Nice abstraction. I like it. 
This is great. 
We need the guard to guard the access to shutdownSchedule,      private var shutdownSchedule = UNSCHEDULED This can be non-volatile since it is protected by guard withGuard  We can rewrite this function as:  if (futures.decrementAndGet() == 0) guard withGuard {   if (uuids.isEmpty) {      shutdownSchedule match { ... }   } }
I'll experiment a bit with this, thanks for being on top of it!
I committed your suggested fix above. If there are any other changes you'd prefer let me know
Oh, I tinkered with it, got the final tests running now, will check your latest commit!
Thanks man, the other methods were too lucrative compared to these. Now you can do:  camel.activationFutureFor(foo).foreach {   a => a ! SomeMessage   a ! SomeOtherMessage   a ! SomeThirdMessage }  And never block yo
thats whassup! blockin's for suckas! :)
Looks good. The consumer can wait in its own ec for registration, while others can get an activationFuture on their ec, seems more isolated. That should prevent the possibility that someone is somehow blocking the consumer registration because it all happens in one queue on one ec /dispatcher somewhere? 
Yup, and it also doesn't use something arbitrary as the default dispatcher.
Cool, well caught!
this creates two methods: one with type Nil and one with type Iterable[String], where one is a bridge method; I think we should avoid that when not necessary
yeah, always use explicit return parameters for user-facing APIs
different proposal: simply make `within` abstract in ScatterGatherFirstCompletedLike, so that it will be picked up from ScatterGatherFirstCompletedRouter; apply the same treatment to nrOfInstances and routees, since custom routers do not really have to do this if they dont want to.
having this in here makes sense, other routers may use it also (potentially)
and not only because it MAY trip up certain IDEs
Good catch. I will implement this immediately.
Props[TestActor] is nicer. 
Use: actorContext.actorOf(Props[DemocratActor], "d") Same below. 
All samples should really have Typsafe copyright headers. Add a ticket for going through and add that. 
Why use 'tell' here? And why not 'sender tell fib' ? More idiomatic Scala. 
Nicer with: RoundRobinRouter(5)
Use 2 spaces indentation below.  Should not be /user/router but /router
Perhaps worth mentioning that you get these odd names $b etc since no address is specified. 
Use backticks for code not '. 
Why * used in this section. Use backticks. 
Why use AtomicInteger inside an actor? Does it escape? If not use an Int. 
Same here? Needed? 
On the other hand, could be nice to be explicit. So I'll say keep it. 
Good job. I like it. 
Thanks for your feedback. I will update accordingly and push.
Ack, sorry guys thought we flushed all these out.  lol'ed at the "hysterical" @viktorklang :)
Anything that fails spuriously is hysterical in my world...
These default values are marked final now but the type annotations stop them from being constants. Is the intention for them to be constants?
Yes, that was the plan, why would the type annotation mess with that?
From what I remember a constant definition has to have final val and no type annotation. I would also guess that it only covers some types, like primitives, String, arrays of constants.
Can confirm that it's final val with no type annotation, and that general objects can't be constant expressions. So:      final val defaultDeployId = ""  constant     final val defaultDeployId: String = ""  not constant     final val defaultSupervisor = None  not constant  
Why what? The no type annotation? I think it was always spec'ed that way.
But what sense does it make?
Missing a ` here
Very nice, douche. 
Thanks... I guess...
since it is part of reference.conf this will always be true
I.e the pool config should be commented out ;)
_routees = _routees filterNot abandonedRoutees.contains
should this happen at each message send?
Couldn't this be handled inside Pool.resize instead?
I think that line is meant to call: def resizePool(props: Props, context: ActorContext, currentRoutees: IndexedSeq[ActorRef])
Ah, it shouldn't, since the pool is immutable. I get it :-)
that is not true, note that I use config.hasPath, which is the user defined config, as opposed to deployment, which has the default fallfacks included val deployment = config.withFallback(default)
if it's not done on each message send, when is it then to be done? Do you think of some kind of modulus? In that case I think it should be implemented in the RouterPool,  and it belongs to the advanced use cases that the user can implement themselves.
What do you mean by "I think that line is meant to call: def resizePool" Isn't that exactly what it does?
yes, and I wanted to hide this from the user, if they implement their own RouterPool
Ah, I didn't see that it was in RouterConfig
ah, sorry, I somehow managed to forget how this works.
wrt every call: yes, I agree that it should be handled in the pool implementation, so the hook should stay as is here. Down in the default impl I think it should only resize on every Nth message, based on the elasticity parameters. Otherwise one flip of the AtomicBoolean per message send would be too costly. I propose an atomic counter with a threshold, and the reduction is done by the one doing the resizing; anybody else just always increments.
Or you just do a TTAS
On Tue, Jan 10, 2012 at 10:09 AM, Roland Kuhn < reply@reply.github.com > wrote:  > wrt every call: yes, I agree that it should be handled in the pool > implementation, so the hook should stay as is here. Down in the default > impl I think it should only resize on every Nth message, based on the > elasticity parameters. Otherwise one flip of the AtomicBoolean per message > send would be too costly. I propose an atomic counter with a threshold, and > the reduction is done by the one doing the resizing; anybody else just > always increments. >  Hmm, right now the AtomicBoolean is in RouterConfig, so that user's doesn't have to implement concurrency stuff when writing their own Resizer. What about placing the counter and the Nth message check there also, but the Resizer decide how often by providing a method:  def resizeOnEachNthMessage: Int  The config of that belongs to the Resizer impl.     > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/8b71bf5bea435cc0ef2caa68a3e6afa4ac01da77#commitcomment-852491 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Sorry, what is a TTAS?  On Tue, Jan 10, 2012 at 10:10 AM, viktorklang < reply@reply.github.com > wrote:  > Or you just do a TTAS > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/8b71bf5bea435cc0ef2caa68a3e6afa4ac01da77#commitcomment-852494 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
yes, sounds good, might be good to call that method on each check, though so that the Resizer has full dynamic control.
TTAS == CCAS ==  if (!something.get && something.compareAndSet(false, true)) Avoids CAS if not needed
On Tue, Jan 10, 2012 at 10:33 AM, Roland Kuhn < reply@reply.github.com > wrote:  > yes, sounds good, might be good to call that method on each check, though > so that the Resizer has full dynamic control. >  Yes, that's the plan. Thanks for good suggestion.   > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/8b71bf5bea435cc0ef2caa68a3e6afa4ac01da77#commitcomment-852541 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Completely forgot to rewrite this in the 'Akka' style. I just had a few minutes to try it and commit it when it worked. I will fix it up. 
nice work! BTW: it looks like cheating, but I was surprised that it actually prevents me from doing the wrong thing. Strange, how does that work?
I only need to return Future[Any] from the shift methods so I can capture exceptions. The continuations themselves carry the proper type of the method until the last continuation, which I stuff the result of that into the Future created in the flow method. The problem, as you found, is having the inferencer find the correct type for the result of the reset method's annotations. I don't care about it, I just make it a Future[Any], because I only use that to get the exception.  And probably magic. Definitely a bit of magic in there.
Hey Derek. This commit seems to break scaladoc generation on publish-local. Can you take a look at that?
Yeah, probably the same as the problem with Future.apply. I'll fix it.
Commited fix, publish-local should work again
It is really strange that type inference works when compiling, but fails when generating scaladocs. Thats twice now I have seen that.
Yes, strange. Could be useful for Scala team to have a ticket, if it doesn't already exist.
ouch: of course you didn't cherry-pick the merge commit. my bad. please also add the doc change in there to release-2.0. 
Which ones, I don't have time to scan the entire commit dude :-)  https:github.com/jboner/akka/commit/b4fcc3b2f2d7e534908057750d0086b9c6e20764
just checkout dispatchers.rst from master and commit to release-2.0
On Mon, Feb 27, 2012 at 2:20 PM, Roland Kuhn < reply@reply.github.com > wrote:  > just checkout dispatchers.rst from master and commit to release-2.0 >  I thought I already updated those in both branches?   > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/cf6e8b1f1bef43dd12e4efa9f4a193746092a9df#commitcomment-1014854 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
I've pushed the changes to releas-2.0 now   2012/2/27 iktor lang <viktor.klang@gmail.com>  > > > On Mon, Feb 27, 2012 at 2:20 PM, Roland Kuhn < > reply@reply.github.com > > wrote: > >> just checkout dispatchers.rst from master and commit to release-2.0 >> > > I thought I already updated those in both branches? > > >> >> --- >> Reply to this email directly or view it on GitHub: >> >> https:github.com/jboner/akka/commit/cf6e8b1f1bef43dd12e4efa9f4a193746092a9df#commitcomment-1014854 >> > > > > -- > Viktor Klang > > Akka Tech Lead > Typesafe <http:www.typesafe.com/> - The software stack for applications > that scale > > Twitter: @viktorklang > >   --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
At least I thought I did, seems to be messed up over at GH right now:  getting an ssh denied  2012/2/27 iktor lang <viktor.klang@gmail.com>  > I've pushed the changes to releas-2.0 now > > > 2012/2/27 iktor lang <viktor.klang@gmail.com> > >> >> >> On Mon, Feb 27, 2012 at 2:20 PM, Roland Kuhn < >> reply@reply.github.com >> > wrote: >> >>> just checkout dispatchers.rst from master and commit to release-2.0 >>> >> >> I thought I already updated those in both branches? >> >> >>> >>> --- >>> Reply to this email directly or view it on GitHub: >>> >>> https:github.com/jboner/akka/commit/cf6e8b1f1bef43dd12e4efa9f4a193746092a9df#commitcomment-1014854 >>> >> >> >> >> -- >> Viktor Klang >> >> Akka Tech Lead >> Typesafe <http:www.typesafe.com/> - The software stack for >> applications that scale >> >> Twitter: @viktorklang >> >> > > > -- > Viktor Klang > > Akka Tech Lead > Typesafe <http:www.typesafe.com/> - The software stack for applications > that scale > > Twitter: @viktorklang > >   --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
"which is the actor that created it"?
"Do not pass mutable actors between..."
I'm guessing the commit of the file called "q" was accidental?
q-file is badass
Scary, wonder where that came from? Thanks for noticing. Can you please remove it, if not already done.  /Patrik  22 dec 2011 kl. 00:50 skrev Peter Vlugter<reply@reply.github.com>:  > I'm guessing the commit of the file called "q" was accidental? >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/1bce4c39c3e8844037dd5d6feb8402dedfac7374#commitcomment-814357
Nice! Added to the rst doc for dispatchers?
Is it possible not to provide an upper limit?
Make a trait for ``BufferPool`` and make this class private[akka] so it's just an implementation detail. Then I can optimize the crap out of it. :-)
Any particular reason why this is an AtomicInteger and not an AtomicBoolan? Just for having nice names Unlocked and Locked?
you can release the lock prior to the allocation
this does not need to be volatile as it is guarded by a volatile piggyback of the state
If you make `takeBufferFromPool` private and it's always called from `acquire`, you don't even need `case Nil` because it's guarded by `poolSize == 0` in `acquire` above.  Maybe one should measure how much spinning costs compared to allocating a new buffer so one could just allocate a new buffer when there's contention (though this would maybe just shift the contention to `malloc` underneath).
Thinking more about it, I think we should allocate an array instead, then the adds and removes do not incur any allocations.
Right. Since it is currently fixed size anyway an array would fit perfectly.
Would you use a simple array or a scala.collection.mutable.ArrayStack to make the management of adding/removing items easier?   On Thu, Jan 17, 2013 at 7:43 AM, Johannes Rudolph <notifications@github.com>wrote:  > Right. Since it is currently fixed size anyway an array would fit > perfectly. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/commit/7d89aefb634e6dd2c31ed8f13b6963be4f338352#commitcomment-2453179>. > >
negative size == take random size?
multiple releases is possible here, making the pool corrupted
I thin @drewhk has a point here, rename this to ``locked`` and make it an AtomicBoolean and drop "Unlocked" and "Locked" and use true/false
I don't understand. Aren't concurrent accesses synchronized through the lock in `addBufferToPool`? 
Scrap that comment about it being guarded. It isn't of course. Maybe that's what you meant above.
I think it's deceptive to _not_ release it and not returning a way for the user to know if the release failed.
That depends on what exactly the contract of `release` is. I see `release` as just a notice from the user to the pool that the user won't use this buffer any more and that it can be now freed or reused on a best effort basis. I don't see why this would be deceptive. It's just a looser contract. The buffer will be GC'd sooner or later in any case.  If you use `free` (as in `malloc`) you also don't know if a particular buffer will be put back into a free list or given back to the OS at the time of the call or only later on.
collection of native memory is fraught with dangers. It's not collected on normal GC sweeps.
So what do you suggest? If we can't control or force releasing native memory, the only thing I see we could do is to limit the amount we allocate in the first place. But then we could just rely on the JVM limit to do that.  What we could do is put buffers bigger than the configured size into an extra collection sorted by size (maybe only weak referenced) and fallback to a more expensive buffer lookup when a bigger buffer is needed.
Perhaps the solution is to not be able to request a size at all, the size you'll get is the size in the config. The only use of the buffer is to gain performance and a smaller buffer should not affect the semantics of your program.
BTW: What the JDK implementation is doing is very similar than what we have here. However, they use private API to really free the buffer when they can't or don't want to take it back.  http:hg.openjdk.java.net/jdk7/build-gate/jdk/file/a06412e13bf7/src/share/classes/sun/nio/ch/Util.java
Thanks for the link. I still think that a simpler solution as outlined above is good enough bang for the buck.
Still, we need a potentially unlimited number of buffers. What should we do with them if we don't need them any more if we don't let them be garbage collected?
It's a good question, I can see a couple of different solutions: 1) We only create at most N _direct_ bytebuffer (these are the ones we pool) outside of those buffers, we only allocate normal ByteBuffers and these will be collected by Normal GC. 2) We only ever create _direct_ ByteBuffers but they are always of the configured size, we only pool N of those and let the JVM handle the collection of that native memory  I definitely want to avoid having our own freelists, it's just adding a crapload of liability without adding that much value (i.e. we do not know if the JVM uses freelists for the malloc call also)
Regarding 1) that would certainly be possible. My only complaint would be that this solution would increase workload when we run out of pooled buffers and start using heap buffers (because when calling channel.read/write the JDK will then use its own direct buffer pool and copy everything over). If pool exhaustion can be seen as a sign of high load having to fallback to heap buffers could then further increase load.   Regarding 2) I don't understand what you mean with "let the JVM handle the collection of that native memory". Do you mean we have buffers that we don't pool but which will be garbage collected normally after use? Taking big-sized buffers out of the picture it seems it seems to be almost what is already implemented here.  What's the difference between a pool and a freelist?  FYI: I checked how garbage collection of direct buffers is done in openjdk 7: Some time after a direct buffer becomes eligible for garbage collection the reference-handler thread runs `Cleaner` code associated with a direct bufer which will then free the resources. See  http:hg.openjdk.java.net/jdk7/build-gate/jdk/file/a06412e13bf7/src/share/classes/sun/misc/Cleaner.java  The total size of memory allocated in direct buffers is limited by a flag. When allocating a direct buffer the JDK checks first that this limit is not yet reached. If it is reached it will run `System.gc()` before attempting to allocate a direct buffer which will free unreferenced direct buffers.
The difference between a pool and a freelist _for me_ is that a freelist handles differently sized pooled items.  I suggest that all buffers will be allocated with the preconfigured size (i.e. the one in the config file), then we cap the pool size at a fixed number, specified by the conf.  We shouldn't optimize depending on a specific JVM implementation, because people use both older, newer and different implementations.  Thoughts?
So the `Cleaner` code is present in JDK6 as well, and relying on `java.lang.ref.Reference` and friends in some way or another to do cleaning is really the only way. I can't imagine that any other  JVM does it to differently.  That said, I think that any pool that we have should be simple as @viktorklang suggested. The heuristics for what to pool of what size will be hard to configure/tune. It also needs to use `SoftReference` to not "starve" the JVM if we have a large pool that we stop using, but still keep around.
I agree to not optimizing to a specific JVM implementation. I would go with your proposed 2) solution. I.e. we have fixed size buffers from which we pool a fixed number and let everything above this fixed number be garbage collected by the JVM, right?  I can't really decide on SoftReferences. I guess it would be the right thing to use them but I don't know how much overhead they bring over hard references.
If you're talking about access overhead, then after optimization only one extra pointer dereference.
log.debug("Requesting channel from {}", connection) -- comment applies to all log statements.
should the reconnection timeout be configurable?
I may be wrong, but I don't think you need to bother with the if (log.isDebugEnabled) if you use the more efficient log.debug("Received channel {}", channel)
log.debug("Applying {} registered callbacks on channel {}", registeredCallbacks.size, channel)
Move this to configuration.  Reference the branch that I made.  It's probably not perfect, but will provide some idea on how to create an extension.
val channelFuture = connectionActor ? CreateRandomNameChild(channelActorCreator) mapTo manifest[ActorRef]
should avoid blocking here, and just work with the future.
timeout should be configurable
channelActor ? ExecuteCallbackTo(callback) mapTo manifest[T]   though I haven't used this pattern with a type param, so my comment may be inappropriate.
Can you safely go to Disconnected if you swallowed an Exception above?  Seems like you might get into a state of runaway connections.
stopping here for now, running for coffee and hoping to enjoy some sunshine...
should be yes, must have missed that.
I don't know if creating a string is as efficient as checking a boolean?
I tried to use the extension example you created already. Will add these timeouts there too
For as far as I know this exception can only happen if close() is called while the connection is already closed (in case of some other error happening on the side). I can build in an additional connection.isOpen check to be really sure in case of the exception
The literal is interned, no cost...
yeah but String.format is slow compared to the logging libs string formatting
alternately, it might fit in the connection properties if it is something that could vary with different connections.
Why does this exist?
So the underlying rabbitmq layer doesn't end up with a non-meaningful 'pool-x-thread-y' situation when we look it in i.e. JVisualVM
the connection actor should probably be a child of a context rather than the system.  
Should be a WordSpec with MustMatchers. (Or, we do it in a seperate ticket, all at once, but it is probably better using the boyscout rule)
I would apply the boy scout rule when there are major changes not minor tweaks like this.
It will have to happen at some point before integrating wip-camel into akka master. So add a ticket for changing all tests to WordSpec and MustMatchers, or fix it at every pull request, your choice. I rather fix stuff when I see it, to each his own.
I would have fixed it straight away, but sbt build is fucked and also there are some significant changes in master (? is gone) so my local sbt build script which uses akka core snapshot doesn't work either...
If by "sbt build is fucked" you mean the jmxri and jdmk stuff, then just install them locally from the link I sent until Patrik has fixed that. If you mean something else, then I don't know what you mean.
Did you merge wip-camel with latest master? You could also work on top of wip-camel sort of isolated, do all merges there, then when all tickets are done, rebase wip-camel on top of the master at that time, fix it where needed, and do a final pull request, squashed as one commit with all the features in one commit message. I don't use a published snapshot but just the sbt sub project akka-camel build, so you are isolated at the point of wip-camel.
No I didn't merge it. I just want to focus on tasks and don't want to be shaving the Yak, if you know what I mean.
Ok, if you really don't like shaving the Yak :) pull akka sources from wip-camel, use the AkkaBuild, work in the sub project akka-camel and don't depend on snapshot binaries? (well, that might include some Yak shaving, I don't know your setup)
This would be exactly yak shaving.  The big sbt script doesn't work with gen-idea either. So for now I'll depend on defined snapshot from our last rebase to wip-camel, and I'll use the "big script" to reformat the code before commits.
'gen-idea sbt-no-classifiers' works for me, with mpeltonen plugin in my global .sbt dir.  -- Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg  On Jan 24, 2012, at 18:47, Piotr Gabryanczyk<reply@reply.github.com> wrote:  > this would be exactly yak shaving so.  > The big sbt script doesn't work with gen-idea either. So for now I'll depend on defined snapshot from our last rebase to wip-camel, and I'll use the "big script" to reformat the code before commits. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/7e1899a22306e401e40c5621c59c9f926cd6e9a5#commitcomment-897076
I meant 'gen-idea no-sbt-classifiers' BTW  On Tue, Jan 24, 2012 at 6:47 PM, Piotr Gabryanczyk < reply@reply.github.com > wrote:  > this would be exactly yak shaving so. > The big sbt script doesn't work with gen-idea either. So for now I'll > depend on defined snapshot from our last rebase to wip-camel, and I'll use > the "big script" to reformat the code before commits. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/7e1899a22306e401e40c5621c59c9f926cd6e9a5#commitcomment-897076 >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
I did that and ended up with massive project which I don't need and incorrect deps. I dont fancy shaving the project and fixing deps every time something changes in the project. Multi-module setup simply sucks...:(
Strange, I also got the big project, but it loads quite quick on intellij 11, and the deps work fine as well for what I've seen, and I compile/test in sbt. (and yeah I know, it's the standard 'it works on my machine' comment which doesn't help) There is definitely some room for improvement in the IDE support and overall performance of it, but it's a whole lot better than about a year ago.  On Tue, Jan 24, 2012 at 10:26 PM, Piotr Gabryanczyk < reply@reply.github.com > wrote:  > I did that and ended up with massive project which I don't need and > incorrect reps. > I dont fancy shaving the project and fixing deps every time something > changes in the project. > Multi-module setup simply sucks...:( > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/7e1899a22306e401e40c5621c59c9f926cd6e9a5#commitcomment-897972 >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
It might be the canary syndrome - the cannary thinks the life is good in the cage as it haven't seen the life without the cage :) Check this file instead and do gen-idea: name := "tmp-camel"  version := "2.0"  scalaVersion := "2.9.1"  resolvers += "Typesafe Snapshots" at "http:repo.typesafe.com/typesafe/snapshots"  resolvers += "Typesafe Repository" at "http:repo.typesafe.com/typesafe/releases/"  libraryDependencies ++= Seq(   "com.typesafe.akka" % "akka-actor" % "2.0-20120119-000630",   "com.typesafe.akka" % "akka-slf4j" % "2.0-20120119-000630",   "com.typesafe.akka" % "akka-testkit" % "2.0-20120119-000630" % "test",   "org.apache.camel" % "camel-core" % "2.8.0",   "org.scalatest" %% "scalatest" % "1.6.1" % "test",   "org.mockito" % "mockito-core" % "1.9.0" % "test",  	"junit" % "junit" % "4.10" % "test" )  scalacOptions ++= Seq("-deprecation", "-unchecked", "-Xlint")  parallelExecution in Test := true
2.0-M3 is published
@piotrga hehe, that might be true. @viktorklang cool.
Love reading your committ-quarrels, kick some major Camel ass :-)
We use the callback to remove the Future from the Dispatcher's registry, so if a Future times out now it's impossible to trigger that callback. I'll make a ticket, I know how to fix that.
Great catch, also add a test for that. On Apr 27, 2011 8:36 PM, "derekjw" < reply@reply.github.com> wrote: > We use the callback to remove the Future from the Dispatcher's registry, so if a Future times out now it's impossible to trigger that callback. I'll make a ticket, I know how to fix that. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359627
Can't we just keep an AtomicLong that we incr when we add a FutureInvocation to the executor, and decr when the run-method of the FutureInvocation.run has been executed?  Then we don't need to generate an Uuid per FutureInvocation either, wdyt?  2011/4/27 iktor lang <viktor.klang@gmail.com>  > Great catch, also add a test for that. > On Apr 27, 2011 8:36 PM, "derekjw" < > reply@reply.github.com> > wrote: > > We use the callback to remove the Future from the Dispatcher's registry, > so if a Future times out now it's impossible to trigger that callback. I'll > make a ticket, I know how to fix that. > > > > -- > > Reply to this email directly or view it on GitHub: > > > https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359627 >    --  Viktor Klang, Director of Research and Development Scalable Solutions <http:www.scalablesolutions.se>  Code:   github.com/viktorklang Follow: twitter.com/viktorklang Read:   klangism.tumblr.com
Yes. The uuid tracking was kept incase of needing to track or cancel an executing Future. I also wanted to keep behavior as close to the original 'spawn' method in case of other functionality that I might not have been aware of. I can change it to an AtomicLong for sure.  I have become much more comfortable working in the Akka source since I wrote this, it's kinda funny seeing how much I tip toed around inside the dispatcher code. On Apr 27, 2011 3:57 PM, "viktorklang" < reply@reply.github.com> wrote: > Can't we just keep an AtomicLong that we incr when we add a FutureInvocation > to the executor, and decr when the run-method of the FutureInvocation.run > has been executed? > > Then we don't need to generate an Uuid per FutureInvocation either, wdyt? > > 2011/4/27 iktor lang <viktor.klang@gmail.com> > >> Great catch, also add a test for that. >> On Apr 27, 2011 8:36 PM, "derekjw" < >> reply@reply.github.com> >> wrote: >> > We use the callback to remove the Future from the Dispatcher's registry, >> so if a Future times out now it's impossible to trigger that callback. I'll >> make a ticket, I know how to fix that. >> > >> > -- >> > Reply to this email directly or view it on GitHub: >> > >> https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359627 >> > > > > -- > Viktor Klang, > Director of Research and Development > Scalable Solutions <http:www.scalablesolutions.se> > > Code: github.com/viktorklang > Follow: twitter.com/viktorklang > Read: klangism.tumblr.com > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359933
On Thu, Apr 28, 2011 at 12:12 AM, derekjw < reply@reply.github.com>wrote:  > Yes. The uuid tracking was kept incase of needing to track or cancel an > executing Future. I also wanted to keep behavior as close to the original > 'spawn' method in case of other functionality that I might not have been > aware of. I can change it to an AtomicLong for sure. >  Coolio, send me a linky to the branch so I can review it before it goes into mainline.   > > I have become much more comfortable working in the Akka source since I > wrote > this, it's kinda funny seeing how much I tip toed around inside the > dispatcher code. >  Well, I think that's sensible, messing up the dispatcher is probably the worst thing ;-)   Thanks!  On Apr 27, 2011 3:57 PM, "viktorklang" < > reply@reply.github.com> > wrote: > > Can't we just keep an AtomicLong that we incr when we add a > FutureInvocation > > to the executor, and decr when the run-method of the FutureInvocation.run > > has been executed? > > > > Then we don't need to generate an Uuid per FutureInvocation either, wdyt? > > > > 2011/4/27 iktor lang <viktor.klang@gmail.com> > > > >> Great catch, also add a test for that. > >> On Apr 27, 2011 8:36 PM, "derekjw" < > >> reply@reply.github.com> > >> wrote: > >> > We use the callback to remove the Future from the Dispatcher's > registry, > >> so if a Future times out now it's impossible to trigger that callback. > I'll > >> make a ticket, I know how to fix that. > >> > > >> > -- > >> > Reply to this email directly or view it on GitHub: > >> > > >> > > https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359627 > >> > > > > > > > > -- > > Viktor Klang, > > Director of Research and Development > > Scalable Solutions <http:www.scalablesolutions.se> > > > > Code: github.com/viktorklang > > Follow: twitter.com/viktorklang > > Read: klangism.tumblr.com > > > > -- > > Reply to this email directly or view it on GitHub: > > > > https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359933 > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/800840719f54e67ce25e59ff7fc691ff6055e478#commitcomment-359950 >    --  Viktor Klang, Director of Research and Development Scalable Solutions <http:www.scalablesolutions.se>  Code:   github.com/viktorklang Follow: twitter.com/viktorklang Read:   klangism.tumblr.com
isn't it possible to make it even shorter?      actor {       become {         case "hello"  sender ! "hi"        }      }
I agree that this is a nice dream, but this won't work because we need the `new` keyword in order to properly scope our implicits, foremost `self` as the sender for `!`. Thus, the potential for making it even shorter is limited to a gain of two charaters, if the trait's name were a single letter.
Macro to the rescue?
Hmm, now that you mention this, it might be possible. But Eugene said that macros implementing traits is even more experimental than macros are by themselves, so maybe this can wait until Akka 2.2?
Yeah, it can wait. But it will not look that good to either have:  * 2 ways of doing the same thing or  * go through deprecation and removal of a feature introduced in the previous release
I was thinking more along the lines of keeping it source compatible by changing `actor()` to be a macro which will check whether its argument is actually a class definition (and pass that on) or the proposed thing above; although I must say that in the latter case a lot more magic would be needed, because e.g. no `become` is usually in scope, so the code would not type-check before being passed to the macro. This is another reason why I would not want to rush things, well have to see how macro-based APIs should be done.
I'd still think it would be interesting to explore if it is not possible to get closer to Patrik's syntax using macros. 
Im curios: have you actually seen this match?
why not use `immutableSeq` here as well?
building the varargs array is an overhead which was avoided in the old version; I guess your reasons are aesthetic?
yes, this was necessary
If someone has something like that I'm going to bail out early ;-)
Sie sieht mir Rodeln
Because it's a 2N op instead of N
youre a naughty one
Buuuuuut, we are not expecting huge Iterables here right?
not unless they are also Eaterable
Why do we need 2 top-level daemons, can they exist without eachother?
Badass! Who taught you this badass trick?
They have different responsibility, life-cycle and deployment. One is for gossiping between nodes the other for commands between nodes and user -> node.   I guess I could create an empty dummy supervisor for them. And then look them up with actorFor in the Node. Not sure if it is worth it but do you want me to do that? 
Because you didn't want me to use this method in ActorSystemImpl. So I made the code dependent on ExtendedActorSystem and added this one. 
Why. HOCON allows you to not quote strings. More slick IMO. 
HOCON allows you to not quote strings. More slick IMO.
HOCON allows you to not quote strings. More slick IMO.
HOCON allows you to not quote strings. More slick IMO.
I think we should use Duration as much as possible, i.e.  import akka.util.duration._ 10 milliseconds
Use duration. tickDuration = 100ms
Use Duration(getMilliseconds("akka.scheduler.tickDuration"), MILLISECONDS)
Constructor of HashedWheelTimer should take Duration instead of value and unit parameters
Totally agree with Patrik
Cool, I'll update the test, trait and HWT constructor.  On Tue, Nov 22, 2011 at 16:11, viktorklang < reply@reply.github.com > wrote:  > Totally agree with Patrik > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/ac03696d88bf76eae5c760582b29f37f180493de#commitcomment-738004 >
This might be an important TODO. Is it possible to grab the mailbox from the outside somehow? It would be nice with a callback in the mailbox that is invoked when the mailbox status has been changed to Closed, then the mailbox could do it's own cleanup. Is that possible to add?
We already have a "cleanUpMailboxFor" in MessageDispatcher, we could have that method invoke an "beforeCleanup" callback on Mailbox.  You fix?
That is good. I'll add that.  On Mon, Nov 14, 2011 at 1:24 PM, viktorklang < reply@reply.github.com > wrote:  > We already have a "cleanUpMailboxFor" in MessageDispatcher, we could have > that method invoke an "beforeCleanup" callback on Mailbox. > > You fix? > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/4aa1905ce30eee9915ebc0e6675058b81ebeac99#commitcomment-715396 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Why do we need to reset the interrupted-flag on the Thread running the test? It should only be the threads of the Dispatcher that should get interrupted?
I have no logical explanation for it.  It was a 'lets try this and see if it works', which it did. But you can run the tests for yourself with and without the after method and see if it fails on your machine.  I can spend time researching why, but I guess that is not the best way to spend my time.   On Fri, Jul 15, 2011 at 10:34 AM, viktorklang < reply@reply.github.com>wrote:  > Why do we need to reset the interrupted-flag on the Thread running the > test? It should only be the threads of the Dispatcher that should get > interrupted? > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/f93624e7e0ebb38c8ed351fbbd045ef1c4fbd207#commitcomment-478859 >
Based on the stacktrace I concluded that the thread executing the test had received an interrupt, and therefor its interrupted status was set and therefor it fails on the queue.  If I disabled the interruped tests, everything was alright. If I enabled them the other test started to fail. So that was why the most logical place to fix the problem, was at the location of the interrupted tests.  But again.. I have no idea how the interrupted flag was set on the test executing thread.  On Fri, Jul 15, 2011 at 10:42 AM, Peter Veentjer <alarmnummer@gmail.com>wrote:  > I have no logical explanation for it. > > It was a 'lets try this and see if it works', which it did. But you can run > the tests for yourself with and without the after method and see if it fails > on your machine. > > I can spend time researching why, but I guess that is not the best way to > spend my time. > > > On Fri, Jul 15, 2011 at 10:34 AM, viktorklang < > reply@reply.github.com>wrote: > >> Why do we need to reset the interrupted-flag on the Thread running the >> test? It should only be the threads of the Dispatcher that should get >> interrupted? >> >> -- >> Reply to this email directly or view it on GitHub: >> >> https:github.com/jboner/akka/commit/f93624e7e0ebb38c8ed351fbbd045ef1c4fbd207#commitcomment-478859 >> > >
Please note that some Serializers need type information regarding how to reify the Class, you might want to use the same kind of approach as TypedActor: https:github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/actor/TypedActor.scala#L135  
You reply with an exception?
These still need to go :-)
I know, haven't thought of a nice solution yet :-(
Actually I want to have the calling future be completed with an exception. Do I need to do that differently?
Any proposal or thoughts on this? The ack/nack come in async from the rabbitmq broker, so there needs to be some way of completing a previous constructed future for the caller...
case Left(exception) => akka.actor.Status.Failure(exception)
Why not just have a child actor that handles that?
Sample project should be updated to sbt 0.12, like the rest of the akka build.
The one in this currently discussed commit:  https:github.com/akka/akka/blob/da78c3af0fd0e32b10c6cc54cfdce75dee8e0061/akka-sbt-plugin/sample/project/build.properties
Great, thanks Pete
I think we have some inconsistency in how we write those Settings classes, I think making the Settings class itself final is most clear, but here it might be intended to be able to subclass, I don't know
I dont think any of the two buys us anything. What is the purpose of `final` in this context?
to not make it overridable I agree that it doesn't matter
Agree, ignore me. 
should the ticket stay open until we have changed this whole test to be based on TestLatches for verifying actor creation/termination?
Don't know if that's "possible" since the test wants to test a previous value against a "current" value.
well, we want to trigger creation/termination of children via the resizer, and that should happen deterministically, no?
Very Robertly done! +1
Why these vals?  `import ClusterSettings._`
Very good! +1
Sure. They can be removed. But I won't do it in this pull request. 
Good. Thanks. Great reviews. 
Thx for this feature - we'll try to use it in akka-camel module.
a problem with what? "if there was a problem" is not really clear :-)
Lol, it's an Awaitable, it's an abstraction, for Futures its more clear, but at this level all we can really say is that it'll throw up if it tried something inedible ;-)
Ok. So we can't even say that it dies if there was a *user* problem of some sort? 
No, since it might not be user-code related at all. Lets say you'd have a ConnectionPool that returned Awaitable[Connection] so it could for instance create Connections asynchronously, or reuse an existing one. Now the user might not have done anything, but Await.result throws a SQLException because the database went down or somesuch.
Sure. Got it. Thanks.
You're welcome buddy, thanks for questioning it!
Docs for DynamicVariable say that the value will follow any child threads created. If this is true then it is not thread safe as then we would have multiple threads adding to the Stack. That is why I went with ThreadLocal. I didn't check the code to see exactly what it did though, so I may have misunderstood.
On Fri, Apr 29, 2011 at 3:36 PM, derekjw < reply@reply.github.com>wrote:  > Docs for DynamicVariable say that the value will follow any child threads > created. If this is true then it is not thread safe as then we would have > multiple threads adding to the Stack. That is why I went with ThreadLocal. I > didn't check the code to see exactly what it did though, so I may have > misunderstood. >  Ah, ok, good catch, will change back to ThreadLocal :-)   > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/1fb228c06d5fdb67c22d59455357821f5a2290fb#commitcomment-362511 >    --  Viktor Klang, Director of Research and Development Scalable Solutions <http:www.scalablesolutions.se>  Code:   github.com/viktorklang Follow: twitter.com/viktorklang Read:   klangism.tumblr.com
Everything else looks good!
Great!  Running tests as we speak :-)
Does this mean that it will start 16 threads on my notebook? I think this should default to 0.1 or so (unless your comment about it being multiplied by the number of cores is misleading).
No, it will start 2 threads :-)  You need to separate FromFactor from Size
what about this? this seems to indicate up to 64 threads on my notebook? (I'm talking about the comment to the right)
it's a fixed size, not a factor.
then the comment is misleading, is all I'm sayin'
Lol, good point! Can you push a fix? :-)
will push tomorrow, today is my official day off ;-)
Np, I just pushed a fixed version
Job pr-validator-per-commit failed for 671ebf89 [(results)](https:jenkins.akka.io:8498/job/pr-validator-per-commit/81/):  <br>Took 46 min.<br> to rebuild, comment "PLS REBUILD/pr-validator-per-commit@671ebf890931ae5ed72b5663d2b728754e449e7c" on PR 1381
RemoteNodeDeathWatchFast strikes again
It will be my top priority tomorrow
what, you want to sell this? ;-)
LOL, I maded it for u
okay, so we make extension loading fully implicit, then.
That I have not changed. I'm still in the "at load time"-camp, this patch does not change that.
yep, looks good to me!
Then the scaladoc is misleading: getting the thing loaded multiple times means calling registerExtension multiple times. If you supply the wrong key, youll get an exception instead.
My point was:  class F extends ExtensionId[Something]  (new F)(system) (new F)(system)  Will create 2 distinct extension instances of Something
My opinion is that BOTH of them should throw an exception. ExtensionId.apply() should only work after having registered the extension. Alternatively we could go completely the other way and remove loadExtensions() completely, instead relying only on JIT-registration. But that has the duplicate extension problem unless we take special care (and Im not even sure what that means).
Or we have AOT and JIT Extensions AOT throw if not loaded at boot, JIT always does create-if-absent. 
I don't agree that extensions must be defined in config, i.e. loaded before use. I think on demand registration is very good, especially for our built in extensions. To use an extension the user only needs to add the jar dependency and don't care about adding special configuration.  On Fri, Nov 25, 2011 at 11:00 AM, Roland Kuhn < reply@reply.github.com > wrote:  > My opinion is that BOTH of them should throw an exception. > ExtensionId.apply() should only work after having registered the extension. > Alternatively we could go completely the other way and remove > loadExtensions() completely, instead relying only on JIT-registration. But > that has the duplicate extension problem unless we take special care (and > Im not even sure what that means). > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/603a8ed034f0489cb1330e81d1159640b9432e16#commitcomment-745865 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
What is the difference? Why not always do JIT, but have the possibility to define extensions in config to be initialized at startup. AOT via config has a use case for extensions that starts some kind of runtime service, very much as our boot classes. Should we merge the concept of boot classes and extensions?  On Fri, Nov 25, 2011 at 11:05 AM, viktorklang < reply@reply.github.com > wrote:  > Or we have AOT and JIT Extensions AOT throw if not loaded at boot, JIT > always does create-if-absent. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/603a8ed034f0489cb1330e81d1159640b9432e16#commitcomment-745876 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
now THAT sounds very interesting!
Yes, very good idea Patrik!  Open a ticket
`toString` is probably not the right thing to sort over, given that ports <10000 do exist. Id recommend something like      val addrs = Seq(...)     addrs.sortBy(theRealAddressSorter).zipWithIndex.foreach { case (a, i) => members(i).address must be(a) }  where `theRealAddressSorter` must be the algorithm actually applied within the clustering.
That could be supplied by a ClusterSpec base class as `assertMembers(addrs)` or some such.
Sounds good. Just used natural string sorting since that is what Riak does. But they don't have port as part of the name, but just a user-provided string. Add a ticket to change Cluster.scala. You have to provide a default sorter though. 
Ah, okay, didnt read that code. Then the comparator was actually the right one.
On Fri, May 25, 2012 at 10:27 AM, Roland Kuhn < reply@reply.github.com > wrote:  > Ah, okay, didnt read that code. Then the comparator was actually the > right one. >  but still wrong in the implementation :)   > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/akka/akka/commit/3aba8dc424e643f7085821729b19f93926bcae8d#commitcomment-1375782 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
Nice solution to the problem, but shouldn't we be careful with introducing weak references? I think they have a negative impact on stop the world time in CMS garbage collector.
Indeed, there is another option, and that is to keep the uuid and look it up in the registry each time, but it will be slower and generate more garbage, which is why that wasn't the first option. Wdyt? On Jul 12, 2011 7:24 AM, "patriknw" < reply@reply.github.com> wrote: > Nice solution to the problem, but shouldn't we be careful with introducing weak references? I think they have a negative impact on stop the world time in CMS garbage collector. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472101
That depends on how bad weak references are for the GC. I don't know. Probably need to be benchmarked, and may be dependent on the usage (application). If lookup by uuid is an option I think that is more safe. It's only a lookup in ConcurrentHashMap.  On Tue, Jul 12, 2011 at 8:11 AM, viktorklang < reply@reply.github.com>wrote:  > Indeed, there is another option, and that is to keep the uuid and look it > up > in the registry each time, but it will be slower and generate more garbage, > which is why that wasn't the first option. Wdyt? > On Jul 12, 2011 7:24 AM, "patriknw" < > reply@reply.github.com> > wrote: > > Nice solution to the problem, but shouldn't we be careful with > introducing > weak references? I think they have a negative impact on stop the world time > in CMS garbage collector. > > > > -- > > Reply to this email directly or view it on GitHub: > > > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472101 > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472153 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
I totally agree, can you change that in release-1.2 and master?  --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
ok  On Tue, Jul 12, 2011 at 8:53 AM, viktorklang < reply@reply.github.com>wrote:  > I totally agree, can you change that in release-1.2 and master? > > -- > Viktor Klang > > Akka Tech Lead > Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the > Experts > > Twitter: @viktorklang > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472216 >
Perhaps that I'm too stupid, but I don't understand how the scheduler should be used in practice and how many instances you want to have. In more traditional application if some kind of scheduler is needed, no sharing is needed. The service that needs it can create and manage it (or the di container if that is used).  So it appears that this singleton scheduler serves some purpose, but I'm not sure which one. And as a quick hack.. why not remove the lifecycle methods? Just let it be there as a platform service.. like the gc.. or the system clock.. Something that is always up and running. It isn't the best solution because it doesn't scale (everyone using the same scheduler.. so more contention). But at least you don't need to worry about shutdown.
On Tue, Jul 12, 2011 at 9:21 AM, pveentjer < reply@reply.github.com>wrote:  > Perhaps that I'm too stupid, but I don't understand how the scheduler > should be used in practice and how many instances you want to have. In more > traditional application if some kind of scheduler is needed, no sharing is > needed. The service that needs it can create and manage it (or the di > container if that is used). >  Have you've read the docs for it?   > > So it appears that this singleton scheduler serves some purpose, but I'm > not sure which one. And as a quick hack.. why not remove the lifecycle > methods? Just let it be there as a platform service.. like the gc.. or the > system clock.. Something that is always up and running. It isn't the best > solution because it doesn't scale (everyone using the same scheduler.. so > more contention). But at least you don't need to worry about shutdown. >  Yes, this is _exactly_ what I've proposed the past 2 days on the akka-dev ML
What happens if the ActorRef becomes remote? Maybe it is local when scheduled, but then has become remote when the task is executed. I started with this Actor.registry.local.actorFor(uuid), but that doesn't work for the above scenario, I guess.  On Tue, Jul 12, 2011 at 9:26 AM, viktorklang < reply@reply.github.com>wrote:  > On Tue, Jul 12, 2011 at 9:21 AM, pveentjer < > reply@reply.github.com>wrote: > > > Perhaps that I'm too stupid, but I don't understand how the scheduler > > should be used in practice and how many instances you want to have. In > more > > traditional application if some kind of scheduler is needed, no sharing > is > > needed. The service that needs it can create and manage it (or the di > > container if that is used). > > > > Have you've read the docs for it? > > > > > > So it appears that this singleton scheduler serves some purpose, but I'm > > not sure which one. And as a quick hack.. why not remove the lifecycle > > methods? Just let it be there as a platform service.. like the gc.. or > the > > system clock.. Something that is always up and running. It isn't the best > > solution because it doesn't scale (everyone using the same scheduler.. so > > more contention). But at least you don't need to worry about shutdown. > > > > Yes, this is _exactly_ what I've proposed the past 2 days on the akka-dev > ML > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472264 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Yes I did.  I checked the docs from the Akka project: http:akka.io/docs/akka/1.1.3/common/scheduler.html  And I have checked the documentation of the Scala sources themselves.  Based on that information I'm not able to derive much. It appears to be some system service that always is available. But it is more guessing and background knowledge than hard science. * * On Tue, Jul 12, 2011 at 10:26 AM, viktorklang < reply@reply.github.com>wrote:  > On Tue, Jul 12, 2011 at 9:21 AM, pveentjer < > reply@reply.github.com>wrote: > > > Perhaps that I'm too stupid, but I don't understand how the scheduler > > should be used in practice and how many instances you want to have. In > more > > traditional application if some kind of scheduler is needed, no sharing > is > > needed. The service that needs it can create and manage it (or the di > > container if that is used). > > > > Have you've read the docs for it? > > > > > > So it appears that this singleton scheduler serves some purpose, but I'm > > not sure which one. And as a quick hack.. why not remove the lifecycle > > methods? Just let it be there as a platform service.. like the gc.. or > the > > system clock.. Something that is always up and running. It isn't the best > > solution because it doesn't scale (everyone using the same scheduler.. so > > more contention). But at least you don't need to worry about shutdown. > > > > Yes, this is _exactly_ what I've proposed the past 2 days on the akka-dev > ML > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472264 >
On Tue, Jul 12, 2011 at 9:31 AM, patriknw < reply@reply.github.com>wrote:  > What happens if the ActorRef becomes remote?   It cannot. (or did someone develop migration of LocalActorRefs?)   > Maybe it is local when > scheduled, but then has become remote when the task is executed. I started > with this Actor.registry.local.actorFor(uuid), but that doesn't work for > the > above scenario, I guess.   Would also not work with the weakrefs :-)  -- Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
On Tue, Jul 12, 2011 at 9:39 AM, viktorklang < reply@reply.github.com>wrote:  > On Tue, Jul 12, 2011 at 9:31 AM, patriknw < > reply@reply.github.com>wrote: > > > What happens if the ActorRef becomes remote? > > > It cannot. (or did someone develop migration of LocalActorRefs?) >  Ok, I don't know much about the new clustering, and that's why I was asking. I thought a ActorRef could be used transparently, even when the actor was moving around in the cluster.   > > > > Maybe it is local when > > scheduled, but then has become remote when the task is executed. I > started > > with this Actor.registry.local.actorFor(uuid), but that doesn't work for > > the > > above scenario, I guess. > > > Would also not work with the weakrefs :-) > > -- > Viktor Klang > > Akka Tech Lead > Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the > Experts > > Twitter: @viktorklang > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472288 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
> Ok, I don't know much about the new clustering, and that's why I was > asking. > I thought a ActorRef could be used transparently, even when the actor was > moving around in the cluster. >  Alright, that does not exist to my knowledge :-)
Afaik. it is going to be possible to transparently move actors on the cluster in the future. If the system sees for example that in a certain time period 2 actors are talking a lot to each other, the system could decide to bring them to a single jvm so you will have a better locality of reference. So the system can dynamically move actors around.  This needs quite some thought I think if we are also introducing the other types of routing like hash based routing since well.. moving an actor to a different partition would make the actor less cheap to find.  Doesn't exist yet.. but it is part of the future.  On Tue, Jul 12, 2011 at 10:49 AM, viktorklang < reply@reply.github.com>wrote:  > > Ok, I don't know much about the new clustering, and that's why I was > > asking. > > I thought a ActorRef could be used transparently, even when the actor was > > moving around in the cluster. > > > > Alright, that does not exist to my knowledge :-) > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/c8c12ab56b8303918e54c80713561a5b4f44a6cb#commitcomment-472308 >
Nice formatting change ;-) Should switch to the FSMDocTest approach for UntypedActorDocTest (wrt. `@Before` and `@After`), because that still leaks when tests fail.
It's not only formatting change. It leaked. I was surprised that it made a difference to initialize in Before compared to in constructor, but it does, and if you whould have added another test method it would have failed fatally, since After is run after each test method.  Yes we should change UntypedActorDocTest, but there is at least one place where it demonstrates how to create an ActorSystem, so that must be taken in consideration.
Ouch, so what is the ScalaTest equivalent of beforeAll/afterAll? Then we should probably use that. And the one test which needs to do its special thing should just do so while adding `try { ... } finally { system.shutdown() }`.
@BeforeClass/@AfterClass. We have used that in several places, also.
YES! that is very close to ideal, only one stylistic nit-pick: I would prefer the # of the origin-explaining comments to be aligned with the : so that the keys stick out more visually.
Can you notify Havoc of this, he might not read all of our github comments.  On Dec 2, 2011, at 12:40, Roland Kuhn<reply@reply.github.com> wrote:  > YES! that is very close to ideal, only one stylistic nit-pick: I would prefer the # of the origin-explaining comments to be aligned with the : so that the keys stick out more visually. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/eebe068aa5979b459ef8a8c356c39b4bd7f6b509#commitcomment-763671
@havocp, what do you think?
Isn't requiring messages for the config opening the door to get unintentional errors.  For example the HWM must be set before connecting or binding the socket.  when i then have code like this:  ```scala val socket = newSocket(config) socket ! HWM(100L) socket ! Bind("tcp:dkkdk:3394") ```  There are no guarantees that the message HWM will actually be processed before the Bind message unless you put the throughput of the dispatcher to 1.  I also have a question regarding the polling. Because there is a pollReceiveTimeout set the zeromq poller will block the actor for that amount of time and then iterate once to block again. Having that poller in the same actor instead of in a future or a child actor doesn't that lead to messages you send to the socket actor are only processed once per poll-cycle (and with the amount of messages being processed per cycle not being larger than the throughput setting of the dispatcher) ?
The no guarantees sentence is not correct: Akka does not guarantee delivery of any one message, but in practice it cannot fail within the same JVM (if you are sensible enough to configure the JVM to exit upon OutOfMemoryError).  BUT: messages are never re-ordered, and this does not depend on the throughput setting!
What? order of messages are retained on a per-sender basis
As far as I can tell, HWM will be processed before Bind in your code.
Why do the check before and after send?
It's a performance optimization (saving one stack trace and all that).
I think it might be premature since it will do 1 extra volatile read for each listener on every send. Should be rare that listeners die, and it's not good not to unregister when the  listener dies, so I think the toll of an exception will be low. Comment the first check out and leave a Uncomment me if this bottlenecks sounds like a fair compromise?
absolutely! (always keeping on learning)
You're doing awesome work, I'm glad to have you on board!
Why at "info" level?
May I suggest using "actor ? "die"" and then wait for the message to get processed?
Why the sleep here?
Why not:   val name1 = (actorNode1 ? "identify").as[String]
Same comment as above
Why not using: actor ? "die" and then wait for it to be processed?
I'd recommend:  set.add((actor ? "identify").as[String])
Why the sleep here?
Why the sleep here?
recommend .as[String] instead of .get.asInstanceOf[String]
recommend .as[String] instead of .get.asInstanceOf[String]
Shouldn't we also assert that the total count is 1000?
See my comment for the other exactly the same code.
recommend .as[String] instead of .get.asInstanceOf[String]
Why the sleep here?
Why the sleep here?
recommend .as[String] instead of .get.asInstanceOf[String]
recommend .as[String] instead of .get.asInstanceOf[String]
He Viktor, with a dying node that is not possible.
We need to make sure that the node is running long enough to be able to act as a cluster node.  Else the node would shutdown immediately.
Will fix it.
He VIktor, waiting for something to complete while a node is shutdown, currently is not going to work since you get a shitload of exceptions.
Else the node shuts down before it should shut down.
Done, although personally I don't like nested expressions, so I keep the 2 step approach. But  I have simplified the 'toString' stuff.
Else the node shuts down before it should shut down.
I changed them all, will not comments any more on this suggestion.
Good one. Will add it.
Why? You can set a 4 second timeout if you must?
Can't it wait for a ZK watch to complete or something?
This is a great start Patrik. Nice work!
That timeout doesn't seem right.
Great that this was started!
we don't automatically have Timeout in scope, so I think you should add      implicit val timout = Timeout(5 seconds)  and/or      implicit def timeout = context.system.settings.ActorTimeout
perhaps change this to "myownlogsourcename"
thanks for the reminder, will scrap the currently provided default argument which enables running without providing a Timeout instance
Shouldn't this be included in the ticket as well?  https:github.com/jboner/akka/blob/master/akka-testkit/src/main/scala/akka/testkit/TestActorRef.scala#L41
Not sure. I'm not sure if the TestActorRef needs it's own sense of equality.
We will probably have to rethink the TestActorRef a bit after the previous configuration changes and the upcoming supervision changes. Ill keep an eye on it.
does it have to be `AnyRef` here, and is that why you do `Vector[T]() ++ nr` instead of using it as is?
Why abs if i is known to be negative?
I see, it's to make sure it's a Vector
But why Vector? Couldn't it bee an Array, too? You index it with idx(...), but that returns a value exactly in the range 0...n-1, so I don't see the need for a hash trie. WDYT?
Looks cool! I didn't know that Arrays.binarySearch has that nice feature.
So the reason for the Vector is that I had issues with toArray and ClassTags and the Java API having erased types. It was a bit above my Scala-fu level, so I vent for this.  If somebody can turn it back into an Array in a nice way then please do so.
I can take a stab at that. Will try something like:      private val (nodeHashRing: Array[Int], nodeRing: Array[T]) = {        val (nhr: IndexedSeq[Int], nr: IndexedSeq[T]) = nodes.toIndexedSeq.sortBy(_._1).unzip        (nhr.toArray, nr.toArray)     }
Tried that. Welcome to `No ClassTag available for T` ;)
yeah, it's definitely outside of my comfort zone also. Correct me if there is a better way, but adding this compiles fine (without warnings also)      class ConsistentHash[T <: Any: ClassTag]       private val (nodeHashRing: Array[Int], nodeRing: Array[T]) = {       val (nhr: Seq[Int], nr: Seq[T]) = nodes.toSeq.unzip       (nhr.toArray, nr.toArray)     }  I have re-introduced the SortedMap in the constructor, to handle the sorting there instead. No need to re-sort everything when adding single nodes.
Cool. That was what I was looking for.
Can't you just write `ConsistentHash[T: ClassTag]`?
yes, thx I have committed the ClassTag-fu It was a bit nasty for the java case, `create`, but I have tried it out
I think the comparison should be protocol agnostic. We sort the node not the protocol it is looked up with. 
We have said that the system should be enforced to be the same across a cluster. But that is not done yet. 
so what you say is that only host and port should be used, and that makes sense because that is the unique thing
Yes. Host and Port. 
Since we're always and only pushing a function that will run: notify(rest.head) why do we need this at all? (I mean, it's not closing over the "current" head at the time of the call to addToCallbacks)
We also need to decide if one failing (throwing) callback should abort the running of the other callbacks, wdyt?
If I may interject: in general I think that each callback should get its chance. The exception does not make much sense in the context which is doing the notification, so I'd vote for swallowing and logging it.
Yeah, the exception is caught in "notify"
I prefer that exceptions not bring everything else down. This is especially true now that all the callbacks are executed in one place, a single exception could prevent a lot of other callbacks from getting executed.
It was getting late, but I also had the idea of just creating a single function per Future. Looks like you did this as well in your own changes.
Isn't the Scala style convention for constants: ClusterPath ?
I don't like that. But I agree that all caps is ugly. I rather go with clusterPath. Most stuff are immutable vals anyway, so constants loose meaning in Scala. Change it if you like.  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Aug 28, 2011 3:37 PM, "patriknw" < reply@reply.github.com> wrote: > Isn't the Scala style convention for constants: ClusterPath ? > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-555913
If it starts with capital letter you don't need to backtick it in pattern matching... On Aug 28, 2011 3:37 PM, "patriknw" < reply@reply.github.com> wrote: > Isn't the Scala style convention for constants: ClusterPath ? > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-555913
Ok. If the general opinion is to go for ClusterPath then go for it.  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Aug 28, 2011 6:44 PM, "viktorklang" < reply@reply.github.com> wrote: > If it starts with capital letter you don't need to backtick it in pattern > matching... > On Aug 28, 2011 3:37 PM, "patriknw" < > reply@reply.github.com> > wrote: >> Isn't the Scala style convention for constants: ClusterPath ? >> >> -- >> Reply to this email directly or view it on GitHub: >> > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-555913 > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-556024
Personally I don't care as long as we are consistent. We should try to stick to style guide, or document the deviation in Developer Guideline.  On Sun, Aug 28, 2011 at 7:18 PM, jboner < reply@reply.github.com>wrote:  > Ok. If the general opinion is to go for ClusterPath then go for it. > > -- > Jonas Bonr > CTO > Typesafe - Enterprise-Grade Scala from the Experts > Phone: +46 733 777 123 > Twitter: @jboner >  On Aug 28, 2011 6:44 PM, "viktorklang" < > reply@reply.github.com> > wrote: > > If it starts with capital letter you don't need to backtick it in pattern > > matching... > > On Aug 28, 2011 3:37 PM, "patriknw" < > > reply@reply.github.com> > > wrote: > >> Isn't the Scala style convention for constants: ClusterPath ? > >> > >> -- > >> Reply to this email directly or view it on GitHub: > >> > > > > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-555913 > > > > -- > > Reply to this email directly or view it on GitHub: > > > > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-556024 > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/4d317517932a162539602537b88d229c0ad6ed1f#commitcomment-556059 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
+1 - I think we should drop all caps for constants
Where is ifNode defined? What is its semantics?
Its the one method without ScalaDoc in MultiNodeSpec. I have already created a ticket for improving its syntax (has too many parentheses right now).
Thanks. I'll look into it. 
Should we implement this as a completeWith alternative?  def completeWith(fun: () => Future[Blah]) = try {   this completeWith fun() } catch {    case e: Exception =>       EventHandler.error(e, this, e.getMessage)       this completeWithException e  }  Because I guess this is a rather common use-case?
Why is this @cps[Future[Any]] and not @cps[Future[A]]?
For some reason I cannot compile your branch:  I get this:  [info] == akka-actor-tests / test-compile == [info]   Source analysis: 39 new/modified, 0 indirectly invalidated, 0 removed. [info] Compiling test sources... [error] /Users/viktorklang/Documents/workspace/akka/akka/akka-actor-tests/src/test/scala/akka/dispatch/FutureSpec.scala:65: value === is not a member of Nothing [error]     assert(future1.get === "WORLD") [error]                        ^ [error] /Users/viktorklang/Documents/workspace/akka/akka/akka-actor-tests/src/test/scala/akka/dispatch/FutureSpec.scala:66: value === is not a member of Nothing [error]     assert(future2.get === "WORLD") [error]                        ^ [error] /Users/viktorklang/Documents/workspace/akka/akka/akka-actor-tests/src/test/scala/akka/dispatch/FutureSpec.scala:77: value === is not a member of Nothing [error]     assert(future1.get === "WORLD") [error]                        ^ [error] three errors found   I've done clean, clean-lib, update, reload, test-compile
Yeah, forgot to commit the revert. I'll get that up
Because it most likely isn't a Future[A]? The simple answer is that it's best to just ignore the annotations, they are just there for the compiler plugin. The return type is A, and that is really what matters.  The "how do I make this work, hitting my head against the wall isn't helping!" answer is that all shifts contained within a reset must have the same annotation. My first try at this actually used the @suspendable annotation, which is an alias of @cps[Unit], and is the most commonly used for this kind of thing (HawtDispatch continuations use it). The only reason why I had to switch is that there was no other way of failing fast (or at all) with a Future containing an Exception. If an exception was thrown in the original code, it was lost and the whole Future.flow block would time out.  At the end of the day, it's best just to not think about it too much.
On the verification of transition, I did find that throwing exceptions did help me catch a race condition under test - but I can also see the perspective that you should just ignore invalid transition attempts.
Definitely a better solution to the previous lock-unlock approach
Looking back at this, is there a way to factor this call to notify listeners on enter() out?  It's common to all three states
The semaphore is no longer here I see.  I was using that to ensure that only the first call could be attempted while in half-open state.  This is part of Nygard's design, and is a differentiator from Closed state.  If we want to diverge from that - that's fine, as long as it's not an oversight.
You have a race here
is it ok if this fails?
is it ok if this fails?
case _ => callFails()
if(failureCount.incrementAndGet() >= maxFailures) tripBreaker()
Looks great Patrik.  Since this is general purpose - does it belong in akka.util?  Or does that introduce an unwanted dependency on akka.dispatch?
In what way is this interesting?
This got brought over from my PR.  I had included it so that the caller could get some idea when another attempt could succeed, if they wanted to put in some kind of backoff logic, or report something to the user.  When creating custom exceptions, I prefer to add context to help the catcher decide how to handle.
@scullxbones this is one of the things that you can help with. What do you think about passing in the *from* state as method parameter?        if (currentState.compareAndSet(from, CircuitBreakerOpen))         CircuitBreakerOpen.enter()  All these could delegate to a      def transition(from: CircuitBreakerState, to: CircuitBreakerState)
yes, as implemented here it should be ignored Especially transitions from Closed will happen more than once, since several calls run at the same time and increase the failure count beyond threshold. That can perhaps be prevented by using == maxFailures. @scullxbones please think through the state transitions and see if we should not ignore false transition attempts 
You are right, I missed that, and I think it would be useful to have. Please add it again. Perhaps it is more lightweight to use an AtomicBoolean instead of Semaphore since there is only one permit.  
My suggestion is to place it in akka.pattern instead of akka.util, since it is a higher level construct built on top of other things. What do you think about that @viktorklang ?
On these callbacks I originally had:  ```scala def onOpen(callback: => Unit): CircuitBreaker ```  And changed them at Viktor's request to:  ```scala def onOpen[T](callback: => T): CircuitBreaker ``` Should I be bringing that forward again, or was there a decision that wasn't needed?  Being a callback the return type isn't used, but not requiring Unit is a more flexible approach.
For the transitions, I like the use of == maxFailures to make all consistent.  Both resetBreaker and attemptReset are only called once per trip through - one by the scheduler (attemptReset) and the other by the guard only admitting a single thread into half-open's invoke call.  Given the choice, I do prefer the approach of fail-fast rather than having assumptions about concurrent behavior fail silently.
I changed from:      def onOpen[T](func: ()  T): CircuitBreaker  It's more convenient on caller side without ()  Regarding T or Unit I have no strong opinion, but I don't understand what benefit T would add for these.
Type inference with Unit can be an issue for partial functions, so either use T or Any. I prefer T though.
ok, then there is a reason. use T
Job pr-validator-per-commit failed for 4ceb70c4 Took 46 min. [(results)](https:jenkins.akka.io:8498/job/pr-validator-per-commit/691/):<br> <br>To retry exactly this commit (if the failure was spurious), comment "PLS REBUILD/pr-validator-per-commit@4ceb70c4f7834ab27aae7d10833ecfcea9b08bc2" on PR 1699.NOTE: new commits are rebuilt automatically as they appear. There's no need to force a rebuild if you're updating the PR.
My understanding why this (might) work is the following:  1. The version of a gossip itself cannot be lower than or be in conflict with any of the versions inside its seen table  2. A state (with version) is *candidate for convergence* if there are no other gossips (and states) circulating that are newer, or in conflict.  So *if a state is candidate*, then we are only interested in tracking its own progress -- we do not care *exactly* which earlier versions were seen by other nodes (since those states won't converge anyway).  *If a state is not candidate*, then by point (1) it cannot contain information about the progress of the *actual candidate* (if such state exists), so its seen table will be of no use in case of merges anyway.
Do you have numbers how much we gain by this change space wise? This seems to be significant.
Sidenote: Point 1. can be proven inductively. Let assume that all nodes have some state S with a seen table T satisfying (1). A versioned state can be updated in two ways:  - the node itself makes modifications from S resulting in S'. The version of this state v(S') dominates all versions in T since by the induction criterion v(S) >= than every entry of T, and v(S') > v(S). Also the T' table only differs at one place from T -- the node's self entry.  - the node merges S and S' into S'' and T and T' into T''. Since v(S'') > v(S) and v(S'') > v(S'), using the induction criterion (1) on S, T, S', T' it follows that v(S'') is larger than any version in T or T' and the only version that can be larger in T'' than the previous tables is the node's self entry.  Therefore any valid update of a versioned state and its seen table keeps property (1) intact.  QED ;)
doesn't this potentially put back removed members into the seen Set, previous mergeSeenTables had allowed members?
do we need this method at all?
I think this is a huge win. Should be carefully tested (repeat job).
I like this large red blob
Agreed! Is this trick known? Potential blog post if correct?
No, I haven't checked any numbers yet, but the seen table is probably the bulk of the message.
So we only merge the seen tables when the gossip versions are the same, so that shouldn't really be an issue. The fact that we needed to filter them out before was that we never cleared the seen table during version bumps.
I'd recommend:  foreach cluster.release(_)
Is the None classloader appropriate here?
Shouldn't we add a ticket for that?
Qualify this with : Throwable so the intent is clearer
Qualify this with : Throwable if it's intended to cover even OOMEs etc
I see "transactionLogNode + "/" + id" shouldn't this be made into a method so it's easier to maintain?
He Viktor.  I don't know. I just extracted some methods, but I didn't do much on the inside.
We should ask jonas :) Since it is his comment. But I can make a ticket for it.
I can do that. But I did not write this code.. just some cleanup.  You are reviewing code of Jonas :)
He Viktor.  A lot of comments are about code Jonas write. I can clean it up, bit imho not part of the ticket.
Boy scout rule!
I have no problem with the boy scout rule.  But I also try to limit the scope of the ticket. I already did some cleaning to make it easier readable.  On Sun, Aug 7, 2011 at 8:27 PM, viktorklang < reply@reply.github.com>wrote:  > Boy scout rule! > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/aaec3aef77d7780a006bcac0ce0d6de2db77e7b7#commitcomment-517981 >
I always clean code up when I visit it, it's the only way to ensure that the codebase's health is preserved, that's the only reason I commented it. "Maintenance" is an implicit cost for all tickets that involves touching code :-)
Ticket created: https:www.assembla.com/spaces/akka/tickets/1084-clusterdaemon-failover-connection-gc-call
Are you complaining about *my* code? Should *my* code be cleaned up? That is not possible. ;-)
I'm only limiting scope :)  And I was not the one reviewing it.. you should look at somebody else :)  On Tue, Aug 9, 2011 at 2:23 PM, jboner < reply@reply.github.com>wrote:  > Are you complaining about *my* code? Should *my* code be cleaned up? That > is not possible. ;-) > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/aaec3aef77d7780a006bcac0ce0d6de2db77e7b7#commitcomment-521297 >
Lol! There is no such thing as _your_ code :D There is only *our* code!
Sure. Just my bad sense of humor.  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Aug 9, 2011 1:27 PM, "viktorklang" < reply@reply.github.com> wrote: > Lol! There is no such thing as _your_ code :D There is only *our* code! > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/aaec3aef77d7780a006bcac0ce0d6de2db77e7b7#commitcomment-521306
I loled, didn't I? :-)
On Tue, Aug 9, 2011 at 1:39 PM, viktorklang < reply@reply.github.com>wrote:  > I loled, didn't I? :-) >  You sure did.   > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/aaec3aef77d7780a006bcac0ce0d6de2db77e7b7#commitcomment-521328 >    --  Jonas Bonr CTO Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner <http:twitter.com/jboner> Google+: http:gplus.to/jboner
Great, we can expand on this for M2
Do you really have to specify /user/ first? Thought Roland removed that need. Check with him.  Same in the Scala sample. 
There's no docs on actorFor here for Java
I checked with Roland. It is correct as is. Thanks.
Yeah, sure. I just wanted to get something in for M1.
Yes, we _must_ have this for M1
I'd change this to: To obtain an ActorRef to an Actor on a remote node::
Add a brief explanation of:      akka:<actorsystemname>@<hostname>:<port>/<actor path>
actorFor is coming from where? static import? 
Good point. I added that without any code... I'll add the context
Why not use the same sample actor address on the two examples to make it really clear?  Same actor. - This is how to look it up on local - This is how to look it up on remote 
I added the oneliners because I wanted to save some time. We should, of course, add better examples of how to use remoting in M2.
Sure. But it takes 1 min to change it to the same address. But skip it if you are tired. 
No, we really need those for M1, it's otherwise pointless to add any docs about it at all. Just add a quick example on local actorFor and remote actorFor, you basically have all the code there already, you just need to put it in a Scala file and import it into the docfile.
Aren't these two examples enough?    val actor = context.actorFor("/serviceA/retrieval")   val actor = context.actorFor("akka:app@10.0.0.1:2552/user/serviceA/retrieval") 
Yes, just clarify the syntax of that URI in a note statement as I wrote earlier in this thread and all should be fine.
Yes, I've added that as well. I'll push a new version and you can have a final look at it.
Great. Thanks buddy. 
Ok, looks good. Great work Henrik.  Merge with master and close pull request, close the 2 associated tickets and call it a night.  Great work today!
Should we log the messages as well? 
I was thinking about that, and I didn't want an attacker to be able to flood the logs more than needed...
Good point. In that sense it should perhaps even be possible to completely turn it off. 
update isn't something you typically do with sbt 0.11
According to Viktor's checkstyle rules this should be : Unit = ;-)
For Java we have ActorSystem.create()
I see no reason to have ActorSystem as static field here, move it to calculate method.
same with this
add plusOrMinus tolerance when comparing doubles
Fair enough. Don't want to upset Viktor and his checkstyle rules :-)
No, I've heard he's pretty badass
It is used on line 116 to add actors.
use getContext().actorOf() (if that does not work, its a bug)
exactly the reason I wanted system to be out of scope ;-)
It doesn't compile even. You meant instead of using system.actorOf I suppose?
From within an UntypedActor you can use context().actorOf()
Add TODO FIXME 2.0
long lines makes it very hard to read on GitHub. 
Very good. Extremely clean code. A joy to read. 
use mapTo for type coercions (casting is cheaper though)
Demonstrate:  Promise successful 3
This isn't parallel, if that should be the case
Might be worth adding a .. warning:: section on that
I'd probably use only Futures instead of mixing Futures and actors. the mapTo-stuff seems cluttery.
Perhaps document:  "failed"- the failed projection of a Future onSuccess / foreach onFailure filter Promise.successful Promise.failed 
Might want to clarify why.
And what that means that you should avoid (Await)
Perhaps clarify here that you should prefer Promise.failed/Promise.successful for constants
great work Patrik!
yes, there is a description in the doc with example of mapTo, but when blocking it makes more sense to do an ordinary cast
It describes in the doc text, that f1 has its original value even though map has been used (of course). I can move that assert out of the doc #map include tag, if you don't like it
I don't know how to do that, or what you mean. Can you elaborate (with code)
That was all :-)  Promise.successful(3)
Creates an already completed future with that value.  Promise.failed(new FooException()) creates a completed promise with that failure
something like: note::   The execution of futures a, b, and c are not done in parallel, in above example.
Exactly, because b is only evaluated once a is done etc.
ah, it's already described in the document Something to keep in mind when doing this is even though it looks like parts of the above example can run in parallel, each step of the for comprehension is run sequentially.
I only converted the existing examples. We can improve the samples later.
Created ticket #1537, for later
I have removed that line, because it is actually discussed more further down. To that I have added: The blocking operations are located in ``Await.result`` and ``Await.ready`` to make it easy to spot where blocking occurs.
great.  All done? If so, push and merge.
thanks, then I'll push the minor things and merge  On Thu, Dec 15, 2011 at 11:11 PM, viktorklang < reply@reply.github.com > wrote:  > great. > > All done? If so, push and merge. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/3b6c3e28d3eabfdfa5c38071528f777834fa7306#commitcomment-799047 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
http:stackoverflow.com/questions/2419410/passing-this-in-java-constructor  We need to devise a correct solution. DelayedInit?
delaying initialization does not solve the entangled structure it currently has, the only clean solution is defining, documenting and implementing a clean sequence, where we probably will have to break up the ActorRefProviders initialization into constructor and init(), with the ability to start local system services like logging in between.
Or switch to first allocating the objects, then call some init-method:  class AkkaApp {   val remoting = new Remoting   val pigdog = new Pigdog   def start() = {     remoting.start(this)     pigdog.start(this)   } }
I wonder why "newRoutedActorRef" was chosen when the established standard is "actorOf"?  How is this used from Java?  How do I specify a custom Router implementation?  Why does it return a RoutedActorRef (leaking implementation detail)?
Should probably use composition instead of inheritance to provide the Router-functionality, otherwise CustomRouter will never work.
Should this class be public?
UnsupportedActorRef already implements ScalaActorRef, why is it needed here as well?
This never forwards ?/ask futures, and doesn't give an error, leading to silent breakage, is this the intended behavior? Why not propagate the channel to "route"?
Is this a public method intended for end-user usage?
Use the new impl of RemoteActorRef start, that doesn't set it to Running if stopped
What is the intended Java API for this?
Why doesn't this take a Channel to defer the semantics to the Router impl?
Same remark with Channel
Shouldn't this use forward?
Are all exceptions to be regarded as Dead Actor?
Should direct never fail over?
Yeah, good idea
Why does this extend ScalaActorRef?
I like that you split up the routing tests, very much needed! Good job! :-)
> I wonder why "newRoutedActorRef" was chosen when the established standard is "actorOf"?  It was based on the same naming used as in the akka.cluster.Routing  > How is this used from Java?  The newRoutedActorRef can't be used from Java easily, that is why I added the other method newRoundRobinActorRef  > How do I specify a custom Router implementation?  Not through these methods. We already talked about that this morning and I suggested that the current design isn't very exensible for this reason. But that is how it was modelled with the ClusterActorRef the 'new' routing is based on. It certainly isn't the way how I would have modelled it.  > Why does it return a RoutedActorRef (leaking implementation detail)?  I think that RoutedActorRef is going to be a part of the API just like the LocalActorRef where you can do Router specific stuff on, like adding/removing connections etc.  But.. In Java I would have set this all up through interfaces or in Scala terms, 100% abstract traits. Currently the traits in Akka are not.. 
> Should probably use composition instead of inheritance to provide the Router-functionality, otherwise CustomRouter will never wor  That is what we talked about this morning. But to do a redesign on this part, I do not consider a part of this already to big task. But we both agree upon that the current approach is not very extensible friendly. But this is the approach used in the ClusteredActorRef.   But a Akka developer that wants to provide a custom router can of course always do something like:  new RoutedActorRef(actorAddress, connections) with CustomRouter if he wants to provide his own implementation.
> Should this class be public?  If it should be part of the Api like the LocalActorRef or the RemoteActorRef, it should. But I don't like making 'abstract' classes part of the API instead of 'interfaces'.   > UnsupportedActorRef already implements ScalaActorRef, why is it needed here as well?  I forgot to remove the one in the routedactorref. I had some problems with the self type if you remember and I fixed it by adding the scala ref... but forget to remove it from the routed ref. Will remove it.
This is how it is implemented in the ClusteredActorRef, the design of the RoutedActorRef was based on.
I can remove that as well. I added the ScalaActorRef to the UnsupportedActorRef and forgot to remove it from RoutedActorRef and ClusterActorRef.  Will fix that.
Currently they are. But there is a ticket for the failover that also needs this question to be answered.  So when we are going to work on the failover, we need to come up with a clear definition of failure. https:www.assembla.com/spaces/akka/tickets/1064-routedactorref-should-be-able-to-deal-with-failover
I'll write a ticket for it.  What should be done is the following.. when a actor fails that accessed by a direct router.. it will be reconstructed somewhere else because de events are stored and can be replayed (TransactionLog).   Currently afaik there is no functionality for a failover of such an actor.  
> It was based on the same naming used as in the akka.cluster.Routing  Ok, what do you think about pros/cons of having Routing.actorOf(): ActorRef?  > The newRoutedActorRef can't be used from Java easily, that is why I added the other method newRoundRobinActorRef  Can't that be solved with overloading Routing.actorOf to accept a java.lang.Iterable?  > Not through these methods. We already talked about that this morning and I suggested that the current design isn't very exensible for this reason. But that is how it was modelled with the ClusterActorRef the 'new' routing is based on. It certainly isn't the way how I would have modelled it.  How much work do you estimate it to be to have a non-trait based impl?  > I think that RoutedActorRef is going to be a part of the API just like the LocalActorRef where you can do Router specific stuff on, like adding/removing connections etc.  I'm still unsure if we should expose implementation details or not, perhaps we should discuss it on the Akka team meeting on wednesday?
Ok, open a ticket for fixing it so that ?/ask works
Even when it has a list of local actor refs?
I cannot comment on the actual effect of the config, but now the lines are internally inconsistent. Either its 5000 millis or 5 seconds...
These values use the time unit declared in the config. So now it's wrong?
These values use the time unit declared in the config. So now it's wrong?
Good observation :-) Wrong of me. Fixed  On Tue, Apr 12, 2011 at 1:44 PM, rkuhn < reply@reply.github.com>wrote:  > I cannot comment on the actual effect of the config, but now the lines are > internally inconsistent. Either its 5000 millis or 5 seconds... > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/03e5944bbe185e7c327a9a5916f5ed2928d0827d#commitcomment-338721 >
This line prevents the use of Duration.Inf in the following case:  Await.result(someActor ? someMsg), Duration.Inf)  since Duration can't have its length "seen".
Don't use inf?
`Await.result(someActor ? someMsg, Duration.Inf)` is perfectly fine, what is not fine is passing an infinite (implicit) `ask` timeout, because that would inhibit proper cleanup of the synthesized PromiseActorRef.
That solves the problem, obviously. But if the following is possible,  (someActor ? someMsg)(Timeout.never)  that is, a send-and-receive-future that will never timeout (the created actor waiting for the response will never timeout). Its "strange" that the following only fails in the Await part, not in the ? part:  Await.result((someActor ? someMsg)(Timeout.never), Duration.Inf)  I'm pretty sure that this is not a common case, and this shouldn't be the way to use it maybe, but in this case I know that "eventually" I'll get my answer back and I really don't want a timeout to occur.  P.S.: Sorry for my missing parenthesis earlier :)
Sorry, my mistake. My apologies. It's as rkuhn says: it's the infinite Timeout on the `ask` that makes it fail. Still, my argument still stands. I'm "sure" that I'll get an answer back, so I didn't want a timeout to occur.  And I'm sorry for using this as "discussion". I was going to ask it in the mailing list.
In that case: how sure are you that it will be within the year? month? day? hour? Then just set the timeout accordingly.
Since Akka doesn't guarantee delivery you do not know that there'll be a reply, so just put a decent timeout there.
Akka doesn't guarantee delivery, but other abstractions can be built with it (like the Uniform Reliable Broadcast from "Introduction to Reliable and Secure Distributed Programming") that guarantee delivery. Besides, in a local JVM, it's very unlikely to lose a message.  I was referring to this as a conceptual thing. I can actually put 1 year as a timeout since I know that it will take mere moments to get the reply, but this breaks the "eventually" concept. Eventually means that I will get it, it can even be 10 years from now, but I'll get it. And I don't want to deal with timeouts just because of possible leaks (and it wouldn't even be a leak, I really want that created actor to wait until it gets a response).
If you don't want to deal with timeouts just define your own implicit Inf which is 1 year and then all is good?
If Timeout supports Infinity (with `Timeout.never`), I don't see why I should make it "look like" it's infinity. Again, I can, but I was seeing this from a conceptual perspective.  From what I've seen, it would need just a few lines of code in `object PromiseActorRef`. But I know that it's never like that.  I'm sorry for all this. It wasn't my intention. I don't actually have a practical problem here, I was merely trying to suggest an enhancement, in my opinion.
Imho, potential infinite leakage of memory and infinite blocking of threads is not an enhancement.
Fair enough. But that should be the developer's problem. The documentation already states that it can be dangerous. "Dragons lie ahead" should be enough to deter most, unless they really know what they're doing.
In my experience people will fall back to never/Inf instead of thinking. So offering them that will only harm them.
I have no argument against that one. I only think that's how, sometimes, we end up harming those that do think. Thanks for indulging me in these comments, I know you probably have a ton of work, providing the best Akka you can. Again, thanks.
You're very welcome, thanks for the discussion, one should always be prepared to explain why things are as they are.
I don't know about you guys, but at least on my setup I get a nasty StackOverflowException after applying this patch. I've also tried removing my local ivy2 cache as well as doing a git clean -n -d -x.  [debug]         /home/karim/w/akka/project/AkkaBuild.scala [debug]         /home/karim/w/akka/project/Publish.scala [debug]         /home/karim/w/akka/project/Unidoc.scala [debug]         Invalidated direct: Set() [debug] Incrementally invalidated: Set() [debug] Copy resource mappings:  [debug]  java.lang.StackOverflowError         at scala.collection.immutable.List.foldRight(List.scala:45)         at scala.collection.LinearSeqOptimized$class.foldRight(LinearSeqOptimized.scala:120)         ...
Have you've downloaded the 0.11.0 launcher?
Yes, I've downloaded this one:  http:typesafe.artifactoryonline.com/typesafe/ivy-releases/org.scala-tools.sbt/sbt-launch/0.11.0/sbt-launch.jar
Can you remove your project/plugins directory?
I assume that you mean 'project/sbt7/plugins' directory? If so, I still get a StackOverflowError.
No, project/plugins I have no idea about the problem you're experiencing. Are you sure you're launching with sbt 0.11 launcher and you have cleaned out any potential residue?  You could ask for input on the sbt mailinglist.  Cheers V
I did a fresh checkout, removed '.sbt' and '.ivy2' from my home and the problem persists. The directory you're referencing does not exist.  karim@development:~/w/akka$ find -name *plugins* ./project/target/streams/compile/defined-sbt-plugins ./project/sbt7/plugins ./project/plugins.sbt ./akka-tutorials/akka-tutorial-second/project/plugins ./akka-tutorials/akka-tutorial-first/project/plugins 
I also double checked that I'm using sbt 0.11.0.
Look at your ~/.sbt directory. Try to remove the boot subdirectory as well as everything in the plugins subdirectory except for build.sbt.
So, after removing '~/.sbt' and '~/.ivy2' as well as doing the following:    http:pastebin.com/K8pqt6Li
Removed ~/.m2?  You sure the merge went alright?  Cheers,   --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
Now we are talking, mate! This is the issue: java -jar /home/karim/bin/sbt-launch.jar Please take a look at the sbt website and create a suitable launch script. Suitable means increasing heap and stack size.
viktorlang:  It's a fresh checkout so I assume that the merge went right.  hseeberger:  http:pastebin.com/pfVL7DEh
Don't you think that to deal with a stack overflow you should increase the stack size, not only the heap size?
Increasing stack size should work:      java -Xss2M -jar /home/karim/bin/sbt-launch.jar 
Yeah, that's it. Thanks dudes!
IIRC Udp sockets can be reconnected and rebound later. Is there a rationale for not supporting this?
How do you specify that a send is to be discarded if it cannot be sent right now? Is the round trip to the selector here really necessary? For UDP you usually expect sending on a best-effort basis. Couldn't you remove the pending send altogether?
Do you check somewhere that the buffer is big enough to accommodate the complete message? It seems you are throwing the rest of the message silently away here (you usually don't have big UDP packets to avoid fragmentation but you *can* have UDP packets being 10ks in size and the buffer size may be lower than that).
> Do you check somewhere that the buffer is big enough to accommodate the   > complete message?  How is this possible? All I could figure out from the API that it either   returns 0 for read or the complete size. I would be happy to report that   it did not fit, but I don't know how to do that.
Hmm. It might make sense, maybe we want a separate Reconnect message?
You can check the size of the buffer before (ByteBuffer.capacity) or else `copyToBuffer` returns how many bytes were copied. The question is how to react on this. Since it isn't possible to split the data up like in the Tcp I would probably use ByteBuffer.allocate(Direct) to allocate a buffer big enough for that case or fail with a message that you need to configure bigger buffers.
Dude, this is a total hack. There _has_ to be another way
He Viktor,  the code isn't the prettiest but it deals with the ambiguity replication vs replication-factor.  On Tue, Jul 19, 2011 at 11:25 AM, viktorklang < reply@reply.github.com>wrote:  > Dude, this is a total hack. > There _has_ to be another way > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484440 >
But can't put it in Configuration. That is a generic parser. Put it in Deployer where the deployment parser is.  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Jul 19, 2011 1:37 PM, "pveentjer" < reply@reply.github.com> wrote: > He Viktor, > > the code isn't the prettiest but it deals with the ambiguity replication vs > replication-factor. > > On Tue, Jul 19, 2011 at 11:25 AM, viktorklang < > reply@reply.github.com>wrote: > >> Dude, this is a total hack. >> There _has_ to be another way >> >> -- >> Reply to this email directly or view it on GitHub: >> >> https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484440 >> > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484664
Hi Jonas,  I'm currently looking at another solution that solves the problem more generically.  On Tue, Jul 19, 2011 at 2:59 PM, jboner < reply@reply.github.com>wrote:  > But can't put it in Configuration. That is a generic parser. Put it in > Deployer where the deployment parser is. > > -- > Jonas Bonr > CTO > Typesafe - Enterprise-Grade Scala from the Experts > Phone: +46 733 777 123 > Twitter: @jboner >  On Jul 19, 2011 1:37 PM, "pveentjer" < > reply@reply.github.com> > wrote: > > He Viktor, > > > > the code isn't the prettiest but it deals with the ambiguity replication > vs > > replication-factor. > > > > On Tue, Jul 19, 2011 at 11:25 AM, viktorklang < > > reply@reply.github.com>wrote: > > > >> Dude, this is a total hack. > >> There _has_ to be another way > >> > >> -- > >> Reply to this email directly or view it on GitHub: > >> > >> > > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484440 > >> > > > > -- > > Reply to this email directly or view it on GitHub: > > > > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484664 > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484686 >
Great  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Jul 19, 2011 2:08 PM, "pveentjer" < reply@reply.github.com> wrote: > Hi Jonas, > > I'm currently looking at another solution that solves the problem more > generically. > > On Tue, Jul 19, 2011 at 2:59 PM, jboner < > reply@reply.github.com>wrote: > >> But can't put it in Configuration. That is a generic parser. Put it in >> Deployer where the deployment parser is. >> >> -- >> Jonas Bonr >> CTO >> Typesafe - Enterprise-Grade Scala from the Experts >> Phone: +46 733 777 123 >> Twitter: @jboner >> On Jul 19, 2011 1:37 PM, "pveentjer" < >> reply@reply.github.com> >> wrote: >> > He Viktor, >> > >> > the code isn't the prettiest but it deals with the ambiguity replication >> vs >> > replication-factor. >> > >> > On Tue, Jul 19, 2011 at 11:25 AM, viktorklang < >> > reply@reply.github.com>wrote: >> > >> >> Dude, this is a total hack. >> >> There _has_ to be another way >> >> >> >> -- >> >> Reply to this email directly or view it on GitHub: >> >> >> >> >> >> https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484440 >> >> >> > >> > -- >> > Reply to this email directly or view it on GitHub: >> > >> >> https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484664 >> >> -- >> Reply to this email directly or view it on GitHub: >> >> https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484686 >> > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484695
I think this is better:  def getSection(name: String): Option[Configuration] = {     val l = name.length + 1     val m = map.collect {       case (k, v) if k.startsWith(name+".") => (k.substring(l), v)     }     if (m.isEmpty) None     else Some(new Configuration(m))   }  I have ran all the tests and they are fine. If no one has an objection, then I'll commit this.  On Tue, Jul 19, 2011 at 3:14 PM, viktorklang < reply@reply.github.com>wrote:  > Great, thanks! > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484703 >
Looks better! :-)  for performance you might want to hoist "name+"."" out of the guard.  val section = name + "." case (k, v) if k startsWith section => (k substring l, v) 
He viktor,  no problem.  On Tue, Jul 19, 2011 at 3:56 PM, viktorklang < reply@reply.github.com>wrote:  > Looks better! :-) > > for performance you might want to hoist "name+"."" out of the guard. > > val section = name + "." > case (k, v) if k startsWith section => (k substring l, v) > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/5635c9f76a5950d6231c40c3e705648d9a5f9158#commitcomment-484773 >
coolio! But wheres my ExtensionKey[T]? ;-)
What's the Java API for using it? ;-)
Yes, but how do you declare it in the config file?
Should it use java here when there is no java mapped?
first: youre reviewing a merge ;-) second: "java" comes from reference.conf
Will the message be received (in application code) in each traversed actor, or handled by Akka under the hood. Probably the latter. Needs to be clarified. 
Should we also provide a special Resolve message that will be handled completely under the hood and reply with ResolvedActorRef message(s)?
Why do we use nul instead of null? I think null is more recognized in java/scala community.
My implementation plan is to transform it into a List-like data structure in actorFor and then handle this type of message automatically:      case class Parent(msg: Any) extends AutoReceiveMessage     case class Child(pattern: String, msg: Any) extends AutoReceiveMessage          actorFor("../*") ! msg -> self ! Parent(Child("*", msg))
This would have to be tagged with the initial request and responses would come in one-by-one. But my feeling is that this is so little gain over just getting the replies that we should not put it into Akka itself. Basically Resolve would be an implicit Ping-Pong or EchoRequest.
because `null` has one character too many ;-)
we could call it `nil`, though.
Yes, nul look like a typo to me. Why the limit of 3?  On Mon, Nov 21, 2011 at 8:38 AM, Roland Kuhn < reply@reply.github.com > wrote:  > we could call it `nil`, though. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/a2a09ec5a922c5a766472a054949cd83aba378ce#commitcomment-734183 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
General geekiness: `nul:` was on MS-DOS what `/dev/null` is to Unix, plus all top-level directories in Unix have three characters.
There are other references as well, like the bubble walker, the dead letter etc
I'd say that the most canonical form is to bootstrap thought ActorSystem.actorOf, then expand using context.actorOf inside actors, to form the supervision tree.
I think this is bad, for the following reason:  1) watch/unwatch need to work from the first moment. 2) I'd expect to have the path resolved when I do actorFor, not at some arbitrary, potentially racy, even later on
What's the benefit of doing it like that and what are the alternatives?
Way cool, but this unresolved business gives me the creeps.
Yes, but they are not intended for public consumption (deadLetters is just a local actor reference)
Nothing in this is more or less racy than doing the lookup synchronously. The reason for this approach is simple: should actorFor block for potentially never-returning network messages?  Think of unresolved as reified query which you can send to. This also means that you cannot meaningfully watch/unwatch it. Maybe the unresolved reference should be a supertype of ActorRef so that we can distinguish it on the type level.
The benefit is that it works (at least I did not find any blockers while pondering different strategies). I cannot think of any alternatives which have this property (with the implicit requirement of not making our remoting layer a huge mess).
Yes, I thought so ;-) So, what about making them explicitly special in the type system?
I should probably add a section about the -or-create part of actorFor. My idea here is that an absolute lookup of a declared service will start it if not already done. As we said, all such services are supervised by a guardian, and we can easily make a new one for this purpose, just to keep the structure as clear as possible:      "akka:host:port/srv/configured-service"
yes, will expand that section. I wrote this doc with reference to the written-already-in-my-head-but-not-yet-dumped introduction to hierarchical actor systems.
Might be better to have something more uniform, like:  akka:/app/service-b (If host:port) doesn't exist, it's local 
My evil plan was to maybe in the future offer an actor system registry so that you can initiate inter-system communication using these URIs, hence the need to include the systems name.
The user shouldn't need to care or know about details of a ref. an ActorRef is an ActorRef is an ActorRef
Add the section with a TODO so it doesn't get overlooked.
All ActorRefs need to support watch/unwatch, otherwise it's not transparent and all is lost.
And what are the performance implications?
So how would you get an ActorRef to something you look up?
Suggestion  akka:my-system@hostname:port/app/service-b (with remoting) akka:my-system/app/service-b (without remoting)
Yes, concerning sending to it and passing it around. This section is background information for those who want to know more about the implementation. Given this, I will add a little note about the special one-off refs.
will commit an update instead, just want to wait until we settle on the same page
Okay, agreed. Proposal:      class ActorQuery {         def !(msg: Any) { ... }     }     class ActorRef extends ActorQuery  Then you can also pass queries around and reply to them etc. Of course the docs will explain why ActorQuery cannot be a subclass of ActorRef.
Well, obviously you route messages via different actors, so there definitely is a non-negligible cost. You pay for the flexibility it represents.
Send it a message and watch out for replies, then store away the respective sender references. Patrik suggested an AutoReceiveMessage just for this purpose, but since at least the replies need to be explicitly handled by the user, I tend to think that typical use patterns will make this superfluous. Why would you look something up if not intending to send it a message?
Well, that was my point, if including them is for enlightenment on how it's implemented, why settle for anything less than everything? ;-)
Really dislike the name ActorQuery  trait Endpoint/Destination/Receiver {   def tell   def ! Scala*?   def ask  Should be here?   def ? Scala*?
Is this in the case where it represents an ActorRef or something else?
This is why DeathWatch needs to work for them.
Otherwise I have no way of finding out whether the endpoint is valid or not
host:port is kinda tightly coupled with the Netty Transport layer. We should open up the possibility that whatever goes after the @ and the first subsequent slash is the identifier in the used remoting transport.
"One important aspect is that actor paths never span multiple actor systems or JVMs."  I think this needs clarification
The unresolved/endpoint/... actor ref is like a remote actor ref. You don't know if there really is an actor on the other end. Watch/unwatch really need to be async (everything is, right?). For me, this fits in with the general transparency idea and that actor refs should all be considered as distributed by default, not locally known and resolved by default.
Sounds good, but then it should be resolved/optimized under the cover so that users don't need to do the resolve by reply msg themselves due to performance reasons.  /Patrik  21 nov 2011 kl. 22:02 skrev Peter Vlugter<reply@reply.github.com>:  > The unresolved/endpoint/... actor ref is like a remote actor ref. You don't know if there really is an actor on the other end. Watch/unwatch really need to be async (everything is, right?). For me, this fits in with the general transparency idea and that actor refs should all be considered as distributed by default, not locally known and resolved by default. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/a2a09ec5a922c5a766472a054949cd83aba378ce#commitcomment-736015
Yes, I agree. Optimised locally to a local actor ref, and can be cached on remote nodes.
Hej, good to see youre back! I also had a little chat with Viktor a few hours ago and will try to weld all the ideas and requirements into v2 of this document tomorrow. Then we should probably have a skype meeting about it.
If it's an actor ref then it needs to fully be an actor ref, that means that watch/unwatch needs to work etc.
For sure. Monitoring will need to work asynchronously for remote and cluster actor refs, which is similar to unresolved local actor refs.
If I do:  watch(actorFor(somePath))  I _must_ get the Terminated message when its terminated, or a Terminated-message if it doesn't exist, otherwise it breaks the contract of the death watch
You have '/remote' which is more than three.   I'd prefer '/null' I think.   And why not call '/app' '/user' or '/usr' instead? 
Okay, you are right: either "/rmt" (which I dont like) or go the full way:      "/user"     "/system"     "/null"     "/temp"     "/remote"  PS: /usr stands for Unix system repository
I like that. Less cool, but more clear. Also like 'user' more than 'app'. 
Should be '/user'. 
youre looking at an old commit ;-)
On Tue, Dec 6, 2011 at 11:41 AM, Roland Kuhn <reply@reply.github.com> wrote: > youre looking at an old commit ;-)  Sounds good then.  > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/a2a09ec5a922c5a766472a054949cd83aba378ce#commitcomment-771972    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
this code snippet doesn't make obvious sense, what is bangIsInMyCustomMailbx  I think the whole section is very internal implementation specific. Why does the user have to care. Things just work as expected. It possibly belongs in scaladoc.
and same comment here
You are right that it also belongs into the ScalaDoc, will fix. But it should be documented here as well, because if you implement a custom mailbox and use it for top-level actors, then there will be an observable difference in semantics. Ill try to make the code snippet more pseudo and more obvious.
ok, I see
I'll add a support for the rest of the socket options if this looks good enough.
Why not make it an ADT?  sealed trait 0MQSocketOption  case class Linger(value: Int) extends 0MQSocketOption
Ok, that will work when setting a socket option. How about accessing one?
What is the use-case of accessing it?
    sealed trait SocketOptionQuery     sealed trait SocketOption     case class Linger(value: Int) extends SocketOption     object Linger extends SocketOptionQuery      def getOption(q: SocketOptionQuery): Option[SocketOption]     def setOption(o: SocketOption)
use-case may be: if Linger is set to 0, I will affect TCP-RESET, if not it will be TCP-CLOSE, which may or may not be interesting to know before calling close() (I have not much of an idea about 0MQ yet, this example is just for TCP stuff)
Viktor, I am adding such support because the ZeroMQ API allows the user to do so, i.e. if a user uses non-default values for some socket options, akka module should allow it. Linger here is just for RFC.
Moving CallingThreadDispatcherModelSpec.scala broke the build on my machine. Does it still work on yours?
Oops, that happens when doing just editorial changes. I fixed it  (/me goes searching for brown paper bag)
This was the first time since v2.0 was merged into master that I tried compiling it, so wasn't sure if it was a problem on my end or not. I just have bad luck I guess :) glad you were able to fix it.
I think Play Mini is a bad name. But we should talk to Peter about that. 
Why is this line hardcoded? 
Excellent. Great job. 
I just wanted exactly that, not the whole line as it also contains ".mapTo[Int].asPromise.map {" which could be confusing. That meaning of that is explained  in a later section. 
Before calling `resume()` you might want to check that the `_ + 4` and `_ * 2` operations have *not* been performed, ie that the suspension actually worked.
`nmber` => `number`
`new AtomicInteger(Off)`?  Whether `0` or `Off`, should it be the same as the value you `compareAndSet` in the `attach` method?
This uses `ref.transformAndGet` internally, but you could just use `ref.single.update` (since you don't need to get the value to know what it was set to).
I'd definitely prefer an ADT over magic value (-1)
Same as above
There still is an adt 'autoreplicationfactory'.  So that has not gone missing. Only it provides the polymorphic property 'factor' that behaves the same as the previous existing function. But if you want that instead of the polymorphic property, I can change it back. So your say.  On Tue, Jul 19, 2011 at 11:28 AM, viktorklang < reply@reply.github.com>wrote:  > I'd definitely prefer an ADT over magic value (-1) > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/a145e0773cafee3825eaece65065238dd15c4c68#commitcomment-484443 >
I definitely prefer ADT over magic value
Hi Viktor,  I'll change it back. I didn't know this was a recommended practice in Scala.  On Tue, Jul 19, 2011 at 3:12 PM, viktorklang < reply@reply.github.com>wrote:  > I definitely prefer ADT over magic value > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/a145e0773cafee3825eaece65065238dd15c4c68#commitcomment-484700 >
second argument of getOrElse is by-name, which is why I used the empyMap withDefault.
puilding up the intermediate list just to add it to the map seems costly, use same approach as add?      cache = (cache /: changes) {       case (ca, (c, cs)) => ca.updated(c, ca.get(c) match { ... })    }
Realized I was being unclear about this earlier, the main reason of caching this is due to the withDefault.
Ah, now I see. Can't use a default on the cache though. Is it ok to just have a val outside the fold?
Haven't had a chance to look at this yet, but I am pretty sure I changed filter to Any => Boolean to make things easier when working with Actors, although it is incorrect. If the new Actor '?' method returns a Future[Any] we should be able to change the test back to the way it was before, and have a correct filter method.
Yes, this might be a good reason to introduct ActorRef.? now already, although the bulk of stuff from my queue will only travel towards master in a few weeks time, as I first want to get 1.2 ready.
Problem is that if we go for Future[Any], we need to provide some coercion from Future[Any] to Future[T]
Yes, definitely. Among my additions is Future.as[T:Manifest] which will check conformance right away (on the erasure only, of course). The previous .asInstanceOf[T] approach removed type checks, leading to possible errors far down the line, but for the Java API that can't be helped.
But how can you do the conformance-check if you don't add the Manifest to the Future itself, because without a value, you have no reified type to conform to.
The one I have checks on the read-side: https:github.com/jboner/akka/blob/testkit-1.2/akka-actor/src/main/scala/akka/dispatch/Future.scala#L348
Requiring a manifest on every ActorCompletableFuture would probably be a major performance hit: I was astonished to see the byte code (it is always dynamically created instead of preallocated in the using class).
Problem is that it's blocking. Right now we have 3 blocking methods:  1) get 2) await 3) await(atMost)  Ideally I'd want to keep blocking as close to 0 as possible.
Have you've tried ClassManifest instead of Manifest?
My aim was to make the transition from (actor !! msg).as[String] as painless as possible, but I agree that a single blocking call would be more consistent. Ideally Future.await() would be the only one, and it would yield a different type (AlreadyCompletedFuture?) for which as[T] would convert immediately, while Future.as[T] would be my mapTo[T].  Will check out ClassManifest, IIRC Manifest was what I tried some time ago.
This is already in place, has exact same semantics as (actor !! msg).as[String]:    /**    * Implicitly converts the given Future[_] to a AnyOptionAsTypedOption which offers the method <code>as[T]</code>    * to convert an Option[Any] to an Option[T].    * This means that the following code is equivalent:    *   (actor !! "foo").as[Int] (Deprecated)    *   and    *   (actor !!! "foo").as[Int] (Recommended)    */   implicit def futureToAnyOptionAsTypedOption(anyFuture: Future[_]) = new AnyOptionAsTypedOption({     try { anyFuture.await } catch { case t: FutureTimeoutException => }     anyFuture.resultOrException   })   I'm not too much of a fan of FutureTimeoutException tho.
... and I thought I'd save one allocation by putting that method directly on the single class that you pimp it to ...  BTW: ClassManifest and Manifest generate very similar byte code:     29:  getstatic       #51; Field scala/reflect/ClassManifest$.MODULE$:Lscala/reflect/ClassManifest$;    32:  ldc     #53; class scala/collection/immutable/List    34:  getstatic       #51; Field scala/reflect/ClassManifest$.MODULE$:Lscala/reflect/ClassManifest$;    37:  ldc     #34; class java/lang/String    39:  invokevirtual   #57; Method scala/reflect/ClassManifest$.classType:(Ljava/lang/Class;)Lscala/reflect/ClassManifest;    42:  getstatic       #32; Field scala/Predef$.MODULE$:Lscala/Predef$;    45:  iconst_0    46:  anewarray       #59; class scala/reflect/OptManifest    49:  checkcast       #38; class "[Ljava/lang/Object;"    52:  invokevirtual   #42; Method scala/Predef$.wrapRefArray:([Ljava/lang/Object;)Lscala/collection/mutable/WrappedArray;    55:  invokevirtual   #62; Method scala/reflect/ClassManifest$.classType:(Ljava/lang/Class;Lscala/reflect/OptManifest;Lscala/collection/Seq;)Lscala/reflect/ClassManifest;  So, two manifests, one wrapped array allocated for "List[String]" (plus whatever else goes on inside the library). I'd think that all this could also be done by a static initializer.
Why did you remove the dispatcher? 
Crap, this was meant for wip-props only :( I'll fix it.
Can this be backported in time for the akka 1.3 release, please? We're having issues with objects getting into old gen when we need long timeouts, just because the scheduler thread still holds on to the onTimeout runnable, even though the future has long been completed.
Awesome, thanks a lot.
When did we decide to use daemonic = on as default? It has consequences, and I'm not totally convinced of the benefits to have it default on. 
We had a vote at the office today. I won. :-)  No, but seriously, I think we should have only 1 daemonicity flag, not per dispatcher, so either the ActorSystem runs as a service, or it doesn't. WDYT? (then we need to add ActorSystem.join as well)
I agree with that we should have one daemonic flag and was going to suggest      akka {       daemonic = off       actor {         default-dispatcher {           daemonic = ${akka.daemonic}  But we could also remove the daemonic config on the individual dispatcher/scheduler/remote things and only use akka.daemonic. +1 for that.  If the default value should be on or off needs some discussion though.  Somehow I think the first impression when trying Akka is very important and that we don't have to explain to many things to be able to run the first HelloWorld. This would be a fair attempt:         object HelloWorld extends App {       val system = ActorSystem()       val a = system.actorOf(Props[HelloWorld])       a ! "hello"     }      class HelloWorld extends Actor {       def receive = {         case x => println(x + " world")       }     }    This will not work. "hello world" will most often not be printed. We will have to explain thread related things in initial sample. As first time user I would be more disappointed that my sample doesn't work at all than that the jvm doesn't exit automatically.  Also, it feels weird that we propagate non-blocking everywhere but the very first thing we need to explain is that you must block, and spare one thread for keeping things alive.  I think default should be daemonic = off. The everything runs until system.shutdown().  Independent of this I think it is a good idea to add the server.join() method for the daeomic = on use case. join is pretty well known, but in akka we don't use it anywhere else so I think we should stick to more existing concepts. system.awaitTermination() Even better would be to make system an Awaitable. Await.ready(system, 2 hours) The result could be an exit status code, nonzero indicates abnormal termination. 
Great write-up Patrik!  On Fri, Jan 20, 2012 at 08:10, patriknw < reply@reply.github.com > wrote:  > I agree with that we should have one daemonic flag and was going to suggest > >    akka { >      daemonic = off >      actor { >        default-dispatcher { >          daemonic = ${akka.daemonic} > > But we could also remove the daemonic config on the individual > dispatcher/scheduler/remote things and only use akka.daemonic. +1 for that. > > If the default value should be on or off needs some discussion though. > > Somehow I think the first impression when trying Akka is very important > and that we don't have to explain to many things to be able to run the > first HelloWorld. This would be a fair attempt: > > >    object HelloWorld extends App { >      val system = ActorSystem() >      val a = system.actorOf(Props[HelloWorld]) >      a ! "hello" >    } > >    class HelloWorld extends Actor { >      def receive = { >        case x => println(x + " world") >      } >    } > > This will not work. "hello world" will most often not be printed. We will > have to explain thread related things in initial sample. As first time user > I would be more disappointed that my sample doesn't work at all than that > the jvm doesn't exit automatically. > > I agree with that the first impression of Akka is really important and the HelloWorld example is a very good illustration of how beginners would expect Akka to work.   > Also, it feels weird that we propagate non-blocking everywhere but the > very first thing we need to explain is that you must block, and spare one > thread for keeping things alive. > > I think default should be daemonic = off. The everything runs until > system.shutdown(). > > Independent of this I think it is a good idea to add the server.join() > method for the daeomic = on use case. join is pretty well known, but in > akka we don't use it anywhere else so I think we should stick to more > existing concepts. system.awaitTermination() > Even better would be to make system an Awaitable. Await.ready(system, 2 > hours) > The result could be an exit status code, nonzero indicates abnormal > termination. >  I really like this idea too.  > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/03bc15feb11a4bed8bf2efa42dde2c87cef9d0af#commitcomment-885011 >
Thanks for this write-up, completely agreed with all points. But as usual one nitpick: Await.ready(system, ) does not at all look like a shutdown to me, so keep it simple and just stick with awaitTermination(timeout). 
Await.ready(system, ) is not shutdown instruction, it is waiting for the system to complete (shutdown), but awaitTermination is probably even more self describing.
looks good, now I only need to adapt that when I merge in my .stop() removal
Doesn't "new Props(Worker.class)" work?
Since yesterday it does.
use duration, 5 seconds reads better
I don't thing we should place it in akka, and I said why in the review of gracefulStop
do we need to expose this in ActorRefProvider interface?
existing package is akka.pattern we normally don't use plural for package and class names akka.actor akka.dispatch
This is doc spec, must show what user's need to do, so some kind of import is needed
ah, crap, forgot to boy-scout ;-)
not if we expose addTempPath/removeTempPath instead, which would be more generic. Will try it out.
sorry, took this over from Nikolay without checking, changed now.
no, this is Scala doc, so object Patterns is not needed; import akka.pattern.ask is done below (in the doc-visible part)
Add a log INFO here that a new scheduler is created and what needs to be changed to run on the same scheduler?
Instead of spreading this logic out, may I suggest wrapping the system.scheduler on line 452 in a Scheduler with Closable and just not doing anything on close? And then the type of the clusterScheduler can be "Scheduler with Closable".
That's interesting, but wouldn't that mean that I need to create a wrapper class implementing/delegating 6 methods for the Scheduler. Is that really better?
and also confusing, because it looks like I'm closing the default scheduler when reading the code in Cluster.shutdown, which is something that absolutely shouldn't be done
Yup, but at least we don't need to remember to treat them differently in the code, so when/if we only use one scheduler, it's only one place to remove it :-)
You shouldn't see the default scheduler in the Cluster.shutdown, you should only see clusterScheduler, which is semantically what you want to shut down.
I don't get that point, compiler will help us if we remove the useDedicatedScheduler function
Yes, but useDedicatedScheduler is something that will be used on more than one place. I prefer to keep conditional logic contained to as few places as possible, and with minimal exposure to consumers.
I'm doing the change now, but I still it is a tradeoff that doesn't pay, when the price is to duplicate the interface of the Scheduler to the delegating wrapper
Thanks. I appreciate and respect that you have and voice what you believe is right. Keep up the great work, you're doing some excellent work on the clustering!
Great commit message. Very descriptive. 
NAAT (Voice of Borat)
Is verry niiice
LOL  On Mon, Dec 19, 2011 at 3:37 PM, viktorklang <reply@reply.github.com> wrote: > Is verry niiice > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/0f3a720f3e4974437b6e804eafa5487452cfcafe#commitcomment-806206    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
How does this look from Java? Have you thought about a Java API for Path? 
Yes, this is the one that will need a java method. Could be child? path.child("a").child("b")
Or "to"?  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Nov 10, 2011 4:27 PM, "Peter Vlugter" < reply@reply.github.com> wrote:  > Yes, this is the one that will need a java method. Could be child? > path.child("a").child("b") > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/3f7cff141ddeaf3941da6e208312f8d78ade0012#commitcomment-708414 >
This is epic! :)
Why call it ``preStartProcessor``? Why not something like ``preStartRecovery`` or ``preStartReplay`` or something with semantic meaning? That is of course if the main purpose of it is to perform recoveryas the docs state. 
I guess my point is: why not have methods with good names and well defined semantic meaning rather than just blindly adopting the preX/postX scheme from Actor (which are so generic just because it is *only* the user implementing them). In this example just call the method: ``recover`` or ``replay``. 
What happens if I call ``confirm`` on a message not delivered through a channel? no-op?  The destination actor can't, and should not know, if it receives messages through a channel or not right? 
> What happens if I call confirm on a message not delivered through a channel? no-op?  yes, a no-op.  > The destination actor can't, and should not know, if it receives messages through a channel or not right?   the destination actor (or any of its downstream actors) is responsible for confirming messages delivered through a channel. Therefore, it must know.
Ok. Is ``confirm`` used anywhere else? If not why is it on the generic ``Persistent``?  Have you thought about having a special ``ConfirmablePersistent`` or ``ChannelRoutedPersistent`` message or wrap it in an envelope with the ``confirm`` method or similar?  So it is clear what the intent is. Just asking. 
The processor-specific life cycle hooks are planned to be removed anyway, as discussed at:  - https:github.com/eligosource/akka/commit/12f242ecc9fb69f10170d5658ffb6439558c0c95#commitcomment-4098360 and - https:github.com/eligosource/akka/commit/de19c60e03f03c24fcfb178a24d8c2c3a6e923d0#commitcomment-4110432  Pull request is coming soon. The recovery logic will then directly be in `preStart` and `preRestart`. We still can factor the recovery logic out into separate (overridable) methods.
I like the idea with `ConfirmablePersistent`. Need to think more about possible implications. Added to my todo list. 
Ok. I think that could make sense. preStart/preRestart are even less specific. But there might be drawbacks. 
why is this protected[this]? (it's going to be public for Java, and it's not documented to be internal use only)
why is this protected[this]? (it's going to be public for Java, and it's not documented to be internal use only)
Why a val?  if you want to keep the code noise down you can do:  import this.context.system.settings.config
Ideally, since these are per CamelExtension, shouldn't they go into CamelExtension as camel.settings ?
Shouldn't this be configurable?
Why do you need the system here? If you only need the configuration, shouldn't this take CamelSettings from the CamelExtension?
At this point I think that we can safely remove this line, there is no code left from the original implementation, right? 
yeah, that's very true :-)
move "abstraction." up to the previous line
wohooo! SPEED ;D
Don't have time to read all of it :(
A shame really as it is such a master piece!
These needs to go into the https:github.com/typesafehub/typesafe-docs/tree/master/tutorials repo. Can you do that (later)? 
This is the nuclear bomb of git-land, are you sure it is a good idea to provide it in this ready-to-use form? It can only be applied locally anyway, as published branches changing in this way would break all repos having a clone.  I am pretty much in love with the immutability principle of git.
Might be dangerous in the wrong hands. Feel free to remove it.  -- Jonas Bonr  http:scalablesolutions.se http:jonasboner.com http:akka.io http:letitcrash.com  On 5 Apr 2011 19:00, "rkuhn" < reply@reply.github.com> wrote: > This is the nuclear bomb of git-land, are you sure it is a good idea to provide it in this ready-to-use form? It can only be applied locally anyway, as published branches changing in this way would break all repos having a clone. > > I am pretty much in love with the immutability principle of git. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/b42417a2783d08ada468df81e34e89f5794b0413#commitcomment-330107
It's nice, but you are probably going to get similar feedback as with tryAll from Viktor on this. It might be a general style thing in Akka. If I were you I would preempt that and just use the 'java like' try catch stuff that was in my review. It might nog be the coolest,. but it is less code.
It's not about cool, it's about readability, and using abstractions. I can stand doing: try x finally y  but:     try {       template.start()      } catch {        case e=> {          try {            context.stop()          } catch {            case e=>  swallow, so the offending exception is thrown          }          throw e        }      } Is beyond my threshold and it is not a good practice. It is mixing the levels of abstractions in a method.  We need abstractions for this things the same way that we need abstractions for concurrency like actors instead of threads. We could do things with threads, queues etc. but we choose to use actors instead. How is it different for error handling?  BTW. connect-otherwise block I actually took from akka examples, so it is the same principle as already used within akka.  
Also akka is full of usages of Either class which is another abstraction (in my opinion too low level) for error handling. What about Option class? All this are abstractions which people use to hide low level stuff. It is just a matter of getting used to them and they will be as clean as try-catch which is a part of the language.
I'm just saying what I would expect from the previous review. I understand your point of view. Ask Viktor what he thinks. This is very generic stuff, and of course you are now only using it here, but it is something that needs to be accepted, or not, from a global perspective.  Let's say I go along with your points. redefining try with try_ can be confusing, you might read passed it. And I would rename DangerousStuff to something like TryExtensions or something like that (who would want to import dangerous stuff? ;-) is the inline really necessary? 
I've had a similar review a long time ago on the use of partial functions, orElse. But that was in a hot path in the remoting. This of course is not.
Akka tests should always use WordSpec with MustMatchers,
I'm not against a exception-handling-thing in akka-util, but if it should be there it needs to be very lightweight. i.e. verify inlining and preferrably 0 allocations.
In this case you might want to consider Scala ARM.
Ok. From now on... But please don't ask me to rewrite the existing stuff:) In the original camel module, there was a mixture of everything, so I assumed that as long as it is readable and part of ScalaTest it is allowed...
Existing stuff should be encompassed by the boyscout rule. A good intro to the developer guidelenes is in the docs: http:akka.io/docs/akka/2.0-M2/dev/developer-guidelines.html
And the boyscout rule then means that I will probably rewrite it ;-)
I read the developer guidelines some time ago, and they didn't specify it must be MustMatchers and WordSpec. I know that example is showing them, but I didn't realise it is restricted to the two above. Maybe it's worth saying it explicitly to avoid late surprises?
Will look into it
You think the example is ambiguous? https:github.com/jboner/akka/blob/master/akka-actor-tests/src/test/scala/akka/ticket/Ticket001Spec.scala
We sould implement our own AtomicReference that has this operation by default :-)
Why not iterate over entrySet?
He Viktor,  for the router changes, the interface of the router needed to be changed anyway. Instead of a map[ClusterAddress, ActorRef] being returned by connections, a Iterable is now returned containing ActorRefs.  But as soon as we are going to add failover and node addition, then we need to have another very close look at the routers.  On Thu, Jul 28, 2011 at 5:27 PM, viktorklang < reply@reply.github.com>wrote:  > Why not iterate over entrySet? > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/96cc0a00b42fe6d428c3791c7b596a7df334a678#commitcomment-501093 >
Why the ; at the end?
Open brace here is not needed
Why case e: Exception here and e: Throwable in the route method below?
Not possible to do "clusteredRef ? "Die"".await?
not possible to do "clusteredRef ? "Die"".await?
why this section instead of:  intercept[RoutingException] {   clusteredRef ! "Hello" }
Iterating over the entry set is more complex since you do not have the InetSocketAddress by hand if you get the ActorRef (entry).  The InetSocketAddress is required for remove method.
Nope. Since you will get shit load of exceptions.  But the waiting stuff sucks since it introduces phantom failures that are very hart to reproduce and also lead to an increase in build time.  Therefor we need some kind of elementary waiting structures that are not using the clustering itself (since that is the stuff that is being made unstable on purpose). But have some kind of  node.getProcess.await or something.  But this is the same technique Jonas uses in his test, so I concluded that they were good enough.
Call it a Java remainder :) WIl fix it.
But now we cannot see the stacktrace on a remote machine?
This is no good. This was the whole idea with AkkaException. 
Pete, can you fix this and also add a unit test that verifies that the serialized and reassembled instance has the correct toString with regards to the stacktrace?
Do you have a suggested fix? If you have printStackTrace within toString you have an infinite loop as printStackTrace calls toString. It would need to be a separate method. I'm completely surprised this has only just showed up.
Implement something like the ASF did for Harmony?  http:www.docjar.com/html/api/java/lang/Throwable.java.html  See line 175 and forward
Yes, we need a separate method. Either our own print stack trace, or a separate toString method that's used by remoting. I think parts of printStackTrace are private but we can copy the implementation over.
Right. I think it would be worth fixing this. Can't believe it has not happened before. This code have been running in releases for years (since day one almost). If you have time, please fix it. But prioritize it as normal. I'll open a ticket.  On 5 April 2011 22:30, pvlugter < reply@reply.github.com>wrote:  > Yes, we need a separate method. Either our own print stack trace, or a > separate toString method that's used by remoting. I think parts of > printStackTrace are private but we can copy the implementation over. > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/bb55de44a45fe7df4b58ac8b41cd868750dd167c#commitcomment-330449 >    --  Jonas Bonr  Specialist at Large work: http:scalablesolutions.se code: http:akka.io blog: http:jonasboner.com twtr: @jboner
Shouldn't this be a SecureRandom with a seed with a tad more entropy? (not an expert in the field of java.*.Random)
Couldn't hurt. Change that. 
You can coalesce this if-check with the ones above
isEmpty is probably preferred here since that is what you're testing
Should probably include that it throws a bunch of exceptions :-)
I'm not a huge fan of log and rethrow, either swallow and log or just rethrow?
filterNot is generally preferred for removal operations:  oldAvailableNodes filterNot failureDetector.isAvailable
Nice stuff :-) Makes me remember some of the code I wrote for the JGroups clustering
Very nice and clean and functional and immutable stuff!
I'll leave that to the excellent authors of this amazing feature :)
LOL. I'll do it. Thanks. 
Could change it. Just that I already have the value size already calculated, so I could use it instead of querying it again. 
Yeah. See previous comment. Not sure what to do. 
Yeah. Or we don't throw these. The question is what to do on Status == Failure and timeout. 
Ah, didn't know about that one. Thanks. 
LOL. May it rest in peace. 
...and thanks a lot for the review. 
In my experience Jenkins (on our current slow box) will manage to violate this. We generally dont go below 300ms multiplied by the dilation factor of 5, and we still see spurious failures from time to time. Yes, thats 1.5sec :-(
Ok, is the dilation automatically applied to all durations in tests, or do I have to do something special?
Look for .dilated, which is pimped onto Duration by akka.testkit.duration2TestDuration. TestKit does it internally already for all maximum durations, e.g. in within(2 seconds).
Job pr-validator-per-commit failed for e58a661f [(results)](https:jenkins.akka.io:8498/job/pr-validator-per-commit/389/):  <br>Took 5 min.<br> to rebuild, comment "PLS REBUILD/pr-validator-per-commit@e58a661fe3e3b90185c85c132eb332537530175f" on PR 1559
Why 4 spaces indentation mixed with 2? Matters since it is part of docs IMHO. No biggie but...
This line might be too long for PDF.
Loooooong line. Makes it hard to read on GitHub ;-)
Actually, the first version(s) of Smalltalk was async. But Alan Kay was convinced to change it. Could be fun/worth mentioning that. 
Nicer if you backtick the Java stuff in the text. ``Props``. Same with other stuff belowe and above. 
it's Java, it has 4 spaces
I like the story-style of writing. 
No official release of Smalltalk was async, right?
Unit in Scala, but void in Java. 
I'll write a greasmonkey script that solves that :p
Why not complete the sample with showing what to do with the Future? 
That's a part of the Future docs.
Nice. Good job. 
Yeah, but you are mixing 2 and 4. 
I don't know.
Good. You have got the hang of the java coding. ;-)
Job pr-validator-per-commit failed for 61e47c11 Took 47 min. [(results)](https:jenkins.akka.io:8498/job/pr-validator-per-commit/690/):<br> <br>To retry exactly this commit (if the failure was spurious), comment "PLS REBUILD/pr-validator-per-commit@61e47c115da8cfbcb1c2abfda37752cb436fd7eb" on PR 1699.NOTE: new commits are rebuilt automatically as they appear. There's no need to force a rebuild if you're updating the PR.
Comment: Comparison state can traverse in the following directions (starting from Same): Concurrent <- Before <- Same -> After -> Concurrent
Yes, I'll add that to the comments so things might be more readable.
Comment: Short circuiting is available if:  - no full comparison requested, only validation for a certain ordering (ordering ne FullOrder)  - state Same has already violated (see states and allowed transitions in my comment below) and the requested ordering does not match with the current state then that state will be never reached again  - NB: checking for concurrent can only be done with FullOrder -- otherwise this route is invalid
probably rename parameter "order" to "currentOrderState" or similar
btw, is it not possible to immediately return here if order eq Concurrent? That state will never change.
what if order eq Concurrent here? Is it a problem? If it is, than you can guard against it by short-circuiting before (see previous comment)
Yes, but we never loop around when `order` becomes `Concurrent`, so it can't really be that here.
Ok, I see short-circuiting is inline (just returning Concurrent and not recursing)
Comment: Since Concurrent state is always immediately returned (absorbing state) the invariant of this method is that order is one of {Same, After, Before}
I could not find any obvious mistakes.
shouldn't this be placed in `object VectorClock`?
shouldn't this be placed in `object VectorClock`?
what does this compare? the node name hash?
I would see this commit running in a repeat job for a while, and I would like that you add an extra runtime check in that test run that verifies that old and new impl of `compareTo` always returns the same result
That is indeed a very good idea!
Sure, I'll move it (and the one below).
Yes, it's the node hash.
Or more to the point the Nodes.
ok, I think I got confused because I forgot that they were located in a TreeMap
This is the way to do it. Scope it like this. Great. 
What is the benefit of having 'ConsistentHashable' and 'consistentHashKey' rather than the more generically usable 'Hashable' and 'hashKey'? 
final class for all messages? 
I think the method should be named 'withHashMapping' or just 'withMapping' since it is already scoped as a ConsistentHashingRouter.  
Very good. Thank you. 
to make it very clear that this has nothing to do with `hashCode`
not required - enough noise here anyway
I think 'hashKey' is clear enough. 
ok `withHashMapping` what about the scala name `consistentHashRoute`? should I name that `hashRoute`? I'm not sure about `Route`. I picked that because we already used `Route` for a very similar partial function in the routers. Is `hashMapping` better there also? 
It is best practice for immutable classes in Java. I think we should be idiomatic in our samples.  http:www.javapractices.com/topic/TopicAction.do?Id=29
Ok, I will change for `ConsistentHashableEnvelope` and `ConsistentHashMapping`.  But I will keep the verbose name in `ConsistentHashable`, because if user is implementing `ConsistentHashable` they will typically have both `hashCode` and `hashKey` which will cause confusion.
I do think we should not prefix with 'consistent' everywhere. No need to constrain ourselves like that.  Where is 'consistentHashRoute'? Can't see it. What does it do? 
Change what? The names have to match up. 
https:github.com/akka/akka/commit/a4dd6b754754fbdd61e1fcfa76497664380985df#L2R72  So I change to `hashRoute` or `hashMapping`.
hashMapping sounds good I think. 
and all case classes should also be final, alright, I don't disagree with you so I change
No      trait ConsistentHashable {       def consistentHashKey: Any     }      case class ConsistentHashableEnvelope(message: Any, hashKey: Any)       extends ConsistentHashable with RouterEnvelope {       override def consistentHashKey: Any = hashKey     }  The reason, once again, is that implementation of ConsistentHashable goes into user code, and hashKey is not unique enough to avoid confusion and also possible naming conflicts. 
Ideally yes, but I see now that we have not followed this practice before either, so it is up to you. 
Yeah. Intuitive what it means. 
`hashMapping` it will be
ah, sorry for the previous nitpick: you fixed it already
Looks like a job for @inline.
Indeed, the problem in my experience is that it silently fails. Has that been fixed?
Paul seems to rely on it for the debug logging in the compiler. He'd be the best guy to ping about the limitations.
Alright, thanks for the heads up, I'll talk to my colleague ;-)
doesnt => do not
I think a picture would be very helpful to explain the subtle differences between restart and recreation, what is an incarnation, and how does death watch fit into this.
yes, let's create that and revise docs again when all related tickets have been fixed. I'll create a separate ticket for it, so it's not forgotten.
no need to copy props for each x, do it once only
isn't that the same as xs.toIndexedSeq ?
Don't specify akka.conf, use default ActorSystem("PiSystem"), which will load /application.conf from classpath
I like this withRouting in Props
fixed by abstracting out into RouterConfig.createRoutees
changed to use breakOut
Henrik changed it back to just defaults, I kind of like to demonstrate that you should name things, hence took in your suggestion.
Yes, the system name should be there.
I'll keep the name when I wrap this one up later this evening.
@viktorklang this line had been missing and caused the ActorLifeCycleSpec to fail (consistently, thank God!)
Awesomely done Roland!
You can drop the braces if you want to.
You're reversing them in-place? NICE
I promised no extra allocations, remember? ;-)
I don't really enjoy log+rethrow unless it actually serves a purpose, could you add a comment here on why it's doing log+rethrow?
Good work roland!
Hmm, primary reason was that it always did throw, the new thing being that more messages have been dropped, hence the logging. This will (probably) be caught only in the ExecutorService; interestingly enough, we have a finally{} in run(), but happily throw up (pun intended), even rescheduling ourselves. We should probably rethink that.
The reschedule is in the finally block, no?
Yes, which means that we reschedule in the face of an OOM, ThreadDeath, etc.
You should create a .rst file out of this. Not urgent but get back to that when you have time. All docs should be reST.  Also, have you linked to it from the docs?  Great job Henrik. 
Ah, it *is* a reST file. But the file ending is missing. 
No need for Tuple2
No need for Tuple2
Awesomeness. Good job. 
Ok, I'll remove the Tuple2 
Removing call-by-name will result in an allocation of the Event including a potential call to string.format, every invocation regardless if the logging level matches or not. I'd like to revert to call-by-name for both notify methods.  
For this method notify(event: Any) there is no reason to use call-by-name because there is no condition in the impl and it is evaluated directly anyway in notifyListeners. However, your point is valid for the other method notify[T <: Event : ClassManifest](event: => T) I considered that but thought that the use case for it is very limited, but sure, we can put it back. Call-by-name parameters can't be used from Java, so we need this event: Event variant also, and then we have the problem with overloaded methods will be considered to have same signature.  I have done a fix for it, please look at: https:github.com/jboner/akka/commit/e8ee6b321a3e5ffff5615d9f37f802d19f4a4b2f
You still have only one of them call by name. Can you make both call by name?  -- Jonas Bonr  http:scalablesolutions.se http:jonasboner.com http:akka.io http:letitcrash.com  On 7 Apr 2011 12:51, "patriknw" < reply@reply.github.com> wrote: > For this method notify(event: Any) there is no reason to use call-by-name because there is no condition in the impl and it is evaluated directly anyway in notifyListeners. > However, your point is valid for the other method notify[T <: Event : ClassManifest](event: => T) > I considered that but thought that the use case for it is very limited, but sure, we can put it back. Call-by-name parameters can't be used from Java, so we need this event: Event variant also, and then we have the problem with overloaded methods will be considered to have same signature. > > I have done a fix for it, please look at: https:github.com/jboner/akka/commit/e8ee6b321a3e5ffff5615d9f37f802d19f4a4b2f > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-332600
It matters in both notify methods since notifyListeners is also call by name so the event will not be evaluated if there are no listeners registered.  I'll look at the patch.  -- Jonas Bonr  http:scalablesolutions.se http:jonasboner.com http:akka.io http:letitcrash.com  On 7 Apr 2011 12:51, "patriknw" < reply@reply.github.com> wrote: > For this method notify(event: Any) there is no reason to use call-by-name because there is no condition in the impl and it is evaluated directly anyway in notifyListeners. > However, your point is valid for the other method notify[T <: Event : ClassManifest](event: => T) > I considered that but thought that the use case for it is very limited, but sure, we can put it back. Call-by-name parameters can't be used from Java, so we need this event: Event variant also, and then we have the problem with overloaded methods will be considered to have same signature. > > I have done a fix for it, please look at: https:github.com/jboner/akka/commit/e8ee6b321a3e5ffff5615d9f37f802d19f4a4b2f > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-332600
I don't think it is necessary, if you pass in an Any (or something that evaluates to Any) it will use the method def notify(event: => Any) def notify(event: Any)  Those have no conditional check of the level and will evaluate the function immediately anyway.  It is problematic to add def notify(event: => Any) because of signature clash with def notify(event: Any)  /Patrik  On Thu, Apr 7, 2011 at 1:17 PM, jboner < reply@reply.github.com>wrote:  > You still have only one of them call by name. Can you make both call by > name? > > -- > Jonas Bonr > > http:scalablesolutions.se > http:jonasboner.com > http:akka.io > http:letitcrash.com > > On 7 Apr 2011 12:51, "patriknw" < > reply@reply.github.com> > wrote: > > For this method notify(event: Any) there is no reason to use call-by-name > because there is no condition in the impl and it is evaluated directly > anyway in notifyListeners. > > However, your point is valid for the other method notify[T <: Event : > ClassManifest](event: => T) > > I considered that but thought that the use case for it is very limited, > but sure, we can put it back. Call-by-name parameters can't be used from > Java, so we need this event: Event variant also, and then we have the > problem with overloaded methods will be considered to have same signature. > > > > I have done a fix for it, please look at: > > https:github.com/jboner/akka/commit/e8ee6b321a3e5ffff5615d9f37f802d19f4a4b2f > > > > -- > > Reply to this email directly or view it on GitHub: > > > > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-332600 > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-332613 >
But a system without listeners isn't a reality. Therefore I actually changed it to notifyListeners(message: Any)  Correct me if I'm wrong, but if the parameter is not a function there is an overhead of using call-by-name, since the parameter will be wrapped in a function0 object. That will always be the case for Java.  /Patrik  On Thu, Apr 7, 2011 at 1:17 PM, jboner < reply@reply.github.com>wrote:  > It matters in both notify methods since notifyListeners is also call by > name > so the event will not be evaluated if there are no listeners registered. > > I'll look at the patch. > > -- > Jonas Bonr > > http:scalablesolutions.se > http:jonasboner.com > http:akka.io > http:letitcrash.com > > On 7 Apr 2011 12:51, "patriknw" < > reply@reply.github.com> > wrote: > > For this method notify(event: Any) there is no reason to use call-by-name > because there is no condition in the impl and it is evaluated directly > anyway in notifyListeners. > > However, your point is valid for the other method notify[T <: Event : > ClassManifest](event: => T) > > I considered that but thought that the use case for it is very limited, > but sure, we can put it back. Call-by-name parameters can't be used from > Java, so we need this event: Event variant also, and then we have the > problem with overloaded methods will be considered to have same signature. > > > > I have done a fix for it, please look at: > > https:github.com/jboner/akka/commit/e8ee6b321a3e5ffff5615d9f37f802d19f4a4b2f > > > > -- > > Reply to this email directly or view it on GitHub: > > > > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-332600 > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-332614 >
I'm fine with the EventHandler change but you can't change the notifyListeners method in ListenerManagement. It is used for much more than EventHandler. I *must* be call-by-name. Please change that back.  On 7 April 2011 13:30, patriknw < reply@reply.github.com>wrote:  > But a system without listeners isn't a reality. > Therefore I actually changed it to > notifyListeners(message: Any) > > Correct me if I'm wrong, but if the parameter is not a function there is an > overhead of using call-by-name, since the parameter will be wrapped in a > function0 object. That will always be the case for Java.
Ok, I will change it back, no problem. Thanks for review.  /Patrik  7 apr 2011 kl. 21:26 skrev jboner<reply@reply.github.com>:  > I'm fine with the EventHandler change but you can't change the > notifyListeners method in ListenerManagement. > It is used for much more than EventHandler. I *must* be call-by-name. > Please change that back. >  > On 7 April 2011 13:30, patriknw < > reply@reply.github.com>wrote: >  >> But a system without listeners isn't a reality. >> Therefore I actually changed it to >> notifyListeners(message: Any) >>  >> Correct me if I'm wrong, but if the parameter is not a function there is an >> overhead of using call-by-name, since the parameter will be wrapped in a >> function0 object. That will always be the case for Java. >  > --  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-333266
Great. Thanks a lot.  -- Jonas Bonr  http:scalablesolutions.se http:jonasboner.com http:akka.io http:letitcrash.com  On 7 Apr 2011 21:36, "patriknw" < reply@reply.github.com> wrote: > Ok, I will change it back, no problem. Thanks for review. > > /Patrik > > 7 apr 2011 kl. 21:26 skrev jboner<reply@reply.github.com>: > >> I'm fine with the EventHandler change but you can't change the >> notifyListeners method in ListenerManagement. >> It is used for much more than EventHandler. I *must* be call-by-name. >> Please change that back. >> >> On 7 April 2011 13:30, patriknw < >> reply@reply.github.com>wrote: >> >>> But a system without listeners isn't a reality. >>> Therefore I actually changed it to >>> notifyListeners(message: Any) >>> >>> Correct me if I'm wrong, but if the parameter is not a function there is an >>> overhead of using call-by-name, since the parameter will be wrapped in a >>> function0 object. That will always be the case for Java. >> >> -- >> Reply to this email directly or view it on GitHub: >> https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-333266 > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/35812951e2e2bb26a1d85b6830cf5a0ffae86946#commitcomment-333279
Wicked dawg. Could you sprinkle some ScalaDoc on that?
ScalaDoc sauce? :-)
Great work Henrik!
Sauce coming straight up!  On Wed, Nov 23, 2011 at 11:14, viktorklang < reply@reply.github.com > wrote:  > ScalaDoc sauce? :-) > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/7ca5a4161bb279d94dc1de1750cdfb966ca39151#commitcomment-740752 >
You can make this more readable by using the implicit conversion available in import akka.util.duration._ Then you can write 50 milliseconds instead of Duration(50, TimeUnit.MILLISECONDS)
picky, please don't use () when it's not side effecting, i.e. duration.toMillis
wrong comment, it is up to the user to decide unit, it doesn't have to be milliseconds describe what a tick is instead
Timeout is already a wrapped Duration, so you should do t.duration instead of creating new Duration
Nice. I love durations!
Is there anyway we can read 100ms or 30ns into a duration directly? Right now I use this: val SchedulerTickDuration = Duration(getMilliseconds("akka.scheduler.tickDuration"), TimeUnit.MILLISECONDS) That's the reason why I added the comment above.
This is a Java class so I guess it's okay?
That is correct way of doing it. If user specifies tickDuration = 1s you will end up with  SchedulerTickDuration = 1000 milliseconds
ah :-) sorry
Cool. Thx. I'll remove the comment.
Did you add a bunch of binary files (file-based/mailbox_user__X)? 
Ah, you removed them? 
Yes, for some douchy reason they are not excluded in the .gitignore
why break user code with this visibility change?
Having it protected doesn't provide any protection anyway, that's what the ActorRef is for. Also, it's needed to get the receive-block out of the Actor instand and into the ActorCell, the alternative would be to do context.become(receive) in the Actor, but that would complect the ActorCel code, so if we can get away with this it'd be preferrable.
wouldn't protected [akka.actor] solve the access issue without forcing user code to change?
protected[akka.actor] would be public to Java, and that would be a change from protected to public
this nukes any user-created stack from the constructor: better use a Deque which initially holds a place-holder and switch that out under the other elements after `newActor()`
Awesome. Great set of instructions.
Thanks. I needed to spare the next poor schmuck from deciphering a late night chat transcript between you and Viktor ;)
You have our eternal gratitude!
Awesomely done Bjrn!
Fixed bug in FaultHandling and impoved the SupervisorHierarchySpec stress tests according to Roland's suggestions. All tests green for me.
There is one thing I personally dislike: the elevation of Failed into an Envelope and storing it in currentMessage. Maybe there is a cleaner way?
Well, there is a ticket for making Failed into a SystemMessage :-)
This _is_ that ticket :) This commit is my fix of Roland's branch. But there is a hack where we behave like a Failed was received like a user message.
The fix in `SupervisorHierarchySpec` could be related to the elusive race in https:www.assembla.com/spaces/akka/tickets/3037 
Would it make sense to encode the states as bits so you could do:  status & Suspended status & Scheduled  etc...
Nice!  I'm a bit worried about the performance though, since this will really be in the hot path. With all those matches I suspect the JIT will have a hard time inlining this method, thoughts?
Yes, this is important
Overall: Great work!  I'm a bit worried about performance though, having everything encoded in one field means that the contention on the cache-line that holds the status will be greatly increased. Also, the inline:ability of "become" is questionable. We should look into this.  Thanks for the hard work Roland!
Its not really in the hot path, only register/suspend/resume, but I can also split it into three separate methods to get rid of the outer match, and the inner match would go away by more cleverly encoding the bits as you suggest above. Will investigate 
okay, so Ill remove the TODO and vagueness from the comment
We should measure performance instead of guessing ;-) But FWIW, my guess is that the Scheduled state changes by far outnumber suspend/resume which means that this solution should have similar contention compared to the old dispatcherLock itself.
Can you add these to multi-jvm tests also?
Had to change our custom reporter slightly, but now it's done.
thanks, very useful
This will do a lookup in the system-internal map for every invocation, is this intentional?
changed to val
It was simpler to get the existing tests to run that use mockito, but I fixed it now, not lazy anymore. Not a big fan of mocking frameworks btw.
Personally I never use them.
Be careful though since there can be initialization ordering issues depending on mixin ordering.
You can replace all the latch, actor etc by just sening the scheduled actor to the testActor and then do expectMsg(timeout)(msg)
We should consider having the TimerTasks just execute the runnables inside the main dispatcher, in that way a blocking TimerTask won't mess up the Scheduler
I think "if (...) ... else ..." is the commonly used for on boolean patternmatch ;)
timeout is not a val, and as such it is not safely published. Make it a val.
On my system it's py2.7. Can we use this?      $(LOCALPACKAGES)/akkastyles-*.egg 
or change it to $(LOCALPACKAGES) as I am not sure how the asterisk would be interpreted in that location (especially if nothing exists).
Sure. Just `$(LOCALPACKAGES)` sounds good.
So how can I use it now? 
read akka-dev ;-)  (or try `startSystem()` in the REPL)
Got it. Works like a charm. 
in java it should be Class<?> or just write Class everywhere
above comment was meant for the java file of course :-)
in java I think it is system.eventStream()
yes, good catch, will fix
yeah, you could add new stuff like 1 second sleep easily now.
    object later     implicit object laterConvert extends Classifier[later.type] {       type R = Later       class Later(d: Duration) {         def apply(f: => Unit) = ...       }       def convert(d: Duration) = new Later(d)     }      10 seconds later {       checkIt     }
I like it. I like it a lot.
I don't think now should be public. It's not a real Duration.
It would make more sense to make it a Deadline instead and add + and - on that. I instinctively did not do that because of the extra allocations, but we are talking about a testing DSL here with views and type classes, so I guess I need to readjust into non-paranoid mode when writing stuff like this ;-)
While this is clearly badass I think we can do without it.
You dont say   Bonus points for explaining why this can never ever be printed! (Hint: youll have to read the mq sources, which I have not done yet)
It never goes into poll timeout?
I don't understand the semantics of this method.
Should we really create an empty frame?
receive one message, which may consist of several parts; mq docs say that delivery of multi-part message is all-or-nothing, which is why the ContinuePoll never happens (at least Ive never seen it happen); shall I remove it?
abused as poor mans option
thats what the previous code did, and I think empty frames can actually be transferred, so, yes, we should.
Document with ScalaDoc and Java API
Why not use your one-stop-shop here?
Quite the busy bee, Mr Rocketscientist
you mean on the secondary constructor in addition to on the class?!?
look at line 227 ;-)
Ah, I thought it was you don't need to split Id and Extension into two...
this will make tests hang instead of fail, because now it waits for an unbounded period of time. I think we cannot keep the method with such a change in semantics.
same here, this will trip up users.
I'm not so sure about this, the timeout should logically stop the AskActorRef, but I don't wnat it to complete the future, I'll take a stab at that.
"as" will be removed
"get" will also be removed, I've started going from stepping-stone to stepping-stone to have stable points inside the refactoring.
ah, yes, of course.
ugh, that's strange behavior by the compiler
why add val if not needed?
nope, its not strange (it is documented and Martin doesnt like adding syntax for special things), but Viktor, have you really verified that this works for reference types at all? I though only AnyVal could be constant  And even if String could work, String.intern is NOT a compile-time constant anyway.
for a constant value it needs to be:       final val MessageExchangeId = "MessageExchangeId"
Can you still write /a/b/c/actor ?  If it should be default then the router should be "direct" not "round-robin". 
Isn't 0.0.0.0 only default on Mac OSX? 
Does the new parser not support 'on/off'. Can we add that to it? 
You should be able to use ConfigFactory.empty() here instead of parseMap(Map())
chaining withFallback() like this unfortunately does not do the right thing right now; I think I have to fix the API or implementation somehow, since this is the obvious way to use it. I wrote some docs explaining the problem yesterday, see withFallback doc comment in ConfigMergeable. Basically if you have in three configs: foo={}, foo=null, foo={} and you do first.withFallback(second).withFallback(third) then the result of first.withFallback(second) is foo={}, and then the foo in third gets merged in to the foo from first because when merging third, the foo=null has already gone away. but really because of the foo=null in second, anything in third should have been ignored. It is a minor corner case but you'd use foo=null if you wanted to "clear" an object found in a fallback. Anyway I guess you don't need to change this, I just need to make it work instead of documenting a crazy workaround.  I'll fix it.
It would be nice if the parser in the Config lib were useful for durations here (see Config.getMilliseconds) but unfortunately it doesn't have the adjustable default time unit feature. To support that I guess we'd need some kind of Config.getDuration that would return a number along with either a TimeUnit or null, and you'd have to write a method that used that and plugged in the default time unit if none were specified in the file, or something. But it would be easy to accidentally use getMilliseconds() which would not use the default time unit. I don't know. I guess another approach would be to have Config.withDefaultTimeUnit(); it would add an extra word of storage to every config object but may not be a big deal. Let me know if you would use any feature along these lines or if I should leave it as-is.
you could just use Some() here rather than Option() I think, not that it matters a whole lot
This file (and the others in a properties-like format) should have worked without reformatting as far as I can see - reformatting it is fine, but want to be sure you didn't reformat because the parser was upset about the original. 
Yes, in the application conf you will write /app/service-ping Ok, I'll change to direct.
0.0.0.0 was the previous default value in the code, i.e. what was used. I can change default to 127.0.0.1
It didn't. Now it does. I'll change back to on/off.
Thanks, I read the docs and was puzzled, because it is really this way we built up the config. We even add the reference afterwards, i.e. it is impossible to use the withFallbacks. If you fix the issue I think you should remove withFallbacks. More realistic is foo {a =1} withFallback foo {b = 2} with Fallback foo {a = 0, b = 0, c = 0} which should become foo {a = 1, b = 2, c = 0} Clearing (nulling) isn't something I would have expect working at all.
Personally I think it is totally wrong to have the default time unit at all. I think all durations should be specified with value and unit explicitly. Everything else will only cause confusion. I'll talk to the others about it.
Well, if System.getProperty("akka.config") is null then I immediately get a None and dont have to try parsing file null.
First I thought this format wasn't supported, but it was another bug that tricked me. After reformatting them I didn't revert that because I think this format is more nice.
I agree with Patrik here, the default time unit is not that useful.
Thanks for supporting that. I created ticket http:www.assembla.com/spaces/akka/tickets/1363-remove-default-time-unit-in-config  On Fri, Nov 18, 2011 at 4:59 PM, viktorklang < reply@reply.github.com > wrote:  > I agree with Patrik here, the default time unit is not that useful. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/4b8f11ea92ab3fb4809e2ca84b11ac93771477bb#commitcomment-730808 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
(re-throwing the exception)
good point, will add
Yes! resultOrException should be removed: why this extra allocation which is thrown away immediately anyway? Putting it in one place only is the right thing to do so it can be optimized away.
I'm not so sure about the name "sync" though, but I wanted something that would imply blocking/synchronization. The alternative was "onAndYield", but it isn't sexy...
I agree the name sync is unclear. I think Block communicates that everything there is blocking. Suggestion: Block.get
"get" sounds cheap to me. I'd rather go with something like "load"
Why not eval?
I thought we agreed on that 'dsl' (and 'DSL') was a bad name
We discussed that wrt. the name of the Actor subclass which adds the internal DSL (on which Philipp is working), IIRC, and there I still agree. But all of those pieces will form something which in effect is a DSL for using/writing actors, so I started calling the object which you need to import `ActorDSL`. The settings now are at `akka.actor.dsl` which also sounds fitting, or do you disagree? How else should it be named?
We came up with a better name, didn't we? I don't remember what it was. But dsl is so vague, does not really say anything what is it about. 
youre thinking about the `trait Act extends Actor`, I think; this DSL thing here is one level up, so to speak
Right. Ok. Makes sense. But I'd still prefer a better name than 'dsl'. But I don't have one now so let's leave it for now. 
 but the name is immediately available for reuse.
the actual stop is always asynchronous, also with ActorSystem, the waiting is only for confirmation that the name is free again. Actually, I think this can be optimized, but better to document as it is.
hmm, then it is an implementation detail that users normally don't have to care about, since we do what is expected? I don't think we need to document that detailed in rst docs, but it can sure be included in scala docs. Is this correct:  Actors are stopped by invoking the :meth:`stop` method of a ``ActorRefFactory``, i.e. ``ActorContext`` or ``ActorSystem``. Typically the context is used for stopping child actors and the system for stopping top level actors. The actual termination of the actor is performed asynchronously, i.e. :meth:`stop` may return before the actor is stopped.  On Wed, Dec 14, 2011 at 8:30 PM, Roland Kuhn < reply@reply.github.com > wrote:  > the actual stop is always asynchronous, also with ActorSystem, the waiting > is only for confirmation that the name is free again. Actually, I think > this can be optimized, but better to document as it is. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/7b2349c0d9292a90f3ec2aa1605426b5b9c42bec#commitcomment-794904 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
The important point is that immediate reuse of the childs name is only possible if stopping your own child or a top-level actor via `system`. Otherwise it is correct.
Okey, the actor.rst (and untyped-actor.rst) needs to be sprayed with information about the actor name/path. That is much missing now. We have ticket #1448 that should cover that. I suggest that you do that part.  On Wed, Dec 14, 2011 at 8:39 PM, Roland Kuhn < reply@reply.github.com > wrote:  > The important point is that immediate reuse of the childs name is only > possible if stopping your own child or a top-level actor via `system`. > Otherwise it is correct. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/7b2349c0d9292a90f3ec2aa1605426b5b9c42bec#commitcomment-794937 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
these can probably go now
Why this change? I have a big aversion against "new"
making them defs would allow these to be computed on-demand, and this trait does not really need them to be stable, right?
I can change it to use def instead of val.
That change is there because I removed all parameters from the VoteCountRouter and therefore could not keep it a case class. Do you think I should do some other way? This will be an example in the documentation so it is important that we all agree on what best practice is.
Create a companion object with an apply method?
why not just make it an object?  also: `case class A()` does work (but why would one?)
K, I'll make it an object then.
I think this is going to be a source of many confusion since you need to remember to add new routers here, and users cannot hook into this at all.
The alternative was to make routers in akka-actor remote-aware, which is not nice at all. Unless you have a better proposal 
In what way do they (Routers) need to know anything about remoting?
Well, the remote routers need to handle the nodes setting, which the local ones do not. Hence there must be a translation between local and remote (unless the RemoteDeployer would not use local parsing at all, which is obviously even worse). If that translation is part of the router interface (so that the RemoteDeployer could do it generically) then local routers need to know about their remote counterparts, which is not possible due to cyclic dependencies between the packages.
What about having a RemoteRouter-wrapper that handles the nodes-setting and wraps the original "local" router? Or a "root-path-provider" that gets put into the Router?
> What about having a RemoteRouter-wrapper that handles the nodes-setting and wraps the original "local" router?  I need to think about this, but Im only taking a little time off now: were on the move. So itll have to wait until next week. Care to open a ticket?  > Or a "root-path-provider" that gets put into the Router?  Its not that easy. And while were at it we should solve it in a way which fits seamlessly into clustering.
I'm technically on vacation. So no tickets :-)
Lol. Me too.
when Im in that mood, strange things may happen: I created the ticket nonetheless ;-)
Could you revert this formatting change? 
Could you revert this formatting change? 
Could you revert this formatting change? 
Could you revert this formatting change? 
Could you revert this formatting change? 
Could you revert this formatting change? 
Could you revert this formatting change? 
ah, so it bypassed the log level check, but isn't that done for publish also (but for the logger instead of stdout logger)?
Yes it bypasses the check, but the loggers are only subscribed to the right type of log events, so they will not print the event if they shouldn't.
I see, great
Much cleaner, thanks!
heh, not as clean as I'd like it to be, I want to avoid entering the guard, I should really open a ticket on redesigning the whole Dispatcher lifecycle management.
Doesn't Switch.switchOn already protect against a race?
On Thu, Apr 28, 2011 at 4:44 PM, derekjw < reply@reply.github.com>wrote:  > Doesn't Switch.switchOn already protect against a race? >  Problem is that we don't have the guard taken in dispatchFuture, so for example:  if (uuids.isEmpty() && futures.get == 0) {             active switchOff { Could get to here if we don't ensure we have the guard prior to switchOn               shutdown  shut down in the dispatcher's references is zero             }           }   > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/e4a5fe9abd242639b207a1d976410491ce0bba74#commitcomment-360939 >    --  Viktor Klang, Director of Research and Development Scalable Solutions <http:www.scalablesolutions.se>  Code:   github.com/viktorklang Follow: twitter.com/viktorklang Read:   klangism.tumblr.com
Why do we want to make the user type more by removing all the methods from Actor?  So now he has to pollute his code with 'context.' everywhere. I don't like that, need some convincing here. 
The user shouldn't pollute everything with context everywhere. "self" is on Actor, and if you don't use any of the context-things there's no additional typing, and if you want to have the full scope, you simply do: import context._ In this way we do not pollute the namespace on Actor by default, we can add new methods to Context without accidentally breaking peoples Actors etc.
I am just worried that there is too much things going on. The user needs to learn when to use either of: 1. Actor API 2. self 3. context It is not simple.  On Tue, Dec 6, 2011 at 11:46 AM, viktorklang <reply@reply.github.com> wrote: > The user shouldn't pollute everything with context everywhere. "self" is on Actor, and if you don't use any of the context-things there's no additional typing, and if you want to have the full scope, you simply do: import context._ > In this way we do not pollute the namespace on Actor by default, we can add new methods to Context without accidentally breaking peoples Actors etc. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/3204269f6a0fb2aec94595f504e2c423d9344b5c#commitcomment-771983    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
I can understand your worries, the current suggestion was the least of all evils:  Actor == The behavior self    == The identity context == The context
I think its simple enough, viewed like this:  - for creating actors you need to know where to get them from (that is what you mean by Actor API, right?) - for the typical things we have `self` and `sender` directly available, as that covers probably 90% of the use cases - if you want more special actor saucewhich is become/unbecome and watch/unwatch, system and childrenthen its always close by, just `import context._` (or better: choose specifically)
I agree to this as the most important reason. It gives us freedom to evolve the api without breaking user code.  On Tue, Dec 6, 2011 at 11:46 AM, viktorklang < reply@reply.github.com > wrote:  > The user shouldn't pollute everything with context everywhere. "self" is > on Actor, and if you don't use any of the context-things there's no > additional typing, and if you want to have the full scope, you simply do: > import context._ > In this way we do not pollute the namespace on Actor by default, we can > add new methods to Context without accidentally breaking peoples Actors etc. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/3204269f6a0fb2aec94595f504e2c423d9344b5c#commitcomment-771983 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Might be the best overall choice. But it needs to be *very well* documented. In a way that makes sense and is easily remembered. 
Well-documented is Roland's middle name
https:www.assembla.com/spaces/akka/tickets/1438-doc--document-the-differences-between--actor-api--self-and-context  On Tue, Dec 6, 2011 at 1:36 PM, viktorklang <reply@reply.github.com> wrote: > Well-documented is Roland's middle name > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/3204269f6a0fb2aec94595f504e2c423d9344b5c#commitcomment-772192    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
yup, my thought exactly
A bit OT, but: Would it be possible to make actor suspend / resume accessible in the public API (ActorRef)? I think that would be very useful in some scenarios (e.g. making an actor wait for something without blocking a thread). Or is this already possible in some way?
Definitely not from the outside, and on the inside it doesn't make sense since it can't resume itself.  What's the use-case?  Cheers, V On Jun 21, 2012 11:30 AM, "oschulz" < reply@reply.github.com> wrote:  > A bit OT, but: Would it be possible to make actor suspend / resume > accessible in the public API (ActorRef)? I think that would be very useful > in some scenarios (e.g. making an actor wait for something without blocking > a thread). Or is this already possible in some way? > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/akka/akka/commit/2e459f5f1d6b88472118b8c479f3eded3740926a#commitcomment-1486334 >
Resume would have to be possible from outside, of course. The use case I was thinking of was: An actor gets a message, instructing it to do some long-running operation. The actor spaws a child (or children) to do all that stuff, but until it's finished (for example some real-world interaction), the primary actor should not react to new commands. Of course this could also be solved with message-queues. It's just that when I saw this pull-request, I got the idea that an actor suspending itself and beeing resumed by the child(ren) doing the actual work would be an easy and elegant solution for such cases (blocking an actor until something is done).
On Thu, Jun 21, 2012 at 2:14 PM, oschulz < reply@reply.github.com > wrote:  > Resume would have to be possible from outside, of course. The use case I > was thinking of was: > An actor gets a message, instructing it to do some long-running operation. > The actor spaws a child (or children) to do all that stuff, but until it's > finished (for example some real-world interaction), the primary actor > should not react to new commands.   What if the child dies? Or the message never reaches the child? What if someone is sending messages at a high rate so blocking for 10 seconds will actually put millions of messages in the mailbox, potentially putting the system in an OOME situation?   > Of course this could also be solved with message-queues. It's just that > when I saw this pull-request, I got the idea that an actor suspending > itself and beeing resumed by the child(ren) doing the actual work would be > an easy and elegant solution for such cases (blocking an actor until > something is done). >  In this case just use "become" + Stash and you can accomplish the same thing, but without the dangers of suspend/resume.  Cheers,    > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/akka/akka/commit/2e459f5f1d6b88472118b8c479f3eded3740926a#commitcomment-1486891 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
> What if the child dies? The child doing the actual work could be watched by an intermediate supervisor, which would handle this case,  i guess.  > In this case just use "become" + Stash and you can accomplish the same Sure - I'm not saying it's necessary, I was just wondering if it might be an easy alternative.   > What if someone is sending messages at a high rate? That also applies to "become" + Stash, I guess? Can one defend againt that using a bounded mailbox? 
On Thu, Jun 21, 2012 at 5:00 PM, oschulz < reply@reply.github.com > wrote:  > > What if the child dies? > The child doing the actual work could be watched by an intermediate > supervisor, which would handle this case,  i guess. >  Suspension of a parent implies suspension of children, as suspend/resume is used exclusively for supervision-management.   > > > In this case just use "become" + Stash and you can accomplish the same > Sure - I'm not saying it's necessary, I was just wondering if it might be > an easy alternative. >  I'm fairly sure that there's no easy alternative, however, it is a topic worth exploring more. But handing out suspend/resume as end-user API isn't going to cut it.   > > > What if someone is sending messages at a high rate? > That also applies to "become" + Stash, I guess? Can one defend againt that > using a bounded mailbox? >  Yup, and it will lead to messages getting dropped, which may or may not be what you want.   > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/akka/akka/commit/2e459f5f1d6b88472118b8c479f3eded3740926a#commitcomment-1487693 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
> Suspension of a parent implies suspension of children  Uhm, yes, this was originally about hierarchial suspension, wasn't it? There's the flaw in my plan ... just forget everything I said - sorry! :-) 
Does it really matter if it's 1 or N errors?
yes, for me it does: I want to report as much and as early as possible, otherwise users are annoyed.
I meant:      sender ! (narrowCheck(ru)(channelListTypeTag.tpe, tt.tpe) match {       case Nil  CheckTypeACK	       case list  CheckTypeNAK(list mkString ("errors:\n  - ", "  - ", ""))     }) 
It should not say transparent remoting, but location transparency. 
We need to say something more about the serialization I think. 
Good it is a start. Surprisingly little. But perhaps not needed to have more. 
There is a ticket to document Serialization.
That's good then. I also see that the serialization stuff is removed from the config (I mean Debasish's pluggable stuff). Why?  On Thu, Dec 15, 2011 at 8:50 PM, viktorklang <reply@reply.github.com> wrote: > There is a ticket to document Serialization. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/commit/94017d8b7a8982a27cc977d3e61199b182902ccc#commitcomment-798525    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
Because it doesn't exist anymore.  Have a look at the Serialization Extension for the new serialization implementation.
Where is it? 
Roland and I discussed it and came to the conclusion that this would be enough for now. We probably need to add some example code for the "real" release though.
What do you mean?  akka.serialization.Serialization.Settings does use config:   akka.actor.serializers   akka.actor.serialization-bindings   On Thu, Dec 15, 2011 at 9:00 PM, viktorklang < reply@reply.github.com > wrote:  > Because it doesn't exist anymore. > > Have a look at the Serialization Extension for the new serialization > implementation. > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/94017d8b7a8982a27cc977d3e61199b182902ccc#commitcomment-798576 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
It's not removed. Is it? https:github.com/jboner/akka/blob/master/akka-actor/src/main/resources/reference.conf#L182  On Thu, Dec 15, 2011 at 9:13 PM, Patrik Nordwall <patrik.nordwall@gmail.com>wrote:  > What do you mean? > > akka.serialization.Serialization.Settings does use config: >   akka.actor.serializers >   akka.actor.serialization-bindings > > > On Thu, Dec 15, 2011 at 9:00 PM, viktorklang < > reply@reply.github.com > > wrote: > >> Because it doesn't exist anymore. >> >> Have a look at the Serialization Extension for the new serialization >> implementation. >> >> --- >> Reply to this email directly or view it on GitHub: >> >> https:github.com/jboner/akka/commit/94017d8b7a8982a27cc977d3e61199b182902ccc#commitcomment-798576 >> > > > > -- > > Patrik Nordwall > Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts > Twitter: @patriknw > > >   --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Sorry guys. I expected it to be in akka-remote module and reference.conf since that is where it was before.
Henrik, can you link to this config snippet then.  Or better, create a serialization document page and write that it will be documented soon, but point to the config snippet how to configure serialization? 
ok, if we are anyway going to break this, why not make a real scala setter also, to support      receiveTimeout = 10.seconds
Make it clear that this is Java. I think we have used  v2.0 Java API::
no, this does not break (appearances may be deceiving, this did not change from 2.0); having Scala setters would come at the price of ugly things in the Java auto-complete
awesome, dude! So, this is how we roll  got perf numbers?
Very cool. Great job.
I'll run the bench (before and after) when I some low priority spare slot appears in my schedule.  On Mon, Nov 21, 2011 at 11:11 PM, viktorklang < reply@reply.github.com > wrote:  > Thx buddy > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/4fdf69861119f3f9883c2cbd8b4b1e3028a0b86a#commitcomment-736168 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
good that you work on this! I just remembered someones wish about having source line information included in log messages, which we currently dont support at all (ISTR a ticket that the source information always shows SLF4J.scala). Might be interesting to fix in a similar way, if possible (didnt look too closely into this; it would be desirable to achieve it without taking full stack traces because that is so damn slow; or configure it only above a certain log level, so you get WARNING and ERROR with line info but DEBUG/INFO without)
what, they finally did that? ;-)
That is ticket: http:www.assembla.com/spaces/akka/tickets/857 I think we have more core 2.0 stuff to do so I will not solve that now.  On Tue, Nov 29, 2011 at 8:22 AM, Roland Kuhn < reply@reply.github.com > wrote:  > good that you work on this! I just remembered someones wish about having > source line information included in log messages, which we currently dont > support at all (ISTR a ticket that the source information always shows > SLF4J.scala). Might be interesting to fix in a similar way, if possible > (didnt look too closely into this; it would be desirable to achieve it > without taking full stack traces because that is so damn slow; or configure > it only above a certain log level, so you get WARNING and ERROR with line > info but DEBUG/INFO without) > > --- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/19a78c0e58bf1ce4c2315da6407d39b4bc770d51#commitcomment-753050 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Is this Scala Iterable or Java Iterable, what is the Java API for this?
Is this Scala Iterable or Java Iterable, what is the Java API for this?
Is this Scala Iterable or Java Iterable, what is the Java API for this?
I think I prefer 1 version for Java Iterable and one for Scala Iterable, wdyt?
What is think is that the Iterable should be completely removed from the interface of the Router. It makes the composition to a nightmare.   So I would pick that up as soon as task https:www.assembla.com/spaces/akka/tickets/1075-routedactorref-clusteractorref-should-use-composition is done.  Another thing that should be looked at, is to add some kind of mechanism to let the router know that the connections have changed. This makes it possible they use their own internal (so optimized for their needs) collection structure for storing a 'snapshot' of the existing connections, and as soon as the connections change, they can update their structure. This will make a much more efficient routing possible.
https:github.com/jboner/akka/commit/43031cb94be79aa6c7a4735bec68d6adfbca6459#commitcomment-514899  On Thu, Aug 4, 2011 at 9:59 PM, viktorklang < reply@reply.github.com>wrote:  > I think I prefer 1 version for Java Iterable and one for Scala Iterable, > wdyt? > > -- > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/commit/43031cb94be79aa6c7a4735bec68d6adfbca6459#commitcomment-513808 >
use the systems' ThreadFactory, and make the tick interval configurable.
What's the downside of having tighter settings for the default scheduler and just use the system scheduler?
Why do these need to be manually cancelled?
Do we need this check?
Isn't the min delay the tick interval?
nanoTime? Perhaps use Deadline?
Why not just use a latch?
If there is a reason to cancel them manually I want to know why. Who wants to maintain a codebase full of "why not code"? I don't want to consider "why not"-code when I refactor, do you?
LOL. Why not be sure that all resources you have created when starting up are released before you can call it a proper shut down? You mean that they will be shut down automatically? By who? 
Yeah, I totally agree with that resource management being important, but now when Patrik added a dedicated Scheduler, he shuts that down on ActorSystem shutdown, which should take care of everything scheduled on it.
Ah, didn't know that. *That* sounds like a better solution for sure. 
The scheduler *runs* scheduled tasks on shutdown (close), which is something I don't want in this case. Therefore I cancel them. I'll add a comment about it here.
shutdown can be invoked onTermination or by user. In the onTermination case the actors and guardian are already terminated and this prevents some error logging
My thought was that the default scheduler is configurable by user (for other purposes) and we need better resolution here than might be needed for other things. I'll do something in between; if default scheduler has good enough resolution I use that, otherwise I create a dedicated one. Configureable by the tickDuration.
Alright, that explains it, things like that have to be commented so someone doesn't "optimize" it ;-)
Guarding the stopping of something that might already be stopped shouldn't be an issue. Waht kind of logging does this approach evade? DeadLetters?
Yeah, I just want to make sure that we don't create new Threads if we don't have to ;-) Ideally I'd like to remove the ThreadFactory and only rely on a passed in ExecutionContext, a man can dream, can't he?
no, it can be scheduled to the first bucket, but I can use the minTick anyway if you think that makes sense
yes, definitely nanoTime (thought about that 1 minute after pushed ;-)
well, it doesn't make a difference, but I can rewrite to latch and then measure total time and assert the rate, same thing, but different
sure, I agree
    [ERROR] [06/12/2012 12:04:08.911] [ClusterSpec-akka.actor.default-dispatcher-6] [ActorSystem(ClusterSpec)] Failed to run termination callback,          due to [sending to terminated ref breaks promises]     akka.pattern.AskTimeoutException: sending to terminated ref breaks promises 	at akka.pattern.AskSupport$class.ask(AskSupport.scala:74) 	at akka.pattern.package$.ask(package.scala:43) 	at akka.pattern.AskSupport$AskableActorRef.$qmark(AskSupport.scala:151) 	at akka.actor.ActorSystemImpl.stop(ActorSystem.scala:505) 	at akka.cluster.Cluster.shutdown(Cluster.scala:536) 	at akka.cluster.Cluster$$anonfun$5.apply$mcV$sp(Cluster.scala:465) 	at akka.cluster.Cluster$$anonfun$5.apply(Cluster.scala:465) 	at akka.cluster.Cluster$$anonfun$5.apply(Cluster.scala:465) 	at akka.actor.ActorSystemImpl$$anon$4.run(ActorSystem.scala:583) 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.runNext$1(ActorSystem.scala:707) 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.apply$mcV$sp(ActorSystem.scala:710) 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.apply(ActorSystem.scala:703) 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.apply(ActorSystem.scala:703) 	at akka.util.ReentrantGuard.withGuard(LockUtil.scala:15) 	at akka.actor.ActorSystemImpl$TerminationCallbacks.run(ActorSystem.scala:703) 	at akka.actor.ActorSystemImpl$$anonfun$terminationCallbacks$1.apply(ActorSystem.scala:580) 	at akka.actor.ActorSystemImpl$$anonfun$terminationCallbacks$1.apply(ActorSystem.scala:580) 	at akka.dispatch.DefaultPromise.akka$dispatch$DefaultPromise$$notifyCompleted(Future.scala:920) 	at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:897) 	at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:897) 	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) 	at scala.collection.immutable.List.foreach(List.scala:76) 	at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1.apply$mcV$sp(Future.scala:897) 	at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply$mcV$sp(Future.scala:387) 	at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:379) 	at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:379) 	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) 	at akka.dispatch.Future$$anon$4.run(Future.scala:379) 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:117) 	at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381) 	at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) 	at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974) 	at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1478) 	at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Ouch, that's an ugly one.  def stop(actor: ActorRef): Unit = {     implicit val timeout = settings.CreationTimeout     val path = actor.path     val guard = guardian.path     val sys = systemGuardian.path     path.parent match {       case `guard`  Await.result(guardian ? StopChild(actor), timeout.duration)       case `sys`    Await.result(systemGuardian ? StopChild(actor), timeout.duration)       case _        actor.asInstanceOf[InternalActorRef].stop()     }   }  I think we should fix the above.
ok, but then you know the reason for the check
Yup, I'll open a ticket on the other thing. It's unintuitive that stop might throw an exception to the caller.
Add a comment at the check with a FIXME link to http:www.assembla.com/spaces/akka/tickets/2221-actorsystem-stop-shouldn-t-throw-exceptions
This is rare. Normally it is detected as unreachable and moved to the unreachable set (as UP) then moved to DOWN (still in the set).  You can DOWN a node that is UP and healthy but normally you never do that I suppose. Make this clear in the scala doc.
I.e. a conflict. 
Good that you added this. Let's add to it along the way as things come up. 
yes, true, my intention was to document the normal cases, not this rare case
Just a question, where did you find the standards/guideline for adding examples to akka?
Zonk...Is there another standard I missed?:)
No that's not what I meant, haven't added examples before, it looked structured in a very specific way, so I was wondering if that was described somewhere, how it is done (as part of the main tree, with the numbers in the names etc) It would be nice to do examples the same everywhere, so maybe there is no standard for that?
Ah, you are using the Memory Leak Pattern ;-)
Yes, I just found out right now. Ill take out that daemonic=on setting.
Why {{ ?
oops: context fail! (I was thinking you were still commenting on the initialCommands stuff)
Thats Javas awesome syntax for when you write an anonymous constructor.
Great. The Java testing API looks quite alright now. 
Worth sending out an email to akka-user about this once it's up in the docs. 
I believe the correct parlance is "instance initializer"
Really? I've never seen it. Ok. Thanks. 
Thanks! Just think how much of the boilerplate can go with Java 8 
As 'Connected' is really 'Connecting' (renamed) this may fail as the client may not yet be connected to the server. In practice this means that the test may fail because the message is never received as the client has not yet connected. I don't know what's a good approach to take this into account in the test case. Ideas?
I think it would make sense to think about how failure is handled. What kind of supervision strategy do you envision?
Oh, what I mean is that the message is lost because of a race.
What is the test testing?
That the subscriber receives a message that the publisher has sent.
Is that guaranteed? (If so, the test is rightfully failing since that guarantee doesn't exist in the code) and if you only guarantee that some messages are received, you'll need to encode that in the test. Makes sense?
Due to the race condition, it's not, but once the client is connected, delivery is guaranteed.
Then make the test statistical, send messages at a rate, with monotonically increasing values. then in the subscriber test that after a certain period, atleast one message was received, and all messages that were received should have monotonically increasing values.
Why do you pass in an ActorSystem?
Why do you pass in Node? What kind of Node is it, is it the extension? If so, why do you pass it in?
Why this line?
override def unhandled
actorOf is asynchronous, that child might not exist at the time of your lookup.
Both of these is not needed I understand now. Not used to Akka 2 yet. Thanks.
Is it automatically inferred if I don't specify anything? 
Right. I realized that myself. Have already changed it to def. 
But I need to define a receive method, so then I would need a dummy receive PLUS an unhandled handler. Not that nice IMO. 
Naturally, otherwise you'd have to define it always.
Ok. Nice. Thanks. 
Still fewer Loc:  def receive = Actor.emptyBehavior def unhandled(msg: Any): Unit = log.error("/system/cluster can not respond to messages - received [{}]", unknown)
2.0 is really badass :-)
Ah, Actor.emptyBehavior is nice. Fixing.
When I started thinking what is going to happen if there is an exception thrown in reciveAfterProduce I am leaning towards getting rid of sync optimisation. It will be confusing for the user as the exception thrown in sync mode might be caught by camel async processor. The same exception in async mode will be caught by akka dispatcher. This might be really hard issue to detect. WDYT?
Also, I wonder what @krasserm has to say about it?
I'll look at it tomorrow, but you have a point there about the confusion that could happen.
@piotrga good catch ... drop the sync optimization as it will result in less confusing behavior under error conditions. Need more time to think about alternative optimizations but we can also introduce them later without breaking public contracts.
Isn't merge quite performance sensitive?
Is this optimal?
it's not done that frequent, once per 3 second it's much more performant now than it was before
Just curious: when can a future be completed with NonLocalReturnControl?
You can merge this line with the last in match with 'case _ => source'
Good point! Thanks
It might makes sense, yes
Ok. Maybe add comment which one is inbound and outbound
Nice that you pulled inside
You've made it pure, nice!
I pushed some improvements
Nice work Patrik!
Do you prefer this tailrec version?        @tailrec     def resolve(stack: Stack[Class[_]]): Option[Serializer] = if (stack.nonEmpty) {       val c = stack.head       serializerMap.get(c.getName) match {         case null            val stack2 = if (c.getSuperclass eq null) stack.pop else stack.pop.push(c.getSuperclass)           val stack3 = stack2.pushAll(c.getInterfaces)           resolve(stack3)         case x  Some(x)       }     } else {       None     }  I don't think it's a big difference since it manages the stack itself -- but it is tailrec ;-)
Hehe, no, it's ok
if you want to make it into M4 youll need to be quite fast, now  But I dont see much harm because RC1 is not far out at all.
Very useful. What about Java API for UntypedActors?
If my comments are fixed today I'll include this in RC3! :-)
I have verified with my little test app that it works. Good!
Excellent! So a review from Roland and then we're golden?
Yes, looks good!
I have added a commit for the validate-pull-request task here as well (ticket 3558). One potential disadvantage with that approach is that it is not possible (afaik) to run the tasks in sequence for all projects to fail fast, e.g. no tests are run before everything compiles.  In the jenkins job we currently run "clean clean-files update test:compile test"  What is clean-files good for? It's a SettingsKey not a TaskKey. It is not something that can be put in dependsOn.
This no longer merges...
Ok it does :)
@rkuhn pointed out that `Props(new Actor with Stash { def receive = Actor.emptyBehavior })` is not possible, and I will document the limitation
LGTM to the doc update
Kitty is happy. Merge?
Thanks for the quick update.
(kitty-note-to-self: ignore 23429803) :cat: Synchronaising! :pray:
The ticket for this is in milestone 2.2.1. Should we merge and close the ticket?
well need to add such a task to release-2.1 and release-2.2 as well
The title said forward port so I assumed that it is at least in release-2.2. I will need to eventually close the ticket in milestone 2.2.1 that is why I asked.
It is in release-2.2. I think it can be be done on demand in release-2.1
LGTM, (uncertain about the Inbox)
May I merge this? Need to backport...
please fix the module abbreviation (should be tes according to Patriks mail to akka-dev ;-) )
Probably "kit" would be the nicest
PLS SYNCH  The kitteh picked up the wrong jenkins job :(
(kitty-note-to-self: ignore 23660292) :cat: Synchronaising! :pray:
This will be split up into two separate pull requests.
You sure there was no correlation between the timeouts? (i.e. that they are chosen to be some distance from eachother?)
Not that I could see. I tried to look for that. It doesn't test expected callTimeout failures.
Then it LGTM
Fixed! See commit comment for details.
Comments aside, this is amazing work Patrik, keep it up! :-)
Fixed relevant review comments. Anyone planning to review this more, or shall I merge?
Looks great. A lot nicer, more readable and intuitive.  Can't believe you argued in favor of Either ;-)
LGTM! Reads very nice.
@jboner I wasn't against Try as a standalone entity, I was against baking it into Future ;-)
nice! I think there are some changes to public api, which should be mentioned in migration guide.
@patriknw Yes, absolutely, just wanted to get aconfirmation that this was the way to go first. :-) I'll add migration.
given the lines counts I do understand why you of all people have picked this ticket ;-) LGTM
Any news here?
I am waiting for the pull request to be accepted on the scala-zeromq binding: https:github.com/valotrading/zeromq-scala-binding/pull/15  I didn't get any update for a while though. If you have another solution to push the update, feel free to propose.  Alexandre
Hi,  So my pull request has just been accepted on the Scala-ZeroMQ project and they made a new release today.  I am going to use this new version of the library now in Akka in my pull request, see if everything work as expected, make a rebase against the lastest Akka master branch and then I think the pull request will be ready.  Do you see anything else I should do to have it accepted ?  Alexandre
no, sounds good; but this will not go into 2.1, Id like to have this together with Viktors Frame->ByteString work on 2.2-M1
Ok, do you have any idea on when is 2.2 to be released ?  Would it be possible to have it included in a minor release also ?  Alexandre
The change is neither source nor binary compatible, hence we cannot apply it during the 2.1.x series. The 2.2 cycle will be shorter than the 2.1 cycle for sure, we are aiming for two months. We will publish a 2.2-M1 as soon as we have merged the bigger changes for the release, though.
Hi,  I just pushed something but it's not good, it does not compile. I will have to do the rebase again, maybe during the weekend.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/212/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/212/
something went wrong with this rebase; rebasing across a merge with master can be problematic history-wise, in that case it might be best to get your branch into the shape you want the end result to be (and have that committed), then `git reset --soft` to origin/master and commit, which will leave you with a single commit rebased on top of master.
What is the status of this PR? 
Hi,  We would still like to publish the change, I need to find a bit of time to do the rebase properly.  I was thinking of actually creating a new branch from master, reintegrate the changes and push a new pull request, as the amount of change on master since I originally created this one are significant.  I will try to do that as soon as possible.  Alexandre
thank! as a heads-up, we are aiming for 2.2-M1 next week
It was getting complicated to do a rebase of couple of months of commits so I made a new branch with just one commit on top of current master. There is another pull request for it. You can close this one.  Thanks  Alexandre
Hi. This pull-request is closed and I didn't find fixes in master. Does it mean that the problem with request/reply performance is resolved any other way? Can I rely on akka-zeromq and have reasonable performance?
I don't know the status, but there is another open pull request by @alexandreagular: #1285
Yes, I closed this pull request because it was getting very old and I thought it would be simpler to get a fresh master branch and put the changes on them.  There is another one indeed on which some comments have been made that I need to work on. I will try to do that soon in my spare time. 
Hey! Have all contributors to the code signed the CLA?  typesafe.com/contribute/cla
I haven't done that yet. I need to check it with my manager, which won't be until monday.
Great, let me know when you've signed or if you cannot sign. Without CLA signature I cannot do anything with this PR but to close it.
Hi,  Just committed a new version and added some more sample code. the CLA signature should come soon.
Great. Let us know when you have it signed. 
I am Alexandre's manager (@jacobkolind), and I have just signed the CLA. Are we good to go if he also signs the CLA?
Hi Jacob,  everyone who has contributed to the PR has to sign the CLA. However, we have the possibility of issuing a corporate CLA for the company so that all employees' contributions are covered by that, is that something that would be interesting?
We discussed this with our CEO, but he was of the opinion that he would rather that we signed the CLA individually. The company has waived all rights to any IP that we contribute to Akka.
I have also signed the CLA so now everyone contributing to this pull request have signed the CLA. Let's hope it can be used :)
Excellent. First of all, could you split it up into logical chunks? (1 PR for the performance fix(es)) and one for the docs and samples?
Hi,  I haven't checked that, thanks. What is the minimal version of ZeroMQ that we should support ?  I can make a fix for older versions, but it's not going to be as neat. I would rather keep that one for the current and newest version of ZeroMQ and make the fix specifically for oldest versions. Would that be acceptable ?
Latest stable release is 2.2,so that has to be supported.
So should I leave it as it is ?
Please apply this change to ConcurrentSocketActorSpec so you can run the tests:  def checkZeroMQInstallation =     try {       zmq.version match {         case ZeroMQVersion(x, y, _) if x >= 3 || (x >= 2 && y >= 2)  Unit         case version                 invalidZeroMQVersion(version)       }     } catch {       case e: LinkageError  zeroMQNotInstalled     }
So, it doesn't work on 2.2: https:www.assembla.com/spaces/akka/tickets/2538-0mq-integration-broken-on-2-2#/activity/ticket:2538
Ok, I am going to check that and do the fix on checkZeroMQInstallation now.
Hi,  So I have found the problem, it turns out that the setReceiveTimeout method is not working. The reason is that the scala-zeromq binding does not allow this setting with zeromq < 3.0.0.  As this does not match the zeromq 2.2 spec, I was thinking that the proper way to fix it would be to change the scala-zeromq binding, this file in particular: https:github.com/valotrading/zeromq-scala-binding/blob/master/src/main/java/org/zeromq/ZMQ.java (see the setReceiveTimeout and setSendTimeout methods)  And upgrade the required version for the scala-zeromq binding in akka  Does that sound acceptable to you ?  The other solution we have is a duplicate implementation for zeromq < 3.0.0 and >= 3.0.0 which won't be as neat.  Alexandre
What happens if one uses the java-zmq-binding?
I checked the implementation of java-zmq-binding, it seems correct, we could use it: https:github.com/zeromq/jzmq/blob/master/src/org/zeromq/ZMQ.java line 586 you can see the requirement is 2.2.0
Alright, so I think your proposition is OK: Fix it in the scala-zeromq-binding When that's done we can then let users either use the scala or the java binding to their liking, they will, however, have to upgrade to atleast 0mq 2.2.  How does that sound?
Ok that sounds good to me.  I will work on that and come back to you when I am done. 
Sounds great, thanks
Hi,  So I fixed the problem on the scala binding. I made a pull request for that: https:github.com/valotrading/zeromq-scala-binding/pull/15  I will probably fix other things I have noticed on it but that is not relevant for this pull request.  In my last commit, I changed the required version for the scala-zeromq binding to 0.0.8-SNAPSHOT, which you will have to download from my branch, compile and publish locally to make it work.  With all that (plus a small fix I had to do) the unit tests pass.
Alright. We won't be able to do anything until the artifact is published with a stable identifier (no SNAPSHOT deps allowed) Great work improving the binding btw!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/26/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/26/
I have no clue about this
Not enough.  You need to also publish to an ivy repository, like this one: http:www.scala-sbt.org/community-plugins.html#communityrepo  I even know the company who provides that repo.  I can hook you up.
Added the publishTo mod
Needs docs but otherwise LGTM!
I changed the MultiNodeSpec to not depend on AkkaSpec or on a specific test framework and removed all dependencies to ScalaTest from the published artifacts.  This means that we have to mix in STMultiNodeSpec in our MultiNodeSpec tests to get ScalaTest functionality.  Would like a second review.
Good solution! :+1:
Aside from comments, LGTM
Do we have to change anywhere to make these published? I thought someone said that akka-cluster was not released as milestone, but I can't find anything in build or release script.
I've always excluded it manually, but please consult AkkaBuild.scala
@rkuhn did you also exclude akka-cluster and akka-remote-tests manually from the latest milestone release? In that case everything is fine, since we want them to be included now.
yes, see b1768582910e3d8accc5532073282664cbc19e8f for the manual exclusion
thanks, that means that samples are published, why?
because I just did whatever Viktor had done, while Peter was watching; thats all I know.
yeah, it was just an observation, I'm not blaming you, I'll create a ticket, I see no reason to publish samples
now the only thing missing is to add the possibility of a failure after creating children, so that the new Creation logic is also tested.
The logic is tested by the minimal case I added, but yes it should be stressed as well.
 which is why I also dont expect more problems (famous last words).
Nobody expects the Spanish Inquisition!
That's what I get for using the Web UI on my iPhone on the bus.
awesome that you have nailed the bastard LGTM
yup, well possibly move real code into akka-incubator once that exists, right?
Why not call it 'Patterns'?  What about the Java side of things? 
There are no summer of blog posts with Java. I double-checked.
But patterns are general. Then we should at least link to them and say that the pattern applies but the code is in Scala for now and that they would have to translate. We can't have the Java side be just void. 
Didn't I write exactly that in the java section a few weeks ago?  /Patrik  10 sep 2012 kl. 18:05 skrev Jonas Bonr <notifications@github.com>:  > But patterns are general. Then we should at least link to them and say that the pattern applies but the code is in Scala for now and that they would have to translate. We can't have the Java side be just void. >  >  > Reply to this email directly or view it on GitHub. > 
You don't know?  If so, then good, if not, then do it. 
The Java section links to the scala section for the parts that are scala-only (code wise) otherwise I see no point in completely replicating and have 2 files with essentially the exact same info.
It wasn't a question, sorry. LGTM http:doc.akka.io/docs/akka/snapshot/java/howto.html  /Patrik  10 sep 2012 kl. 18:08 skrev Jonas Bonr <notifications@github.com>:  > You don't know?  > If so, then good, if not, then do it. >  >  > Reply to this email directly or view it on GitHub. > 
Absolutely no need to copy paste. Looks good. Thanks. 
I have incorporated all your awesome feedback. I would appreciate if you take another look at the CAS impl in in `updateConsistentHash` so it match your suggestions.  I dismissed Viktors suggestion about function to map msg => hashKey, so if others have opinions so speak up. I guess we can have both ways for super duper flexibility.  Should I benchmark rangeImpl or do we consider it good enough as first impl? It's only internals so it could easily be changed.  I will also convert the doc sample to Java.
Damn UI, I mistakenly clicked close...  I was about to say go with the rangeImpl since we can change it later. No need to optimize that for RC1.  Shouldn't `ConsistentHash` be `private[akka]`?
Why would a consistent hashing router always route to remote actors? I see a lot of use-cases for having an in-process consistent-hashing router, like if you have a partitioned datastore and you have an actor representing and manipulating each partition, you want a router that sends the right messages to the right partition.
We had a discussion about the mapping of msg to hashKey, and the conclusion was that there are valid arguments (use cases) for all three ways. 1. Envelope - sender knows the key 2. Interface - key is part of the message and it's convenient to define it together with the message definition 3. function in router definition - makes the decision transparent for the sender
Performance improvement for review
More to review: function to map message to hash key
apart from those two nitpicks: :+1:
Java doc sample ready for review. I don't know of anything more outstanding in this pull request, so give me your final verdict.
Renamed a few things as agreed with @jboner 
Congrats, you earned 62 lines ;-) :+1:
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/672/
One conf setting should also be removed: `max-gossip-merge-rate`.
Thanks. Nice catch.
some minor comments great work :+1:   Thank you @pvlugter for suggesting this!
LGTM after comments, really nice work guys!
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/672/
Rebased and fixed
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/680/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/680/
apart from the one thing I didnt fully penetrate: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/680/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/680/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/674/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/674/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/675/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/675/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Apart from my comment, LGTM! This is a very useful change.
Are there any other actors in the remoting that needs special logging (by default everything is logged, as before)? Any chance that I swallow anything that should be logged? EndpointDisassociatedException, EndpointAssociationException?
I don't think so. As all of the exceptions that lead to EndpointExceptions   are published as remoting events, this should be correct. And as I see you   log "unexpected" exceptions, which is nice.
ok, good, I'll do some manual testing with the cluster sample also
Fixed review comments and did some manual testing
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/681/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/681/
LGTM; refining the usage of the new logging configurability will be an ongoing process
I changed log-remote-lifecycle-events=on as decided.
How to turn off log-remote-lifecycle-events was documented.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/678/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/678/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/677/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/677/
so the problem was that RoleName sorting was not compatible with Member sorting?  In that case: LGTM
Yes, that was the exact issue. Joining nodes can not be the leader, but they are sorted first.
Seems like a safe bet until we decide on things coming back from `Unreachable`. I still think that `Unreachable` should be a terminal state.  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/679/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/679/
LGTM, good reasoning
Waiting for kitteh approval
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/686/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/686/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/685/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/685/
any more reviewers on this, so it can be merged?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/684/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/684/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
seams to be same problem as in PR 1111 maven plugin download, I'll try to empty my cache to see if this is a too old plugin version
``` [ERROR]   The project com.typesafe.akka.akka-sample.dining-hakkers:parent:2.2.0-SNAPSHOT (/localhome/jenkinsakka/workspace/akka-pr-validator/akka-samples/akka-sample-osgi-dining-hakkers/pom.xml) has 1 error [ERROR]     Unresolveable build extension: Plugin org.apache.felix:maven-bundle-plugin:2.3.7 or one of its dependencies could not be resolved: Failed to collect dependencies for org.apache.felix:maven-bundle-plugin:jar:2.3.7 (): Failed to read artifact descriptor for org.apache.felix:maven-bundle-plugin:jar:2.3.7: Could not transfer artifact org.apache.felix:maven-bundle-plugin:pom:2.3.7 from/to moxie-everything (https:moxie.typesafe.com:8497/nexus/content/groups/everything): Not authorized. -> [Help 2] [ERROR]  [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR]  [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http:cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException [ERROR] [Help 2] http:cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException ```
I have not yet been able to verify this, but pulling it into my branch; that maven download problem might need to be fixed in Lausanne, will see
Merging needed it seems.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/683/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/683/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
What is required to make Faield a SystemMessage without any other changes, I'd like to have only those changes in a single commit so I can try some queue ideas out in isolation.
2013.03.12. 16:46:50 dtumon Viktor Klang () <notifications@github.com>   rta:  > What is required to make Faield a SystemMessage without any other   > changes, I'd like to have only those changes in a single commit so I can   > try some queue ideas out in isolation.  The problem is that that commit came after the other changes, but more or   less starts with commit 26daf42.
Alright, that looks like a hassle to distill. I'll try to implement a new SystemMessageQueue with stashing builtin
You might look into Roland's original branch and my fix for that. That   might be a better place to start for you.
Do you want the ticket as well? ;)
Lol. we'llsee, I'm just tinkering atm. Is everything present here? wip-2299-Failed-SysMsg- (I saw you made some commits to it)
Yes, it has everything, but you have to remove the "stash-like"   constructs. My only commit to that is a bugfix that made the tests pass.
I do like this approach, great work Endre! The important part of confining the message stashing/unstashing/processing in one place is already done, so if we want to replace the system message queue implementation that will be rather easy. I see no reason not to go ahead with this irrespective of Viktors possible improvements.
Fixed comments and rebased (it was a hellish rebase... whew)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/713/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/713/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Ah, stupid me...
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/714/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/714/
This can be merged as soon as one more thing is added: a section in the message ordering guarantee docs that communication of failure does not obey the normal rules and a mention (plus link) in the actor restart docs for Java and Scala plus an info box in the supervision.rst that supervision-related parentchild communication uses messages of a special kind which cannot be influenced by the user.
Updated documentation, added marker traits proposed by Roland
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/714/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/714/
Squashed, rebased, merged, fixed. Now waiting for kitteh.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/739/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/739/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/740/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/740/
LGTM (with that one nitpick)
LGTM, please backport to release-2.2
I need to add test case, then backport.
(kitty-note-to-self: ignore 23005434) :cat: Synchronaising! :pray:
Added test case
I can't see the test case?
Now its really there :)
use imperative present tense style for commit messages
If the licensing on the fancy Mozilla logic is a problem there are other (maybe less sophisticated) heuristics, e.g. the relatively simple one described here: http:www.w3.org/International/questions/qa-forms-utf-8.en.php
Thanks @sirthias that is another option, and of course simple ASCII detection might be enough. I did this PoC as a discussion starter. I changed quite a lot the java port of the Mozilla code, but it is still derivative I think.
Btw, I think the Mozilla code is simply a matcher state machine for the pattern described in the link you pasted.
I did sign your CLA btw, although I don't think that's needed for a change like this. 
Thank you for contributing. As a side note, I think we should provide a general section for throttling approaches, since this is basically a variation on the Stop-and-Wait flow control mechanism.
+1 for having a general section for throttling approaches. I'm not familiar with stop-and-wait, but it sounds like it's blocking? The linked article discusses a couple of alternative approaches that mostly suffer from blocking..
No, stop-and-wait is a family of flow-control protocols (and also error recovery protocols -- the two are closely related), it has nothing to do with blocking. It only means that the producer does not send a new message/work until the previous one has been acknowledged/new work has been requested. 
Thanks for spreading the word!  Concerning the CLA: weve been asked to require a CLA for everything that shows up in the git history (must be a lawyer thing).
Thankyou for your contribution.  Would you mind signing our CLA?  http:www.typesafe.com/contribute/cla  (unfortunately Im forced to ask you for this, even though it is just a one character fix)
Signed it.  If I didn't sign, would it have legally prevented you from fixing the spelling mistake, or could you have performed a cleanroom implementation of my patch? :-)  Edit: also, what's the reason for the 10ms limit on Windows?
LGTM, I'm not sure how strict we should be with the commit comment syntax. https:github.com/akka/akka/blob/master/CONTRIBUTING.md
We use a special encoding for the commit messages, as described in https:github.com/akka/akka/blob/master/CONTRIBUTING.md Please update the commit message to:      =act Fix typo in scheduler error message
Windows does strange things when you ask for sleeps <10ms and most of our tests run on Linux and MacOS so we stay clear of the strange places.
Thankyou for your contribution.  Would you mind signing our CLA?  http:www.typesafe.com/contribute/cla  (I know it feels unnecessary, but otherwise I cant merge anything)
No pb, I'm currently in holidays, but as soon as I'm near a computer I will sign it.  Cheers Le 7 aot 2013 09:54, "Roland Kuhn" <notifications@github.com> a crit :  > Thankyou for your contribution. Would you mind signing our CLA? > > http:www.typesafe.com/contribute/cla > > (I know it feels unnecessary, but otherwise I cant merge anything) > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1628#issuecomment-22235329> > . >
OK, now I'm neer a computer, and I'm not sure I understand the process for the CLA. Does I really need to *print* the individual CLA and mail it to Typesafe ? With actual paper ? Or does the "complete the CLA signing with your GitHub account" is sufficient ?  If it's the first, well it's really a burden for just a poor correction in some documentation (not to say I won't do it, but hell, you just don't encourage user to contribute). Why don't you use an online signing service like https:www.echosign.adobe.com ? (Ah, just noticed they were bought by adobe).  If I misunderstood something and submitting the form where "I hereby accept...." is ok, sorry for the noise :) (well, in that case, perhaps you should add an explanation somewhere about what exactly we have to do to accept the CLA). 
OK, sorry for the noise, in the midtime I received an email saying that the CLA is signed, so I guess the form is sufficient ;)
LGTM Thanks for contributing. (CLA sign verified)
We use a special encoding for the commit messages, as described in https:github.com/akka/akka/blob/master/CONTRIBUTING.md Please update the commit message to:      =doc Update ZeroMQ pub/sub port mismatch between doc and code example
nothing to be sorry about, but you should actually force-push your new commit ;-)
Updated commit forced-pushed. 
backported to release-2.2
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/653/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/653/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/652/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/652/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/658/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/658/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/658/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/658/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/661/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/661/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Rebased this. I will merge it when Kitty is happy. I have run this code many (30+) times on build servers so I feel confident.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/661/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/661/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Ticket 2930 (timeout changing throttler mode) makes Kitty sorry. Unrelated to this, so I will merge.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/660/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/660/
The build was a success, so shall I merge? Id like to get the snapshot docs published.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/656/
very good catch :+1: 
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/656/
Great work, Bjrn! This is indeed a nice catch, and an impressive shortening of cluster convergence times.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/663/
Fixed based on review.
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/663/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
The failure is in an accidentally pushed old test, that was removed again.  PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/664/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/664/
Looks great to me!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/668/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/668/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
So the test that failed _normally_ takes around 4.1 seconds and this run it timed out at 5 seconds. I think the timeout in the test is too tight.  PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/669/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/669/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/659/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/659/
BTW, I will add something to documentation also.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/671/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/671/
Apart from formatting, LGTM
I second that. LGTM apart from the formatting.
Added documentation and fixed formatting
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/671/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/671/
Looks Great To Me!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/670/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/670/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/666/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/666/
perhaps time to merge this?
Thanks for the reminder!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/667/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/667/
fixed review comments
@gakesson, please sign the CLA: http:www.typesafe.com/contribute/cla
We use a special encoding for the commit messages, as described in https:github.com/akka/akka/blob/master/CONTRIBUTING.md Please create a ticket and update the commit message to:      =act #ticketnumber Improve semantics for BoundedBlockingQueue 
@patriknw   Oh, silly of me to believe you didn't have a process for these kind of things. :-) I need to become a watcher (shepherd like Jules in Pulp Fiction!) for the Akka space and the project owner can grant me this. My assembla username is gakesson .
@gakesson you should be able to become a watcher of the Akka space yourself
So, how is the process to close this issue?  Also (I'm more familiar with Gerrit) but I don't quite understand why the regtests don't run after each commit...
Yes I did, and I signed it again to get twice the effect ;-)
CLA verified, then it needs a final approval by @viktorklang 
Thanks for contribution, @gakesson 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/624/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/624/
The last commit was just to fix a typo in the commit message.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/625/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/625/
Apart from comments. LGTM
As Bjrn said: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/645/
Updated based on Bjrn's comments. Now includes deadlock report, and a test.  Found a scalac bug too: https:issues.scala-lang.org/browse/SI-7203  ```` Deadlocks found for monitors and ownable synchronizers: "deadlock-thread-a" Id=431 BLOCKED on java.lang.Object@12e64ad7 owned by "deadlock-thread-b" Id=432 	at akka.testkit.AkkaSpecCoronerSpec$$anonfun$1$$anonfun$apply$mcV$sp$1$$anonfun$2$$anon$1.recursiveSync(AkkaSpecCoronerSpec.scala:52) 	-  blocked on java.lang.Object@12e64ad7 	at akka.testkit.AkkaSpecCoronerSpec$$anonfun$1$$anonfun$apply$mcV$sp$1$$anonfun$2$$anon$1.recursiveSync(AkkaSpecCoronerSpec.scala:52) 	at akka.testkit.AkkaSpecCoronerSpec$$anonfun$1$$anonfun$apply$mcV$sp$1$$anonfun$2$$anon$1.run(AkkaSpecCoronerSpec.scala:44) 	at java.lang.Thread.run(Thread.java:722)   "deadlock-thread-b" Id=432 BLOCKED on java.lang.Object@31aa6131 owned by "deadlock-thread-a" Id=431 	at akka.testkit.AkkaSpecCoronerSpec$$anonfun$1$$anonfun$apply$mcV$sp$1$$anonfun$2$$anon$1.recursiveSync(AkkaSpecCoronerSpec.scala:52) 	-  blocked on java.lang.Object@31aa6131 	at akka.testkit.AkkaSpecCoronerSpec$$anonfun$1$$anonfun$apply$mcV$sp$1$$anonfun$2$$anon$1.recursiveSync(AkkaSpecCoronerSpec.scala:52) 	at akka.testkit.AkkaSpecCoronerSpec$$anonfun$1$$anonfun$apply$mcV$sp$1$$anonfun$2$$anon$1.run(AkkaSpecCoronerSpec.scala:44) 	at java.lang.Thread.run(Thread.java:722) ````
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/645/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/646/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/646/
Updated based on Roland's comments.  I'd also be keen to know if my usage of java.util.concurrent objects (CountDownLatch/Semaphore) is the best it can be. I just did a quick skim of the package javadoc and picked what looked right!  I am getting test failures on my local builds (sample.cluster.transformation.TransformationSampleSpec, akka.io.TcpConnectionSpec) but I'm hoping they're more to do with general build instability at the moment
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/647/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/647/
I think it would be better if this was a trait that can be mixed in to various types of tests, not only AkkaSpec. For example MultiNodeSpec is not a AkkaSpec, and it would be very useful for those tests as well. 
yes, I agree that making it a trait would be better; I always forget that not every test suite somehow extends AkkaSpec 
Made into a separate object so it can be reused. There wasn't a straightforward mixin I could make for tests, because non-AkkaSpec tests have different before/after methods and it's difficult to know which ones to override.  I looked at Timer and ScheduledThreadPool. I wasn't sure that ScheduledThreadPool was an improvement over using a single thread since it's not as simple as just scheduling the task (and then possibly cancelling it). I'd need to shutdown the ScheduledThreadPool too to avoid leaking the thread, right? (Timer has automatic thread cleanup, but with some quirks that seem to make it undesirable.)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/647/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/647/
LGTM, would you care to add it to `MultiNodeClusterSpec` also?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/657/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/657/
Made StressSpec duration configurable.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/676/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/676/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/627/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/627/
Thanks for the explanation. LGTM
Ah, I was completely lost in the dungeon :)
I might be missing something, but somehow this feels like an awfully wide interface for something which conceptually just has two operations (and offers some convenience methods on top).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/630/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/630/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
It doesn't compile...
Saw that, Java code in akka-docs/rst/java/code/docs/agent/AgentDocTest.java wasn't using the singleton's factory method.  Do we have a convention to use e.g. `create` for java interop here, or should Java just stick to using `apply()`?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/634/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/634/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/635/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/635/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Doesn't compile mate ;-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/635/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/635/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Don't use the Build Kitteh as an IDE, make it compile and pass test locally before pushing.
Something ain't adding up here... The triggering PR contains: 	     -    Agent<Integer> agent = Agent.create<Integer>(5, ec);     +    Agent<Integer> agent = Agent.create(5, ec);  But Kitteh says:      [error] ...AgentDocTest.java:38: ')' expected     [error]     Agent<Integer> agent = Agent.create<Integer>(5, ec);  How can kitteh be erroring on the pre-delta version of the offending lines?  It definitely builds cleanly on my local machine.
Seems weird, try to rebase and squash commits?
Trying now, not sure how happy github is about squashing already-pushed commits though.  If it all goes pear-shaped I'll recreate the branch and send a fresh PR with everything in a single commit
it's your own branch so pushing -f to squash shouldn't be an issue.
New ball of string for kitteh, let's see if she plays nicely with it :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/638/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/638/
Yay! Happy Kitteh, wonder why she go so upset before?
Now that things worked out for the Kitteh, I still wonder whether the interface `Agent` should have so many abstract methods. Would it not make sense to do it more like `Traversable`, where you implement `foreach` and get all the rest for free? Here it would probably suffice to give `get` and `alter` (one or both or those), would it not?
I'm not sure that there's much reuse to be had, you're assuming that all Agents will be using STM directly? In Kevin's example he just wants to shim something on top of a SecretAgent, right?
ah, okay, there were some missing pieces of information here; so all is good now: LGTM
@viktorklang build something on the Agent interface, yes.  Delegating to another instance - See here for the use-case: https:gist.github.com/kevinwright/5038696
:+1:  I like the SecretAgent name :)
aside from the comments LGTM and `SecretAgent` is alright (since it is private), Agent007 would be even more fun 
Patrik, I knew there was a reason to ping everyone: what would we do without you?
good cop/bad cop ;-)
so theres one doc comment to fix and the migration docs to write: it would be okay with me if you want to keep all the glory to yourself, otherwise Ill pick it up and add one commit tomorrow
I'm happy to do the doc comment and make SecretAgent final, but not sure where the migration docs go.
migration guide: https:github.com/akka/akka/blob/master/akka-docs/rst/project/migration-guide-2.1.x-2.2.x.rst you can just edit the existing section "Brand new Agents"
Cool, expect another commit then :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/638/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/638/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/629/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/629/
Updated with a new take on the problem.  If this actors mailboxes are empty when we try to get the lock, then we can back off since someone else has picked them up and processed them.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/632/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/632/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
That's weird. Out of diskspace on the jenkins? Alert ops-related?
Nope, not out of disk space max 60% used on any volume. Seems like a2 lost the mapping to `/home` or some maybe some credential glitch. Will restart the jenkins slave.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/636/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/636/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Very interesting failure
Very interesting failures indeed.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/639/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/639/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
This is not ready for review. Closing.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/641/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/641/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/649/
Cleaned up and rebased.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/649/
Great job! This is a really nice improvement!
Any more comments? Good to merge after fixing Rolands comment?
LGTM, only style comments
Fixed and squashed. Waiting for validation.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/649/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/649/
LGTM!  Imports are usually a very important part of Scala code, and most documentation out there omits them.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/622/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/622/
LGTM  FYI, see also the related ticket suggesting @implicitNotFound annotations: https:www.assembla.com/spaces/akka/tickets/3095
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/622/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/622/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/651/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/651/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/631/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/631/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/642/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/642/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/640/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/640/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/648/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/648/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/650/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/650/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Preparing another attempt
LGTM apart from my minor nitpick
backported to release-2.2
I think the scenario is correct. I had test runs with the increased timeout and it is OK now. LGTM
Great work @drewhk and @rkuhn 
Baaah sorry wrong button. Reopened.
backported to release-2.2
unsealed should be ready for merge and backport
backported to release-2.2
I have worked through akka-actor now, puh
It was SupervisorHierarchySpec
LGTM: you mopped up quite a few warnings, its a shame that we dont have the tools yet to ensure that the thicket does not grow back. I hear SBT 0.13 has some handy features (linking to other docs) which might eventually allow us to have PR validation fail when breaking doc links.  This is the first step, right? I.e. just ScalaDoc warning removal, no significant improvements to content; or did you not encounter old cruft / undocumented methods?
improvements of content (mostly reduction of wrong content) was done in the second commit: https:github.com/akka/akka/commit/0f25106b853b077f027995c804f9748b2cf079ca  but it is "only" akka-actor so far
backported to release-2.2
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/610/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/610/
is there a ticket for fixing or removing `"stop writing in cases of backpressure and resume afterwards"`?  apart from that: LGTM
1198 has been merged
...so can we merge this in?
Just a moment...
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/643/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/643/
Why?? We have to guarantee binary compatibility in all our dependencies??
I mean, we don't expose any Netty API I know of.
Otherwise it looks correct, but I am pretty surprised by this revert.
If the user has to upgrade Netty due to our upgrade, and that breaks his/her usage of Netty, then we are not BC. We cannot change dependencies in a patch release unless those are BC as well.
Well, fine for me.
Shouldn't this reported then as a known issue?
yes, excellent point!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/609/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/609/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/606/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/606/
The rst docs must be changed also.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/611/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/611/
any reason for another syntax of the commit line? I thought it was supposed to be      =pro:3488 update to config library 1.0.2
I was trying out something: this one works better in flowdock (i.e. generates a clickable tag which will show you the whole history of this ticket number). Do you see disadvantages?
I just wanted to know, so that we all can use the same. This is fine, better.
backported to release-2.2
Could also mention: We follow the imperative present tense style for commit messages (more info here) http:tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html   
yes, sounds good, will amend
backported to release-2.2
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/619/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/619/
ill push a dry'd up version tomorrow  Am 26.02.2013 18:19 schrieb "drewhk" <notifications@github.com>: > > LGTM > >  > Reply to this email directly or view it on GitHub.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/620/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/620/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/621/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/621/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/621/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/621/
LGTM, once you've processed the other comments.
LGTM, after processing the comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/623/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/623/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/616/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/616/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/617/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/617/
Nice catch, LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/612/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/612/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/615/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/615/
LGTM, apart from one minor comment.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/626/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/626/
Thanks, Josh!  LGTM
Anytime @rkuhn.  Thanks for the review!
ah, new policy: please prefix the commit messages first line with =pro #3019 
@rkuhn is that correct?
LGTM, but should there not also be a Java version?
use imperative present tense style for commit messages
(kitty-note-to-self: ignore 23005454) :cat: Synchronaising! :pray:
Can I merge this?
LGTM, no need to backport?
It needs to be backported. It is in the PR description
ah, then I think it deserves a ticket
yes, ticket & merge & backport
Should we release the plugin with this patch, or should we just make this switch regardless? What are the implications?
Should the akka-sbt plugin be cross published against 0.12.x and 0.13, or do we only support the sbt version currently used by this akka version?
I don't know. Is it difficult to cross publish the plugin? I don't know how to do that.
I also dont know whether that is a good or bad idea, lets ask @harrah for his opinion.
If you use 0.12.x for your build, you can build against both 0.12 and 0.13.  (You'd apply or revert this patch as appropriate.)  It might be a good idea to build against both since there will be 0.12 users for a while.  I don't think 0.13 can build against 0.12 at this time- see [this thread](https:groups.google.com/d/topic/simple-build-tool/MGx7EN0uBGE/discussion) (which I haven't looked at yet).
Thanks for the info. Would it be possible to integrate both plugin builds at the same time so we cannot forget? I guess wed need some project duplication magic; can we just make a new project which does the cross-build and gets the source files from this (unmodified) project?
Yes, I think that should work.  `def pluginProject(sbtVersion: String, scalaVersion: String): Project = ...` and then use it once for each sbt version.
take a look at https:github.com/jrudolph/sbt-cross-building, it's pretty easy to do builds for 0.12 and 0.13 with it 
Any news on this? :)
@bantonsson will publish 2.2.1 version for sbt 0.13 and adjust the build for future cross publishing
Currently only Scala, will port to Java after the text is reviewed
Looks great! Port to Java, squash and push, please.
Updated, added Java, squashed (+fixed misdirected arrows in image)
backported to release-2.2
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/748/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/748/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/744/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/744/
changed UdpPacket -> Udp
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/744/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/744/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/750/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/750/
good name change!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/779/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/779/
+1, I like the new names better
Thanks! I worked on an outdated branch and this change seem to have not survived the merge.  LGTM!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/753/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/753/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/753/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/753/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/747/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/747/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/756/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/756/
Good work, Endre!
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/829/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/829/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/829/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/829/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/856/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/856/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/755/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/755/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/755/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/755/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/757/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/757/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/759/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/759/
Fixed all remaining pieces, will squash after final review. Wait, a few FIXMEs that you can help me to decide what to do with: * Do we need to support temp actors? That would require the SelectChildName dance in VirtualPathContainer also. * How to replace actorFor in camel ActorComponent? * How to replace actorFor in ActorTransportAdapter?
Of course we need to support temp actors otherwise you won't get any responses to "ask" that go to other JVMs, amirite?
no, AFAICS Patrik is only asking about `system.actorSelection("/temp/blah")`, and there Id say: no
yes, it was about actorSelection, and my vote is also: no
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/797/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/797/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/797/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/797/
Fixed last set of review comments and squashed Will merge when kitty approves.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/804/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/804/
LGTM (the arrows are not aligned though :) )
that is up to scalariform, I think there is some limit when it skips aligning when the difference is too big
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/761/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/761/
(kitty-note-to-self: ignore 23145738) :cat: Synchronaising! :pray:
great work, looks much better this way!
Updated according to review.
LGTM next time; you don't have to close pull request, git commit --amend git push -f origin <the branch>
LGTM for 2.2
@oakwhiz Thanks for contributing. I will cherry-pick to 2.2.x
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/741/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/741/
LGTM; what was it used for?
it is still used, it just that the explicit configuration of the plug-in is not needed `mvn exec:java` is corresponds to `sbt run-main`
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/743/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/743/
Great work! :100: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/745/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/745/
apart from nitpicks: LGTM
Fixes according to review.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/752/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/752/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
The failure was in an unrelated test `akka.cluster.SplitBrainWithFailureDetectorPuppet`
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/752/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/752/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/784/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/784/
I guess you mean backport to release-2.2, right?
yeah, I mean 2.2
reverted the rsync removal
(kitty-note-to-self: ignore 23037693) :cat: Synchronaising! :pray:
Tested on Windows, it works. :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/728/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/728/
epic ;-) LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/727/
Great work Bjrn! Apart from comments, LGTM.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/727/
LGTM, aside from the clusterView refresh
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/729/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/729/
apart from the `isTerminated` business it LGTM
Cleaned up the `isTerminated` business by adding a watched ask to the ThrottlerManager.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/732/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/732/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/725/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/725/
No packets were harmed during the construction of this PR. (LGTM)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/724/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/724/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/738/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/738/
Updated based on feedback. I reduced the wait time for expectNoMsg(). I'd previously used a marker message, like you suggested, but sometimes the messages arrived out of order.
This checks oversize on write side only, right? Should we also check it on the read side? Could protect somewhat against malicious clients.
I think this should be configurable. If we were able to decode the   message, then we might just be happy and deliver it :)  My proposal would be that the policy should be configurable - take+log,   drop+log, take+nolog
Configurable would be nice, with default of drop+log.  LGTM
I would vote for the default to be take+log. If we have a healthy, decoded   message, why would we drop it?
A malicious system could flood a system with oversized messages causing OOM.
A malicious system can totally ruin a system via remoting in many other   ways.
Yes, so we should just add one more? I think that the default should be symmetrical on send and receive.
I think we need to enforce the maximum frame length in both directions, no exceptions; ideally it would be done on the transport level, leading to closing the stream if the length field is out of bounds.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/738/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/738/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/751/
Improved info in error messages. Check payload size on read side, discarding ovesized payloads and logging an error.  I definitely agree with a default of drop, because higher layers might make assumptions about the size of data that they receive.  Not sure if I see a strong case for configurability, given that the max payload size is already configurable. e.g. If you set the max payload size to 10000 bytes, why would you want to ignore that value in some cases? The only thing I can think of is a use case for having different payload sizes for reading vs writing. But perhaps we could wait for someone to actually request this feature.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/751/
Good point about the nested actor; I've changed that.  In the new version hopefully I've got the meaning of `localAddress` and `inbound` correct. I'm assuming that when EndpointReader is created, that `handle.localAddress == localAddress` (I tested with an assert). I also capture the EndpointWriter's value of `inbound`. Hopefully that's correct.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/754/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/754/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/766/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/766/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/734/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/734/
I have a slight feeling that it is not this simple. The leader can have those statuses also, for special cases when there is no member in Up. Could this change result in convergence for those cases when it should not? See `Gossip.leaderOf`
@patriknw I've been thinking a bit about this, and since we only care about the convergence on the leader (as of now) to do the leader actions, I don't think that it's a problem.
Yes, as I said, it was just a feeling, and I could not come up with a concrete scenario where it would be a problem. Good that you thought it through.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/734/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/734/
500 threads? really?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/731/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/731/
@viktorklang I lied, it peaked on my machine at 498, but then I only have 4 cores. :wink:
That is scary. How could I have missed that? :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/730/
Cool :) +1
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/730/
 the wonders of async events and sync testing   LGTM
Tricky stuff. LGTM, but my understanding is somewhat limited.
LGTM after fixing those two cases
Good catch! LGTM
well, rather embarrassing (for me and all reviewers), I blame vaccation
I actually saw the error logged and made mental note of it -- just forgot when chasing another bug :/ It is good to see it fixed!
yes, indeed, we should buy a set of brown paper bags ;-(  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/722/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/722/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/723/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/723/
apart from the two questions it LGTM
PLS SYNCH  (kitteh is blind)
(kitty-note-to-self: ignore 23158986) :cat: Synchronaising! :pray:
PLS REBUILD ALL
(kitty-note-to-self: ignore 23248777) :cat: Roger! Rebuilding pr-validator-per-commit for 396e3104, e67ac3df, c9cba183. :rotating_light: 
(kitty-note-to-self: ignore 23330920) :cat: Synchronaising! :pray:
Thanks for porting, @patriknw. LGTM
Please sign Typesafe CLA: http:www.typesafe.com/contribute/cla
I have not completed the review. I will continue tomorrow.
@akara I think this will be a great contrib
Typesafe CLA signed using github account.
Thanks for the updates. You should create a ticket for this feature: http:doc.akka.io/docs/akka/2.2.1/project/issue-tracking.html  Then squash the commits into one, and use our special commit message encoding, which here would be something like:      +con #ticketnr Aggregator contribution  I will take care of cherry-pick to master when review is completed.
Ticket created. Please do one final review. If all OK I'll squash the commits and use the proper commit message encoding. 
Thanks for this submission, having this code available will further the research for how to write these kinds of actors in a nicer way with less pitfalls!  Please add one more commit addressing the two documentation issues and consider using an immutable persistent data structure instead of the mutable WorkList.
I think I have addressed all the comments (except the use of Vector for reasons given in the comment). I'm ready to pack this into a single commit with proper identification as Patrik requested. Before I do so, I'd like to do one last check if all are OK with the current state of the pull request. Will finalize once OK received. Thanks.   On Tue, Sep 10, 2013 at 4:30 AM, Bjrn Antonsson <notifications@github.com>wrote:  > LGTM > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1676#issuecomment-24152816> > . >
LGTM: on further thought this WorkList is appropriate for high-rate scenarios
Thanks! Will squish the commits and finalize (with proper commit message).  -Akara   On Wed, Sep 11, 2013 at 10:13 AM, Roland Kuhn <notifications@github.com>wrote:  > LGTM: on further thought this WorkList is appropriate for high-rate > scenarios > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1676#issuecomment-24258195> > . >
This pull request can be closed and replaced by https:github.com/akka/akka/pull/1728 which has the same contribution squashed into a single commit properly named according to Akka processes and requirements.  Thank you all for the great input and discussions on this pull request.
Has been superseded by #1728 
(kitty-note-to-self: ignore 23257862) :cat: Synchronaising! :pray:
this is a heroic effort
As the horse salesman said: "it is not blind, it is brave"!
I couldnt verify all details, but it LGTM from a little distance: great work, Endre!
(kitty-note-to-self: ignore 23327286) :cat: Synchronaising! :pray:
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/720/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/720/
after the fix: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/711/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/711/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/721/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/721/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/717/
yes, looks good; Patrik will see how it works ;-)
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/717/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/706/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/706/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/703/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/703/
I need to merge this now, since there is no genjavadoc 0.3 for 2.10.1 the build failed.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/708/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/708/
Yes, this looks good at first glance. Since the whole thing is driven by the gossip I dont see a problem in adding dynamic changes here, since e.g. the routers will respond correctly as it is. Since this is the case, we can defer the decision on that feature.
alright, sounds good
Pretty impressive! Like!
Completed all sub-tasks. Ready for final review.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/715/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/715/
some nitpicks, and the cluster docs could probably use some beautification (i.e. its -> it is and friends), but in general it LGTM.
Fixed review comments. I will let jenkins test it once more before merging.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/715/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/715/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/707/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/707/
ouch, you are right, that ticket cannot go into 2.2.1
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/704/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/704/
@joa, Please sign the CLA: http:www.typesafe.com/contribute/cla We can't merge anything, not even removal of 3 characters in the documentation without that in place. Thanks
We use a special encoding for the commit messages, as described in https:github.com/akka/akka/blob/master/CONTRIBUTING.md Please update the commit message to:      =doc Correct typo in cluster docs 
CLA has been signed and commit message changed.
LGTM  Thanks. Signature Verified.
backported to release-2.2
Please just rebase on top of master and squash your commits.  Thank you
Merged and squashed.
fixed incorrect call, using no-arg call.
LGTM Thanks for contributing! (CLA signature verified)
Great. Thank you!
We use a special encoding for the commit messages, as described in https:github.com/akka/akka/blob/master/CONTRIBUTING.md Please update the commit message to:      =act #3547 Resolve high contention on unsubscribeAddresssTermination 
Great work (finding and fixing)! Welcome to the exclusive club of those who have contributed to the dungeon ;-)
backported to release-2.2
Very unexpectedly :)  When backported version will be released?
soon, we aim at next week or the week after that
Good news. :) Thank you!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/702/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/702/
I added the update to genjavadoc 0.4 on top of this; seemed logical.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/702/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/702/
Thanks @rkuhn exactly what I wanted to do once you checked it in.
So I would like to have some review of this one so I can merge and build the bits.
my review not the one you're looking for, but: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/699/
Also, a good question is if we should update the Scala version with this PR or we should do that separately. It was just very hard to verify this change otherwise.
As of now it is easy to test it in release-2.1 as well (the all-test target is in AkkaBuild), but I see no problem with updating the version, since this was the only real issue.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/699/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/700/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/700/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/701/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/701/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/698/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/698/
LGTM: thanks for this pull request! Please sign the [CLA](http:typesafe.com/contribute/cla) so we can merge it in.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/694/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/694/
I will also add a section about "ActorRef equality and sending to remote actors" in the migration guide.
Great work, Patrik! Apart from the DeathWatch thingy it looks solid.
See also my mailing list suggestion: https:groups.google.com/d/msg/akka-dev/bXpaIEQ_CdE/gqxbZgqW1ekJ
I have changed DeathWatch, optimized toString and fixed other minor things. Remaining is to go through the documentation (addressing.rst) and add to migration guide.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/712/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/712/
Added the migration guide section
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/716/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/716/
Can you take a look at this so we can merge it. * optimized toString * documentation updates, I'm sure more clarifications and revisions are needed but we also have the related tickets for actorFor and Terminated * changed `toRawString` to to `toSerializationFormat`. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/716/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/716/
after fixing those few small things in the docs: LGTM
toString and toStringWithAddress now use "same" optimized string builder. Fixed the docs. Let me know if you want to review more if I can squash and merge.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/733/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/733/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
thanks Kitty, will fix
added doc Ref vs. Path
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/733/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/733/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
the second kitty failure was a hick-up. Same build number as the first one.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/736/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/736/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/737/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/737/
Can I merge this now?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/693/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/693/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Wonder why that test didn't fail when I ran the tests locally. But at least we found an issue in our own tests :wink:
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/697/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/697/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/691/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/691/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Ah, setting SO_KEEPALIVE is problematic on Linux as well. I should not have reenabled that test.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/692/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/692/
Aside from comments, LGTM!
I don't know what it means, but trust you, LGTM
LGTM (incl +1 for Viktors comment)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/709/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/709/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/689/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/689/
inverted the double-negative logic and prepended "akka." as per Bjrns suggestion
is there any way we can force this to be always enabled when doing a release, because that would mean that I could safely turn it off in my local dev (without having to remember to use different sbt settings for release)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/690/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/690/
just use `sbt -Dakka.genjavadoc.enabled=false` when you start your sbt shell, the release script will not do the same
I still think this is a bad idea. And we should do as we do for the scaladoc plugin.
That means that i have to have two different sbt alias. It doesn't feel that difficult to force it for release mode - I might create a pull request.;-)  /Patrik  On Mar 13, 2013, at 13:27, Roland Kuhn <notifications@github.com> wrote:  just use sbt -Dakka.genjavadoc.enabled=false when you start your sbt shell, the release script will not do the same   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1241#issuecomment-14838283> .
Okay, Ill look into what that could mean. But that will take some time.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/695/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/695/
Ive added `-Dakka.genjavadoc.enabled=true` to the akka-docs2/akka-nightly jobs on the slow jenkins and to akka-nightly/akka-multi-node-nightly on moxie.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/688/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/688/
I should probably have made a bit more of an explicit description: I have taken this across the SBT river as far as I can see, now it would be great if Console experts could chime in.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/688/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/688/
LGTM: great work, Patrik and Endre!
I rebased and fixed the ResendState match
(kitty-note-to-self: ignore 23110633) :cat: Synchronaising! :pray:
Thanks a lot for contributing.  We use a special encoding of the commit messages. Please rewrite the commit message to:      =con Add a few more details to the cluster client docs.  (CLA sign verified)
LGTM Please squash the commits (it is ok to force push to this branch to update the pull request)
PLS SYNCH  (This is to make our pull request validator jump back into action)
(kitty-note-to-self: ignore 23005667) :cat: Synchronaising! :pray:
@taylorleese Thanks for contributing. I will cherry-pick to 2.2.x
Should be backported to 2.2
LGTM (for the code I see; I cannot judge whether you missed some actors)
I searched for all actorOf and systemActorOf inside the remoting project -- should be fine AFAIK
By the way, I think there are some future callbacks that should use this dispatcher as well. NettyTransport has an  implicit executionContext that looks scary
@patriknw that is true, but the Netty driver already has a dispatcher setting, so it probably should use that one. 
you mean `use-dispatcher-for-io` setting. I think that should be reserved for the pure io stuff.
Yes, I referred to that setting. I don't know. The only reason we use another dispatcher and not the threadpool is that we use our nicer futures. But the wrapped futures still run on the io threadpool. I don't see any reason to use the remoting dispatcher here instead of th io one.
ok, convinced, use the io dispatcher there then and check other usages of future callbacks, or scheduled tasks or whatever that requires an ec
You convinced me too, so if dispatcher-io is given, it will use that, if not, it attempts to use the remoting dispatcher, and only finally falls back to the default one.
makes sense, LGTM
(kitty-note-to-self: ignore 23005469) :cat: Synchronaising! :pray:
So what were the symptoms? We have this ticket https:www.assembla.com/spaces/akka/tickets/3064 where the TcpConnectionSpec just freezes.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/589/
The symptom is that this test fails: https:github.com/spray/akka/commit/bbc7ae7132954d1c078fc908230e2c6112ac882e#L0R48
I dont think issue 3064 is related to this bug. The problem fixed with this pull request is that the `TcpListener` does not send an `AcceptInterest` message to its selector if accepting of connections was not stopped by the configured `BatchAcceptLimit`.
@sirthias Thanks.  LGTM
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/589/
Btw, we accidentally fixed the same typos :)
Damn. I thought I can beat you guys to it... ;-)
@drewhk could you do the merge?
What's the status here?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/588/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/588/
I think you are weakening the test too much; if you can add something like my comment then LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/588/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/588/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/590/
Wow, weird. LGTM
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/590/
great find! LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/590/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/590/
I think this is in release-2.1 also  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/596/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/596/
LGTM  A slightly related question, what is the sender when the supervisor strategy is invoked? It would be convenient if it was the the failing child, because then one could define different strategies for different children.
it is; I might as well add this in here as well
Yeah, there is some code that uses that fact ;)
I also added a test case while I was at it (so that it breaks in the obvious way if someone tries to make Failed a SystemMessage ;-) ).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/592/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/592/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/591/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/591/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Failure was in `UnreachableNodeRejoinsClusterSpec`
high time that we nuke that feature and its test 
yes we shall nuke rejoin, but all these failures in UnreachableNodeRejoinsClusterSpec is all about "timeout changing throttler mode"
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/604/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/604/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/603/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/603/
Do we want to permit null messages? Usually logging works better when it includes a log message. :)  Can you change your commit message to match the usual Akka conventions: http:doc.akka.io/docs/akka/snapshot/dev/developer-guidelines.html
I see now that null messages are in master already so ignore the first part of my comment.
Theres no need to delete any repos, starting out at the head of your old branch:      git reset HEAD^^     git commit -am 'fix logging of null messages, see #3086'     git push -f origin <your-branch-name>  Ill merge it even though it has a slightly strange commit message ;-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/602/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/602/
Timing is the only issue? What could have caused the timing difference to older versions?
@drewhk 500 ms margin on the slow jenkins is not something I would trust
That is true. +1
@drewhk you made me think more about this. The log show AskTimeoutException. Shouldn't only Some or None be returned from these methods, no exc. Reopened the ticket. Have fun!
Well played :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/595/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/595/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/608/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/608/
LGTM apart from nitpick
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/607/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/607/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/605/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/605/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/605/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/605/
LGTM apart from the scaladoc for gracefulStop.
fine, another way of doing it that doesn't involve a global singleton is https:github.com/akka/akka/pull/1146 (and that pattern is used at other places too) Just sayin'
Yes, that is actually much nicer. I'll rewrite it.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/555/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/555/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/556/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/558/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/558/
added more info
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/562/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/562/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/557/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/557/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/560/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/560/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
aside from the warning it looks great
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/573/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/573/
LGTM: squerge! (squash & merge)
actually: squergefp (incl forward-port)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/563/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/563/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/566/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/566/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
pls rebuild all
@richdougherty I think that it has to be uppercase. PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/571/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/571/
looking great! carry on!
Thanks! I will apply all of your recommendations.
endre, could you close this PR and squash your commits and rebase onto master and open a new PR?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/25/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/25/
Closing iteration 2. Preparing new PR.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/569/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/569/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Looks good (or does it hide something highly non-obvious?) to me.
It's basically a rename/move/cleanup in the sbt-multi-jvm project according to what we decide at the conference, and a publishing to scalasbt.artifactoryonline.com.  No real code changes in the plugin compared to 0.2.0-M5.  The branch has been tested on scalable1 and it pulled down the dependency from scalasbt.artifactoryonline.com without problems, and ran all tests including multi-node.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/568/
doesnt look bad, but with SBT you never know unless you cut yourself ;-) (translation: I trust you)
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/568/
You shouldn't trust me @rkuhn, I think I just noticed a bug when doing this gist https:gist.github.com/bantonsson/5004611  I added it there if anybody gets an urge to test it out :wink:  It has all versions set to 2.1.1 (which doesn't work yet of course) so you have to change the akka versions to 2.1.0 and the plugin version to 2.1-SNAPSHOT and publish the plugin locally.
Sorry about the confusion. The fix is solid.
Great. Good job.  But you need to provide Java versions of all samples as well. 
Yes, it should be java also (eventually)
Good. I knew I could trust you. :-) Is there a ticket for it? So it is not forgotten.
eventually... pop.. there it is .. https:www.assembla.com/spaces/akka/tickets/2502-java-version-of-cluster-usage-documentation  On Fri, Sep 14, 2012 at 11:58 AM, Jonas Bonr <notifications@github.com>wrote:  > Good. I knew I could trust you. :-) > Is there a ticket for it? So it is not forgotten. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/703#issuecomment-8556696>. > >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
I pushed the fix for LeaderChanged to this branch, https:www.assembla.com/spaces/akka/tickets/2518
Wow. TL;DR ... Superficial code review done, LGTM
I don't expect more review here, so I merge this before continuing with next section. 
Ah, I knew I had forgotten something: the casts are necessary right now, because those operations are not optimally typed in Scala 2.10.0-M7. I have created https:www.assembla.com/spaces/akka/tickets/2504-remove-casts-to-finiteduration so that the casts are removed once we update to the next milestone/RC.
Aside from comments: LGTM!
Unless you object Ill take this to mean that after fixing the relative imports I can merge.
Nothing in cluster?
Nothing much in remoting either, but Ill double-check. (the change is supposed to be very small, so I wasnt surprised)
There won't be any more milestones AFAIK
aside from my nitpicking it looks great 
Sorry about the 'Changes from PR' commit message
Patrik the updates are in.
DataStream duration -> FiniteDuration is in. Removed all 'Duration's aside from this previously.
Looked at the diff once more. Found a few minor things as you can see. @viktorklang, will you take another look at it? I think it is good enough to go in before RC1. It's low risk that it breaks anything else. It can probably be improved a few more iterations, but I think it will be natural to do that when we start building the routers on top of this.  In other words, you got my :+1:. Let's see if Viktor thinks it should go in before or after RC1.
@patriknw Alright, I'll have a stab at it tmro morning.
Latest suggestions applied.
Looks gorgeous!  Please squash all commits into one and merge that in.  Excellently done!
Congrats! Awesome work Helena.  On Fri, Sep 21, 2012 at 2:08 PM, Helena Edelson <notifications@github.com>wrote:  > YaY! > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/706#issuecomment-8762559>. > >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
Awesome to work on the cluster! 
Great job Bjorn. LGTM.
Great work Bjrn, merge it!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/576/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/576/
Added a small fix so we only log log initialization errors directly to the `StandardOutLogger`.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/584/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/584/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/577/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/577/
LGTM (after small fix)
LGTM, and +1 for the fix
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/582/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/582/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/575/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/575/
Id like to keep the doc build warning-free, so please leave the toc in; either set the depth to one (or zero, need to try it out) or move it to the bottom of the page.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/574/
As we discussed, the warning was because of the wrong link, not the   absence of toc-tree, so there are no warnings now -- the toc can be   dropped safely.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/574/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/586/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/586/
I assume you're talking about http:download.java.net/maven/2/com/sun/jersey/contribs/jersey-scala/1.1.0-ea/ ?
Yes. Exactly as well as the jersey lift stuff.  Care to help out with that?   
jersey-scala should be a 15 minute integration. I'll do a read-up on jersey-lift and see what's needed. I should probably have time during the weekend to try to integrate it :)
Also, the code in http:is.gd/1KjhI uses ThreadLocal, I'm assuming that it could be a problem for Actor usage. We just need to make sure that we're on the same Thread when we access the ThreadLocal
jersey-scala integrated here: https:github.com/viktorklang/akka/tree I updated the Scala Sample Service as well
I'm giving up on jersey-lift , I simply cannot get Grizzly to do the right thing (i.e. initialize the LiftFilter properly). I'll move onto try to integrate Atmosphere. But I guess the question is how it's supposed to interact with the services. Perhaps we should have a discussion about it?
Thanks for trying Lift integration out. If it's a problem with Grizzly we could perhaps move to embedded Jetty. Grizzly was just the simplest thing that could work.   Regarding Atmosphere I was simply thinking binding an actor as a Comet actor by annotating it the same way as done with jax-rs. Do you think there will be issues? Write up your thoughts and ideas and I'll try to reply.  (But I haven't looked at Atmosphere impl yet). Thanks a bunch for trying this out. It would be a killer to have Akka actors comet enabled. 
Check the latest commit
is this a keeper?
okay, all fixed. merge?
I've added ThreadLocalRandom from jsr166y into akka.jsr166y.*
Cool. I switch to ThreadLocalRandom. 
Ok. Fixed some of the issues and some new ones. 
Ok. Can we merge now? 
+1 for merge -22 C outside
Cool. I'll merge.  I meant we have +14 C *inside*. Freezing our asses off. 
Ledsen om jag lt lite grinig.  -- Jonas Bonr CTO Typesafe - The software stack for applications that scale Phone: +46 733 777 123 Twitter: @jboner Blog: letitcrash.com  On Feb 5, 2012 9:32 AM, "patriknw" < reply@reply.github.com> wrote:  > +1 for merge > -22 C outside > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/297#issuecomment-3816318 >
Hey man, sorry for the delay, could you sign the CLA (online) so I can accept the PR?  http:www.typesafe.com/contribute/cla
I totally forgot about CLA - I just signed it.
Whats the point of the partial rename? It should be either all or nothing, but unless there is some very compelling reason Id side with nothing here, because pipeTo is semantically clearer and more intuitive than the rather meaningless pipe.
pipeTo(a,b) doesn't denote what gets piped to where. a pipeTo b does however.
`pipe(a, b)` is not clearer than `pipeTo(a, b)`, and Id strongly prefer the names of the imports to be equal so that not two are needed. What concerns the direction: it doesnt make sense to pipe an actor to a future, only the other way around, so the types are what makes the prefix thing really clear. Its exactly the same problem & solution as for ask.
If you see "pipe(a,b)" do do not see what is an actor or what is a future, or if it has anything to do with either actors or futures. You _must_ agree that pipeTo(a, b) is nonsensical from a grammatical point of view. If you want both methods to share name, then you need to come up with something that works for both. Or come up with a better argument for your cause :-)
Well, `pipeTo(a, b)` is exactly at the same level of nonsensical as `+(a, b)`. What I mean to say is that it is quite common to make the transform between prefix and infix notation in Scala. And the situation is exactly the same for ask. Plus, we are looking for a verb, because the name shall designate an action, and the only English sentences which begin with a verb are using the imperative form: pipe A to B.  Hey, wait a minute: `pipe(a) to(b)`. Problem solved. Only one method needed (add .to() to PipeToSupport). This would even get rid of the stupid compiler warning. I think we have a winner! ;-)  If you do that, then please rename all occurrences, including PipeToSupport. Thinking about that, PipeSupport.pipeTo should probably be the only one which retains the preposition.  One thing I wondered all along: why dont we also make forward a pattern, and Im thinking it should be exactly like my proposal above: `forward(msg) to(actor)`. Or scrap it altogether and tell people to use `ref.tell(msg, sender)`.
If pipeTo was commutative as +, then I wouldn't have a problem with it.  pipe(a) to(b) == a pipeTo b? What's the added value?  Good idea with forward, I can definitely buy that.
Haha, I'm a bit daft this evening. you mean:  pipe(a, b) and then have pipe(a) to(b) instead of pipeTo?
The point of the (currently in master) pattern implementations `pipe(future, actorRef)` method was to allow optimizing away the (hopefully HotSpot-elided) allocation of the PipeToSupport.PipeableFuture object if someone wanted that. It will probably much more common that the infix `pipeTo` is used because of the chaining possibilities and general easy on the eyes factor. So, I believe we need the explicit `pipe(a, b)` and the implicit `a.pipeTo(b)`, and the more DSLish `pipe(a) to(b)` could be just thrown in for good measure if we like it.  So, rename the `implicit def pipeTo` also to `pipe`, making it `import akka.pattern.pipe` for everything, and possibly add a `.to(actor)` method on the support class. But, oh my, this would also allow the following:      Future { doSomethingExpensive } to actorRef
I'll take a stab at it tomorrow
I reviewed the code and read your discussion, and can only say that I was also confused of why the double imports {pipe, pipeTo}.
Revised all this, merge?
Alright, last check before merge?
Looks great, +1
I concur, looks good. While youre in there, could you update the Scaladoc for the concrete strategies to not claim to always restart, but instead include something which mentions those directives?
Fixed the Scaladoc also.
I think this is ready, but please take a look, and especially, is there any more place in Future that should be completed with ExecutionException?
Fixed review comments, but the null actor. That is something we will have to investigate further, I guess.
yes, Id say open a ticket for the null actor case and leave it alone in this ticket. Looks good!
Created ticket for the null actor case: http:www.assembla.com/spaces/akka/tickets/1768-actorcell-actor-null-after-fatal-error-in-create
looks good for me :-)
Ok. Thanks guys. Great reviews.  On Tue, Jan 31, 2012 at 3:21 PM, viktorklang <reply@reply.github.com> wrote: > +1 merge > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/281#issuecomment-3739750    --  Jonas Bonr CTO Typesafe - The software stack for applications that scale Phone: +46 733 777 123 Twitter: @jboner
Excellent work Roland, no docs to update?
true, true; just wanted to push before drained battery forces me to go to sleep :-P
I'm fairly sure I removed those a couple of weeks back. but +1 from me
This is from master anyway. So unless you removed them in a branch they are still there. Ok. I'll merge. 
great thanks mate
Awesome to get this in. Can't wait to see it in action.
Committed fixes after review
Just updated to latest sources from Doug and made it the default dispatcher
:+1: Great job!
Great work Patrik!
What caused the problem?
Different call chains for pinned and thread-pool, so it was never done for pinned. I moved the name adjustment to a place which is used by both.
ok, if nobody has no comments I am intending to merge it tonight
time is ticking... anybody has some comments?
oops, too late. sorry I have been very busy. I did look at it briefly (not all) and didnt see any major issues.
Hi,  Thanks for contributing!  have you've signed the Akka CLA? If not, ping me at: viktor dot klang at typesafe dot com and I'll send you the CLA for you to sign and send back.  Also, this fix is only partil, it (yet) doesn't fix this: https:raw.github.com/jboner/akka/master/akka-docs/java/logging.rst
Theres a new form for signing the CLA at http:www.typesafe.com/contribute/cla, we can merge this as soon as youve signed (please add a comment here so we notice).
Hey man, sorry for the delay, could you sign the CLA (online) so I can accept the PR?  http:www.typesafe.com/contribute/cla
Great work Patrik!
good change, keeping the codebase clean after removing things which complicated other things is probably the most distinguishing feature of good development :-)
alright, I'll do the additional improvements when I'm done with the other thing I started
Fixed as discussed
PS I did sign the CLA.
Very nice catch, sort of embarrassing that this slipped through reviews.
:) Thanks for merging!
Have you seen torvalds guidelines on writing good commit messages?  	The body of the commit message can be several paragraphs, and 	please do proper word-wrap and keep columns shorter than about 	74 characters or so. That way "git log" will show things 	nicely even when it's indented.
aside from my comments LGTM
Aside from the comments, _excellent_ work Eugene!
Great, thanks Eugene!
wow, that must have been exhausting 
Fixed after review comments
I really like this sample, because it is non-trivial. We need more stuff like this!
Looks very cool, nicely done Patrik!
Ping me when it's merged Patrik, so I can commence the release.
Merged. I will do the Java equivalent in M4, ticket #1722  On Tue, Jan 24, 2012 at 4:39 PM, viktorklang < reply@reply.github.com > wrote:  > Ping me when it's merged Patrik, so I can commence the release. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/257#issuecomment-3634268 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
I don't see what broke the build, no tests were modified. I did run them prior to the push.
@helena No Worries. The failure is unrelated. I've created a ticket.  https:www.assembla.com/spaces/akka/tickets/2582
I introduced a bug in previous refactoring, I'll update test and fix.
LGTM, have you ran the test (with timing tests turned on) and verified that it works?
I kicked it on scalable1: successful
Yes I verified it  Sent from my iPhone  On Oct 7, 2012, at 7:17 AM, Viktor Klang () <notifications@github.com> wrote:  > LGTM, have you ran the test (with timing tests turned on) and verified that it works? >  >  > Reply to this email directly or view it on GitHub. > 
Thanks!  Sent from my iPhone  On Oct 7, 2012, at 8:43 AM, Patrik Nordwall <notifications@github.com> wrote:  > I kicked it on scalable1: successful >  >  > Reply to this email directly or view it on GitHub. > 
Im sorry for the delay, just a heads-up: Ill look into this when I come back from vacation next week.
Again, Im sorry it took a bit longer to get back to you, Rick, but I think this is nearly there. After fixing the few little things I commented on, please merge master into this branch and after you push make a comment on this pull request containing the text `PLS REBUILD ALL` to verify that it compiles and passes all tests.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/59/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/59/
Corrected and fixes pushed as commented.  PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/188/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/188/
fixed imports  PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/189/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/189/
The Build kitteh is not a replacement for compiling and testing locally. Please compile and test locally before pushing.
Yeah, I know. But building locally with sbt doesn't show any errors. I'm not used to sbt. Sbt compile seems to compile only scala sources. I don't know how to build the project then.
The build KITTEH actually merges your branch with master and builds the result: there have been changes which you need to adapt your code to, since your branch is quite old. Please merge master into your branch, start SBT anew and build again, you should then see the same failures locally.
While checking your changes (all looking good apart from the kitteh failure, thank you!) I see that you already did that merge; could you possibly have left SBT running without a reload?
In maven you say simply "mvn clean install" - that's all. As I noted. I don't know sbt and documentation is very sparse. By accident I found something like build-release. What is the workflow to build akka with sbt?
see [here](http:doc.akka.io/docs/akka/2.0.4/dev/building-akka.html#Building_Akka): just `sbt test` would do it (which runs ALL tests and takes half an hour or so). You can select a closer scope by saying `sbt akka-docs/test` (since your work is only in that sub-project.
Local tests passed. Try again...  PLS REBUILD ALL
Thanks, the KITTEH will hopefully pick it up within the hour.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/190/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/190/
Thanks for submitting this! The comments I made are meant to make it a bit more generic.  Could you please also sign the [CLA](http:typesafe.com/contribute/cla)?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/24/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/24/
I have added `-Dakka.test.tags.exclude=long-running,timing` to all akka jobs at jenkins.typesafe.com so it is prepared for merge.
I sneaked in a fix of the remoteConfig for sbt console also. RemoteActorRefProvider was not defined correctly.
You can wait with reviewing this. There will be another change, which will make this super awesome.
Thanks for the suggestion Roland. Now it is super slick!
Very nice indeed! Is this odcumented in the migration, stuff?
yes, I wrote a section int migration guide
switched the settings to on in the docs
change all off to on, and add the ref apart from that, great with this summary
Fixed both DEBUG and ref to testkit
Apart from the one issue I think this sample is demonstrating a lot of good practices.
Aside from the comments it looks great, thanks Patrik!
Closing this without merge. Started new pull req, rebased on master https:github.com/jboner/akka/pull/257 After review I will convert to java.
The return value is only used in one place `akka-actor/src/main/scala/akka/actor/dungeon/Children.scala` line 195, where the whole bug/fix originated.  Should probably be removed to be symmetric with the other lifecycle calls.
It looks good to me, but I cannot claim complete understanding of the context.
Is this ready, guys?
I did one PR and then remembered seeing the akka dev guidelines about rebasing in git. I ended up closing the other PR in an attempt to figure out how rebasing works.  Also, I have applied the implementation changes you suggested.
The parameter list for `nodeConfig` is nice.  I'm not sure if the advanced `runOn` and fNode` syntax adds that much value. Almost same as the more explicit:      val ports = Map(send -> "1991", receive -> "1992")     runOn(send, receive) {         system.settings.config.getString("akka.remote.netty.port") must be(ports(myself))     }       By the way, `ifNode` syntax should be improved: https:www.assembla.com/spaces/akka/tickets/2126
so instead of:  ```scala ifNode(send, receive) {          "1991"       }  {         "1993"       } ```  you'd favor  ```scala if(isNode(send, receive)) {          "1991"       } else {         "1993"       } ```  I like the more advanced runOn syntax since I think that situation would come up fairly often.  I agree that the isNode version of ifNode is preferable and my advanced ifNode syntax now seems like overkill to me and feels cleaner as a match statement.  ```scala port = myself match {      case send => "1991"      case receive => "1992"      case _ => "1993" }  system.settings.config.getString("akka.remote.netty.port") must be(port) ```
I like `isNode` but we should be aware of that it can be written as      if (Set(send, receive) contains myself) "1991" else "1993"  
the pattern match is also nice, but remember backticks, or name the role names with initial upper case
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/23/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/23/
@rkuhn please comment, we talked about this, but I'm not sure if we decided anything specific  My opinion: - vararg parameters to nodeConfig: OK - tuple params to runOn: No - remove `ifNode` - add `isNode`  
@patriknw I agree: better remove all custom control structures and use normal `if` expressions, the gain of the custom syntax is not worth its own cognitive weight.
Superseded by pull request #837 
When I look in the project I see `ReliableProxySpec` using `blackhole` as well, but it's not in the diff.  Apart from that. LGTM.
My PR job told me so also :-) Thanks for noticing. Fixed.
Yes, LGTM. The only surprising thing is that so few tests needed to be changed.
nice work Nikolay and Roland
Aside from my comments I think it's cool
okay, fixed all non-disputed comments, merging now ;-)
looks good, now I just should not forget to merge that into the remote-cleanup branch
Fixed everything that was commented
Looks great to me!
Fixed everything you excellently commented. That's what I get for doing that way too late :-)
I must apologize.  I did not read the Developer Guidelines until after I submitted this pull request.  I'll redo this, but I might not get to it until next week.
thanks for contributing, but this test is not verifying very much one should run more advanced things, including remoting and stuff to be sure real applications are invaluable for testing this, and jconsole to look at threads
Agreed, also with ticket 1693 there's a one-stop-shop for daemonicity
Thanks for the responsiveness,  WDYT about something along these lines?      diff --git a/akka-actor/src/main/scala/akka/actor/Scheduler.scala b/akka-actor/src/main/scala/akka/actor/Scheduler.scala     index 02c67c6..153c70a 100644     --- a/akka-actor/src/main/scala/akka/actor/Scheduler.scala     +++ b/akka-actor/src/main/scala/akka/actor/Scheduler.scala     @@ -9,7 +9,7 @@ import akka.util.internal.{ TimerTask, HashedWheelTimer, Timeout <E2><87><92> HWTimeout,      import akka.event.LoggingAdapter      import akka.dispatch.MessageDispatcher      import java.io.Closeable     -import java.util.concurrent.atomic.AtomicReference     +import java.util.concurrent.atomic.{ AtomicLong, AtomicReference }      import scala.annotation.tailrec      import akka.util.internal._      import concurrent.ExecutionContext     @@ -137,14 +137,17 @@ class DefaultScheduler(hashedWheelTimer: HashedWheelTimer, log: LoggingAdapter)          val continuousCancellable = new ContinuousCancellable          continuousCancellable.init(            hashedWheelTimer.newTimeout(     -        new TimerTask with ContinuousScheduling {     +        new AtomicLong(System.nanoTime + initialDelay.toNanos) with TimerTask with ContinuousScheduling {                def run(timeout: HWTimeout) {                  executor execute new Runnable {                    override def run = {                      receiver ! message                       Check if the receiver is still alive and kicking before reschedule the task                      if (receiver.isTerminated) log.debug("Could not reschedule message to be sent because receiving actor {} has been terminated.", receiver)     -                else scheduleNext(timeout, delay, continuousCancellable)     +                else {     +                  val driftNanos = System.nanoTime - getAndAdd(delay.toNanos)     +                  scheduleNext(timeout, Duration.fromNanos((delay.toNanos - driftNanos) max 1), continuousCancellable)     +                }                    }                  }                }     @@ -162,11 +165,12 @@ class DefaultScheduler(hashedWheelTimer: HashedWheelTimer, log: LoggingAdapter)          val continuousCancellable = new ContinuousCancellable          continuousCancellable.init(            hashedWheelTimer.newTimeout(     -        new TimerTask with ContinuousScheduling {     +        new AtomicLong(System.nanoTime + initialDelay.toNanos) with TimerTask with ContinuousScheduling {                override def run(timeout: HWTimeout): Unit = executor.execute(new Runnable {                  override def run = {                    runnable.run()     -              scheduleNext(timeout, delay, continuousCancellable)     +              val driftNanos = System.nanoTime - getAndAdd(delay.toNanos)     +              scheduleNext(timeout, Duration.fromNanos((delay.toNanos - driftNanos) max 1), continuousCancellable)                  }                })              },
Be careful to prevent negative drift (if the task is potentially longer than the scheduling period). 
Patched with suggestion by Viktor. Tested...  A negative drift is only possible when execution is ahead of schedule. It's very unlikely because of the way wheel timer works. Anyway, if there is a negative drift, then delay before the next execution will be longer (delay - negative_drift). This will take execution back to schedule.  If you mean negative delay, not negative drift, then the "max 1" guards for negative delay in the expression (delay.toNanos - driftNanos) max 1. So delay will never be less than 1 nanosecond.  I've tested corner cases (like: frequency << tick-duration). It works fine.
I meant negative delay, sorry. I missed the "max 1" statement. Looks good to me!
I like this, because it makes it possible to remove my implementation of it `akka.cluster.FixedRateTask`. I tried this new scheduler with `akka.cluster.FixedRateTaskSpec` and it pass the test fine. :+1:
Yupp, there was implicit conversion. Unfortunately FiniteDuration.max isn't an option as well since its return type is Duration, not FiniteDuration and I don't want asInstanceOf... I replaced it with Math.max which will be inlined by JVM...  (computation of delay.toNanos might also be optimized (moved out of scheduling loop), but I am unsure we want to sacrifice readability in sake of tiny bit of performance)
Duration.max is fixed in Scala, but the fix is not part of 2.10-M7; we're waiting for RC1 to come out to get rid of some casts to FiniteDuration.
:+1: good job!
:+1: Good job!
I'm assuming that all you guys currently working on the Camel stuff review eachothers code, I'm way too busy at the moment.
Great, thanks.  Will haev more look next week, prepping for M3 now, docspree tomorrow, craploads of stuff to do
Generally you'll open a pull request per ticket.
And don't forget to remove the remote branch once the pull request is merged.
LBATM (Looks BadAss To Me)
merged after fixing review comments
This is ready to go in, but I guess I wait until PR #725 is in, since I refer to stuff there.
#725 is in
only genuine with the backing vocals of the Klang (singing What is love?)  I trust that you have taught jenkins this new trick?
LGTM, even though I can't hear the music
The idea here is that since neither publish repo nor maven central is set, it should publish to default and then rsynch like the others.
Aside from comments, :+1:!
I wonder if we should have some kind of Gotchas section. Thinking of non obvious things like: - don't shutdown the first node, since that is the controller - shutdown can only be done from the first node, the controller - don't ask for `node` (address) of a node that is shutdown, grab the `node` before shutting down - don't use player/conductor from other threads than the main test thread, such as from inside an actor, future, scheduled task
:+1: Perfect!!! I will refer to this when completing the "how to write cluster test" https:www.assembla.com/spaces/akka/tickets/2437
The gotcha section is an awesome idea. Will add it after the sample.
please add a description of how to run a single test      multi-jvm:test-only sample.multinode.MultiNodeSampleSpec
Sure, it's in the multi-jvm documentation, but it never hurts to repeat it.
All review comments have now been addressed
pull request is still not automatically mergeable, have you've based it on the wip-camel branch?
It's finally done now... Thank you git for being so simple(NOT :( ). And thank you documentation writers for adding all these comprehensive examples of how to do thinks in git.(sarcasm...)
Git is extremely simple, which is not the same thing as being easy :-)
great work :-)
Alright, for me to be able to merge this into master it needs documentation, both rst docs and ScalaDoc. Is it feature-complete otherwise?
Please note that we're going to attempt to ship M3 tomorrow evening so we're in a bit of a hurry if this is going to make it in there (which I'd love).
I think it's feature complete. I don't know if I'm able to make it with the documentation by tomorrow evening though. I'll copy what was there for 1.3 and change it to be correct for the rst.  intellij really needs a gen doc refactoring :)
Don't forget ScalaDoc :-)
Nicely done Ivan, hope to see you again at ScalaDays :-)
I've copied most of the docs from the zeromq api docs, they had far more inspiration than I did.  But scaladocs and rst docs are present now
And they are correct? (no rest for the wicked)
yes they should be correct AFAIK.  For the rst document I changed what was in the 1.3 branch
You mean "release-1.3"?
Very high code quality all over, great work!
while this looks good, shouldnt we also change the reference.conf default? (or did I miss that one?)
I vote for changing reference.conf  size-max = 4096 to size-max = 64  Still good to have rather big size-factor, since I would guess that many use IO.
+1 for using max 64 by default,  think the size factor should be more like 3
I changed reference.conf. All fine with the new values?
+1 :) merge
Apart from question. LGTM.
have you checked plugins.sbt? it might be correct, just asking
Yeah, nothing too bad there, it also doesn't affect the generated poms of our artifacts.
LGTM   19 sep 2012 kl. 16:21 skrev Viktor Klang () <notifications@github.com>:  > Yeah, nothing too bad there, it also doesn't affect the generated poms of our artifacts. >  >  > Reply to this email directly or view it on GitHub. > 
What are the prerequisites to using schoir?
Having N machines with ssh access to the outside world and that are able to communicate to each other on any port.
Works on Windows?
Also git and sbt should be running.
I tested the master on cygwin, but the actual slaves only on linux.
The docs should probably include setup instructions.  --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
There's not much to setup, but I'll add what is required.
Great, thanks, it's just going to be less hassle on the ML if it's clear in the docs
Btw, did you see this: http:www.assembla.com/spaces/akka/tickets/1468
Yes, did you see my comment? :)
Commented on the ticket :-)
Please create tickets for the different things that needs to be done, as we've talked about in the comments. Prefix the name with CAMEL:
Alright, so here's what I think should be done.   Create a branch @jboner/akka named "wip-camel" based on current master, then redo the pull request onto that branch, then merge the pull request into that.  Then, when the Camel stuff is ready for testing by users, and all junk is removed and there's rudimentary docs, we merge it into master. We're going to release M3 _real_ soon, and by the number of open ticket for Camel, my guess is that it won't make it, but I'd gladly be proven otherwise.  WDYT? Workable?  Who of you guy don't have any commit-rights to jboner/akka? Piotr and Nene?
Re timeline: As far as untyped actor goes, it looks to me that if the producer part is ready, then the rest should be cosmetics. So, there is a chance. Raymond, Dhananjay, how far are you with your tickets? Do you need a hand? Viktor, what happens if we miss it? What's the next date?  Re commit-rights: I don't have them yet. Re branch: could you please create one for me in the meantime. I'll redo the pull request.
I've created and pushed the branch and e-mailed Jonas to add you guys.
I've just sent a pull request to wip-camel
I have commit rights on jboner/akka
I'll wait for the pull request to finish, and then break up my work in tickets, and do pull requests per ticket, on wip_camel.  Is that correct guys? I'm going to have a lot of time tomorrow to do this, so let me tell you how it goes end of tomorrow (about the M3 release)
Ray: Yes, that sounds like a good plan.
Added you guys. 
I love you man
I've changed the semantics to cascade only failures.
Can I merge now plz?
u can haz me smiley: :)
Looks like something you're really enjoying ;-)  +1 from me
well, I certainly enjoy good debug output when encountering bugs ;-)
okay, feedback was good (also necessary) and incorporated. send smileys my way ;-)
is one + enough for merge?
Not all +'s are created equal ;-)
Aside from my comments, all looks fine
ok to merge?
See my latest comment: https:github.com/jboner/akka/pull/170
Fun to see this take shape for 2.0!
How are things going here guys? I'd _love_ to be able to ship this with 2.0-M3 that is due at the end of this week, will it be ready?
I addressed all issues except for the restarting of a concurrent socket actor.  The documentation for 1.3 is wrong too because the restart of the socket loses all configuration for that socket, as they are all added through sending messages. So to fix that it will need to change to be a constructor parameter I guess  I did make the last changes on a different branch, I rebased on master out of habit but I think that messes with the history of this pull request. https:github.com/casualjim/akka/tree/2.0-zeromq-ivan
This is ready for a second round of review.
any thumbs up or down here?
Y U NO LEADER? :(
merge with current master and push, so we can just skip the mem verification, I take it.
I think I misread the JMX docs regarding a heap mem max assertion in relation to used and committed, by reading an 'and' to be a plus. So the change relects that with the addition of '='  I had not seen it proved otherwise in my env.
Build is back to normal again :-)
Sorry about that!
Apology accepted :-)
Shouldn't there be something in the rst docs also? A separate directory. An index file listing the contribs, and similar description as in the README.md
I think experimental/index.rst should be adjusted also. Remove the last sentence, starting with "Another reason for marking a module..". That was written with contribs in mind.
Added a full-fledged pattern impl to akka-contrib and integrated with the latest and greatest sbt-site plugin (not yet published, need to clone and publish-local to make this branch run).  Please review.
I'll review the pattern tomorrow
Great. I like it! The pattern solves a specific problem in a nice way. We should just remember that it doesn't solve end-to-end guaranteed delivery, and that is clearly stated. That should be done at the application level.
When I tried out the branch, it looked like the akka-contrib tests weren't compiled by the normal test:compile. Is that intentional?  There was also an error scalariform for akka-camel during test:compile.   Otherwise great example. LGTM!
no, good catch!
I redid the branch, because there was some weirdness going on (those camel commits were not supposed to be in there, for one); force-pushed.  The comments should all have been addressed, now this needs to wait until sbt-site is published.
* Rebased and all tests run. * A dry-run release done. * Change jenkins `akka-docs` job to `sphinx:generate` instead of `sphinx`.  Merging...
I have done some changes based on review feedback. Please review again. Don't want to mess up the Guardian.
even though we have some implicit checks, it would be nice to add a direct test case which exercises this logic; especially since the implicit check does not lead to a failed test result
I think I added such a test in ClusterDeathWatch, see PR #749
that only tests the AddressTerminated business, but not the code which is supposed to properly terminate the remote-deployed children when the remote system is `.shutdown()`.
got it, I will add a test for ordinary shutdown
Fixed review comments. - lock for removeChild - test for shutdown - test for Switch
LGTM.  Great with a test even though the `verifyShutdown` flag isn't that clean. No idea how to solve it otherwise.
Several LGTM. Does that mean that I can merge, or do you need more time to review it?
Its about as dirty as it gets ;-)
All fixed. Merge or more review?
Go ahead and merge, +1
First, thanks for your great feedback and this contribution, I and others will review it with great interest. Second, have you signed the Akka CLA? We need that to be able to accept contributions.
No, how do I sign the CLA?
Great work! You mentioned other obligations, hence my question: shall I take it from here or do you want to polish and merge it?
Please feel free.  I just sent the signed CLA to Patrick. I saw there are quite a bit of comments. If it was left to me I could not get to this until this or next weekend.  -Nikolay    ________________________________  From: Roland Kuhn <reply@reply.github.com> To: Nikolay Botev <bono8106@yahoo.com>  Sent: Monday, January 9, 2012 4:27 AM Subject: Re: [akka] Ask 2 (#201)   Great work! You mentioned other obligations, hence my question: shall I take it from here or do you want to polish and merge it?  ---  Reply to this email directly or view it on GitHub: https:github.com/jboner/akka/pull/201#issuecomment-3410745
Okay, pulled into my own clone, will push to akka repo once Im done with a few finishing touches and will then open a new pull request.
No comments makes me sad :(
What does it solve? What is the ticket?
https:www.assembla.com/spaces/akka/tickets/1612  But only by accident, what it really does is that it clears up some accidental complexity within the active remote client.
Ok, I have nothing to add. Looks good.  On Tue, Jan 10, 2012 at 9:57 AM, viktorklang < reply@reply.github.com > wrote:  > https:www.assembla.com/spaces/akka/tickets/1612 > > But only by accident, what it really does is that it clears up some > accidental complexity within the active remote client. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/205#issuecomment-3427014 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Needs a squash (to remove all those merges) on your branch do: git pull git merge origin/master git reset origin/master git add -A git commit -m "Wonderful message about how this is the cluster metrics" git push origin name-of-your-branch  That should do the trick
Never refactored a file name after removing the ClusterMetricsCollector trait. Refactoring now.
[info] MetricsCollector  [info] - must not raise errors when attempting reflective code in apply (1 millisecond) [INFO] [09/24/2012 21:31:33.557] [pool-6-thread-8] [MetricsCollectorSpec(akka:MetricsCollectorSpec)] Hyperic SIGAR was not found on the classpath or not installed properly. Metrics will be retreived from MBeans, and may be incorrect on some platforms. To increase metric accuracy add the 'sigar.jar' to the classpath and the appropriateplatform-specific native libary to 'java.library.path'. [info] - collect accurate metrics for a node *** FAILED *** (11 milliseconds) [info]   1489316104 was not less than or equal to 954466304 (MetricsCollectorSpec.scala:100) [info]   org.scalatest.exceptions.TestFailedException: [info]   at org.scalatest.matchers.Matchers$class.newTestFailedException(Matchers.scala:155) [info]   at akka.testkit.AkkaSpec.newTestFailedException(AkkaSpec.scala:54) [info]   at org.scalatest.matchers.MustMatchers$MustMethodHelper$.mustMatcher(MustMatchers.scala:884) [info]   at org.scalatest.matchers.MustMatchers$LongMustWrapper.must(MustMatchers.scala:1278) [info]   at akka.cluster.MetricsCollectorSpec$$anonfun$2$$anonfun$apply$mcV$sp$6$$anonfun$apply$mcV$sp$10.applyOrElse(MetricsCollectorSpec.scala:100) [info]   at akka.cluster.MetricsCollectorSpec$$anonfun$2$$anonfun$apply$mcV$sp$6$$anonfun$apply$mcV$sp$10.applyOrElse(MetricsCollectorSpec.scala:85) [info]   at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:37) [info]   at scala.collection.TraversableLike$$anonfun$collect$1.apply(TraversableLike.scala:283) [info]   at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:130) [info]   at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:286) [info]   at scala.collection.TraversableLike$class.collect(TraversableLike.scala:283) [info]   at scala.collection.AbstractTraversable.collect(Traversable.scala:105) [info]   at akka.cluster.MetricsCollectorSpec$$anonfun$2$$anonfun$apply$mcV$sp$6.apply$mcV$sp(MetricsCollectorSpec.scala:85) [info]   at akka.cluster.MetricsCollectorSpec$$anonfun$2$$anonfun$apply$mcV$sp$6.apply(MetricsCollectorSpec.scala:79) [info]   at akka.cluster.MetricsCollectorSpec$$anonfun$2$$anonfun$apply$mcV$sp$6.apply(MetricsCollectorSpec.scala:79) [info]   at org.scalatest.WordSpec$$anon$2.apply(WordSpec.scala:2179) [info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974) [info]   at akka.testkit.AkkaSpec.withFixture(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$class.invokeWithFixture$1(WordSpec.scala:2176) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2185) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2185) [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198) [info]   at org.scalatest.WordSpec$class.runTest(WordSpec.scala:2185) [info]   at akka.testkit.AkkaSpec.runTest(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2250) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2250) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249) [info]   at scala.collection.immutable.List.foreach(List.scala:309) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:265) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249) [info]   at scala.collection.immutable.List.foreach(List.scala:309) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249) [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326) [info]   at org.scalatest.WordSpec$class.runTests(WordSpec.scala:2250) [info]   at akka.testkit.AkkaSpec.runTests(AkkaSpec.scala:54) [info]   at org.scalatest.Suite$class.run(Suite.scala:2303) [info]   at akka.testkit.AkkaSpec.org$scalatest$WordSpec$$super$run(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2297) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2297) [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362) [info]   at org.scalatest.WordSpec$class.run(WordSpec.scala:2297) [info]   at akka.testkit.AkkaSpec.org$scalatest$BeforeAndAfterAll$$super$run(AkkaSpec.scala:54) [info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213) [info]   at akka.testkit.AkkaSpec.run(AkkaSpec.scala:54) [info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:180) [info]   at org.scalatools.testing.Runner2.run(Runner2.java:16) [info]   at sbt.TestRunner.delegateRun(TestFramework.scala:57) [info]   at sbt.TestRunner.run(TestFramework.scala:51) [info]   at sbt.TestRunner.runTest$1(TestFramework.scala:71) [info]   at sbt.TestRunner.run(TestFramework.scala:80) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:190) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:178) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:121) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:121) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) [info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info]   at scala.collection.immutable.List.foreach(List.scala:76) [info]   at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) [info]   at scala.collection.immutable.List.map(List.scala:76) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:121) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:121) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$5.work(System.scala:71) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232) [info]   at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) [info]   at sbt.Execute.work(Execute.scala:238) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232) [info]   at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [info]   at sbt.CompletionService$$anon$2.call(CompletionService.scala:30) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:166) [info]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:166) [info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [info]   at java.lang.Thread.run(Thread.java:679)
Ticket created: https:www.assembla.com/spaces/akka/tickets/2546
Not fixed yet
Added more failing tests and fixed them. Not too proud of the solution. The whole `SubclassifiedIndex` and the cache should probably be cleaned up/rewritten. 
diff --git a/akka-actor/src/main/scala/akka/event/EventBus.scala b/akka-actor/src/main/scala/akka/event/EventBus.scala index 07c49f9..e61f1d3 100644 --- a/akka-actor/src/main/scala/akka/event/EventBus.scala +++ b/akka-actor/src/main/scala/akka/event/EventBus.scala @@ -159,7 +159,7 @@ trait SubchannelClassification { this: EventBus     }      private def removeFromCache(changes: Seq[(Classifier, Set[Subscriber])]) { -    cache ++= changes.map { case (c, s)  if (cache.contains(c)) (c, cache(c) -- s) else (c, Set.empty[Subscriber]) } +    cache ++= changes map { case (c, s)  (c, cache.getOrElse(c, Set.empty[Subscriber]) -- s) }    }
As part of this ticket we should also fix the issue with remote deployed routees, see the ticket. https:www.assembla.com/spaces/akka/tickets/2535?comment=168349053#comment:168349053
We solve the other pieces as separate tickets, so merging this.
Another try in a few moments...
This is horrible, can't you search/replace "\r\n" => "\n"?
what do you mean? the "\r" in there is the whole point of the change. Or is that not necessary anymore on certain broken platforms?
perhaps I have misunderstood the problem. I thought the \r was added by multi-line string at compile time on windows, and similar to stripMargin, we could add a replace so that the code is maintainable
no, the multi-line string picks up whatever source file encoding the last editor used, whereas the current solution fixes things in a really obvious way
> perhaps I have misunderstood the problem. > I thought the \r was added by multi-line string at compile time on windows, and similar to stripMargin, we could add a replace so that the code is maintainable  The problem was that on Windows \r was added on the *nix script, and on *nix the \r was omitted on the Windows batch file. Now it is ugly, but generates the correct output no matter what. We can probably find a better solution though.
ok, I still think that s.replaceAll("\r\n", "\n") and s.replaceAll("\n", "\r\n") is more obvious than all the \ encoding in the string, anyway - you have a release to cut
> ok, I still think that s.replaceAll("\r\n", "\n") and s.replaceAll("\n", "\r\n") is more obvious than all the \ encoding in the string, anyway - you have a release to cut  Except the replace should be platform dependent, since the multi-line string will not contain \r\n on *nix, and will not contain lonely \n on Windows. So we need some fixLineEndings(stringToFix, isInputWindows, isOutputWindows) that fixes all four cases.
Aside from missing test, LGTM
I did not mean to close this. It just... happened... somehow...
LGTM, nice with a simple bug for once
Yes, thank you for the help, Patrik
nice one :-) LGTM
Nice! Maybe a few words in the RST docs?
LGTM (just to make it appear in the issues page)
LGTM, please backport to release-2.2
regarding the RST docs; the changed code samples are shown in rst, I think that is enough
This is ready for merge, and backport to release-2.2
backported to release-2.2
LGTM, only reds -- freakin' awesome
Yes, thanks to Bjrn, who spent all the greens on this ;)
LGTM, please backport to release-2.2
LGTM: I would have forgotten that _other_ file again 
This is ready for merge, and backport to release-2.2
Included ticket number in commit. Now it is ready for merge, and backport to release-2.2
does this want to be backported? to rephrase: does the 2.2.0 release process benefit from this?
it is not needed, it can wait
in that please merge and add a ticket for backport to 2.2.1
ok, backport ticket created
All code in docs must be compiled and imported into the text, see the other .rst files for info on how it works.
code extracted - ready to roll
Coolio, will have a look tmro morning. Thanks!
Would love to merge it in, but is not possible, could you catch up with master? Thanks!
Excellent cleanup and bugfix @patriknw!
LGTM: nice boy-scouting! Please backport to release-2.2.
This is ready for merge, and backport to release-2.2
backported to release-2.2
what do you think about also describing how to change the default mailbox to the MPSC mailbox, and there also highlight its limitations?
LGTM, after alignment of scala/java docs create separate ticket for how to change default mailbox
@patriknw but there's docs for both java and scala here?
yes, but they don't look the same for no reason. You added for example `Configuration name` bullets to the java docs only
backported to release-2.2
in principle LGTM, but lacking docs (RST and reference.conf)
please use the correct commit message encoding 
Wicked :-) Thanks!
I like extending the sbt prompt but I also think prompt changes are better in your own global.sbt - works across all sbt projects and can be customised per user. 
How about having this one as the official Akka prompt and then one can override it in his/her own global.sbt...
Sure. Sounds good.
Somehow I like your version better than the current, so Ill base my upcoming fix on this instead of on master.
I added the LARS fix to this pull request
Don't forget to remove the unused shift variable at line 513 (or wherever it ended up now)
This means that we will always log: * AssociationErrorEvent * RemotingErrorEvent  Could that be too much? How does it look like if you run the simple cluster sample, and don't start the first seed node immediately?
Here is another example of logging that can be annoying: http:stackoverflow.com/questions/17360303/akka-remote-system-shutdown-leads-to-endpointdisassociatedexception  My opinion is that we should log the serialization and oversize messages, but not life cycle events at error level unless log-remote-lifecycle-events=on (default).
That kind of logging should not happen now, since the SHUTDOWN reason is sent with the Disassociate message.
what about deserialization errors in the EndpointReader? Otherwise LGTM
What about them?
they are not handled symmetrically with serialization errors in the EndpointWriter: a missing deserializer (or a missing message class) will be published as association error and fail the whole thing, but that seems like wielding a really large hammer to me
It was a decision before that we terminate associations in case of serialization errors.
thinking more about this: any possible change of the readers error handling would be too large for 2.2.0, so if you do something along those lines please open a new PR for that
So you want to fine-grain deserialization errors?
? serialization errors are called transient and ignored (but logged) while deserialization errors are currently fatal 
Originally all serialization errors were fatal, but the team wanted the serialization case to be transient. The case of deserialization is different, since if it is caused by framing errors then that association is completely broken and can be only recovered by reassociating. 
my assumption is that below serialization we still have framing (actually multiple layers, since deserialization errors ending up in that spot occur after having decoded the out protobuf message envelope already), meaning that the failure is transient more likely than not; but we should not worry about that right now, please create a ticket on Rollins to revisit this  Summary: this one LGTM
No, if a framing header is corrupted (bit-flip) then the framing _still_ decodes the frames but with incorrect length, and every subsequent frame will have incorrect length -- causing for example ProtoBuf to choke on each and every message after. Also, fine-graining the deserialization errors is problematic since the exceptions can be Serializer specific without any preliminary knowledge.
But in any case, we should sit down and decide on what should be logged at what level, under what conditions, and what should be published as an event, what should cause association abort and what should not, what should be logged during startup/shutdown, what is considered transient, what is considered fatal, what should be configured and to what extent. And _document_ all this.
which is why I usual favor a magic word in combination with enforced message size limits, giving enough redundancy to make framing reliable enough; and yes, we should sit down after the vacation period and properly talk all this through
I definitely think reassociating and therefore resynching framing fits the "reliable enough" criteria :)
nope, you didnt get my point: deserialization errors may well be due to wrong but correctly transferred content, where reassociation is neither necessary nor beneficial
How do you define wrong but correctly transferred content? What kind of exception from a Serializer counts as wrong but correctly transferred content?
All of them, that is my point. What you are proposing amounts to a layering violation, the transport is responsible for transporting the bytes, the Akka layer is responsible for serialization/deserialization.
Ok, but then we need to add some kind of error detection code to the akka protocol. That is doable of course.
could you create a ticket so we dont forget?
Tickets were created, this one is ok to backport and merge
backported to release-2.2
CLA signed. :-)
Hey man, I'll give this a final review tomorrow before I merge it in.  Thanks!
Great - thanks Viktor!  The next thing I'd like to add to the ByteString classes is a custom iterator, also changing a few other things to remove boxing/unboxing for many operations (no API changes, but maybe some additions). I have something similar in my current project, and I've done some comparisons with ByteString - if all goes well, this will speed up some things by almost an order of magnitude. I was waiting whether CompactByteString will be accepted in principle, so I know what code to base it on. This will be a new branch and merge request, of course, hope I can show a prototype next week or so.
the Java side of things might also want to get this treatment; otherwise LGTM
The Java doc did not mention mailbox config. What should I add there?
the feature is the same, so the docs should also be the same; that they were not to begin with is something that is covered by the boy-scout rule ;-)
The language is not the same ;)
So I compared the two, and all the differences were language specific, except the note that I added to scala. Since that note mentioned a marker trait, I watered down that description in the Java docs.
looks good, could you squash so I can merge & pick?
backported to release-2.2
adjusted according to suggestion
This is ready for merge, and backport to release-2.2
backported to release-2.2
LGTM; backport to release-2.2
This is ready for merge, and backport to release-2.2
backported to release-2.2
as usual: backport to release-2.2
fixed the info box (`note::`)
This is ready for merge, and backport to release-2.2
backported to release-2.2
Sorry man, I can agree on having side-effecting methods have parens. I sort of let the previous PullReq slide since impact was low and it's not our own method (currenttimeMillis), but you'll have to sell me the idea that we should treat referential opaqueness as side-effecting.
I don't care about whether being not referentially transparent should be called side-effecting or not. What bothers me is the fact that a def without parentheses looks like a val. This is misleading. Didn't you already see issues with passing the sender (should be sender()) along to some computation that might happen in the future and/or in another thread?  To speak with Daniel S. (who shares my point here): In my opinion not using parentheses is just wrong.  By the way: If you look at the Akka code, you will see that some occurrences of the methods I fixed already used parentheses. You should at least be consistent.
You'll be able to use it without parenthesis no matter if it is declared with or without parenthesis.
Sure, this is a little annoying. Hence it is even more important to be consistent on the definition side!  What's the best documentation? Code! While the majority of the Akka code shows excellent style, it offers poor documentation in this regard.  Also look at autocompletion in IDEs: If you use the parentheses they will show up in the autocomplete preview, at least in the latest Scala IDE for Eclipse.
The problem is that it will offer no help whatsoever, since you do not immediately understand when you're closing over "this" or not. IMHO the problem of capturing "this" needs to be solved by some other means than sprinkling parens and hoping people understand the consequences.  I do, however, agree that we should strive for uniformity no matter which side of the fence we are on.
Yes, we need better protection, if possible. But using the parens is not useless. It is a convention, like so many others. It will make at least some users (like me) aware of the issue.
I prefer having parens for a slightly different reason: it reduces breakage if you replace the def with a val returning a Function0. Ideally there shouldn't be a difference between a method and a function in Scala, and so I try to make my code reflect that (although I do break this rule often, it isn't on purpose).
Another disadvantage of not having parentheses at definition side: Some users (like me) would like to use parentheses on call site. This is not possible with the current no-parens style.
That, on the other hand, is a very valid argument.
All right, if this is a valid argument, then let's merge this one ...
Didn't Josh say that the semantics of () had changed?
He is wrong, nothing changed. You can try yourself:  ```  tmp$ scala-2.10 Welcome to Scala version 2.10.0-20120420-191528-3c9c18ddcc (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_31). Type in expressions to have them evaluated. Type :help for more information.  scala> def foo() = () foo: ()Unit  scala> foo()  scala> foo  scala> def bar = () bar: Unit  scala> bar  scala> bar() <console>:9: error: Unit does not take parameters               bar() ``` 
Having carefully danced around Walters still image, I must confess that I did not find any obvious deficiencies. Apart from the spiritual one (i.e. allowing instance re-use at all). But as you say, there may be use cases 
Can we do something to get rid of the ridiculously slowness of : akka.cluster.AccrualFailureDetectorSpec ?  It also fails intermittently on my machine:  [info] - mark node as available if it starts heartbeat again after being marked dead due to detection of failure *** FAILED *** [info]   true was not false (AccrualFailureDetectorSpec.scala:90) [info]   org.scalatest.TestFailedException: [info]   at org.scalatest.matchers.Matchers$class.newTestFailedException(Matchers.scala:150) [info]   at akka.testkit.AkkaSpec.newTestFailedException(AkkaSpec.scala:56) [info]   at org.scalatest.matchers.MustMatchers$MustMethodHelper$.mustMatcher(MustMatchers.scala:873) [info]   at org.scalatest.matchers.MustMatchers$AnyMustWrapper.must(MustMatchers.scala:901) [info]   at akka.cluster.AccrualFailureDetectorSpec$$anonfun$1$$anonfun$apply$mcV$sp$5.apply$mcV$sp(AccrualFailureDetectorSpec.scala:90) [info]   at akka.cluster.AccrualFailureDetectorSpec$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(AccrualFailureDetectorSpec.scala:76) [info]   at akka.cluster.AccrualFailureDetectorSpec$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(AccrualFailureDetectorSpec.scala:76) [info]   at org.scalatest.WordSpec$$anon$2.apply(WordSpec.scala:2161) [info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1968) [info]   at akka.testkit.AkkaSpec.withFixture(AkkaSpec.scala:56) [info]   at org.scalatest.WordSpec$class.invokeWithFixture$1(WordSpec.scala:2158) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2167) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2167) [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:168) [info]   at org.scalatest.WordSpec$class.runTest(WordSpec.scala:2167) [info]   at akka.testkit.AkkaSpec.runTest(AkkaSpec.scala:56) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2232) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2232) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:226) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:215) [info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info]   at scala.collection.immutable.List.foreach(List.scala:76) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:215) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:231) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:215) [info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info]   at scala.collection.immutable.List.foreach(List.scala:76) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:215) [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:282) [info]   at org.scalatest.WordSpec$class.runTests(WordSpec.scala:2232) [info]   at akka.testkit.AkkaSpec.runTests(AkkaSpec.scala:56) [info]   at org.scalatest.Suite$class.run(Suite.scala:2286) [info]   at akka.testkit.AkkaSpec.org$scalatest$WordSpec$$super$run(AkkaSpec.scala:56) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2279) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2279) [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:318) [info]   at org.scalatest.WordSpec$class.run(WordSpec.scala:2279) [info]   at akka.testkit.AkkaSpec.org$scalatest$BeforeAndAfterAll$$super$run(AkkaSpec.scala:56) [info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213) [info]   at akka.testkit.AkkaSpec.run(AkkaSpec.scala:56) [info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:148) [info]   at sbt.TestRunner.delegateRun(TestFramework.scala:61) [info]   at sbt.TestRunner.run(TestFramework.scala:55) [info]   at sbt.TestRunner.runTest$1(TestFramework.scala:75) [info]   at sbt.TestRunner.run(TestFramework.scala:84) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:183) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:183) [info]   at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:195) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:183) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:183) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:115) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:115) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:194) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:194) [info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info]   at scala.collection.immutable.List.foreach(List.scala:45) [info]   at scala.collection.TraversableLike$class.map(TraversableLike.scala:194) [info]   at scala.collection.immutable.List.map(List.scala:45) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:115) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:115) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$5.work(System.scala:67) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:221) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:221) [info]   at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) [info]   at sbt.Execute.work(Execute.scala:227) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:221) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:221) [info]   at sbt.CompletionService$$anon$1$$anon$2.call(CompletionService.scala:26) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:138) [info]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:138) [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [info]   at java.lang.Thread.run(Thread.java:680)
I get strange errors from time to time!  Here is what happened when I wanted to test:   akka-cluster > test [info] Formatting 2 Scala sources {file:/data/3-projects/scala-projects/akka/}akka-cluster(test) ... [info] Formatting 2 Scala sources {file:/data/3-projects/scala-projects/akka/}akka-cluster(compile) ... [info] Formatting 1 Scala source {file:/data/3-projects/scala-projects/akka/}akka-testkit(compile) ... [info] Formatting 4 Scala sources {file:/data/3-projects/scala-projects/akka/}akka-remote(compile) ... [info] Formatting 1 Scala source {file:/data/3-projects/scala-projects/akka/}akka-testkit(test) ... [info] Formatting 5 Scala sources {file:/data/3-projects/scala-projects/akka/}akka-actor(compile) ... [info] Formatting 3 Scala sources {file:/data/3-projects/scala-projects/akka/}akka-actor-tests(test) ... [info] Compiling 5 Scala sources to /data/3-projects/scala-projects/akka/akka-actor/target/classes... [error] /data/3-projects/scala-projects/akka/akka-actor/src/main/scala/akka/pattern/PipeToSupport.scala:4: class file needed by package is missing. [error] reference type PipeToSupport of package akka.pattern refers to nonexisting symbol. [error] package akka.pattern [error]              ^ [error] one error found [error] {file:/data/3-projects/scala-projects/akka/}akka-actor/compile:compile: Compilation failed [error] Total time: 9 s, completed Apr 24, 2012 6:51:18 PM  Any idea?
do a clean and a recompile
Ok, great. I could run the tests again but I don't get any test failure except some transient exceptions.  The line  [info] true was not false (AccrualFailureDetectorSpec.scala:90)  in your log does not appear at my machine when I run the test. 
Slowness of AccrualFailureDetectorSpec tests are mainly due to several Thread.sleep which is totally around ~ 18 s
Yup, so... can we get rid of them?
getting rid of them == deleting the tests  I haven't written the tests, so maybe it is better to ask @jboner 
Hey, you touch, you own ;-)
This reminds me of you recent tweet: "hAkking + Eye of the tiger" or something like that! ;)
I think it is hard to cut it down much. The FailureDetector's action/no action is based upon heartbeat vs timing.  However we could (and should) tag it as LongRunningTest so it is excluded in regular 'sbt test'.  Can you fix that? 
Can't we seed the FailureDetector with a Clock, and then seed it with a TestClock that runs faster than the normal one?
@jboner yes, absolutely. I'll fix it.  @viktorklang that would be a work around of course. But I think it can violate the Failure Detector behavior if we have (for some reason) byzantine actors that feed the Failure Detector with wrong clocks. What do you think?
It is not that easy. It is possible to reduce the 'threshold' to get a quicker detection of failure. But when I did that some time ago the tests became very unstable. Simply too small error margin to run on a non-real-time OS and VM I suppose. Feel free to change it, but I would prefer slow and correct compared to fast and perhaps wrong. 
So, then I assume that my Clock proposal won't work? Can't we simulate input and output to the FailureDetector?
I would of course work if we rewrite the FD to not work with "real" time but fake time that we manage the ticking for.  Currently it is controlled by the 'newTimestamp' function which currently only is 'currentTimeMillis'.  All we need is a Long. So it should be possible to fake it. Abstract it to allow passing in the 'clock' from the outside.  Open a ticket if you think it is worth spending time on. 
The more tests we can run quickly on our dev boxes, the shorter the time from change to feedback, so instead of having to wait for a lot of long-running tests to run before we know if something breaks, we get instant feedback. I think this is a worthy cause. Especially in this case.
So the decision is to abstract the time and fake it for tests. Can you create a ticket and assign it to me Viktor?
Created: http:www.assembla.com/spaces/akka/tickets/2021-abstract-over-time-for-accrualfailuredetector  I couldn't find you in the list on Assembla, can you assign it to yourself?  Thanks!
I agree it would be nice. Best would be if we could do it for the "real" cluster/membership tests as well. Let's think about it. 
Absolutely. Or atleast have as much as possible as normal tests and only the ones we cannot make faster as longRunning
Good ideas. I agree. Let's start hAkking it :)
Thanks Amir. You are already making yourself indispensable. 
Thanks Jonas :)
Does this really help in the case of load?
I assume that it will help if other actors are busy using all threads of the default dispatcher.
I did a test with cluster.StressSpec. Changed /master-node-1/workers max-nr-of-instances-per-node  from 1 to 8. That change doesn't increase the number of messages, just number of actors processing them. Then "must use routers with normal throughput" fails with UNREACHABLE detected. Added this dispatcher config and it it is fine.
We might want a setting to use an isolated dispatcher for remoting if needed?
Yes, that is needed. I created a ticket for that the other day.
This is ready for merge, and backport to release-2.2
backported to release-2.2
Just a question, why is this not a default?
Cluster in itself doesn't consume much resources so it can normally run on default dispatcher, sharing threads with others, but if you run many actors or blocking (oh no) on default disp this is a possibility to isolate. I might change opinion after real world feedback.
Would that not be true for many system level (instead of user level) actors?
adjusted according to comments
This is ready for merge, and backport to release-2.2
backported to release-2.2
backported to release-2.2
Very cool! Does it work?
Wow, great work!
It was originally your idea Viktor :)
Scchhhh, I'm donating credit here :-)
LGTM; we have a winner, then :-)
Agreed, thanks for the comments!
backported to release-2.2
thanks @2beaucoup; fixed
this should go into 2.1.1
I think this should rather go into 2.2.1 ;-) But it still needs that fix which currently separates it from my LGTM
yeah, 2.2.1 it is actually I verified the primitive type from java, but you are right that in scala it is not correct, and isPrimitive is better      scala> val i: Int = 0     i: Int = 0      scala> i.getClass.isInstanceOf[AnyRef]     res0: Boolean = true      scala> i.getClass.isPrimitive     res1: Boolean = true 
changed to `!found.isPrimitive`
if you want to test the case you just fixed, youd have to make the constructor choice ambiguous:  ~~~ scala def this(i: Int) = ... def this(x: AnyRef) = ... ~~~  Then passing `null` should successfully find the second one, where your previous code would have found both and bailed out.
good suggestion, I will add something like that to the test
added some more combinations in the test
backported to release-2.2
Good cleanup. @nraychaudhuri, we can merge this when you have signed the CLA. http:www.typesafe.com/contribute/cla Thanks for contributing.
As a Typesafe employee I thought I am already good there ;)  Signed   On Fri, Jul 5, 2013 at 3:05 AM, Roland Kuhn <notifications@github.com>wrote:  > LGTM > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1594#issuecomment-20503910> > . >
no special treatment for non-Legends ;-) (Legends get to sign on paper and are free to choose the type of ink)
backported to release-2.2
I switched to using tryComplete instead of success/failure as you suggested.  Do you think there is value to having trySuccess/tryFailure methods on Promise, or would that crowd the API too much?
trySuccess and tryFailure is already available in SIP-14[1], so they'll be there when we migrate.   [1] http:docs.scala-lang.org/sips/pending/futures-promises.html
I thought I remembered reading it in the SIP!  So, is "object Future" staying in Akka (along with this pull request?) or will it be migrated as well?
That is currently undecided.
I'll close this here, this method should be discussed on the scala-sips mailinglist so if it is added it's added to scala.concurrent.Future
ready for merge and backport
backported to release-2.2
Thanks Viktor, this is great feedback - appreciate your time.  I need to brush up a bit to get the implicit working within DurableMessageQueue, but that shouldn't be a problem.  Following the point about openAsOf, will move failure count and the semaphore to their respective states as well.
Thanks a lot for taking a stab at this, I believe this type of functionality would greatly enhance the reliability of the durable mailboxes. Please take my comments as the constructive critique I tried to convey.  Please don't hesitate to ask if you think I'm wrong or if something is unclear.  Cheers, 
Great contribution! Very much needed.
I think I've incorporated all the feedback so far.  Let me know what else needs done, or if i've misunderstood.  Also something to think about, does it make sense to age off the failures in closed state so infrequent failures don't cause the breaker to open?  Thanks for your time guys...
Have incorporated latest round of feedback - thanks for that Viktor.  Have added configurability, and async handling - it's fairly simple, but I think it should work.  It seemed to integrate well with the mongo durable mailbox for example.  Have also injected the logic into the existing durable implementations.  I'd like to get a system test running before I say these are "done" - but put them in the push for discussion.
Just realized that the async impl only tracks the state, but doesn't apply the behavior.  So that's incomplete, and i'll take a stab at that tonight.
Great, I'm currently in London, will get back to you next week.
Added async behavior impl, and async timeout tracking.  Haven't done system test yet - planning on putting together a simple system using durable mailboxes and then turning off e.g. mongo, redis, etc  As I was implementing the async timeout tracking, it made me think the timeout tracking shouldn't be at the instance level - rather there should be some mapping of an instance of the call to a deadline, with some kind of handback token for the asyncSuccess and asyncFailure calls, although not sure if that's overkill.  
Convinced myself that the timeout tracking should be at the call level, implemented that and got the test coverage of the async side up.
Doing some basic stability testing has me wondering if I have the circuit breaker in the right place.  What i'm doing is sending messages to a local actor on a schedule and restarting the external system responsible for the mailbox (e.g. redis or mongo).  Here's a gist: https:gist.github.com/2622718  From what i'm seeing, and the code seems to back this up, it looks like on the first failure the dispatcher thread croaks and there's no recovery attempt.  I need to add more logging to be absolutely, since i'm not seeing enough errors in the logs with my simple case.  What I am seeing is that the actor stops writing messages to the log, and in some cases i'm seeing a System.exit(-1), although I haven't located where that's coming from yet.  I've tried putting a parent actor in to act as a broadcaster, and drive that with the scheduler vs. what I was doing before which is driving the individual actors with the scheduler.  I guess i'm used to supervision of actors - there's really no supervision of dispatchers, right?  In any case this seems to make the circuit breaker not very useful, since there's never an opportunity to transition states.  For durable mailboxes, since IMO they should be treated as unstable external systems - is dispatcher supervision a missing concept?  Or am I missing something?  I don't have an OTP background, but am researching this now.  Am also going to see if I can adapt a node.js tcp proxy to test timeout behaviors.
More logging cleared up my understanding of what is going on, it's not the dispatchers that are crashing.  It's specific mailbox implementations that become unresponsive.  Beanstalk mbox: Doesn't recover from a restart of the service Mongo mbox: Occasionally recovers depending on when I restart mongo, very noisy even on happy path with an (apparently irrelevant) exception on every single message processed ZK mbox: Does recover, hammers away while it tries to reconnect, no backoff apparent Redis mbox: Doesn't recover, and causes a System.exit(-1) due to a heap space OutOfMemoryError:  More info on the Redis issue. https:gist.github.com/2625351  No matter what, i'm not seeing a lot of value from the circuit breaker at this point, am thinking the mbox impls need shored up a bit first.
I found the NetworkFailureTest trait and was able to find a similar function in Linux (tc) to simulate delays, dropped packets, etc.  Also, my actor hierarchy was not set up correctly to restart the receiving actors, which I fixed in the latest version.  All the mailboxes are significantly more resilient to timeouts and dropped packets than service restarts.  For what it's worth, now i'm seeing a lot more use from the circuit breakers.  So i've gotten a bit ahead, am happy to go whatever direction makes sense.  Look forward to some more feedback.  Thanks!
Sorry for the delay in review, lots of things going on right now, will check ASAP.
No worries.  I appreciate all the time you've been able to put in so far.
Crap, sorry for the delay here, I'll get someone on top of this ASAP. Apologize for the delay
I'm on this. We have an idea of using Futures for the async circuit breaker. I'll let you know as soon as I have something working.
Looking forward to it, Patrik!   You guys may want to think about upgrading the redis driver sooner rather than later due to the unfriendly crash behavior that I covered above on redis server restart/crash.  I can open a new ticket if that helps...
Based on your work I have done a proposal that integrates the async part with akka Futures, see https:github.com/akka/akka/pull/459  To motivate addition to akka it should support the async case as you started and I have rewritten it be integrated with akka Futures. The synchronous case is still there as a convenience.  Thanks for you great work. Looking forward to comments.
Redis and several of the other durable mailboxes have been removed from akka, but we encourage to continue them as separate community open source projects.  /Patrik  16 maj 2012 kl. 18:15 skrev scullxbones<reply@reply.github.com>:  > Looking forward to it, Patrik!  >  > You guys may want to think about upgrading the redis driver sooner rather than later due to the unfriendly crash behavior that I covered above on redis server restart/crash.  I can open a new ticket if that helps... >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/427#issuecomment-5744971
Got it.  Some feedback for you guys, do with it what you will:  * Communicate more - if you know someone is working on something and decide you are cutting it loose from maintenance, let them know.  While the decision to cut support makes a lot of sense to me - it would have been nice to know what was coming.  I now see Viktor's post in akka-user and the removal of everything but file-based on 5/8 - which again, based on my experience on this task as documented above is a good call to make.  * I'd be lying if I said, the "thanks we'll take it from here" approach doesn't sting a bit.  Use that sparingly because it is discouraging as a contributor.  I appreciate the attempt to soften the blow, and I do like Patrik's use of Future (without Await).  I'm sure there were good reasons why to take that road rather than steer me a different direction.  * On the positive side, overall it has been a good experience for me, and i've learned quite a bit in the process.  I appreciate you guys working with me on this.  Given where we're at - do you still need a signed CLA (asked for one on the ML, never heard back)?  If so, please email the form to me - I can get it back to you as early as Saturday when i'm within reach of a scanner.  Let me know if there is any reason to leave this pull request open, otherwise will close it out.  Thanks, Brian
Thanks for the feedback Brian,  The "removal" of the mailboxes doesn't really affect your work at all, this CircuitBreaker will be a _very_ important tool for anybody who is implementing a mailbox of their own, or for the already established ones. So you should definitely not take this as something discouraging, it's completely the opposite! In fact, I like it so much I want to put it in akka.util so we can use this for more stuff than merely mailboxes!  Also, "thanks we're taking it from here" was not meant to be like that at all, I asked Patrik to take over the review since I'm currently overworked and felt really bad for not being as responsive to your changes as I'd liked to. A quick informal handover yesterday led to an idea to use Futures for this, as to add even more cohesiveness to the API, Patrik ran with it to see if it was feasible and it actually turned out pretty well. But only because we had the chance to stand on _your_ shoulders here.  So please see this as positive, your work has improved Akka, and for that we are very thankful!  The only thing that's left is the CLA, which you can sign online here: www.typesafe.com/contribute/cla
One suggestion, you may want to mention Brian's work in the commit then:  https:github.com/akka/akka/commit/a35cc848df0c22f85d8d9604ab73528f05459c42  It's a nice gesture to give credit where credit is due.
Well of course we will, and also in the release notes. We use Pull Requests for code reviews.
As you can see in https:github.com/akka/akka/pull/459:  "**Proposal** of CircuitBreaker that integrates with Futures based on @scullxbones pull request https:github.com/akka/akka/pull/427  I need to add some documentation and use it in durable mailboxes.  Not sure where to place it, now in akka-actor."
Thanks for feedback Brian. It's exactly as Viktor said, your work was a great foundation. I don't think I would have been able to steer you in this this direction without writing the code. I was not confident that it would work until I tried with all your excellent tests.  I don't think we are in more hurry than that you can complete the work and do the merge in your name. There are a few things left that we can discuss solutions to - one and only one attempt in open state, which was an oversight from me, thanks for pointing out  - race in transitions - integrate with file based durable mailbox - some docs  /Patrik  17 maj 2012 kl. 14:24 skrev viktorklang<reply@reply.github.com>:  > Thanks for the feedback Brian, >  > The "removal" of the mailboxes doesn't really affect your work at all, this CircuitBreaker will be a _very_ important tool for anybody who is implementing a mailbox of their own, or for the already established ones. So you should definitely not take this as something discouraging, it's completely the opposite! In fact, I like it so much I want to put it in akka.util so we can use this for more stuff than merely mailboxes! >  > Also, "thanks we're taking it from here" was not meant to be like that at all, I asked Patrik to take over the review since I'm currently overworked and felt really bad for not being as responsive to your changes as I'd liked to. A quick informal handover yesterday led to an idea to use Futures for this, as to add even more cohesiveness to the API, Patrik ran with it to see if it was feasible and it actually turned out pretty well. But only because we had the chance to stand on _your_ shoulders here. >  > So please do not see this as positive, your work has improved Akka, and for that we are very thankful! >  > The only thing that's left is the CLA, which you can sign online here: www.typesafe.com/contribute/cla >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/427#issuecomment-5762976
Thank you very much for the clarification Viktor - it's much appreciated.  I'm definitely encouraged to contribute more in the future.  CLA signed.  PR closed.
Sorry, missed your comment Patrik.  Please let me know what I can do to help out with the remaining items.  Does it make sense to clone your wip branch and hack on that?
   17 maj 2012 kl. 15:23 skrev scullxbones<reply@reply.github.com>:  > Sorry, missed your comment Patrik.  Please let me know what I can do to help out with the remaining items.  Great, much appreciated. I'll get back to you tomorrow. In forrest right now, public holiday.  >  > Does it make sense to clone your wip branch and hack on that?  Whatever you find convenient. Copy the text from the two files are just fine by me. >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/427#issuecomment-5763877
I have commented on https:github.com/akka/akka/pull/459, so there you find some outstanding issues you can help with.  Then you can integrate it with with file based durable mailbox, including config settings. I'm not sure if it should be part of the durable mailbox base class, but it should be easy to add to the concreate durable mailbox implementations.  Documentation. Scaladoc and rst doc in the durable mailbox chapter. I have another pull request for that documentation outstanding. Will let you know when that has been merged.  Let's make this an awesome contribution to Akka!    On Thu, May 17, 2012 at 3:31 PM, Patrik Nordwall <patrik.nordwall@gmail.com>wrote:  > > > > 17 maj 2012 kl. 15:23 skrev scullxbones< > reply@reply.github.com > >: > > > Sorry, missed your comment Patrik.  Please let me know what I can do to > help out with the remaining items. > > Great, much appreciated. > I'll get back to you tomorrow. In forrest right now, public holiday. > > > > > Does it make sense to clone your wip branch and hack on that? > > Whatever you find convenient. Copy the text from the two files are just > fine by me. > > > > --- > > Reply to this email directly or view it on GitHub: > > https:github.com/akka/akka/pull/427#issuecomment-5763877 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
The ticket mentions fixing the start scripts, is that something which was incorrectly reported or does it still need to be done?  These changes LGTM, but depending on the previous question they might be incomplete.
I have tested the instructions in the documentation with a fresh akka dist and the bash start script is fine. I couldn't see anything wrong with the bat script. The wildcard classpath is documented to be supported: http:docs.oracle.com/javase/6/docs/technotes/tools/windows/classpath.html
in that case please document your findings in the ticket, thanks
backported to release-2.2
It seems that proguard likes breaking classes by removing constructors; this is probably not limited to the case shown here. Can you switch that off? If that does not fix it please open a ticket at assembla.
Yes, that fixed the issue (well, after a lot of other unfound methods). For reference, my proguard settings: https:gist.github.com/MHOOO/37ec130f6b09193c7ec5  Now however, I'm getting a different error:  ``` I/System.out(25506): [ERROR] [07/03/2013 01:09:22.056] [MobileController-akka.remote.writer-dispatcher-12] [akka:MobileController/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FCuboidGameSystem%40127.0.1.1%3A2552-1/endpointWriter] AssociationError [akka.tcp:MobileController@192.168.1.136:2553] <- [akka.tcp:CuboidGameSystem@127.0.1.1:2552]: Error [key not found: 6] [ I/System.out(25506): java.util.NoSuchElementException: key not found: 6 I/System.out(25506):    at scala.collection.MapLike$class.default(MapLike.scala:228) I/System.out(25506):    at scala.collection.AbstractMap.default(Map.scala:58) I/System.out(25506):    at scala.collection.MapLike$class.apply(MapLike.scala:141) I/System.out(25506):    at scala.collection.AbstractMap.apply(Map.scala:58) I/System.out(25506):    at akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:98) I/System.out(25506):    at scala.util.Try$.apply(Try.scala:161) I/System.out(25506):    at akka.serialization.Serialization.deserialize(Serialization.scala:98) I/System.out(25506):    at akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23) I/System.out(25506):    at akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:56) I/System.out(25506):    at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:56) I/System.out(25506):    at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:74) I/System.out(25506):    at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:704) I/System.out(25506):    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) I/System.out(25506):    at akka.actor.ActorCell.invoke(ActorCell.scala:456) I/System.out(25506):    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) I/System.out(25506):    at akka.dispatch.Mailbox.run(Mailbox.scala:219) I/System.out(25506):    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) I/System.out(25506):    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262) I/System.out(25506):    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) I/System.out(25506):    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478) I/System.out(25506):    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) I/System.out(25506): ] ```  Any idea what this could be all about? Apparently serialization is the problem, but I have no idea what key this is all about. I'm only sending strings.
We dont use github issues for bug tracking, please see http:doc.akka.io/docs/akka/current/project/issue-tracking.html.
2013.07.03. 01:16:15 dtumon Thomas Karolski <notifications@github.com> rta:  > Yes, that fixed the issue (well, after a lot of other unfound methods). For reference, my proguard settings: https:gist.github.com/MHOOO/37ec130f6b09193c7ec5 > > Now however, I'm getting a different error: > > ``` > I/System.out(25506): [ERROR] [07/03/2013 01:09:22.056] [MobileController-akka.remote.writer-dispatcher-12] [akka:MobileController/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FCuboidGameSystem%40127.0.1.1%3A2552-1/endpointWriter] AssociationError [akka.tcp:MobileController@192.168.1.136:2553] <- [akka.tcp:CuboidGameSystem@127.0.1.1:2552]: Error [key not found: 6] [ > I/System.out(25506): java.util.NoSuchElementException: key not found: 6  It means that the receiver side has no loaded serializer with id 6. The reference.conf in the remoting module defines these explicitly:       serializers {        akka-containers = "akka.remote.serialization.MessageContainerSerializer"        proto = "akka.remote.serialization.ProtobufSerializer"        daemon-create = "akka.remote.serialization.DaemonMsgCreateSerializer"      }  I am not really familiar with ProGuard so I don't know what could be the exact problem, but it seems that akka.remote.serialization.MessageContainerSerializer is not available. Basically you need everything under akka.remote.serialization.  The error message is not very helpful though, so I created a ticket: https:www.assembla.com/spaces/ddEDvgVAKr3QrUeJe5aVNr/tickets/3487#/activity/ticket:  -Endre
@drewhk You are right. I was missing the serializers values. I assume this happened, because I only copied the "remote" part of the config from the online manual, and not the "actor" part. Thanks for answering here!  @rkuhn, sorry for not using assembla, I can see why you'd want to use it, as it is (from what I glanced so far) far superior to issues on github. But I rather not register for yet another bug tracking system - only to file my first, and probably last, bug report. Thanks for helping anyway!
We can use an even later sl4j, see https:www.assembla.com/spaces/akka/tickets/2615 I will update this PR.
Can't say yes or no, will yield
ok, merging this, it's just boy scouting, the real problem with the barriers remains (see ticket 2583)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/471/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/471/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/473/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/474/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/474/
nice addition: :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/480/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/480/
This is not everything. Search the file system and you will see more references to `akka-remote-tests-experimental`
Are you sure that you are looking at my branch? I cannot find anything. 
I can't see that you have touched multi-node-log-replace.sh, which contains reference to akka-remote-tests-experimental
oh, directories starting with s were under the radar for me until now, thanks for catching this!
:-) yeah, why would someone care about "s" directories then LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/479/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/479/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/446/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/446/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/453/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/453/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
looking at the sad kitteh: it seems that an OSGi import is missing for the ServiceTracker... class, and the maven setup seems to want to publish things which is not allowed on our jenkins setup. For the latter I know too little about the maven plugins used to comment meaningfully.
I haven't see the log about ServiceTracker. but I add a star for the command.import-package. the tests (either maven or sbt built) are going well here) It seams the sbt.process (calling the shell) is not allowed or buggy. This is the tiny fix I used to launch the maven integration tests. I just modify a little bit the command and push. If the shell script is not allowed, we will need to find another way to run the test in maven...
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/455/
Great work, you two! I guess that getting this sample verified successfully by the Build Kitteh will possibly also fix some outstanding bugs ;-)   Itll take us some time to digest everything in there, especially how the test is set up, so that we will then be in a position to react to jenkins failures in the future.  Thanks again for devoting your time and effort to this contribution!
thanks for all your remarks and compliment! You're pointing some parts I did'nt develop myself but I'll try to fix as much as possible.  
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/455/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
thanks for that point (I wanted to be certain this may not be sbt that cause this problem), I'm going to try a fresh build to test it again.
So, I didn't get error on integration test (but did not succeed in correcting the TestProbe problem)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/457/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/457/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
thanks, Jamie, for your corrections! @rkuhn  About the maven integration test, I added a test for maven to check if it's installed (just to be sure)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/459/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/459/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
So, not a problem of maven not found (sorry, I really wanted to be sure of this point.) I'm trying to find other possibilities and find help (maybe tomorrow)
I've just stored the log in the akka-samples/akka-sample-osgi-dining-hakkers/akka-osgi-sample.log, is it possible to get it (as it's not shown in the kitty logs)? Concerning the logs, I get slf4j warning about multiple bindings (I'm not logging expert at all) (http:www.slf4j.org/codes.html#multiple_bindings).
It may be fool, but I replaced the ``mvn clean install`` by a ``mvn clean package`` which should not cause errors as not deploying jars I tried after having clean the supposed maven repo and it worked
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/460/
ouch: the feature is based on maven repo and loads com/typesafe/akka/{api,core,command,uncommons} I'm going to correct this to install does in akka-sample directory
sorry, this means th current jenkins job will fail as those artifact may not have been installed
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/460/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I've changed groupID to avoid deploying in ~/.m2/com/typesafe/akka/ but in ~/.m2/com/typesafe/akka/akka-sample/akka-sample-osgi-dinning-hakkers, let's hope this was just a permission problem... at least, this is more clear now
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/461/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/461/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I thing redirecting mvn outputs in a file was not a great idea finally. I think maven is now publishing in com/typesafe/akka/akka-sample/akka-sample-osgi-.../ directory. Might it still be a permission issue?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/462/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/462/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
So Maven structure leads us to deploy the feature and other bundles on the local maven repository. I've see it's possible to load jar directly in pax exam and I'm planning to do it and I would like to do it without maven if it's possible. (to be continue) 
@rkuhn the problem with akka-osgi-aries tests should be fixed: this was a modification of the blueprint version (back to 0.3.2). I've also reduced the length of the integration test artifacts as those throw fs exception (longer than expected)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/644/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/644/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
``` [ERROR]   The project com.typesafe.akka.akka-sample.dining-hakkers:parent:2.2.0-SNAPSHOT (/localhome/jenkinsakka/workspace/akka-pr-validator/akka-samples/akka-sample-osgi-dining-hakkers/pom.xml) has 1 error [ERROR]     Unresolveable build extension: Plugin org.apache.felix:maven-bundle-plugin:2.3.7 or one of its dependencies could not be resolved: Failed to collect dependencies for org.apache.felix:maven-bundle-plugin:jar:2.3.7 (): Failed to read artifact descriptor for org.apache.felix:maven-bundle-plugin:jar:2.3.7: Could not transfer artifact org.apache.felix:maven-bundle-plugin:pom:2.3.7 from/to moxie-everything (https:moxie.typesafe.com:8497/nexus/content/groups/everything): Not authorized. -> [Help 2] ``` Might it means you should allow some maven plugin in your moxie?
found one wrong password in the .m2/settings.xml.  PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/809/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/809/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
See https:groups.google.com/d/msg/akka-dev/YKW_CxQwvgI/-o4JqL_ifaIJ (fixed branch at https:github.com/vivosys/akka/commits/akkaosgisample).
Submitted new pull #1310  with the fixes.
closing this PR in favor of the new one
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/463/
I think the code does what you say it should, but I think we need a separate ticket and some discussion/brainstorming around why the `LeaderChanged` didn't get published/handled.  LGTM
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/463/
The missing LeaderChanged event can be explained by the the same reason as we discussed for the member transitions. It doesn't have to be convergence on all nodes for each transition. For the member events we solved it by buffering all intermediate events. Same thing must be done for leader changes.
Fixed the missing LeaderChanged problem also.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/466/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/466/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
it's a real failure related to this, I will investigate
the failure was correct, my bad to not run all tests, it would have been solved as expected given some more time
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/467/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/467/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/464/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/464/
`handleInvokeFailure` currently throws InterruptedException; looking at the call sites I am pretty certain that that is a bad idea, see e.g. systemInvoke, which will then lose its todo. Comparing with `processAllSystemMessages` shows that there we make sure to process all of them and just store interruptions for later, after processing has finished. I think this is the right thing to do, so please investigate what would break if FaultHandling.scala:179182 would be thrown out. Then `InterruptedException` would also need to be caught on line 186, followed by emergency stop.
`ActorCell.create` might also want to catch IE, wrap it in the rethrown AIE and set `Thread.interrupt()`
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/486/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/486/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/489/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/489/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/491/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/491/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/469/
Why do we need a Polish API? Isn't English good enough?
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/469/
You cleaned up the woods around the camp site too: LVGTM!
LGTM, yeah, trying to set system properties afterwards is always doomed 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/470/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/470/
Well done: I hope this nails that sucker for good!
Does this mean that one of Endre's earlier non-blocking solutions can be used? New ticket for later?
Bjrn, this is great! :100:  @patriknw yes, I think it worth a try, and it is not hard to implement either.
yes, lets run that throughout the day
SPURIOUS ABORT? -- PLS REBUILD/pr-validator-per-commit@5fb904fb38f5abb1ab52a2289309042e99605e84
(kitty-note-to-self: ignore 18667179) :cat: Roger! Rebuilding pr-validator-per-commit for 5fb904fb. :rotating_light: 
Any objections if I close this without merging?
ok, we will keep the test. I have changed the loop count. Will merge when kitty says yes.
Is this related to ticket https:www.assembla.com/spaces/akka/simple_planner#/ticket:3404 ?
yes, if we can just replace it by something much simpler then why would we not do that?
what was the reason for writing this? was it something with getSimpleName not working?
Yes, as the title of the Pull says _Make AkkaSpec getCallerName work on J9_
okay, LGTM, merge away!
you could make RoutedActorCell final
harmonized `props` factories, and moved the Deploy inside the factory at 2 places
fixed the long line
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/440/
about the Corba question, at least in karaf, it is exported with version 0.0.0, so no problem to import it as it is currently
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/440/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
so, I guess we can close this one due to EOL?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/441/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/441/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
fixed the build failure and acted upon Viktors suggestion
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/442/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/442/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/438/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/438/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/439/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/439/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/444/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/444/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
it seems to be a real failure, at least the throttler which didnt throttle indicates some kind of mess-up
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/445/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/445/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Yup, the failure is legit
Here is a failure run with extra debug logging: https:jenkins.akka.io:8498/job/akka-multi-node-repeat/1155/consoleFull As suspected there is a mix of a4.local:36299 and 10.0.1.234:36299 in the ThrottlerManager. This will require some more thought.
The underlying idea of the design is to use whatever the user had configured (literally, character by character) as the actor systems host part. Give the user full control and dont ever touch it. I had come to the conclusion that that would be the only sane solution for us, so that we dont have to work around broken local setups because the users can do that themselves.  It would be an immense shame if that were impossible to write down without triggering DNS lookups (but then it would not surprise me that much, given how broken the JDK tends to be in such matters of practicality).
I think I have found a workable solution. Take a look.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/445/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/445/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Apart from that one thing I dont get: LGTM  Which means that the throttler failure needs fixing 
alright, I'll look into the failure, I thought I had solved it :-(
then it is probably an issue with DNS@jenkins vs. DNS@yourMac, which is what I meant with that broken assumption earlier; good luck!
but the last failure is referring to the same job (445) that was the first failure, hmm I have run this successfully 4 times in another job PLS REBUILD ALL
ah, sorry, youre right that something felt wrong when I looked at the errors (being too similar)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/458/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/458/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/436/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/436/
LGTM, apart from that Id like to understand that one minor point.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/450/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/450/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/430/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/430/
I have added a clarifying comment explaining the exact nature of the race
So, now Im certain: the `get(bucket)` synchronizes-with the `set(bucket, Empty or null)` in the timer thread, which happens after the `currentBucket += 1`, which means that the `current == currentBucket` is properly synchronized with that earlier write (as in: there is a transitively established happens-before relation between the two). Therefore it is covered by the JMM and is not allowed to fail on J9 (knocks wood).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/431/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/431/
@rkuhn, yes as far as I can see the `get(bucket)` in schedule and the `set(bucket, Empty or null)` in the timer thread keeps `current == currentBucket` safe.  And the `InitialRepeatMarker` dance is to guard us from overwriting when the timer thread have already run the first initial one, right?
I don't know LARS in person yet, so I can only say I trust you.
@bantonsson yes, precisely.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/432/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/432/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Thanks kitty, fixed it.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/435/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/435/
Wow, this is a huge thing  but thorough, AFAICS :-) Great work!  Now the only thing Im uncertain of is whether you have hidden a test case somewhere in there which verifies that the old stuff still works?
I thought that would be overkill. I tested both deprecated properties before changing all tests. I can of course add a test if we think it's that important.
I'd prefer a test so we don't accidentally break it.
Added the test.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/437/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/437/
yup, thanks: LGTM
Could you also expose this as a command line option to the release script? Ive had the problem of running it and failing at a later stage and then being unable to run it again because the plugin was already published.  Otherwise it LGTM.
Excellent idea. Will do.
wow, great, we must remember the two links from akka.io nice icons!
A lot of good cleanup, and I agree with the Terminated change, but the PoisonPill change makes me a bit scared. How do we know that it doesn't damage anything? I would like to hear Endre's opinion.
I agree @patriknw, I wasn't 100% sure either, but the tests pass locally so I don't know what to say. All I can say is that I think there might have been a race between the ``context stop reader`` and the disassociate, the disassociate would have let more messages be processed but the stop would be the next processed message.... Wdyt?
Unfortunately we don't have test coverage for all race conditions :-), they often show up after extensive testing (in different environments, eg EC2). I don't know, which also means that my vote is not worth much.
Do we have any easy way to continuously test this branch until tomorrow morning on scalable1?
ok, I have setup the job at scalable1
Awesome, thanks Patrik!
Just realized that this doesn't solve the problem. However, it should not abruptly kill the reader anyway.
Changed the PR to only do Endpoint.scala cleanup
Really nice! LGTM
deferring this after 2.2.0
after rebase this can go into master, but please add a ticket for backporting to 2.2.x milestone to keep the code bases closer together; we can wait some time until we backport so that possible fall-out is dealt with before that
@rkuhn did you see the compilation error?
when I fixed it github was down, and then I forgot to push it; thanks for the reminder
is it good?
LGTM, apart from the `restarted: Throwable`
please add the explanation I gave on the ticket to the commit message for later reference
LGTM after that
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/511/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/511/
Added Java versions of docs, now actually ready for review. :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/513/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/513/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/508/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/508/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/515/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/515/
LGTM Please sign the CLA: http:www.typesafe.com/contribute/cla
Thanks. I've signed the CLA.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/281/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/281/
Udpated according to review comments.
Udpated: changed forward to tell
Can I merge?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/512/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/512/
LGTM apart from the one thing that starting the cluster now may take one SeedNodeTimeout longer than before (unless I missed something); it should be documented that all seed nodes should be started simultaneously to avoid this waiting period.
yes, it's a correct observation that the trade-off is that it might take SeedNodeTimeout longer, if the other seed nodes are not started
Instead of sending the InitJoin message to the other seed nodes only once (in preStart) it can schedule to do this once per second, and that would reduce the startup time of the first seed node if the others are started slightly afterwards. WDYT?
yes, that sounds reasonable, also in case of network failures
however, that would mean that I can't use watch to detect that the other seed nodes are not started
Aside from comment,s lgtm
sorry, it was too long time since I did this, the observation about longer startup time is not correct either it immediately receives Terminated or it receives Nack (or Ack) from the other seed nodes, so it should not be any delay
but the Terminated from the watch is racy, as we discussed today: you cannot reliably watch something which you have not talked to
ok, so you are saying that I should not base the solution on actorFor watch, alright, I thought we would have an replacement for that, but that is true, that would involve some kind of echo message. alright, I'll change to retries
Revised according to comments. Changed to use retry and timeout instead of actorFor + watch.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/581/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/581/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
failure was in UdpConnIntegrationSpec, created ticket for it
merging now so that it can be verified by the nightly tests also
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/280/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/280/
Im not so certain about the style change: the version with default arguments actually looked clearer to me, unless Im missing something.
fixed review comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/501/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/501/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/502/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/502/
Looks OK to me.
@drewhk could you take a look at the       when(WaitModeAndUpstreamListener) {         case Event((listener: HandleEventListener, mode: ThrottleMode), _)   I didn't change that, but it is changing the `inboundThrottleMode = mode`
I fixed the review comments, and also the aggregated statusPromise at two places, see `statusPromise completeWith Future.fold`
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/283/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/283/
alright, we consider the pr-validator failure (3 second timeout) as a temporary timing glitch for now, it will be seen again if it's a real problem
aside from missing explicit return types: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/507/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/507/
what is the failure mechanism without this change?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/284/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/284/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/505/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/505/
A few more things to cleanup, if not already done: * add internal api scaladoc marker on `private[io]` and `private[akka]` * explicit return type on all public methods. * make UdpConnManager, TcpOutgoingConnection `private[io]` * wrong copyright in SelectionHandler
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/506/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/506/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/506/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/506/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/510/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/510/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/286/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/286/
I have changed the signature of managementCommand and some additional boy scouting
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/285/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/285/
LGTM We also have  2012 Typesafe Inc in the footer of rst docs and akka.io and these must be changed in other projects. I suggest that you initiate those changes also.
I dont know the implications here: you changed 2012 into 2013 in a few cases, but my intuition would have been to make that 20122013. Which is correct/needed?
Does your replacement script check if the file was edited in 2013?
I just edited it. And it's 2013
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/287/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/287/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/516/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/516/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/514/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/514/
Isn't it problematic to put the dispatcher and mailboxtype in the LocalActorRef since it will prevent collection if the ActorRef is held past the death of the ActorCell?
barring scalac bugs no additional fields have been introduced in LocalActorRef
LGTM, that was a lot and at this point it should make us nervous, but I think we have good test coverage for this. akka-local-repeat has been running on this branch over night without failures (10 full builds)
So the issue was that we never removed from the children collection, and kept picking _dead_ children when choosing which one to fail? Apart from the CodeCache of course.
ok, if you say so
Nice, since what protobuf returns is also immutable
I feel that this should get into 2.2. WDYT?
Yes, I think so.
Yes I will, I just dumped early
Don't worry about the first commit failure
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/496/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/496/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/490/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/490/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/488/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/488/
I dont get everything about this PR (in particular why it didnt fail very obviously in the tests), but it LGTM.
what do you mean, there were no "real" reference.conf tests, all tests that were using ssl had defined all ssl conf inside the test class
It didn't fail because the tests that are using SSL set everything they needed in their own config, and there was no test for the general config.
hmm, so we should add a ticket to add a test which tests the pure `reference.conf`
Patrik added those tests as part of this pull :wink: 
I'm a good boy scout, you know :-) https:github.com/akka/akka/pull/1124/files#L5R62
Oh, I should have looked at that file in full, not only diff (thought `system` would be the configured one). Sorry for doubting you.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/487/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/487/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/485/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/485/
Improved based on Viktor's feedback.  Added fix for ticket 3031 here aslo, because they are related.     * Problem may occur when joining member with same hostname:port again,       after downing.     * Reproduced with StressSpec exerciseJoinRemove with fixed port that       joins and shutdown several times.     * Real solution for this will be covered by ticket #2788 by adding       uid to member identifier, but as first step we need to support       this scenario with current design.     * Use unique node identifier for vector clock to avoid mixup of       old and new member instance.     * Support transition from Down to Joining in Gossip merge     * Don't gossip to unknown or unreachable members.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/495/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/495/
Awesome! After review, please squash to one commit and include the ticket number in the commit comment. Thanks!
squashed commits and moved the HashedWheelTimer tests into SchedulerSpec
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/290/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/290/
So heres what happens: when scheduling an integer multiple of the wheel duration into the future, the relative offset will be zero and the remaining rounds greater zero, which means that it will be put into the current bucket. The next tick will look at the next bucket, which means that when the wheel has turned around once, it will find our dear timeout with the initial remaining rounds (e.g. 1 in the test case) and delay it by one more round than calculated. The correct fix would therefore be to decrease the remaining rounds by one if the relative offset is zero.  However, scheduling something by an integer multiple of the tick duration into the future means that the calculated bucket will always find this timeout to have slipped (since by definition the current bucket was emptied in the past). This means that the difference between this PR and the correct solution is just whether it will slip by one or two buckets, which makes no difference => merging.  (and Im heavily pondering writing a Scala implementation completely without locks)
did you check for other occurrences of this bug?  LGTM
@rkuhn I did search for where DeadLetter is created and I think that should be alright (replaced earlier). Do you think it could be any problem with these occurrences?  - BoundedMessageQueueSemantics - BoundedDequeBasedMessageQueueSemantics - deadLetterQueue in ActorSystem - SystemGuardian - Guardian - send in RemoteClient
The problem is more in the `!` or `tell` implementations than in the places which create DeadLetters; if you checked the above, then theyre fine. I just checked all things extending `MinimalActorRef` and found only one offender: theOneWhoWalksTheBubblesOfSpaceTime. If you manage to send to that one, youll get an NPE if you have no sender.
in theOneWhoWalksTheBubblesOfSpaceTime the only use of sender is guarded by `if sender ne null`
You are right, of course, please disregard. So, I conclude that were good :-)
time will tell :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/292/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/292/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/482/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/482/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/484/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/484/
Yes, that looks nicer. Thanks for the feedback.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/492/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/492/
please forward-port to master also
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/481/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/481/
looks good apart from one small comment
yup, looking good! The only thing Im uncomfortable with (apart from that the commits need some squashing) is that there is no test, and I have no setup to just try it out.
good point, I'll add test (probably next week), and then squash the commits
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/289/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/289/
So did some squashing (my first time, I hope this is fine) and included @patriknw remarks. I haven't added test, but used the osgi-sample  to test it. (nb: to test it, you may stop and start the log service, in karaf, it's ususally bundle 3)
thanks for contributing, sorry for nitpicking - I hope you see it as a positive learning exercise
Thanks, sorry some errors are due to the fact I'm testing several modifications and not always correctly commit change before switching. I really enjoy learning and contributing! 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/349/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/349/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
jenkins failures are not related to this ticket AFAICT
So do I think 
Hi Christophe,  currently the merge button is not green, which keeps me from merging this in. Could you please rebase on top of current master?  Thanks,  Roland
aside from the wrong header and the rebase this looks good
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/349/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/349/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
again a stress test on the cluster part, I haven't seen anything about akka.osgi in the reported error.
yes, OSGi seems fine ;-)
Looks great! We might need to generalize it somewhat, when implementations other than TCP are added.
Looks great, thanks for making the language clearer! Apart from the few nitpicks LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/288/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/295/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/295/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/499/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/499/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/497/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/497/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/498/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/498/
LGTM, but I cannot check the CLA right signature right now
Unfortunately we cannot merge this without a [signed CLA](http:www.typesafe.com/contribute/cla) (I know, it sounds stupid for a one-character documentation change, but that is how it is).
LOL. Signed it.
backported to release-2.2
Note : I stumbled on this issue by using ClusterActorRefProvider initially but to reproduce on 2.1 I had to use RemoteActorRefProvider instead.  Please make sure if you fix it to test it also with ClusterActorRefProvider on 2.2.
Note : I am on Windows 7 64 bits. A colleague could not reproduce it on a Linux box.
I have the same configuration as Michel, and I have just reproduced this regression.
Thanks a lot for reporting, but please file the ticket in Assembla: http:doc.akka.io/docs/akka/current/project/issue-tracking.html
I think this is a regression on Windows. I have created ticket: https:www.assembla.com/spaces/akka/simple_planner#/ticket:3450
I confirm that this is fixed in 2.2-RC2. Thanks
Thanks for trying it! /Patrik   On Fri, Jun 28, 2013 at 3:38 PM, Michel Daviot <notifications@github.com>wrote:  > I confirm that this is fixed in 2.2-RC2. Thanks > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/issues/1533#issuecomment-20188448> > . >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  Reactive apps on the JVM Twitter: @patriknw
LGTM, after some renaming
deferred: will review once 2.2.0-RC2 is out
next try after 2.2.0
Thinking about Viktor's comments I realize that this patch might indeed take us down the wrong path. Flexible and (optionally) deep logging is a key requirement for quite a few of our users and we need a proper solution rather than a large number of fragmented little patches like this one. It seems to me that the time has come to start work on a dedicated aspect-weaving tool for stats gathering and deep logging. Is there already something in your drawers something that could serve as a starting point? Otherwise we'll just start from scratch and approach you for feedback when a first cut is done.  Regarding this patch: If we assume we can rely on an aspect-woven tool for inspecting the exceptions in the future the additional exception level breaking the stack-trace chain (as recommended by Viktor) appears to be the best short-term solution. WDYT?
I don't know if I was misunderstood, but just to clarify, "NoStackTrace" can be disabled via system property: https:github.com/scala/scala/blob/v2.10.2/src/library/scala/sys/SystemProperties.scala#L80  So my proposition solves the problem and it's tunable what the user wants to see.
In my opinion the IO layer should not issue any log output above DEBUG level, NIO also does not print to the console. Whether or not a logged exception includes a stack trace (at that level) should be determined by whether or not the message itself uniquely identifies the source location and call stack already.   That said, nothing at all should be logged for events which are signaled to the handler for the connection, and without having checked the code at this point I'd like to assume that that applies to the case for connection reset by peer.   Whether or not to add tracing of failures is a more general question that we should discuss outside of this pull request. 
Thanks for the clarification and feedback, gentlemen, and sorry for not catching your drift earlier, Viktor, I've got you now. (I really shouldn't be doing this stuff on the weekend with my son on the lap.) I agree with the "no logging above DEBUG during normal operation" policy and will update this PR with a respective fix ASAP.
Perfect! Have a great weekend Mathias! :)
Ok, gentlemen, here comes take 2:  Up to now IOExceptions during reading, writing or connecting were treated as fatal actor errors, crashing the connection actor and thus producing ERROR level log messages. This patch treats such exceptions as "expected" during normal operation and prevents them from crashing the actor. Rather, they are logged at DEBUG level and the actor is actively and cleanly stopped.
LGTM apart from nitpicks
Ok, feedback incorporated.
also posted here https:groups.google.com/forum/?fromgroups#!topic/spark-developers/8AlvmcVHPi8 
Hi; Very easy for me to fix on my network. I was thinking of others really. The problem seems to exist in several places in akka and spark, so I am not going to tackle this for now. This is my first two days on akka and spark and only a week or two into scala. Names with colons are a bit duff, but that is the default with the router I have (BT - UK), so I guess a lot of people will hit this problem. Also, when we do move onto IPv6, the IP addresses will have the same problem and I expect any auto-generated names. But for now, I will leave this. Up to you if you want to keep it open as a low priority bug or colse it
Hi!  Please open an issue in our issue tracker: http:doc.akka.io/docs/akka/2.2.0-RC1/project/issue-tracking.html  Thanks!
Sorry - created the user therealnb, logged in and didn't have permission to create the ticket.   From: "Viktor Klang ()" <notifications@github.com<mailto:notifications@github.com>> Reply-To: akka/akka <reply@reply.github.com<mailto:reply@reply.github.com>> Date: Friday, 14 June 2013 12:36 To: akka/akka <akka@noreply.github.com<mailto:akka@noreply.github.com>> Cc: Nigel Brown <nigel.brown@guavus.com<mailto:nigel.brown@guavus.com>> Subject: Re: [akka] machine name containing colon throws exception in examples. (#1534)   Hi!  Please open an issue in our issue tracker: http:doc.akka.io/docs/akka/2.2.0-RC1/project/issue-tracking.html  Thanks!  X Reply to this email directly or view it on GitHub<https:github.com/akka/akka/issues/1534#issuecomment-19452240>. 
Hi!  As the link to the documentation says  you need to "watch" the Akka space to be able to open tickets.  Cheers, 
Hi; I should really read the instructionsK I created a ticket. Regards  From: "Viktor Klang ()" <notifications@github.com<mailto:notifications@github.com>> Reply-To: akka/akka <reply@reply.github.com<mailto:reply@reply.github.com>> Date: Friday, 14 June 2013 12:54 To: akka/akka <akka@noreply.github.com<mailto:akka@noreply.github.com>> Cc: Nigel Brown <nigel.brown@guavus.com<mailto:nigel.brown@guavus.com>> Subject: Re: [akka] machine name containing colon throws exception in examples. (#1534)   Hi!  As the link to the documentation says X you need to "watch" the Akka space to be able to open tickets.  Cheers,   X Reply to this email directly or view it on GitHub<https:github.com/akka/akka/issues/1534#issuecomment-19452883>. 
Updated according to comments and added some further optimizations based on a patch from @viktorklang.
And the performance numbers are now 5ms for same (by removing the GC triggered outliers), and 6ms for merge by skipping the `exists` and only doing `forall` in the `VectorClock.tryCompareTo`.
What's the current perf compared to master?
The last two changes didn't account for enough object allocations to affect the running times. It's still 5ms for same and 6ms for merge.
Ok. And we're sure that we haven't changed semantics in an adverse way? :-)
I can't see that it changes any semantics. All tests are green, VectorClock, Gossip, Cluster, and the stress test replay of cluster messages have the same number of newer/older/same/merge.  I'll kick off a repeat job on the branch.
Excellent. Great work Bjrn!
Cleaned up according to comments.  Made `VectorClock` be `private[cluster]` and changed `Node` into a `String`. Also made `VectorClock` not extend `PartiallyOrdered[VectorClock]` so we now return something more specific than `Option[Int]`.
LGTM, I like the red
LGTM apart from the nitpicks
Made changes according to the last round of comments, and squashed. Awaiting Kitteh, and then I'll merge.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/534/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/534/
fixed review comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/534/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/534/
Apart from test comment LGTM.
this changes semantics of a method within BC compatible branch: see comment in #1021 ; sorry for screwing this up, I should have closed this ticket when I closed the old PR
ok, reverted here: #1167  Changing semantics of misbehavior is normally fine though and from the ticket I read that it was not working as intended/documented.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/299/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/299/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/300/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/300/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/298/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/298/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/535/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/535/
Some updates to the PR:  - added introduction  - added obstruction-freedom  - fixed typos and minor stuff  *Beware!* I have not yet changed debated sections! Unfortunately GitHub marks some of those comments as outdated, but they are important, so please read them.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/535/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/535/
a code example in each section would be good illustrate how it can look like in Akka land (actors futures)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/559/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/559/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/539/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/539/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/301/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/301/
Excellent stuff Roland :-)
LGTM (Looks Great To Me)
LGTM (and it's a great to make it pluggable)
pong (i.e. it is done)
I can't see the difference between the old commit and the new, what was changed? (or should I go through it all again?)
Since I moved things around (to get that clock() thingy working) you would not have seen much anyway, since the rest has been redone more or less completely. Plus I fixed some things in the core algorithm (the null/Empty thing and during shutdown).  So, in this case Id like to ask you to look it all over again.
Ok, will do.
Looks awesome, great work Roland!  Would be nice if we could deprecate the HashishWheelOfTimer
merging it in now; Ill prepare a PR for removing HashedWheelTimer afterwards
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/537/
LGTM if you guys say that it solves the problem
Well, looking at it with some tool cannot hurt :)
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/537/
I actually verified before pushing the fix that without it we got threads that are not from the dispatchers and not normal worker threads hanging around for a while after running the remote tests, and with the fix, they shut down directly. 
LGTM (just to be sure :)
:+1:  merge!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/294/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/294/
Apart from the `to[SortedSet]` I have incorporated all feedback in #1027. Im not sure whether this PR should be merged, for sure it should not be backported since someone might rely on the old behavior of always getting `isCancelled == true` after calling `cancel` (it breaks at least one of our tests). If the new AkkaTimer is just better in all regards, then we should scrap the HWT and then there would be no point in fixing it.  Comments?
closing as per my last comment
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/549/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/549/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/549/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/549/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/293/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/293/
ok, it looks like it could be the reason to the failure :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/541/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/541/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/297/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/297/
Oops, I broke the tests. Fixing.
Closing until fixed.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/296/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/296/
Did this fix the SSL handshake?
I don't know, because it has not failed on my machine, nor on the job I have on jenkins.typesafe.com. But there are other tickets related to updating Netty.
Ok, then lets merge and observe
I am the surface worker, so I will be the first one to file the failure   anyway.
Patrik you are great!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/303/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/303/
Fixed the hash collision problem. Feedback handled. Found the regression detected by StressSpec. 
You are amazing Patrik!
yes, very true. (me not so much because Im guilty of not having reviewed this PR yet)
thanks, but this was real team work -- great ideas!
:-) GO TEAM!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/306/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/306/
Fixed things from last review round. I'll squash when thumbs up.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/309/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/309/
apart from the debug method layout of `def senders` above: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/302/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/302/
Updated according to comments
apart from comments: LGTM
Great one, Patrik!
Please sign Typesafe CLA: http:www.typesafe.com/contribute/cla
LGTM after signing the CLA
Done! Thanks, Mike  On Jun 20, 2013, at 2:02 AM, Patrik Nordwall <notifications@github.com> wrote:  Please sign Typesafe CLA: http:www.typesafe.com/contribute/cla   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1541#issuecomment-19732541> .
LGTM, thanks (I have verified CLA)
Hmm, I'm pretty sure I tried this out locally and it worked; the change LGTM anyway.
there's a regression test added in the PR, so I'm pretty sure that it didn't work without it.
I must have done something wrong, obviously.
Or the STATIC flag works differently on different Java versions for top-level classes.
Well, in any case, it's all gud nao
no additional test?
LGTM, but I have only a vague understanding of this
I have run full builds on this branch 9 times tonight. No failures. https:jenkins.akka.io:8498/job/akka-local-repeat
Thanks a lot!  20 jun 2013 kl. 08:26 skrev Patrik Nordwall:  > I have run full builds on this branch 9 times tonight. No failures. > https:jenkins.akka.io:8498/job/akka-local-repeat >  >  > Reply to this email directly or view it on GitHub. >   -- [scala-debate on 2009/10/2] Viktor Klang: When will the days of numerical overflow be gone? Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January 2038
I would like to see some of the relevant scheduler tests running with `tick-duration=10ms`
added the actual fix for the problem (doh) and a test which verifies it; @drewhk would you please rerun the test on Windows?
Windoz Kitteh aksept job
U CAN MERGE NAO
Nice! No test?
Thanks for the clean-up!
some small fixes needed, otherwise LGTM
fixed the mistakes
I don't think this should be backported to 2.0 Fw port to master of course. Current ticket strategy (if I haven't missed something) doesn't mandate a separate ticket for each version. Set the milestone to the lowest version in which it was fixed and then it's assumed to be fixed in master (later versions) also.
OK, I'll close all the forward/backport tickets I made. I'll wait to close the main ticket until the fix is on both 2.1 and master.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/527/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/527/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/542/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/542/
Apart from comments, LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/530/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/530/
Changed based on review.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/530/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/530/
aside from the test shutdown thing: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/536/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/536/
:+1:  merge!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/538/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/538/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/323/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/323/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/522/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/522/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
also, create ticket: https:www.assembla.com/spaces/akka/tickets/new and refer to the ticket number in the commit message (makes it easier to track for cherry-picks etc)
well that's the last time I C&P from @viktorklang :-P  I'm getting access denied when trying to log in to assembla with my google account.
@patriknw I don't have permissions to create a new ticket at assembla. I've updated the patch.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/583/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/583/
@fommil have you tried the instructions here: http:doc.akka.io/docs/akka/snapshot/project/issue-tracking.html#Creating_tickets  For now, I have created the ticket for you: https:www.assembla.com/spaces/akka/tickets/3086
LGTM, please squash into one commit and include the ticket number 3086 in the commit message Thanks for contributing!
LGTM (after squashing)
@patriknw thanks I'm a "watcher" now. I also have a non-3G connection now, so I'll be able to do this all properly. I don't know how to squash commits, so I'll have to do some research on that.
great, you squash with git rebase --interactive, or simply git reset to the commit before your commits, and then commit again. Thereafter git push -force to this same branch, and the pull request will be updated.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/517/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/517/
Updated according to Viktor's comments (thanks!). I am a bit unsure about the error handling on the receive side, please review it.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/521/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/521/
aside from the minor comments: LGTM
@drewhk Could you please add a test for message-frame-size as well? The code also fixes 3038. I marked it as a duplicate of this.  Aside from that LGTM
Are you sure this fixes that one too? Well, the test will tell...
As far as I can see, we set up the frame size in a `LengthFieldBasedFrameDecoder` in the netty pipeline, and it will throw a `TooLongFrameException` during encode if the payload doesn't fit.
Isn't that a separate pipeline stage than the ProtobufCodec I hooked into?
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/521/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/521/
Added reporting for oversized messages so channels are not closed.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/529/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/529/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I broke the durable mailbox test, will fix.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/529/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/529/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Kitteh reported the previous build.
This seems to be the correct build: https:jenkins.akka.io:8498/job/akka-pr-validator/533/parameters/?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/546/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/546/
Updated akkording ( ;) ) to Patrik's comments.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/547/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/547/
Great work Bjrn!
looking good (although Id like to understand those PoisonPills); good catch with the TestConductor!
Added better comments for the PoisonPill cleanup in the tests.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/320/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/320/
LGTM, merge away!
great (even though I think the PoisonPill stuff was un-dry)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/319/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/319/
Sorry, this looks massive because I've not merged yet. I can't do it on this machine at the moment. The only bit of relevance is the final commit.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/518/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/518/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
@drewhk I think it'll become a separate PR once I merge upstream changes into my repo. Github isn't able to auto merge and I'm currently in a low bandwidth connection and unable to do this locally. :-(  @viktorklang that was what I did the first time but I thought it looked messy. You sure you want it as a one-liner?
Ping us with a comment when there is a fresh pull request with one commit. Is there a ticket for this?
@patriknw aren't pull requests also tickets on github? That has actually always annoyed me slightly.
@fommil, might be, but Akka tickets are in Assembla: https:www.assembla.com/spaces/akka/tickets/new
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/318/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/318/
Oops.  You are right.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/327/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/327/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/519/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/519/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
LGTM Is it important? Backport to 2.1?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/315/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/315/
Passing the envelope through looks good. Thanks.  But, Monsieur Chartreuse, there is a failing test!  Test failure is probably that the envelope creation is no longer wrapped in a try/eventStream.publish.
I'm sorry, that's what I get for pushing while running the tests, :-(  Will fix asap tomorrow
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/316/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/316/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/317/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/317/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/324/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/324/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/525/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/525/
I am not sure about this style. I liked when we had the ifs on the first line. But it seems that I am the minority :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/310/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/310/
I like this style.  LGTM, apart from the unnecessary line wraps and arrow misalignment that @viktorklang already pointed out. 
an improvement in every single case (even the ones I commented upon): LGTM
The extra line breaks introduced by scalariform in eclipse can be avoided by using braces as suggested by @rkuhn 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/524/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/524/
Hmm, dunno what I did wrong back then, I tried it out in the REPL. But good to have a test!
ok, closing this, and will add the test to LoggerSpec when #1162 has been merged
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/308/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/308/
Are we sure we have removed it all now? :-)
There is only config related stuff remaining there, I remove them in the   next round and unify the configuration.
Please double check RemoteTransport, because I removed quite a lot of   stuff from that class that is deprecated (i.e not used in the new   remoting). I want to be sure that everyone agrees on the changes.
I assume we have the same tests in the new remoting, btw
Yes, and many more. The new config does not have a test yet, but that obviously needs the refurbished new config first :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/307/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/307/
LGTM (did only a brief scan)
Failed. It seems It seems I messed up something in my branch. Fixing.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/311/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/311/
Buffer overflow strikes again.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/321/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/321/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/523/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/523/
:+1:  fw port to master if needed
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/526/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/544/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/544/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/545/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/545/
oh, good one! LGTM
Great work Bjrn!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/326/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/326/
Please sign Typesafe CLA: http:www.typesafe.com/contribute/cla
LGTM after CLA
I completely understand the need for a CLA, but respectfully refusing to sign at this point.
lgtm. Thank you.
LGTM, nice test!
When is it going to be merged?
I assume we want this cherried into release-2.2 as well? 
no, Ill move release-2.2 branch onto master when everything is merged (holding off on the one thing that shall not go in)
Excellent, thanks Roland
Has this been verified somehow?
I have made things a bit slower by inserting small pauses in the `AkkaProtocolTransport`, and it reliably fails. That's the only thing that I could think of.
how much slower did you have to make it until it started failing?  in any case: LGTM
Well obviously the total time had to approach 3 seconds, but in total around 2 seconds scattered around the different steps (that localhost machine is really fast at responding).
[spinning off on a tangent] We could repeat on scalable1 with random delays and measure the correlation between total delay and failure, plot the failure density as a function of the delay and then estimate the expected failure rate at delay=0; if that matches the observation of the previous two weeks then were golden ;-)  Long story short: looks good.
LGTM, have you tested it?
I played around a little, is there some test suite for it?
Good, playing around with the simple sample was what I had in mind.
I don't really know how it works so I trust you here.
that is a lot of changes, we should place this under heavy testing
Yes, but they look more scary than they are actually. I did a lot of test runs and also increased the pressure on stress tests.
heavy testing never hurts  (and if it does then we _want_ that ;-) )
I never said do not heavy test it :D Btw, don't mind the first failure above, the second commit is supposed to fix that. (I have not squashed yet so the two commits can be viewed separately)
The last build was successful, just the Kitteh is lazy.
LGTM: keep it as it is
jenkins run was successful
Apart from removing the spinning part...
Now wait for a final kitteh run
LGTM, I have run at least 4 full builds of this branch in akka-local-repeat
Kitteh was happy
Rebased and squashed. I will wait for the next Kitteh round before merge -- since it comes for "free".
Rebased and force pushed.
I have also created an up-to-date version of our message protocol chart: https:dl.dropbox.com/u/4493646/Tcp%20Message%20Protocol.svg  The thing is done with OmniGraffle and consists of a series of small charts that should be well suited for the dev docs. Do you have a preferred way to get this integrated?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/304/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/304/
What's your usual workflow to fix things discussed here? I would just add small commits fixing details, is that the usual way?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/305/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/305/
Really great stuff! I like the test harness.  One thing just crossed my mind: does it support binding to port zero and then getting the real local port number back?
I implemented a completely new strategy how to use direct buffers in 7d89aefb634e6dd2c31ed8f13b6963be4f338352. The thread locals now have gone and we now have a simple pool which connection actors tries to give back its unused buffers to. The pool has both an upper limit of free entries as well as an upper number of bytes in a buffer which it will reuse. Bigger buffers can be created but won't be pooled and reused.  The major advantage of this approach is that when writing we can now copy data into a direct buffer exactly once and keep this buffer (which has always big enough size) until all data is written.  WDYT?
There's no limit on the number or size of buffers you can get, just a limit of how much free buffers it keeps for reusing which I now set to 128KiB * 1000 = 128MiB per default.
So if I want virtually unlimited number of buffers, I just put a large number in there?
Ok, I guess the description of the flag is misleading.   You could put in there a very large number if you never want to free them again (why would you want this?). On the other hand you could set it to zero and still everyone who wants would get a buffer. However, no buffer would get reused.  So what you specify by the product of `max-direct-buffer-pool-size` and `direct-buffer-size` is the maximum number of bytes wasted by currently unused buffers.  Of course, there's a spectrum of possibilities of how to grow and shrink a pool depending on the current usage. It's hard to say which is the best strategy for different use cases. Maybe we could make the pool implementation itself configurable.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/312/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/312/
Beautiful tests, great work!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/314/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/314/
Awesome progress here! So great to collaborate with you guys!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/325/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/325/
I pushed a new version of the DirectByteBufferPool which keeps soft references to a fixed number of fixed sized buffers. If the pool is full a buffer offered won't be reused but will have to be garbage collected.  Is that what you had in mind?
Yes, that's the simple approach that I would like.  A quick comment on the code. The new `DirectByteBufferPool` will not take into account that any one of the occupied slots can be "freed" by the garbage collector (not only the topmost). This could lead to the pool being "full" even though it's mostly empty.  You could add your `SoftReference` to `ReferenceQueue` and then have a scavenger thread that removes those freed entries from the pool, or simply try to compact the pool on every N:th acquire/release operation.
I suggest keeping things simple and avoid complicating things with SoftReference, ReferenceQueues, manual GC and compaction.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/332/
Fine @viktorklang. You're probably right. As long as we document the implications of tuning those pool settings wrt the `-XX:MaxDirectMemorySize` flag.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/332/
I don't have to keep the SoftReferences. When thinking about it again now and including the comment from @bantonsson I don't think they will pull their weight.
Yep, let's optimize/tweak this later, there are probably tons of dimensions to this yet to be considered.
Ok, I've reverted the change regarding SoftReferences.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/335/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/335/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/338/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/338/
Ok, guys, I think we are ready for the next round of reviews. In our eyes we are feature complete with an "ok" initial test coverage.  Bring it on! :) 
I have also updated the [message protocol diagrams](https:dl.dropbox.com/u/4493646/Tcp%20Message%20Protocol.svg) according to the latest implementation.
Great work, guys, this starts looking awesome! Id like to click that green button rather sooner than later so we can start working on the SPI (and trying it out while implementing UDP), so that this can actually go into 2.2-M1.  @sirthias could you commit the flow charts as individual files so we can embed them in the docs? Thanks a lot!
_Starting_ to look awesome? It's been sparkling with awesomeness for ages! :-)
No problem. Where do you want them? Where should I put the omnigraffle file?
Hmm, they show all the details, but Im still leaning towards including them in the user-level documentation for Tcp, which means that they should go in akka-docs/rst/images and will be referred to from rst/scala/io.rst and rst/java/io.rst.
Yes, what I meant is that even relative to the starting level of really good it now becomes awesome :-) It is a delight working with @sirthias and @jrudolph!  23 jan 2013 kl. 15:20 skrev Viktor Klang (): > Starting to look awesome? It's been sparkling with awesomeness for ages! :-) > 
Awesome, thanks a lot for all of your comments and for being as rigorous as you were.
I think this might just be the pinnacle of OSS collaboration!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/520/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/520/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/329/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/329/
Please add a comment about why the line stays in `finishTerminate` (it is due to the emergency exit path taken from `handleInvokeFailure`), squash the commits, and then I think we can merge the very first external contribution to the dungeon!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/333/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/333/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/328/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/328/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/340/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/340/
Ive skimmed it once, will have to redo once you declare it done ;-)
Ready for final review. Everything, including documentation, completed.
This is awesome! Great work, Patrik! And you get bonus points for extra boy-scouting :-)
thanks mate - yeah it was a major effort to cleanup after you :-) FSM rocks!!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/340/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/340/
This is so cool. Have nothing to add. LGTM!
Thanks, I will fix the minor comments and the collect of handOverData in Leader and WasLeader states after lunch, then I will merge this. Nice to have it done this week.
nice, indeed!  verr najs!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/34/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/34/
also cherry-picked to release-2.1
wait, something was lost when... will update
fixed, please review
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/35/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/35/
Just skimmed it, will review it later. Could remote deployed actors cause a problem? I am thinking of name mangling.
That's a valid point, I don't know if it's a problem, and I don't think it is a scenario we need to support. How would you setup such routers? create some remote deployed actors outside the router, pass their references to routees constructor parameter of **several** routers on different nodes --- think that is rather artificial
Merge? We can discuss the other mechanics I was talking about later.
great, I will merge and cherry-pick this tomorrow morning I will change so that the toString is not a val then we can consider Viktor's idea in Coltrane 
cherry-picked to release-2.1
+1 backport :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/36/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/36/
great, I will merge and cherry-pick this tomorrow morning
cherry-picked to release-2.1
Excellently done Bjrn!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/37/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/37/
LGTM. Very nice catch. I will port it to the new netty driver as soon as this is merged.
Merge and backport to release-2.1
LGTM. Awesome that you found this!!
And even greater that we learned something. Btw, do we plan to file a ticket for nss_ldap? The buffer overflow is still not nice...
Yes, I'll file a ticket for the nss_ldap lib.
Nice. Tested the links. LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/38/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/38/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/330/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/330/
Sure, I'll cherrypick
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/336/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/336/
Updated: Refactored from "promise passing" and "onComplete" juggling to future composition.
FYI: going with the inertia, I split TCP and UDP into separate drivers as well. I won't commit it here, but as a separate PR.
PLS REBUILD ALL
yes, great work: LGTM
PLS REBUILD ALL
Kitteh is down. What should be the course of action here?
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/334/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/334/
yes, this design looks good, great work Endre!  (I only reviewed the config file and skimmed the rest, assuming that that are just fixes for the resulting fall-out)
I did a clean build test because the kitteh is out for a stroll. Proceeding with merge, be the Jenkins gods with us.
LGTM! Merge into master and release-2.1 please
cherry-picked to release-2.1 done
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/337/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/337/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/339/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/339/
LGTM  The failure is unrelated to the change. Ticket created. 
Backport or not?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/31/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/31/
Applied review feedback
Can you add an example with the pattern pipe to the section Use with Actors?
Thanks! Have not reviewed it yet, will look into it later
Oh, it closed :)
Opened a new, squashed, one.
I like it! :+1: 
Pure awesomeness! :+1:
Now users of `requiring` will get inlining if they compile with `-optimize`. I compiled the cluster module with `-optimize` to verify this.  (I couldn't compile all of Akka with `-optimize` due to a Scala bug; I added a [comment](https:issues.scala-lang.org/browse/SI-5322?focusedCommentId=62298&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-62298) to the relevant ticket.)
Apart from comments. LGTM
Very nice work Rich. 
Just as a thought: a macro-based solution would get around the inlining / `-optimize` issues right away, no?
well, AFAICS this is only used while parsing configuration settings, which should not really be a hot path 
If we do the change from `ConfigurationException` to `IAE` then there is nothing that is configuration-specific. And since this is a sexy addition I (and others too, I'm sure) would like to use it in our own, non-config and possibly "hot", code as well...
aaaaah, I seeeeeee
Added comments, removed methods without message text.  Thinking about it, do we even need the method that takes a `cond: Boolean`? We'd probably only ever use the `cond: A => Boolean` method.  A macro sounds like fun but it might take quite a bit of work. :)
> Thinking about it, do we even need the method that takes a cond: Boolean? We'd probably only ever use the cond: A => Boolean method.  When you need to reference the value more than once in your predicate the first variant can be a bit shorter:      d requiring (0 <= d && d < 10, "d must be >= 0 and < 10")  versus:      d requiring (x => 0 <= x && x < 10, "d must be >= 0 and < 10")  So, depending on style, it might add value.
Here is the macro-based solution:      object Helpers {       implicit class Requiring[A](val value: A) extends AnyVal {         def requiring(cond: Boolean, msg: Any): A = macro HelpersMacros.requiring0[A]         def requiring(cond: A => Boolean, msg: Any): A = macro HelpersMacros.requiring1[A]       }     }      object HelpersMacros {       type RequiringContext[A] = MacroContext { type PrefixType = Helpers.Requiring[A] }        def requiring0[A](c: RequiringContext[A])(cond: c.Expr[Boolean], msg: c.Expr[Any]): c.Expr[A] = {         import c.universe._         val c.Expr(Apply(_, args)) = c.prefix         val value = c.Expr(args.head)         reify {           if (cond.splice) value.splice           else throw new IllegalArgumentException(msg.splice.toString)         }       }        def requiring1[A](c: RequiringContext[A])(cond: c.Expr[A => Boolean], msg: c.Expr[Any]): c.Expr[A] = {         import c.universe._         val c.Expr(Apply(_, args)) = c.prefix         val value = c.Expr(args.head)         reify {           val v = value.splice           if (cond.splice(v)) v           else throw new IllegalArgumentException(msg.splice.toString)         }       }     }  It inlines everything directly, without boxing and without thunk creation for the condition or message.
yes, the macro is a simple one, but we would not be able to use it without major surgery because then wed need to make a new project with it and akka-actor depending on it, and then merging the JARs  so I think well go with the inlining for now
well leave the Nat_0 glitch in to lure someone else into contributing a small fix later ;-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/30/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/30/
I personally like the use of foreach in the Scala examples, but isn't easier to understand for beginners if onSuccess is used as in the Java examples? WDYT?  Otherwise LGTM.
I think a Scala programmer will understand "foreach"
Sooner or later, yes. At the beginning it may be confusing. It is your   choice though, I am happy either way :)
... and one more class down the drain. Excellent, LGTM!
aside for the hashCode/equals in HashedWheelTimeout: :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/33/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/33/
LGTM!  Backport to release-2.1?
LGTM  Like this one a lot better than the last "fix". 
also cherry-picked to release-2.1
Regarding remState + attach that's a remnant from when remState didn't call attach, good call. Attach is a conditional construct required for guaranteeing invariants, so it should most definitely be in remState. remState is also private, so it is under control. Thanks for reviewing Rich! I'll add tests to both the EC impl and the java API of Agent :-)  Docs for Agents also needs rewriting
Rewrote Agent docs and added some more tests, will commence writing some tests for the new ExecutionContext.
Added tests for the new EC
Addressed Rich's nice catches :-)
Yes, I like it now! (and you know what that means ;-) )
Added suspension test :-)
ah, I cant take it, the _SUSPENSE_  now it only need migration docs
As soon as everyone has OK:ed this change in Agents, I'll add migration docs :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/347/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/347/
LGTM. Really like the `SerializedSuspendableExecutionContext`. 
Oh, I almost forgot. The doc doesn't build. KITTEH PLS COME BACK.
Thanks Bjrn! Currently fixing the docs
LGTM: merge after KITTEH has approved
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/347/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/347/
Great work, thanks for these fixes! Please adapt the two places I commented upon, squash it into one commit and repush (with -f) to this branch, then we can merge it.
okay, heres how Ive done it:      rk:akka rkuhn$ git co pr/1050     Note: checking out 'pr/1050'.          You are in 'detached HEAD' state. You can look around, make experimental     changes and commit them, and you can discard any commits you make in this     state without impacting any branches by performing another checkout.          If you want to create a new branch to retain commits you create, you may     do so (now or later) by using -b with the checkout command again. Example:            git checkout -b new_branch_name      HEAD is now at 9c240cd... Update akka-docs/rst/general/addressing.rst     rk:akka rkuhn$ git co -b tmp     Switched to a new branch 'tmp'     rk:akka rkuhn$ git log --oneline master..HEAD     9c240cd Update akka-docs/rst/general/addressing.rst     52a6f20 Update akka-docs/rst/general/addressing.rst     b3290e1 Update akka-docs/rst/general/addressing.rst     2aaec94 Update akka-docs/rst/general/addressing.rst     45a69cc Update akka-docs/rst/general/addressing.rst     d5eef86 Update akka-docs/rst/general/addressing.rst     a207ce5 Update akka-docs/rst/general/addressing.rst     ab192e4 Update akka-docs/rst/general/supervision.rst     0e1cb63 Update akka-docs/rst/general/supervision.rst     68c8706 Update akka-docs/rst/general/supervision.rst     1e684e3 Update akka-docs/rst/general/actors.rst     rk:akka rkuhn$ git rebase -i 1e684e3^     [detached HEAD 4848e82] various doc fixes (spelling, wrong words, clarity)      Author: Derek Mahar <derek.mahar@gmail.com>      3 files changed, 36 insertions(+), 35 deletions(-)     Successfully rebased and updated refs/heads/tmp.
PLS REBUILD ALL
I don't have merge rights, so after the kitteh says ok, PLZ MERGE
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/60/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/60/
Not merge yet, there is one mistake, sorry.
I didn't say "remove it" I said "set it to 0" :)
Updated according to Viktor's recommendations
Apart from comments LGTM.
apart from the `Exception` LGTM Thanks!!
Fixed according to comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/22/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/22/
Should we backport this to release-2.1?
I don't think it is critical, but it is up to you.
Alright, could you squash this into 1 commit so if we want to cherry-pick it into release-2.1 it's a one-stop-shop?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/29/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/29/
Guys, is this ready to go or not?
I'll fix according to Roland's comments, squash it and update the PR. Any other comments or change requests?
No further comments. LGTM.
Updated PR according to Roland's comments
Should this be backported to release-2.1 once merged?
should be backported into release-2.1 as well, right?
Yes, will do when passed review.  18 okt 2012 kl. 18:12 skrev Viktor Klang () <notifications@github.com>:  > should be backported into release-2.1 as well, right? >  >  > Reply to this email directly or view it on GitHub. > 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/21/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/21/
no, something much more serious is going on here, closing this PR, see https:www.assembla.com/spaces/akka/tickets/2630
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/346/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/346/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
same failure as in the other one
Are we sure that wip-IO built correctly after the original PR? I just looked again and it seems the last time jenkins ran on it, it failed: https:github.com/akka/akka/pull/1030#issuecomment-12512118
right you are, so it was always wrong ;-)
I'll fix the test and send a PR for that first.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/346/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/346/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
This didn't work but I suspect because it didn't include the fix from the other PR. If you look into the log the merge was        Updating 93fc9f1..44d6102  but 93fc9f1 is the version preceding the fix.
yes, it seems that the KITTEH is not always picking up the latest, Im currently trying to figure out why that is; thanks for staying on top of this!
I rebased this pull request on the current wip-IO so the tests should now succeed.
PLS REBUILD ALL
Yes, indeed, the test did succeed (though I had to trigger it manually since the KITTEH is sleepy again).  Thanks for this addition!
@drewhk do you want to transport to your config change manually, or is it better to rebase on top of your branch?
No, I will rebase mine.
sounds good, thanks
I think this can be merged
this PR must be based on wrong branch or something the config version range fix should be merged to master and release-2.1
Thanks for the report! However, your PR doesn't go the full way, I've opened another PR with the correct new version range.  Thanks for reporting this!  Any other problems with OSGi?
Hi Victor - thanks for catching that version range thing, didn't see that.  I'll be playing with Akka and OSGi in the next few days, especially remote actors, and I'll let you know if I find anything.  Cheers,   -Dave  On Thu, Oct 18, 2012 at 6:37 AM, Viktor Klang () <notifications@github.com>wrote:  > Thanks for the report! > However, your PR doesn't go the full way, I've opened another PR with the > correct new version range. > > Thanks for reporting this! > > Any other problems with OSGi? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/811#issuecomment-9564494>. > >    --  Dave LeBlanc <david.leblanc@gmail.com>
Thanks Dave! Looking forward to more feedback on the OSGi support, I really appreciate that.  Cheers, 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/344/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/344/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
closing of connections seems broken by this patch
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/352/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/352/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Thinking about the security implications, there is no precedent for anything being secure at the moment: each actor can send whatever to every other. So I tend to think sending the filename would probably be fine. The configurable dispatcher to run the actual sending on is one way to do it, another could be to wrap the `transferTo` call in `blocking`.  Right now the PR does not merge cleanly anymore.
I had no time yet to look over it again and make it merge cleanly again and fix the remaining issues. I will try to get it done this week.
I'll have to recreate the pull request to target the right branch.
Superseded by #1321
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/846/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/846/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/345/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/345/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
the jenkins failure seams to rely on git merge error, am I true?
Yes, this failure was because the branch can't be merged cleanly onto master. You have to make your branch merge cleanly (either by rebase or merge of master) to make the build kitteh happy.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/345/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/345/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I'm not expert in Jenkins, I've just rebased this commit on akka/master but did not succeed. As this may conflict with the previous PR, please tell me if I need wait to rebase on akka/master. 
hmm, not sure whats wrong here: on the PR page the button is green, but the KITTEH does not agree, will investigate
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/355/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/355/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
ah, okay, now you have something to work with ;-)
thanks, sorry for that mistake (bad rebasing I think) let's hope this time will be the good one 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/355/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/355/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
maybe master is moving to fast for me, I've just rebased
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/355/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/355/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/359/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/359/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/367/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/367/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
you forgot to adapt things in akka-osgi-aries
Sorry for that point, it's actually strange (as I gave default value for OsgiActorSystemFactory.config) Anyway, corrected in BlueprintActorSystemFactory
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/367/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/367/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
so, there are 3 files in this commit and it does not seam to be taken into account in this jenkins job. So sorry, but I don't know
hmm, no idea, but the current state of this branch is not what was tested (thats true) and the merge button is also not green (which is also true ;-) ).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/367/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/367/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
So, tried to create a branch directly on akka/master, without debasing.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/379/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/379/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
the failure was not yours: merging now (sorry for the KITTEH disturbances, I dont know why it dislikes you so much)
now if only we had a build KITTEH   LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/342/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/342/
no changes to rst docs? is covered by another ticket? (I understand why we have the divergence, but we must get them in sync asap)  no changes to samples?  aside from that, which you I guess you will address in another PR, :+1: 
Yes, they are the next PRs. I delayed them until all the changes are in   place.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/342/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/342/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/343/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/343/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/351/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/351/
Apart from comment about `ThreadLocal`, LGTM
Should also be backported/cherry-picked for release-2.1
fixed all comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/364/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/364/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/364/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/364/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
hmm, one more failure (as noted in the ticket)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/377/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/377/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/401/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/401/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/407/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/407/
Thanks for the preview comments!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/365/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/365/
Thanks for the comments Rich, you are my hero :)
After the changes proposed by @richdougherty. LGTM.
I trust you guys: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/365/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/365/
Ummm, this doesn't really seem like it fixes 2618, but perhaps you're adding that flag (preferIPv4Stack somewhere else?
Yes, don't close the ticket. This is only an enabler for the workaround, as I wrote in the PR: "Doesn't solve ticket 2618 but makes it possible to use the workaround, and is useful in general."
Alright, I blame jetlag. Great work Patrik
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/341/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/341/
Thanks Patrik, looking good! I checked the BC questions and the affected classes are `private[akka]` so they only need to keep working, not keep being subclassable by external parties. This means that this PR does not violate BC AFAICS.  This is the last cluster update for 2.1.x, right? Could you please send me a summary of the changes so they can then be listed in the 2.1.1 release notes when that comes out?
I'll give you the list of tickets. Should the ticket milestone be changed, or should I just post a comment on each ticket?
ClusterDeathWatch is not failing on jenkins. https:jenkins.akka.io:8498/job/akka-release-2.1/  git bisect helped me track down the failing commit for ClusterDeathWatch.      [08:16:11] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect good de420ec38a05928d5ee3b2ca9ff28e0c2dd22f2e     Bisecting: 162 revisions left to test after this (roughly 7 steps)     [c5be2096e3e93ff9bf1efdd4f1de2ffd8d12b013] MatchError in RemoteDaemon case AddressTerminated, see #2660 (cherry picked from commit 06199e3f8f6b27e63895495b9b358aee140d0239)     [08:16:36] [patrik@fastlane /Users/patrik/dev/akka]     $ go c5be2096e3e93ff9bf1efdd4f1de2ffd8d12b013     HEAD is now at c5be209... MatchError in RemoteDaemon case AddressTerminated, see #2660 (cherry picked from commit 06199e3f8f6b27e63895495b9b358aee140d0239)     [08:17:05] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect bad     Bisecting: 82 revisions left to test after this (roughly 6 steps)     [91f6c5a94dad08a481ffaea24d3ba55b7edec1ef] Adjust barriers/checks in LeaderElectionSpec, see #2583     [08:22:22] [patrik@fastlane /Users/patrik/dev/akka]     $ go 91f6c5a94dad08a481ffaea24d3ba55b7edec1ef     HEAD is now at 91f6c5a... Adjust barriers/checks in LeaderElectionSpec, see #2583     [08:22:28] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect bad     Bisecting: 39 revisions left to test after this (roughly 5 steps)     [972f669c87fbe1586bd5682916ee6866a75f803b] Adding addressTerminated to UntrustedSpec     [08:27:32] [patrik@fastlane /Users/patrik/dev/akka]     $ go 972f669c87fbe1586bd5682916ee6866a75f803b     HEAD is now at 972f669... Adding addressTerminated to UntrustedSpec     [08:27:43] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect good     Bisecting: 20 revisions left to test after this (roughly 4 steps)     [a471545eecc01222f7c154507fd07e0f06882a1d] Merge pull request #779 from akka/wip-2586-testconductor-transport-patriknw     [08:37:07] [patrik@fastlane /Users/patrik/dev/akka]     $ go a471545eecc01222f7c154507fd07e0f06882a1d     HEAD is now at a471545... Merge pull request #779 from akka/wip-2586-testconductor-transport-patriknw     [08:37:17] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect good     Bisecting: 9 revisions left to test after this (roughly 3 steps)     [73ad31baa4331b44927e1b669740aa1ebc1ee0bb] Merge pull request #789 from akka/wip-2600-tags-opt-out-patriknw     [08:38:49] [patrik@fastlane /Users/patrik/dev/akka]     $ go 73ad31baa4331b44927e1b669740aa1ebc1ee0bb     HEAD is now at 73ad31b... Merge pull request #789 from akka/wip-2600-tags-opt-out-patriknw     [08:38:57] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect bad     Bisecting: 4 revisions left to test after this (roughly 3 steps)     [5b0a2ec7ee44ab82f60dda8ccd5fa0114c00d38f] Merge pull request #782 from akka/wip-null-is-not-tasty-     [08:44:49] [patrik@fastlane /Users/patrik/dev/akka]     $ go 5b0a2ec7ee44ab82f60dda8ccd5fa0114c00d38f     HEAD is now at 5b0a2ec... Merge pull request #782 from akka/wip-null-is-not-tasty-     [08:44:54] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect bad     Bisecting: 2 revisions left to test after this (roughly 2 steps)     [64d50f7001df811f9966a5cedbe1fa07470c178e] fix link to CLA     [08:46:41] [patrik@fastlane /Users/patrik/dev/akka]     $ go 64d50f7001df811f9966a5cedbe1fa07470c178e     HEAD is now at 64d50f7... fix link to CLA     [08:46:47] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect good     Bisecting: 0 revisions left to test after this (roughly 1 step)     [4787c12a29a9af502bd2d7dc735c74d70fd1578c] Fixing a typo in the ScalaDoc for Actor.noSender and clarifying that it is indeed the famous null being used here.     [09:00:10] [patrik@fastlane /Users/patrik/dev/akka]     $ go 4787c12a29a9af502bd2d7dc735c74d70fd1578c     HEAD is now at 4787c12... Fixing a typo in the ScalaDoc for Actor.noSender and clarifying that it is indeed the famous null being used here.     [09:00:16] [patrik@fastlane /Users/patrik/dev/akka]     $ git bisect good     5b0a2ec7ee44ab82f60dda8ccd5fa0114c00d38f is the first bad commit     [09:09:04] [patrik@fastlane /Users/patrik/dev/akka]     $ git show 5b0a2ec7ee44ab82f60dda8ccd5fa0114c00d38f     commit 5b0a2ec7ee44ab82f60dda8ccd5fa0114c00d38f     Merge: 64d50f7 4787c12     Author: Viktor Klang () <viktor.klang@gmail.com>     Date:   Sun Oct 7 22:02:37 2012 -0700          Merge pull request #782 from akka/wip-null-is-not-tasty-                  Changing the default sender of ! to be Actor.noSender to keep null in on...
Hmm, in this case it would make sense to move the tickets to the 2.1.1 milestone, so the list will automatically be taken care of. What Id like to have is a two-line summary of the new features (plus an implied   and lots of small fixes and improvements). Thinking about it, that would fit well into the description of this PR.
bisect is lying :(
The mystery with ClusterDeathWatchSpec found to be due to long timeout of unresolved host on my new network. Increased the test within timeout. Same thing in master, but there shadowed by another error, wrong protocol in the test.
great that you found it!
I have updated the PR description with executive summary. I will move all tickets to 2.1.1 milestone.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/341/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/341/
Cherry-picked (and adjusted sample conf) cluster singleton actor pattern (2895) also
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/341/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/341/
Totally awesome that you nailed it!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/353/
Very good job, Bjrn, that was a tough one!
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/353/
yes LGTM: forward-port and merge?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/353/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/353/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/357/
LGTM; lets see whether the KITTEH agrees
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/357/
thanks, Johannes, now Ill trigger the other two PRs to rebuild
I should have added: this is also a good example for what the boy-scout rule means, keep it up!
LGTM, but I have one question.  Shouldn't we declare a `serialVersionUID` for the messages? I'm just not sure that the compiled case classes will be identical in field and member layout internally in the future even though their serialized form will be compatible.
oh, I had assumed that would already have been the case; thanks, Bjrn, for catching it!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/358/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/358/
Bjrn, Patrik: Good points about serialVersionUID. Actually just adding 'with Serializable' to the SystemMessage trait made the automatic uids different. I've added annotations to preserve the uids, and tests to check we continue to produce the exact same bytes when serializing system messages. Hopefully that will keep us compatible. (We could test byte compatibility for other objects too, if we want.)  Reduced boilerplate.  Read in actual reference.conf for testing.
aside for the reference.conf: :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/358/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/358/
Looks good. Thanks. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/372/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/372/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
failure was in SerializationCompatibilitySpec
I think all timeouts for this test should be looked through
I still find the use of `remaining` to mean `SingleExpectDefaultTimeout` a bit confusing.
Constructive criticism please, I do not know what would be less confusing.
I agree with Bjrn, please use SingleExpectDefaultTimeout explicitly.  I cannot judge the correctness of the fix right now, so Ill trust you guys.
No other test in the entire codebase uses SingleExpectDefaultTimeout.
Yes, that is true. But abusing `remaining` to mean 3.seconds.dilated is not as obvious as it could be. What about exposing this as `defaultTimeout` in TestKit?
No, I literally mean "only wait as long as this test has left to run" i.e  ``remaining``
Apart from the timeouts discussion, LGTM.
Test cahnged to use ``within`` and ticket to fix ``remaining`` has been opened: https:www.assembla.com/spaces/akka/tickets/2971-make-testkit-remaining-throw-an-assertionerror-or-similar-if-used-outside-of-%60%60within%60%60#/activity/ticket:
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/374/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/374/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
this is already forward-ported onto PR #1068 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/375/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/375/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
now with 100% more awesomized DRYness
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/375/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/375/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Rebased and merged with conflicting changes
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/366/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/366/
yes, merge it
If you are sure this is the problem, then LGTM
wow, more than five seconds?  do we know where the time is spent?
I'm not sure. Do you think 5 seconds is beyond all doubt a safe timeout for starting netty? 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/382/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/382/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/382/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/382/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/382/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I can only say that it is mostly instantaneous for me in the REPL; starting a thread pool and binding to a port should not take many seconds. Have we seen this more than once? Maybe it should only be changed for the MultiJVM tests?   Ciao,  Roland  On 28 jan 2013, at 18:40, Patrik Nordwall <notifications@github.com> wrote:  > I'm not sure. Do you think 5 seconds is beyond all doubt a safe timeout for starting netty? >  >  > Reply to this email directly or view it on GitHub. > 
Do we know why it is taking over 5 seconds to start? Is it just because the test server is loaded?  (And: Roland, why are you awake?!)
Closing without merging as we don't want to change the startup timeout based on one single data point.
We now have more data points and they're all on the slow jenkins instance. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/369/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/369/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/369/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/369/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/381/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/381/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/381/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/381/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/381/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
I think you merge right away: failure was in SerializationCompatibilitySpec
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/394/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/394/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
It was the `LightArrayRevolverSchedulerSpec` that failed. Will merge anyway.
yes, that failure was a known one (fixed in #1068 )
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/370/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/370/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/370/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Net negative, Viktor will like it :)   LGTM
The build KITTEH was wrong: SerializationCompatibilitySpec was long fixed   Merge away!
I tested this branch in akka-local-repleat and StressSpec failed a few times. I'll have to look at those failures before merging. The problem is that we had failures in StessSpec before this also.
Fixed the regression. I will run it in repeat a few times before I merge.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/383/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/383/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
that was "Cross-version serialization compatibility" wasn't that fixed? is kitty so far behind?
There was a bug in the pr-validator job: `git fetch origin` should have been `git fetch origin $mergebranch`; fixed that this morning.  I have no idea why the comment was made just now, BTW, since the job was actually started yesterday evening.
Will the `git fetch origin $mergebranch` really solve things? Don't you have merge into the local branch (even if it's just a fast forward?)
feel free to review the script, I restructured it a little; but the recent runs I observed did the right thing now
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/403/
I rebased and got success from jenkins, so I merge this now.
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/403/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
the kitty failure was LARS
LARS is not a cat person
nice one: LGTM
Looks fabulous to me
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/385/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/385/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/385/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/385/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/385/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/385/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
merge: failure was in SerializationCompatibilitySpec
yes, definitely ;-)
I don't think this test is useful right now, so I've submitted an alternative pull request to remove the it entirely: #1083.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/386/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/386/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/386/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/386/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/387/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/387/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/387/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/387/
Apart from the missing `else` LGTM
Changed it to use a mix of quick failing calls followed by awaitCond.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/387/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/387/
LGTM, this supersedes: https:github.com/akka/akka/pull/1081
Me and Rich decided to go with #1081
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/390/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/390/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/390/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/390/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I'll delete the FakeThrowable, just wrapping up the compilation.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/392/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/392/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/392/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/392/
LGTM, can't really see what is changed except the `ignore`
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/391/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/391/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/391/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/391/
good one :+1: 
please create a ticket for it so we have it in the release notes
Check the branch name ;-)
oh, yeah, right  sorry
ticket number somehow in the commit message would be awesome, next time
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/391/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/391/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/399/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/399/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
apart from comment: LGTM
fixed review comments and rebased
What is your opinion about sharing the config using `failure-detector = ${akka.remote.failure-detector}` or not? It's not that much reuse, and we have tests for both so I starting to think they should be separated for the documentation convenience of the user.
I think it is better to leave it more explicit and accept some duplication.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/399/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/399/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
kitty is sorry for https:www.assembla.com/spaces/akka/simple_planner#/ticket:2930
Did the final touch of the reference conf, it's not shared between remote and cluster.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/404/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/404/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
-.- does not seam to be linked to this change
@rkuhn you are true, the comment was false and I misunderstood bnd work: It is not importing akka-actor package as they are in the CP. They need to be either exported or private to be contained in the bundle. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/412/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/412/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
still problems with akka-cluster tests?
yes, but not for you to worry ;-)
Good news: [Akka still can be successfully built on Travis](http:travis-ci.org/#!/gildegoma/akka/builds/1113361).  I investigate how to tune the testing script sequence to get a 100%-sure recipe...  Acutally the troubles come if following warnings appears during compilation:  ``` OpenJDK Client VM warning: CodeCache is full. Compiler has been disabled. OpenJDK Client VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize= ```  I guess that fixing this JVM option should be enough... I'll give some feedbacks.
Memory problem is fixed, and by the way I improved the build time by running the tests in parallel as explained in http:doc.akka.io/docs/akka/snapshot/dev/building-akka.html (now the whole run lasts 15-17 minutes, instead of 20-27 when executed sequentially)  I reworded the original description of this pull request (preamble removed). I think that now everything is ready for integration, if Akka Team wish it...
feature branch rebased to fit in a single commit as required in http:doc.akka.io/docs/akka/snapshot/dev/developer-guidelines.html
I don't understand this pull request.
pull request closed since Assembla issue has been rejected.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/409/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/409/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/409/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/409/
LGTM, please squash so we can merge
Squashed.  Thanks!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/429/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/429/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/410/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/410/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/405/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/405/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/416/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/416/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
yes, looking good, but needs more cow bell ;-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/416/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/416/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/433/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/433/
yes, apart from the two comments and your remaining TODOs in there: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/472/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/472/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/483/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/483/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/483/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/483/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/418/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/418/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/413/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/413/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/413/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/413/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/415/
after removal of manual check: :+1: 
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/415/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/414/
So isn't there a similar problem in `autoReceiveMessage` in `ActorCell`? It uses the same check for `addressTerminated` on the Terminated message. 
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/414/
The replacement for removeChildWhenToAddressTerminated works fine. Here is the ticket: https:www.assembla.com/spaces/akka/simple_planner#/ticket:2993 Please adjust if you think I missed something.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/414/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/414/
LGTM! Incredible detective job!
that was old stuff :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/425/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/425/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/421/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/421/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Very cool. Now we want to find the SPI that would allow us to reuse a lot of this code between UDP and TCP!
Kitteh failure was SupervisorHierarchySpec. The origin of this branch is pretty old, this might have been fixed since, but someone should take a look.
The Selectors for UDP and TCP are almost identical. If you diff the two, most of the changes are removed code in the UDP version and some minor differences.
that is precisely what Im looking at right now; there is obvious room for reuse
And I don't see too much difference in internals when it comes to the connect based UDP API. More interesting will be ZeroMQ.
Precisely; can you start by factoring out the Selector into a generic trait, leaving registration up to the specific impl? My intuition is that zeromq will reuse only the philosophy, but none of the code.
On Thu, Jan 31, 2013 at 5:34 PM, Roland Kuhn <notifications@github.com>wrote:  > Precisely; can you start by factoring out the Selector into a generic > trait, leaving registration up to the specific impl? My intuition is that > zeromq will reuse only the philosophy, but none of the code.   Yeah, something like:  trait SPI  trait SelectorBasedSPI extends SPI  TCP extends SelectorBasedSPI UPD extends SelectorBasedSPI  0MQ extends SPI   >   > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1098#issuecomment-12951173>. > >    --  *Viktor Klang* *Director of Engineering* * * Typesafe <http:www.typesafe.com/> - The software stack for applications that scale Twitter: @viktorklang
Hmm, lets see whether that applies: we are going to share basically only the structure (manager, selector, connection), not so much the code. That trait would probably only require methods giving three kinds of Props and starting the manager itself.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/421/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/421/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
yes, this is going into the right direction
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/447/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/447/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/448/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/448/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/449/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/449/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/451/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/451/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/456/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/456/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/465/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/465/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
[info] TcpConnectionSpec: [info] An outgoing connection  [info] - must set socket options before connecting (26 milliseconds) [info] - must set socket options after connecting !!! IGNORED !!! [info] - must send incoming data to the connection handler (41 milliseconds) [info] - must bundle incoming Received messages as long as more data is available (1 second, 37 milliseconds) [info] - must receive data directly when the connection is established (25 milliseconds) [info] - must write data to network (and acknowledge) (546 milliseconds) [info] - must write data after not acknowledged data (545 milliseconds) [info] - stop writing in cases of backpressure and resume afterwards *** FAILED *** (3 seconds, 48 milliseconds) [info]   java.lang.AssertionError: assertion failed: timeout (3 seconds) during expectMsg while waiting for WriteInterest [info]   at scala.Predef$.assert(Predef.scala:179) [info]   at akka.testkit.TestKitBase$class.expectMsg_internal(TestKit.scala:294) [info]   at akka.testkit.TestKitBase$class.expectMsg(TestKit.scala:281) [info]   at akka.testkit.TestKit.expectMsg(TestKit.scala:641) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$5.apply(TcpConnectionSpec.scala:191) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$5.apply(TcpConnectionSpec.scala:171) [info]   at akka.io.TcpConnectionSpec$$anonfun$withEstablishedConnection$2.apply(TcpConnectionSpec.scala:670) [info]   at akka.io.TcpConnectionSpec$$anonfun$withEstablishedConnection$2.apply(TcpConnectionSpec.scala:655) [info]   at akka.io.TcpConnectionSpec$$anonfun$withUnacceptedConnection$1.apply(TcpConnectionSpec.scala:644) [info]   at akka.io.TcpConnectionSpec$$anonfun$withUnacceptedConnection$1.apply(TcpConnectionSpec.scala:636) [info]   at akka.io.TcpConnectionSpec.withLocalServer(TcpConnectionSpec.scala:509) [info]   at akka.io.TcpConnectionSpec.withUnacceptedConnection(TcpConnectionSpec.scala:636) [info]   at akka.io.TcpConnectionSpec.withEstablishedConnection(TcpConnectionSpec.scala:655) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply$mcV$sp(TcpConnectionSpec.scala:171) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply(TcpConnectionSpec.scala:171) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply(TcpConnectionSpec.scala:171) [info]   at org.scalatest.WordSpec$$anon$2.apply(WordSpec.scala:2179) [info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974) [info]   at akka.testkit.AkkaSpec.withFixture(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$class.invokeWithFixture$1(WordSpec.scala:2176) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2185) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2185) [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198) [info]   at org.scalatest.WordSpec$class.runTest(WordSpec.scala:2185) [info]   at akka.testkit.AkkaSpec.runTest(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2250) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2250) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249) [info]   at scala.collection.immutable.List.foreach(List.scala:309) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:265) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249) [info]   at scala.collection.immutable.List.foreach(List.scala:309) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249) [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326) [info]   at org.scalatest.WordSpec$class.runTests(WordSpec.scala:2250) [info]   at akka.testkit.AkkaSpec.runTests(AkkaSpec.scala:54) [info]   at org.scalatest.Suite$class.run(Suite.scala:2303) [info]   at akka.testkit.AkkaSpec.org$scalatest$WordSpec$$super$run(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2297) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2297) [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362) [info]   at org.scalatest.WordSpec$class.run(WordSpec.scala:2297) [info]   at akka.testkit.AkkaSpec.org$scalatest$BeforeAndAfterAll$$super$run(AkkaSpec.scala:54) [info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213) [info]   at akka.testkit.AkkaSpec.run(AkkaSpec.scala:54) [info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214) [info]   at org.scalatools.testing.Runner2.run(Runner2.java:16) [info]   at sbt.TestRunner.delegateRun(TestFramework.scala:57) [info]   at sbt.TestRunner.run(TestFramework.scala:51) [info]   at sbt.TestRunner.runTest$1(TestFramework.scala:71) [info]   at sbt.TestRunner.run(TestFramework.scala:80) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:190) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:178) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:121) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:121) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) [info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info]   at scala.collection.immutable.List.foreach(List.scala:76) [info]   at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) [info]   at scala.collection.immutable.List.map(List.scala:76) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:121) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:121) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$5.work(System.scala:71) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232) [info]   at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) [info]   at sbt.Execute.work(Execute.scala:238) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232) [info]   at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [info]   at sbt.CompletionService$$anon$2.call(CompletionService.scala:30) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:166) [info]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:166) [info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [info]   at java.lang.Thread.run(Thread.java:679)
The test failure seems to be related to SO_SNDBUF again. Did this test pass in the original branch on Linux? It seems that the OS could allocate more buffer than it was actually requested and the write can just go through without having backpressure in the test.
skimmed it, and apart from comments: LGTM
Awesome work.  The `SelectionHandler` looks usable for any kind of `Selector` now.  LGTM
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/468/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/468/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
[info] TcpConnectionSpec: [info] An outgoing connection  [info] - must set socket options before connecting (52 milliseconds) [info] - must set socket options after connecting !!! IGNORED !!! [info] - must send incoming data to the connection handler (54 milliseconds) [info] - must bundle incoming Received messages as long as more data is available (1 second, 39 milliseconds) [info] - must receive data directly when the connection is established (24 milliseconds) [info] - must write data to network (and acknowledge) (549 milliseconds) [info] - must write data after not acknowledged data (544 milliseconds) [info] - stop writing in cases of backpressure and resume afterwards *** FAILED *** (3 seconds, 53 milliseconds) [info]   java.lang.AssertionError: assertion failed: timeout (3 seconds) during expectMsg while waiting for WriteInterest [info]   at scala.Predef$.assert(Predef.scala:179) [info]   at akka.testkit.TestKitBase$class.expectMsg_internal(TestKit.scala:294) [info]   at akka.testkit.TestKitBase$class.expectMsg(TestKit.scala:281) [info]   at akka.testkit.TestKit.expectMsg(TestKit.scala:641) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$5.apply(TcpConnectionSpec.scala:191) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$5.apply(TcpConnectionSpec.scala:171) [info]   at akka.io.TcpConnectionSpec$$anonfun$withEstablishedConnection$2.apply(TcpConnectionSpec.scala:670) [info]   at akka.io.TcpConnectionSpec$$anonfun$withEstablishedConnection$2.apply(TcpConnectionSpec.scala:655) [info]   at akka.io.TcpConnectionSpec$$anonfun$withUnacceptedConnection$1.apply(TcpConnectionSpec.scala:644) [info]   at akka.io.TcpConnectionSpec$$anonfun$withUnacceptedConnection$1.apply(TcpConnectionSpec.scala:636) [info]   at akka.io.TcpConnectionSpec.withLocalServer(TcpConnectionSpec.scala:509) [info]   at akka.io.TcpConnectionSpec.withUnacceptedConnection(TcpConnectionSpec.scala:636) [info]   at akka.io.TcpConnectionSpec.withEstablishedConnection(TcpConnectionSpec.scala:655) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply$mcV$sp(TcpConnectionSpec.scala:171) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply(TcpConnectionSpec.scala:171) [info]   at akka.io.TcpConnectionSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply(TcpConnectionSpec.scala:171) [info]   at org.scalatest.WordSpec$$anon$2.apply(WordSpec.scala:2179) [info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974) [info]   at akka.testkit.AkkaSpec.withFixture(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$class.invokeWithFixture$1(WordSpec.scala:2176) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2185) [info]   at org.scalatest.WordSpec$$anonfun$runTest$1.apply(WordSpec.scala:2185) [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198) [info]   at org.scalatest.WordSpec$class.runTest(WordSpec.scala:2185) [info]   at akka.testkit.AkkaSpec.runTest(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2250) [info]   at org.scalatest.WordSpec$$anonfun$runTests$1.apply(WordSpec.scala:2250) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249) [info]   at scala.collection.immutable.List.foreach(List.scala:309) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:265) [info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249) [info]   at scala.collection.immutable.List.foreach(List.scala:309) [info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249) [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326) [info]   at org.scalatest.WordSpec$class.runTests(WordSpec.scala:2250) [info]   at akka.testkit.AkkaSpec.runTests(AkkaSpec.scala:54) [info]   at org.scalatest.Suite$class.run(Suite.scala:2303) [info]   at akka.testkit.AkkaSpec.org$scalatest$WordSpec$$super$run(AkkaSpec.scala:54) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2297) [info]   at org.scalatest.WordSpec$$anonfun$run$1.apply(WordSpec.scala:2297) [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362) [info]   at org.scalatest.WordSpec$class.run(WordSpec.scala:2297) [info]   at akka.testkit.AkkaSpec.org$scalatest$BeforeAndAfterAll$$super$run(AkkaSpec.scala:54) [info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213) [info]   at akka.testkit.AkkaSpec.run(AkkaSpec.scala:54) [info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214) [info]   at org.scalatools.testing.Runner2.run(Runner2.java:16) [info]   at sbt.TestRunner.delegateRun(TestFramework.scala:57) [info]   at sbt.TestRunner.run(TestFramework.scala:51) [info]   at sbt.TestRunner.runTest$1(TestFramework.scala:71) [info]   at sbt.TestRunner.run(TestFramework.scala:80) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7$$anonfun$apply$9.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:190) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:178) [info]   at sbt.TestFramework$$anonfun$6$$anonfun$apply$8$$anonfun$7.apply(TestFramework.scala:178) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:121) [info]   at sbt.Tests$$anonfun$makeSerial$1$$anonfun$apply$8.apply(Tests.scala:121) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) [info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) [info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info]   at scala.collection.immutable.List.foreach(List.scala:76) [info]   at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) [info]   at scala.collection.immutable.List.map(List.scala:76) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:121) [info]   at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:121) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:47) [info]   at sbt.std.Transform$$anon$5.work(System.scala:71) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232) [info]   at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232) [info]   at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) [info]   at sbt.Execute.work(Execute.scala:238) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232) [info]   at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232) [info]   at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [info]   at sbt.CompletionService$$anon$2.call(CompletionService.scala:30) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:166) [info]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [info]   at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:166) [info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [info]   at java.lang.Thread.run(Thread.java:679)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/475/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/475/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/476/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/476/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/478/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/478/
Just reviewed the docs (since the rest was already done): LGTM apart from that the samples should be pulled out into compiled code.
The docs LGTM. +1 for samples in compiled code.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/494/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/494/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/420/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/420/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/420/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/420/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
okay, fixed comments and merged master in, lets wait for KITTEHs verdict
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/428/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/428/
converted all todos from the description into separate tickets
I have read the documentation and sample. I like it. Looking forward to try it out on something real. I'll study the impl later. Great work Roland!
thanks for the review!
Great, thanks John, I'll have a look as soon as I can
Have a look at the ExampleSession.scala class.  I've used a Promise based channel callback to handle producer/consumer channel pre/restarts.  Let me know what you think.  I'd like to nail down this lifecycle stuff!
I think you forgot to push
pushed.  thanks!
Great, thanks. I'm technically on vacation the coming 2 weeks, but I'll cheat and give you feedback tonight. Just don't tell anyone.
Cool.  Enjoy your vacation!  I'm going to bombard you with a few questions on your comments above so I have enough to carry me through.  
Did another push with latest work.  I think the next round will be a bit longer since I'm going to tackle the Channel issues and  generally LetItCrash(tm) more.
Should go into both master and release-2.0?
Looks good :-)
Merged into master auto, manually merged into release-2.0 targeting 2.0.1
Great, can you merge it into both master and release-2.0 ?
I don't have write access to push stuff - it merges with no conflicts to current master though
I don't trust people named "Havoc" ;-)  j/k,  I'll handle it. Thanks buddy
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/131/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/131/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/132/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/132/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/135/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/135/
HOLY CRAP. Great find :D
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/134/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/134/
Forward-ported as well?
Yes.  I commented in the ticket, but yes, we should comment here as well when we don't do a pull for it.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/136/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/136/
I'm gonna merge this in 10 if I don't hear anything.
have you've added the caching here as well?
No, I was trying to go with the most conservative minimum fix. but if you want the caching it's just cherry picking the next patch from typesafehub/config master. 
Do I want the caching?
it kinda depends on how deep freeze the release is. if you're really risk averse, don't take any of this pull at all and fix in a point release. if you want the min fix, the caching probably doesn't matter much, it's only saving time at startup or reload. not very important.  if you're feeling more adventurous, the caching is nice and the caching patch has tests and ought to work. finally, if there were several weeks to release you could pull the entire master branch instead of cherry picking, but one of the fixes on master is a pretty big patch that could conceivably break stuff, that's why I've been picking around it.   I'm just not really looped into the release freeze rules and stuff that's why I'm giving info instead of conclusions. but my default would be to just take this pull req as is and then jump to master post release. 
Alright, fair enough. Will pull in for RC4 tomorrow. We've got one final change we need to get in before final
This shouldn't be checked in: akka-kernel/src/main/dist/lib/cmdline-jmxclient-0.10.3.jar
I agree. Should I add it by source? It is just a single file: http:crawler.archive.org/cmdline-jmxclient/xref/org/archive/jmx/Client.html Else we need to add it to Typesafe Maven repo. 
It has to be an executable JAR so I can't embed it as source. Then we have to add it to the Typesafe Maven Repo. 
Add it by source, it's "GNU Lesser Public License" LGPL
Ok, then publish it to the typesafe repo and add it as a dep.
Is this ready to be merged?
Yes. It is just merged with master. 
Alright! So all that's missing now is tests. Create a couple of multi-jvm tests to make sure it works?
Here's the ticket for this: http:www.assembla.com/spaces/akka/tickets/1552-nat-firewall-for-remote-actors#last_comment
Cool, thanks Viktor, will create some tests.
Hey Viktor,   First pass at testing...thoughts?  https:github.com/ticktock/akka/commit/0d28ad31be9c7d87d8dd10965fbfeb0feacf84c1
Hey Scott,  I thought about this some more, and I think I might have gotten carried away here.  Wouldn't we be able to solve the problem by just defining a global address and a local address? So you bind to the local address and you receive things going both to the local and the global address?
Or is there a case where you'd need multiple public addresses? And what about the promiscuous mode, does that make sense in a real-world setting? (accepting any inbound message)
Clsoing this pull request since Scott & I agreed this could be solved by having the notion of a global and a local address, but it needs to be carefully implemented so that it covers all bases (Addresses are tricky beasts to say the least)
Hi Scott, Thanks for the pull request, great that someone is stepping up and takes a stab at this! There are some minor stuff to work out, and in the end I hope we can ship this either in 2.0.x or in 2.1! Thanks again!
Thanks Viktor, I will make those tweaks.  So on the subject of empty whitelist = allow all, In many environments (like the one I am using ; )  the routing can change dynamically at runtime and just allowing all addresses is easier than dynamically updating a routing table at runtime.  So that is the reason for the seperate nat-friendly flag.  nat-friendly = false => no nat nat-friendly = true && nat-whitelist = empty ==> allow all nat-friendly = true && non empty whitelist ==> consult the whitelist  Better / Cleaner way to express this?   
Split it into:  nat-firewall = blacklist | whitelist nat-firewall-addresses = []  blacklist  + no address(es) == all allowed whitelist + no address(es) == none allowed (default) whitelist + address(es) == only allow listed blacklist + address(es) == do not allow listed
Nice, willdo  On Fri, Apr 6, 2012 at 2:59 PM, viktorklang < reply@reply.github.com > wrote:  > Split it into: > > nat-firewall = blacklist | whitelist > nat-firewall-addresses = [] > > blacklist  + no address(es) == all allowed > whitelist + no address(es) == none allowed (default) > whitelist + address(es) == only allow listed > blacklist + address(es) == do not allow listed > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/394#issuecomment-5003288 >
You should also change the wrong documentation in `ActorRefProvider.rootPath` Apart from that :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/120/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/120/
:+1: Will merge into my branch
oops, that cleanup git alias was a bit over-eager
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/115/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/115/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/116/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/116/
So I assume that this is for ticket 2675 *Failure in RoutingSpec on jenkins.typesafe.com*? 
Apart from my comments it looks good. I'm not sure if we should backport to 2.1. It looks like right thing, but should be field tested. If we backport we should run all tests repeatedly for some time.
Hmm, Im not sure about the backporting part. We dont know if it actually fixes something (right?), so Id vote for backporting once we have actually understood the issue and also solved the problem with the CallingThreadDispatcher.
+1 for waiting with the back port
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/128/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/128/
Alright, I consider this ready for merge. What should be backported to 2.1? (We can't ship with the routing bug for sure)
PLS REBUILD ALL
Okay, apart from one comment change (which Ill do), I think this is ready for back-porting into 2.1. Speak soon, or forever hold your piece!
A quick question. RoutedActorCell still calls `init(...).start()` inside the constructor which is called from `newCell`. Is that really ok?  Otherwise :+1:. Nice way to circumvent the hairy startup issues.
Thanks, Bjrn, good catch! (The symptom is unfortunately just a thread leak, no visible test errors)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/140/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/140/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/118/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/118/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/117/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/117/
Why the need to deregister in the first place? restart should be transient, right?
The connection to the camel objects is lost. the producer gets the camel objects at startup from the registry, where the camel objects are kept (separate from the producer of course). When the producer restarts (a new actor instance), the register has no effect cause the producerRegistry thinks its a double registration and the producer does not receive camel objects.  de-register has the (probably nice) effect that if something was wrong with the camel objects, you now have new ones. Now that I'm writing this down I guess I could have also solved the problem by always sending the camel objects at register request, but just not really re-register the double request, in idempotent fashion.
Any downsides in making the registration be idempotent.
hehe, yeah I just thought of that as well. I'm already trying it out.
Downside is that if a camel endpoint would be stuck in some bad state you will have to live with it forever.  But you can always terminate your producer and create a new one on termination. Changed to idempotent is probably best.
Yeah, that's where you have the restart strategy + death watch...
I changed it, don't know if you saw the added commit. Let me know if everything is cool. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/129/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/129/
Should be fine now.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/130/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/130/
Hey Ray, could you squash the commits?
Sure, done.   On Sun, Nov 25, 2012 at 12:49 PM, Viktor Klang () <notifications@github.com > wrote:  > Hey Ray, could you squash the commits? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/893#issuecomment-10692609>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
Aside from last batch of comments: LGTM
Oh I didnt see the last batch of comments, will fix.   On Sun, Nov 25, 2012 at 2:05 PM, Viktor Klang () <notifications@github.com>wrote:  > Thanks Ray! > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/893#issuecomment-10693219>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
fixed   On Sun, Nov 25, 2012 at 2:08 PM, Raymond Roestenburg < raymond.roestenburg@gmail.com> wrote:  > Oh I didnt see the last batch of comments, will fix. > > > On Sun, Nov 25, 2012 at 2:05 PM, Viktor Klang () < > notifications@github.com> wrote: > >> Thanks Ray! >> >>  >> Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/893#issuecomment-10693219>. >> >> > > > > -- > Raymond Roestenburg > > code: http:github.com/RayRoestenburg > blog: http:roestenburg.agilesquad.com > twtr: @RayRoestenburg > >   --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
I will take care of merging and backporting this, now.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/125/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/125/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/137/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/137/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/123/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/123/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/124/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/124/
Looks good, as far as I can tell. However, it is still not binary compatible, strictly speaking (one method removed and signature changed on ForkJoinPool.WorkQueue). This is not supposed to be used externally, I guess, but it is not clearly marked as such.
That method signature change is package protected so it's not reachable.
Merging into master and release-2.0
Have you signed the Typesafe CLA?
Bad joke, if it was not obvious. 
What do we do about the transitive dependencies/3rd party deps that aren't published to Maven Central?
We are planning on doing this for Akka 2.1. Getting rid of some of the deps and embedding others. 
Hey guys, anything you want me to have a look at?
Do you want to merge it?  As far as I am concerned we just need to update the online documentation to reflect the API changes. I can have a look at it after Scaladays.   Piotr Gabryanczyk - Blog: http:blog.scala4java.com Twitter: @piotrga     On 13 Apr 2012, at 17:48, viktorklang wrote:  > Hey guys, anything you want me to have a look at? >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/385#issuecomment-5118758
Can merge when commets are addressed :-)
comments are addressed apart from Any => AnyRef problem in java. Can we merge it please. I promise I'll fix this issue as a priority. I care about this pull request because the scala days demo contains references to this code and it will get people confused if it's not there.
Just make the 2 changes I proposed and I'll merge.  Thanks
pushed now... IntelliJ plays tricks on me with git...
All suggestions eventually applied.
I'll merge and fix manually. Thanks Piotr!
REBUILD ALL PLS
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/104/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/104/
StatsService ruining the day, can we fix it?
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/105/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/105/
apart from nitpicks: :+1: 
Fixed review comments. Merging.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/106/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/106/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/109/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/109/
No info in ticket, and jenkins log is gone. If the issue was to remove the exponential backoff, then LGTM.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/111/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/111/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/97/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/97/
Does it only take that long on the slow jenkins or also elsewhere?  For the change itself: :+1: 
it's normally pretty quick, but it fails with the new values I'd be more sure that there is something else that is wrong
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/98/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/98/
Alright, added shuffling
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/99/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/99/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/100/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/100/
I think a proper fix is to be able to pass in a ClassLoader to createActorSystem, it shouldn't default to classOf[ActorSystem].getClassLoader, this overrides Thread.getContextClassloader etc
Agreed, that should be supported.  I can update the pull request for that but I do think the default behavior of createActorSystem(name: Option[String]) should use the akka-actor classloader as the fallback classloader.  The actorSystemConfig method uses the akka-actor config as the default config reference, and hence, any classes that are loaded dynamically as a result of that config need to be accessible to any bundle that uses the default config.  In this case, the issue can be worked around in at least two ways -- either by avoiding use of OsgiActorSystemFactory entirely and just manually loading configuration and constructing an actor system directly using a proper bridged class loader, or by manually importing akka.event package.  So overall, I think the 2 createActorSystem methods should be modified to include an Option[ClassLoader] parameter that's used as the fallback classloader in the BundleDelegatingClassLoader.  In both cases, the default value of the Option[ClassLoader] param should be Some(classOf[ActorSystem].getClassLoader).  Or alternatively, pass the fallback Option[ClassLoader] as a constructor argument.  Thoughts?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/107/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/107/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/108/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/108/
LGTM. Do we have a ticket for this?
I didn't create an issue, sorry.  Will do so now.
Ticket #2727: https:www.assembla.com/spaces/akka/tickets/2727-actorsystems-created-via-osgiactorsystemfactory(ctx)-createactorsystem-fail-to-load-akka-event-cl---#/activity/ticket:
thanks for contributing, I will merge this now and cherry-pick to master
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/112/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/112/
Fixed review comments, merging.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/114/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/114/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/113/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/113/
better safe than sorry :-) :+1: 
Thanks Irmo.   However I find it a bit odd that you are working in secret on this when you know that John is also working on it (and is close to ready for merge). Why not help him out instead? Open Source is all about collaboration.   We can't have two competing AMQP modules. So what I would like to do is to have John decide if there is anything in your work that is missing in John's, and if so, merge those parts into his branch. I.e. this branch will not be merged in as-is.  Are you up for that John?  Thanks, Jonas. 
Hi Jonas,  This is not in secret but all in cooperation with John ;-)
Good to hear. Then it is all good.  I misunderstood since you wrote that it was done "silently" and you pushed in a complete impl.  So then you will both work trying to get this into a single branch? Thank you.
Silently was maybe not the right word there :-) We needed an updated library in our company that gave more functionality and more stability, but I didn't want to 'waste' time on the additional open-source related work and make it stable and reliable in-house first.  Noticed later that John was working on a new module for 2.0 so I first send him my code for reference. Now, he sort of ran into the same problems I did with the 1.x module which I solved in my new version, so we decided together that I would start a new branch based on my version and when John has signed the CLA, we continue together from there. 
That is very good. Thank you.  The only bad thing is that none except Viktor in the whole team knew about this.  Let's try to be more transparent in the future. We have akka-dev for that. 
Made some additional changes - going out for the weekend, so won't be looking at comments until next week ;-)  
moved to community projects
The events need to be private and should not be exposed. They are for internal use only. Users can use awaitActivation methods from Activation trait. 
Thinking about it, I'll make it private, to not confuse users. It's just that other 'internal' events are published in akka as public classes.
We discussed it some time ago. And we reached the agreement that it is dangerous as they are internals and we are not prepared to expose them, as they are exposing the internals of implementation and break encapsulation.
I've made them internal again.
ah right. minimize exposure.
thank you :)
As a general point: implementing a lazy `path` helps certain use cases, but as soon as somebody puts the ref into a HashMap or compares it to another ref it will be forced. This is kind of natural because hashCode/equals/compareTo are implemented in terms of `path`. It might be interesting to look into overriding those as well and adding another state: name allocated but not registered.
Yes, I thought of that as well but wanted to do one step at a time. I think most cases are the "simple" local ones that wouldn't benefit from such an additional optimization. And the complexity would probably be cranked up quite a bit more ...
Alright, thanks for your great comments! Maybe go for round two of the review?  (Since I've moved the PromiseActorRef class out of the AskSupport trait the diff of the last commit as shown by Github is a bit hard to read, you might want to look at the [actual file](https:github.com/spray/akka/blob/8fc66ce044672603164ccacb73548bd1476939ee/akka-actor/src/main/scala/akka/pattern/AskSupport.scala) instead.  All tests are passing just as before, speedup over the previous version is not really measurably with my crude benchmark.
Apart from the one glitch this looks good (in the sense that my sleepy eyes dont catch anything that sets off warning bells anymore ;-) ). I need to play with it next week, probably writing a few new remoting & lookup tests etc. There have been some really subtle bugs while developing that stuff a few months back, and we had to pull out one optimization literally at the last minute, so forgive me that Im not in the Hooray! state quite yet ;-)
No problem, Roland, it's late here as well... :) And it wouldn't even be a big thing if the team decided to not bring this change into the Akka codebase at all. Just trying to get it done made me learn enough about the Akka core to have a pretty good idea of how to advance our own codebase. And Victors and your comments really have been most valuable! So thank you in any case and have a good night!
One more comment before I forget:  There is one change in behavior from the current (eager) version to this lazy one: Before, the ref was unregistered after future completion but _not_ during a `stop` call. Now it's already unregistered during a `stop`. Somehow the new version makes more sense to me, but (as Victor also hinted at already) I'm unsure about the actual semantics of calling `stop` on a PromiseActorRef (and it's relation to the state of the underlying Future). Just a comment to maybe support you in focusing the further testing of this thing....
Applied Viktors suggestions plus a few smaller things.  One thing I noticed: In my testing I once (and nondeterministicly) got this test failure:      ...     [info] ResizerSpec:     [info] DefaultResizer      ...     [info] - resize when busy *** FAILED ***     [info]   2 was not equal to 3 (ResizerSpec.scala:155)     [info]   org.scalatest.TestFailedException:     [info]   at org.scalatest.matchers.Matchers$class.newTestFailedException(Matchers.scala:150)     [info]   at akka.testkit.AkkaSpec.newTestFailedException(AkkaSpec.scala:56)     [info]   at org.scalatest.matchers.MustMatchers$MustMethodHelper$.mustMatcher(MustMatchers.scala:873)     [info]   at org.scalatest.matchers.MustMatchers$IntMustWrapper.must(MustMatchers.scala:1305)     [info]   at akka.routing.ResizerSpec$$anonfun$1$$anonfun$apply$mcV$sp$6.apply$mcV$sp(ResizerSpec.scala:155)     [info]   at akka.routing.ResizerSpec$$anonfun$1$$anonfun$apply$mcV$sp$6.apply(ResizerSpec.scala:131)     [info]   at akka.routing.ResizerSpec$$anonfun$1$$anonfun$apply$mcV$sp$6.apply(ResizerSpec.scala:131)     [info]   at org.scalatest.WordSpec$$anon$2.apply(WordSpec.scala:2161)     ...  At first glance it appears to be unrelated to the PromiseActorRef implementation. Is this something you have seen before?
That test is timing sensitive.
One more thing I noticed: PromiseActorRef currently caches `provider.DeathWatch` as well as `provider.tempContainer` (as `override val getParent`). It seems to me that the memory footprint of PromiseActorRef could be reduced by at least the DeathWatch val, since accessing `provider.DeathWatch` rather than `this.deathWatch` shouldn't really be much slower. And if the `getParent` call is not frequent enough to make the synchronisation on the `lazy val` of the `provider.tempContainer` matter we might even be able to replace the `override val getParent` with `override def getParent = provider.tempContainer`
I think this is a fair observation. Check if that would work. Thanks
Thanks for this effort! In order to merge this wed need you to sign the CLA at http:www.typesafe.com/contribute/cla
Roland, I already signed this some time ago (for some other path)... Cheers, Mathias
ah, right, sorry I looked in the wrong directory
Currently I am using the .opts files to manually set the IP address of the router to test with.  I am wondering if I should instead use the method suggested in [this stackoverflow question](http:stackoverflow.com/questions/2939218/getting-the-external-ip-address-in-java) to retrieve the external IP address. And then use the .opts files to override that IP when they exist.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/82/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/82/
This branch hasn't successfully built in 13 days, won't make it into RC3
Ok.  I will try to take a look at it tonight and get it working so it can make a future release.  Looking at the jenkins log, I think part of it just needs to be updated to work with the import duration changes between scala-M7 and scala-RC1.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/143/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/143/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/144/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/144/
Jenkins is now compiling this properly.  However it is failing because the router/NAT settings need to be set manually.  I'm not sure if I should rework this, disable it by default or if I should be running a simpler test (see the end of my big comment above).          Cause: java.lang.Exception: multi-node.nat-router property must be set to Router IP! 
Looking through all pull requests: this feature looks like it is very limited in scope, i.e. it only practically works with UPnP (since the ActorSystem will need to know its external address) and only if no local connections shall be made (i.e. NAT-only). You could also implement it without changing Akka by subclassing NettyRemoteTransport to override the `createServer()` method and use that to create your own subclass of `Server` which will unconditionally bind to `0.0.0.0`. Since that should solve your problem without touching Akka source, I would prefer that.  It might be nice to show the resulting code in a blog post on [letitcrash](http:letitcrashcom) so that others can copy it if desired.  One heads-up notice (the knowledge of which influenced my reasoning): the remote transport will be completely replaced by a purely actor-based one in 2.2, which would make your code obsolete in our repository again. @drewhk, how would someone solve this problem in the new remoting?
That looks like an excellent solution.  It may be a little while before I have some time to try it out, but once it is working I'll definitely put something together for a post on letitcrash.
Thanks, looking forward to that! So, Im closing this pull request, but you can of course keep it bookmarked.
Updated with configuration defaults that better mimic the default Netty behavior, so we don't change how things work OOTB.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/83/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/83/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/84/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/84/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/94/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/94/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/89/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/89/
Apart from comments. Looks Godd To Me.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/81/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/81/
It won't matter - if the Bootable setting is set to empty string there will be just the space added that's now between the two "%s"s of main class and Bootable. I have not tested this yet though I will verify this when I get to edit the sample application. Do I have to publish-local for this or is the sample build configuration pick up the plugin from the source automagically?
LGTM, but I think it would be better to not introduce the extra space for the default case
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/95/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/95/
@wookietreiber have you signed the Typesafe CLA? http:www.typesafe.com/contribute/cla After that I will merge this and cherry-pick to master.
@wookietreiber we can not merge this until you have have you signed the Typesafe CLA. It will probably not make it into 2.1 if you don't sign now. http:www.typesafe.com/contribute/cla
I did sign the old akka-only CLA a while back, but didn't commit anything. Is that not valid anymore?
ok, verified, thanks, merging
I will cherry-pick this to master
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/93/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/93/
what about BSONSerialization and filequeue/ ? Im a bit nervous about putting logging in paths which might potentially be used by logging  (especially nice if the loop only happens once in a blue moon in production, believe me)
What if we introduce a TL to signal that we're already doing logging?
Thats the beauty with actor logging: you dont need to get the loop on the same stack ;-)
Yes, I think that is the way to do it.  Other places? In the ticket there was some comment about TypedActor?
TypedActors is hard, what we could do is to check http:docs.oracle.com/javase/1.5.0/docs/api/java/lang/reflect/Method.html#getExceptionTypes() and decide whether we wrap or not. Or we let people add the throws-clause themselves. The only "clean" solution otherwise is to always wrap in a RuntimeException.
Timeout stuff looks good.
I'll review in the morning.
I've had a run-down of the code, sorry if I missed something, way too tired at the moment.
Let me know if I can push this now?
So, added '/system/cluster' top-level supervisor for all cluster daemons. Better I think. Thanks for feedback. 
So is it ready to go? 
No. I could. 
If you are confident about the changes then merge it in, technically no one can review anything but code style until we've sat down and handed things off to the core team.
As confident as I can be. It is hard stuff. I'm sure there are bugs. But I've done my best (in the little time I've had) and all tests are passing. Now it needs hardening. That is the next sprint. I'd be more than happy to walk you through the code tomorrow or any time, but make sure you understand the first half of the cluster spec first. 
Alright merge. We will have to plan 2.1 this week. Thanks
Alright. Thank you. Will do tomorrow. Too tired now. 
You have to take care of yourself man, don't make me force you to take time off! ;-)
Thanks. I'll take it slow the rest of the week. Been very hectic up till today 14:00 CET. :-)
You've done an awesome job. Chillax :-)
Great stuff, I need your email so you can sign the Akka CLA before I can merge it in.
I've sent you an email directly.
Thanks Bruce, I appreciate it!
Should be backported to 2.1
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/92/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/92/
after reducing the `flow` density this should be fine: :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/96/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/96/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/70/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/70/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/69/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/69/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/75/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/75/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/72/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/72/
failure was in cluster (statsFaade), so Ill assume that this PR is okay
Are you going to merge this? I would like to be set to build as soon as all dependencies are available.
Merged to master and backported and merged into 2.1
I don't see it but could have sworn I had AdaptiveLoadBalancingRouter in there, perhaps as the AdaptiveLoadBalancingRouterLike ? Regardless, +1 AdaptiveLoadBalancingRouter name.
I will definitely review this tomorrow!
I have addressed almost all review comments so far, some of the triggered some more major refactoring. See the above commits. I will continue the work on the actual router next week.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/87/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/87/
Ready for final review. I squashed all changes since previous review into one commit, so if you don't want to review it from the scratch you can look at dcde7d3.  This is what has been done in the second round:  * Refactoring of standard metrics extractors and data structures * Removed optional value in Metric, simplified a lot * Configuration of EWMA by using half-life duration * Renamed DataStream to EWMA * Incorporate review feedback * Use binarySearch for selecting weighted routees * More metrics selectors for the router * Removed network metrics, since not supported on linux * Configuration of router * Rename to AdaptiveLoadBalancingRouter * Remove total cores metrics, since it's the same as jmx getAvailableProcessors,   tested on intel 24 core server and amd 48 core server, and MBP * API cleanup * Java API additions * Documentation of metrics and AdaptiveLoadBalancingRouter * New cluster sample to illustrate metrics in the documentation,   and play around with (factorial)
Im once through, looking good as a start (I mean that later a learning algorithm with feedback would be interesting to explore), just a few questions about sigar:  * is it okay to distribute those native libraries * should we have really prominent warnings somewhere that we dont support sigar itself? (we dont have many of those platforms)
@rkuhn regarding sigar distribution, I only included them in the sample, to make it more fun and convenient. We don't publish sample binaries. I'm not sure if it's a problem with the download dist. sigar 1.6.4 is ApacheV2, and is the same version as we distribute with Atmos, which can be important.  Apart from the sample there is no dependency, and it's also the reason for the reflective usage.
I have incorporated feedback in 5eec693  * case object and case class for MixMetricsSelector * Rename decay-half-life-duration to moving-average-half-life * Clarification of decay-half-life-duration and collect-interval * Removed Fields, Java compatibility issue * Adapt for-yield variables * Comment metrics collector constructor that takes system param * Don't copy EWMA if not needed * LogOf2 constant 0.69315 * Don't use mapValues * Remove RichInt conversion * sigar version replace tag in docs * createDeployer factory method to make it possible to override   deployer in subclass * Improve readability of MetricsListener (in sample) * Better startup of factorial sample (no sleep) * Many minor enhancements and cleanups
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/101/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/101/
Ok, I finished reviewing this. Very nice job, I want to try this out very soon :) LGTM
Thanks @drewhk, I have pushed changes based on your latest comments. It would in fact be super awesome if you can try the factorial sample, so we know that it works (with sigar) on windows. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/102/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/102/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/103/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/103/
Damn, this is what I call a PR. Great job. :8ball: 
@drewhk I have done the changes to AdaptiveLoadBalancingRouterSpec, please try again on your machine.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/126/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/126/
Ok, I will do that!
I run AdaptiveLoadBalancingRouterSpec 5 times on my Windows machine and it all passed.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/127/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/127/
Patrik this is awesome. Thanks so much for taking the original nugget to what it is now, looks really great. I'd have added more comments along the way but swamped with work.
I have completed the refactoring to simplify MetricsGossip and NodeMetrics merge. Can I merge this to master now?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/133/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/133/
If it's up to me, I'd say remove it. But Jonas might have a use-case for it
Here is the alternative: https:github.com/jboner/akka/pull/322
We picked https:github.com/jboner/akka/pull/322
okay, heres what Ive got so far:  I found it odd that so many buddies are left in the ConcurrentSkipListSet when only one child had not yet terminated, hence I put assertions around the register and unregister activities and found that sometimes `buddies.remove()` fails.  Detour: I thought hard about what could make it fail and came across a SO question which mentions in passing that System.identityHashCode(x) does not need to be unique, which would make our IdentityHashComparator incompatible with ActorCell.equals(). Replacing it with a .self.path-based comparator did not make it go away, but we might want to keep this in mind.  Finally I saw that the Create() message is dispatched before the ActorCell is attached to the dispatcher, which means that during that crazy test it is well possible that termination happens before registration. Which could possibly explain a whole lot. Now I need to think long and hard whether inverting the order does not break something else.  Of course this does not fix the BD by itself, we still need to find the right way to wake up buddies, but that is part two as far as Im concerned.
So, I think that should fix it. Please have a look. If found okay, Ill remove the scaffolding.
its okay now from my perspective, shall I merge?
Excellent work Patrik! +1
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/65/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/65/
backport to 2.1?
yes   On Mon, Nov 5, 2012 at 10:15 PM, Roland Kuhn <notifications@github.com>wrote:  > backport to 2.1? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/845#issuecomment-10088222>. > >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/64/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/64/
Metric has an isDefined method. Wouldn't it be nicer that when the load-average is not available on the OS then leave the metric undefined instead of a negative value? What if someone just blindly averages with systems where load-average is defined?
I like that suggestion. I will update.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/61/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/61/
Changed to undefined as suggested by @drewhk 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/61/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/61/
Im sure there is a reason for representing the load average as BigDecimal ;-)  LGTM
If these metrics are exposed to ordinary devs, then BigDecimal is the way to save them from themselves. If we use it internally, and performance is a problem, we can use methods like Kahan summation (http:en.wikipedia.org/wiki/Kahan_summation) or other numerically stable algorithms.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/58/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/58/
I guess this is backport material for 2.1?
I vote for backport. Don't want this to fail in 2.1 for no reason.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/52/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/52/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/52/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/52/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/56/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/56/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/53/
ok, I can understand that old is wrong, but I have no clue what is the "preferred" right thing to do.  `%%` feels scary in this case.  Looking at latest sbteclipse, and it's instruction is simply:      addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.1.0")  and it is published here (I think): http:repo.typesafe.com/typesafe/releases/com.typesafe.sbteclipse/sbteclipse-plugin/scala_2.9.2/sbt_0.12/2.1.0/jars/
You should only need to set the scala version to "2.9.2", the version used by sbt 0.12. (and leave the binary version setting as before - otherwise it will use the global setting for this). For sbt 0.11 use scala 2.9.1.
I can take a look at this later today.
As discussed with Josh I think it's a huge bug that SBT doesn't error out if a setting is known to be invalid.
Closing this one. New pull request in #846.
cherry-picked to release-2.1
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/50/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/50/
The PR Validator job broke. I'll look into it.
Lets see if this works. PLS REBUILD ALL.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/50/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/50/
ummm, looks like it's trying to use https
Ignore build 50. Don't know why kitteh posted that one again. Build 51 seems to be building fine, and that's the new one.
So build 51 went fine after I wiped the workspace on the Jenkins slave machine. Seems like there was a glitch after I switched the git repo from http: to git:. 
Forgot to say LGTM.
Hey John! Thanks for this hard work! I've gone through the code (not tests or docs yet, since they'll depend on the refactorings suggested by the comments of the code)  Please don't hesitate to ask if anything I commented is unclear, looking forward to see this into 2.0!  Cheers,
Hey John, could you refresh the pull request to be synched with your latest changes?  Thanks!
Sure.  Will do later tonight when i get home.   Sent from my Verizon Wireless Phone  -----Original message----- From: viktorklang   <reply+i-3349122-22f2160e50d7b0d8e3df380d4bcb862c5b75d022-787382@reply.github  .com> To: jxstanford <jxstanford@gmail.com> Sent: Sun, Feb 26, 2012 17:25:40 GMT+00:00 Subject: Re: [akka] Retrofit of the akka-amqp module to 2.0RC1 (#350)  Hey John, could you refresh the pull request to be synched with your latest   changes?  Thanks!  ---  Reply to this email directly or view it on GitHub: https:github.com/jboner/akka/pull/350#issuecomment-4182759
Ping me when there's more to review :-)
Commented!  This pull request has become quite unmanageable though, way too much things in the diff that shouldn't be there. Did you merge in release-2.0 and then the pull-request is targeting master?
yeah, that's what I did.  If it makes sense, I can dump this pull request, do a new fork off akka/akka, make a branch off the v2.0 commit, put my changes back in, then do a new pull request.  does that seem about right?
I recommend basing it off of release-2.0 since you'll have to open the new pull request against it [release-2.0]. That'd be great, thanks!
the release-2.0 branch is still versioned as 2.0-SNAPSHOT.  The v2.0 tag (commit 6b0f0b080dd844479be239c686a57c451d884e6e) appears to be the official release.  Would it be better to base a new pull request off the v2.0 tag commit?   
Open the new pull request as we discussed. Keep up the good work!
Seems a bit dangerous to me. I think it can go in for now, but in the future it might make sense to think about a cleaner way of FSM composition.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/66/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/66/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/67/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/67/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/68/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/68/
Thanks guys, I'll need to see the espresso machine before diving into this :-)
Skipped the machine and chewed on pure coffee beans during that rebase :-) 
Alright, halfway through, taking a lunch break to reload :-)
Alright!  I'm done with my first rough run-down of the pull-request, over all very nice read! I am a bit concerned by the lack(?) of concurrency related tests for the CamelExtension.  Check my comments and ping me when you're done with them!  Awesome stuff!
Ping me when I am needed :-)
Hi Viktor, only saw your comment now, thanks for the review I'll get started and let you know
Weird. I didn't get any notification either.  And my comments which should explain some of Victor's questions are squashed at the end. Does anybody know what happened?
no idea, strange
Ping me when there's more to review :-)
will do, that d*mn dayjob is getting in the way ;-)
Hey Ray, just ping me as soon as I'm needed!
Will do hopefully I get some work done on it today.  -- Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg  On Mar 15, 2012, at 0:08, viktorklang<reply@reply.github.com> wrote:  > Hey Ray, just ping me as soon as I'm needed! >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/344#issuecomment-4510455
A lot of it is done, but there are some tasks left, hopefully this weekend I can get a lot done. will let you know. thanks for keeping in touch!
Just want to let you know that we care about this awesomeness!
It has to be awesome because I'll be talking about it on ScalaDays 2012 - they accepted my submission at last... :)
Better get to work then! ;-) I'm going rogue for 2 weeks, need to recharge after a 10m long crunch.
Then awesome it will be! ;-)  On Fri, Mar 16, 2012 at 6:15 PM, Piotr Gabryanczyk < reply@reply.github.com > wrote:  > It has to be awesome because I'll be talking about it on ScalaDays 2012 - > they accepted my submission at last... :) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/344#issuecomment-4543958 >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
Viktor, when will you disappear of the grid for your recharge?
At midnight I turn back into a pumpkin and will stay that way the coming 2 weeks. That said, I have plenty of colleagues at the office, so just email akka-dev and they'll help you out.
hehe, ok cool, have a an awesome break!
Finally got to process all the comments, I've separated some of the work into tickets, which I will continue with, so the next pull request will be far less 'big bang' and easier to process. You can check wip-camel to see what changes I made. I have rebased all the review changes into one change on top of the last commit. Let me know if anything more is needed to get this pull request merged. 
This looks good, and I agree with the plan to fix the remaining tickets after the merge. Great work, guys! Assuming that all tests pass go ahead with the merge.
Tests pass. Merged. Thanks Roland.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/42/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/42/
+1 merge and backport :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/41/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/41/
Ill take a look when Im back from vacation next week.
Cool, I'll be available for follow-up changes. Enjoy your vacation!
Great stuff! Even with picture ;-)
The new code (with the bug fix) is available here: https:github.com/hbf/akka/commit/4917680f7348d9abf4962f860f25b71cde75af17
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/45/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/45/
Akka guys, theres just one very little comment outstanding from my side on this, could you please also review? Thanks.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/62/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/62/
Looks great! Consider the timing sensitivity in the tests.
I merged this into master and am in the process of backporting to release-2.1, expect new PR shortly.
Hm, not being a Scala expert, I see in the test:  ```Scala   override def afterAll {     system.shutdown()   } ```  Isn't there a `=` missing? Why does it work at all?
def x { } is called "procedure" style, it is shorthand for: def x: Unit = {}
Appreciated, Viktor, thanks!  (I have added this to Stackoverflow, http:stackoverflow.com/questions/13300038, so hopefully your answer helps others, too.)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/45/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/45/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/43/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/43/
+1 merge! :D
Great stuff! Im through once, and I think it would make most sense if you keep this branch as your new master and generate changes as pull requests into this one (to not let the reviewing efforts grow out of hand)
Great work @drewhk!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/119/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/119/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/230/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/230/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/233/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/233/
PLS REBUILD ALL
We _definitely_ need to do something about that test output, 30MB of text?!
Yes, it looks like there are a lot of stack traces logged now. The raw log is "only" 5.6 MB.
one obvious improvement is to remove the stack traces (i.e. extend NoStackTrace), making sure that actor paths and exception messages are unique by themselves; in the longer term (which might mean medium) this might also force us into finally tackling the issue of configuring logging based on actor paths.
also, consider if ERROR is the correct log level, e.g.      Disassociated     95969503-0b1e-404a-b18d-a1f3e3b6b570akka.remote.EndpointException: Disassociated     	at akka.remote.EndpointWriter$$anonfun$7.applyOrElse(Endpoint.scala:208)     	at akka.remote.EndpointWriter$$anonfun$7.applyOrElse(Endpoint.scala:207) 
Is Dissociated an Exceptional occurrence?
That is precisely what I meant: actor restarts are treated always as errors, but that is not really true; more so the more you drink the supervisor hierarchy kool-aid. There were (infrequent) complaints on the mailing list, I think well see this increase in urgency. Another case of being driven to improve the taste of the dog food.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/234/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/234/
``akka-contrib/multi-jvm:test`` reliably fails on my machine, you do not need to wait for Kitteh to get this feedback
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/39/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/39/
You're using Java7 APIs dude. Use Java 6 to build and test on :-)
Yay :) I haven't realized. Will fix. Thanks kitteh!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/44/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/44/
Automatic merge failure. I need to merge some of the new stuff in RARP.
Woot, going to review now :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/40/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/40/
It's failing due to this: https:www.assembla.com/spaces/akka/tickets/2649
ok, but I guess it doesn't solve the problem, so why is the ticket changed to Test?
The problem isn't reproducible, with this fix the problem will be easier to debug if it manifests again.
I think the idea was that the reason to have AllForOne is that the children are intricately related and one cannot exist without the other. I think the behavior as-is is correct and the "correct" solution is to spawn one child that handles the "temporaries".  WDYT?
With my proposed change it would be quite easy to achieve a one-death-kills-all: just watch() your children and youre done. The opposite would require subclassing AllForOneStrategy, which looks much less nice to me. I am in the process of adding OneForOne vs. AllForOne to general/supervision.rst in terms of describing their general semantics and will add some generic scenarios for how to set up things in different cases. WDYT?
docs added, Id consider this solved.
Looks badass, but I don't recall where it was requested :-)
was requested by my aesthetics checker on the bug picture: we have equivalent functionality on all terminatable entities. Plus, the infrastructure is there by necessity, but it would look butt-ugly rolling your own on top of awaitTermination.
Ok, merge pls.
I'll take that as a "yes"
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/48/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/48/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/46/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/46/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/49/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/49/
Should we merge this into master?
Squashing big commits is great before merging to master, but during review it's hard to see what was done without redoing the whole review.
So how do you guys like this?
for my part: I like it!
Overall I think it looks great. NOW you should also add migration guide for 2.2.
Is this ready to be merged?
Added migration guide for 2.1 to 2.2 and added the immutable stuff in there as well, all good?
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/88/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/88/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/90/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/90/
sry Kitteh, I make beter nao.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/91/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/91/
Sounds like "OK" to me
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/47/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/47/
Oh boy... I will reserve some time to review this tomorrow.
No other comments. LGTM. 
alright, sad story, great job Viktor  we should define some idioms * `import scala.collection.immutable` and explicit `immutable.Seq` vs `import scala.collection.immutable.Seq` ? * `to[immutable.Seq]` vs `to[List]` vs `toList` * `immutable.Seq("foo")` vs `List("foo")`
do we have same problem with `Iterable`? any other?      scala> val i: Iterable[Int] = a     i: Iterable[Int] = WrappedArray(2)      scala> i     res14: Iterable[Int] = WrappedArray(2)      scala> i foreach println     2      scala> a     res17: Array[Int] = Array(2)      scala> a(0) = 3      scala> i foreach println     3
Here's what I think:  import scala.collection.immutable  It's unfortunate, but until Scala fixed Predef.Seq I think it's crucial  .to[immutable.IndexedSeq]  if iteration performance and memory is dear to you .to[immutable.Seq]  For known small things x match { case is: immutable.Seq => ... case other => other.to[immutable.IndexedSeq] }  To avoid wasteful copies use: List(....) or Vector(...) as appropriate instead of immutable.Seq(...), immutable.IndexedSeq(...) or Seq(...)
+1  I think those are good guidelines. 
Sorry for not reviewing this today, I lost myself in bug-hunting land... Totally exhausted now, first thing tomorrow will be this.
caution: based on another topic branch by accident!  can open a new pull req for merging after review is done.
There's one huge drawback here, and that's the extra level of indirection, that's going to have cache effects.
yes, but is it right to sacrifice separation of concerns for an unmeasured difference in micro-performance? Why can't I use my nice PrioMailbox with a BalancingDispatcher?
Overall I like the separation, but I think we should retain the methods on Mailbox and delegate to MessageQueue as to avoid changing too much at this stage.
I could put back the enqueue(), dequeue() and hasMessages(), but implementing MessageQueue would be awkward to the point of being unsafe. On the other hand the compiler verified my changes for me   So, tell me what to do and I will do it today.
I think I'd go for implementing MessageQueue and delegate to the internal one. This also lessens the overall impact of the change. You can then rename the durable mailboxen to MessageQueue but keep the MailboxTypes intact. WDYT?
MessageQueue has the cleanUp(owner, deadLetters) method which should really not be called on a Mailbox, which is why I would prefer to partially delegate without implementing the interface.  The proposal for renaming only the actual MessageQueue implementations is a good one, will do.
I was just trying to avoid breakage at this hour.
Me, too. Mailbox is `private[akka]`, I might overlook something, but it seems to me that it should not have any impact whether it nominally or structurally implements MessageQueue (the parts the make sense, anyway).
Do what you feel is best. I trust your judgement.
One more thing: if someone built their custom MailboxType with pre-RC2, they would not get a compiler error if Mailbox keeps extending MessageQueue. I think this tips the scale my way, we just need to mention this in the release notes.
Alright, make it happen. Thanks
Do it. No review.
merged into master (not automatically closed because was based on a different branch)
You forgot `BoundedMessageQueueSemantics` (did only the Deque one), and I would feel more confident that we wont break it if there were a test for it ;-)
Added test and fixed the other bounded mailbox semantics. And improved the testing facilities of mailboxes.
Good to go in?
I realize I should have tested against 2.2 and squashed - sorry about that.
(kitty-note-to-self: ignore 17795549) :cat: Synchronaising! :pray:
we have a ticket for putting the pgp plugin directly into the akka build; now is as good a time as any
LGTM, removing my global sbt plugin
(kitty-note-to-self: ignore 17805454) :cat: Synchronaising! :pray:
The commit validator succeeded, but kitteh has not updated for some reason
Nice Job!  In the future we might want to rename Leaving and Exiting to something like ExitProposed and ExitAccepted.
thanks, yes I have seen that in my repeat job for this branch as well
Alright, I found the bug. My changes to the gossip merge was not good, so I have reverted them.
I'm glad that I finally found **the** bug. It was fatal, and embarrassing. I leave it as an exercise to the reviewer to spot it.
20 successful full runs of this branch in repeat jobs during the night Please review!
I give up trying to spot the bug. LGTM apart from minor comments.
Great simplification (it is good to see the code become simpler in incremental steps, and more resilient at the same time), but we need to keep working in that direction; there is still too much going on under the covers from which only our great test suite keeps us safe.
Updates after feedback. Take a look at e3cec72. I did some major refactoring of the leaderActions method.
fixed the additional adjustments and rebased I will merge after kitty confirmation
yes, looks good
I will try with these settings:      (netty) connection-timeout = 15s     retry-window = 60s     maximum-retries-in-window = 3
2013.05.10. 11:31:49 dtumon Patrik Nordwall <notifications@github.com>   rta:  > I will try with these settings: > >     (netty) connection-timeout = 15s >     retry-window = 60s >     maximum-retries-in-window = 3  Seems fine
LGTM (though it took me a while to penetrate it; maybe add a comment?)
I have added the change to the connection-timeout and retry window as we tested in EC2.
@rkuhn   It's just a suggestion. I thought the best way to do it was via PR.  An even better approach would be to have akka fail at runtime (ideally compile time but I guess that's harder) if the linked library is not yet supported.  
Well, really it's about the capabilities of zeromq-scala-binding, so if you use a version of that that works with 0mq 3.x, then it will work.  Cheers, 
See it for a second from the developers point of view:  1) Grab the latest stable akka-zeromq 2) Grab the latest stable zeromq (3.x) 3) It doesn't work because of some obscure error (https:gist.github.com/fernandezpablo85/5550102) 4) Double check the zeromq docs on akka.io, there's no mention about version incompatibility (actually do a CTRL+F on that page for "version" just for the fun of it). 5) Dive through the akka-user mailing list archive to find that some users got their problem solved by reverting to ZMQ 2.x, do that and solve the issue.  Agreed that this PR may not be the best solution, but am I the only one that thinks this is suboptimal? Shouldn't people be warned earlier?
Perhaps I should have been more explicit: I think your patch is a good one, but without the second part of the sentence.
closing for inactivity: Ill put warnings in the docs myself
Oops sorry for that @rkuhn missed your reply. It's not a matter of who puts the warning as long as it's there :) thanks for taking care of it.
no worries, your input still has improved Akka :-)
Fine to merge?
So this is more about don't publish when metric gossip received?
yes, it was kind of published to often
ah: is that period of the notification task configurable?
yes, `collect-interval = 3s`
okay, I see, thanks
Can someone review? I want to merge it in. 
I'm reviewing now.
+1 for merging this I have run the tests of it on jenkins @ scalable1 - success 
Thanks Patrik. Added comment now. Will merge. 
Awesome, just what I need. Been debug printing the roles myself and doing some replacement in Emacs.
yeah, I got tired of that :)  On Mon, Jun 4, 2012 at 9:20 AM, Bjrn Antonsson < reply@reply.github.com > wrote:  > Awesome, just what I need. Been debug printing the roles myself and doing > some replacement in Emacs. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/508#issuecomment-6093677 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
Ah, awesome. Love you.  Add a bash script to ./scripts/* to make it easier to run. 
Would be nice to have Jenkins run this one automatically. 
Added ./scripts/multi-node-log-replace.sh Took a look at how to run it on the console out in jenkins. Didn't find an immediate solution to how to run and save it in jenkins. Merging this now. We can improve later. Perhaps doing it as a logger would be better (but then we loose the original logging). 
Ok. Did you open a ticket for integrating this with Jenkins later?  Good job. This is awesome. 
ok, did that now: http:www.assembla.com/spaces/akka/tickets/2176-logger-for-logrolereplace  On Mon, Jun 4, 2012 at 1:54 PM, Jonas Bonr < reply@reply.github.com > wrote:  > Ok. Did you open a ticket for integrating this with Jenkins later? > Good job. This is awesome. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/508#issuecomment-6097285 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
I cannot really comment on the OSGi part, so Ill leave that to others. What I am a bit concerned about is what else you need to do to get the sun.misc.Unsafe stuff working etc. And is the classloader you pass into ActorSystem.apply() capable of loading all necessary stuff (like remote transports and the like from different artifacts)?
Hi Roland,   Thanks for your initial feedback - you're right about a lot of you code-level remarks obviously but that's still very much work-in-progress, my main goal for starting this very premature pull request was to sollicit feedback on whether or not there's actually any interest at all for an akka-osgi modules that provides these two things - a base activator and support for Blueprint.  I'm working on https:issues.apache.org/jira/browse/SMX4-1141 (Add support for Akka to ServiceMix 4) so I'll probably have to do this work anyway, but I would prefer adding it to Akka directly instead of doing it at the Apache ServiceMix project because I think it actually belongs here.  In a previous pull request, we added OSGi headers to all Akka modules so they all install fine now inside an OSGi container, provided that boot delegation is configured for sun.misc (cfr. http:doc.akka.io/docs/akka/snapshot/additional/osgi.html) - for both Apache Karaf and Apache ServiceMix however, that is already set by default.  Up to now, I have only been testing with akka-actor, akka-slf4j and akka-camel (adding examples to the ServiceMix distributions to showcase this new Akka support).  The classloader I'm passing into the ActorSystem.apply() corresponds to the bundle from which the ActorSystem is being started, which should allow users to configure what's visible inside this classloader by adding imports to that bundle.  Not sure this covers all use cases we can imagine, I can only say that the use cases I tested were working fine with this.  I guess we can always add extra magic afterwards if people experience limitations with the current approach.   Regards,  Gert Vanthienen
How much more is needed to have this useable? 
L.S,  These additional commits clean up the code and are adding PojoSR-based unit tests to the proposed akka-osgi module.  With these changes, the new module should be useable - there's a service reference leak that I would like to get fixed tonight or tomorrow, but other than that it should be a good starting point for improving Akka's OSGi support.  Regards,  Gert
With these additional commits, service registration no longer happens automatically, but there's a convenience method available for users to do this themselves and unregistration is being handled properly now.
With this additional commit, the user no longer has to import akka.osgi.blueprint when using the namespace handler.  If there's anything you want me to change in here, just let me now, but as far as I'm concerned this is good to go.
Hey Gert!  Sorry for the delay and silence, I wanted some of our people who are more OSGi-savvy to have a look at this but they've so far been unavailable.  I'll try to get them to review it again.
Gert,  Do I read the code correctly and there is a 1:1 relation between a bundle and an actor system? Isn't this too restricted? Wouldn't it be nice to have an actor system that spans multiple bundles? Or even multiple actor systems per bundle? I would like to better understand the various scenarios, possibilities and restrictions before approving a general-purpose OSGi module. I think a three pages design document would be helpful.  A more specific issue I see is that you are mixing POO (plain old OSGi) with Aries. I think the aries support should be moved to a separate module akka-osgi-aries.  Wrt coding style: I suggest you provide type annotations for all public methods.  Cheers Heiko 
Heiko, Victor,  Thanks for the feedback!    If you only look at the ActorSystemActivator, that one is meant as a convenient way for setting up a single ActorSystem for getting started with Akka in OSGi. It does offer a convenience method for registering the ActorSystem in the OSGi Service Registry, allowing you to reuse the same instance across multiple bundles, but if you want to create multiple ActorSystems in one go, you probably don't want to use this one.  However, all the classloader tinkering or the extra code that is necessary to use Akka inside OSGi has been centralized in the OsgiActorSystemFactory class, so if people want to do a more advanced setup with multiple systems in the same bundle, they can easily reuse that class from whatever it is they're using to create their application (custom Activator class, Spring DM, Blueprint, SCR, ...)  For the namespace handler, you can create as many <akka:actor-system/> instances from a single blueprint file as you want and Blueprint itself has everything to register those as services and to be able to reuse them across bundles already by default.  I can try writing this out in more detail in some kind of design document (probably over the weekend or something, though).  Do you guys have an example of the kind of format you expect or a preferred location for storing these documents?  For the code changes: the current akka-osgi module has the import for the org.apache.aries marked as optional, so there's no runtime dependency preventing this from working in a plain OSGi environment, but I can obviously separate that into a new module as well if you prefer.  And I can obviously make the changes to the public methods as well.  One more question from my end: assuming I can get all of this done in the next few days, do you guys have a roadmap or planning for getting these bits released?  I've finished up my akka-camel example for Apache ServiceMix for inclusion into the 4.5.0 release, but we would need something like a milestone release at least to be able to actually ship it.   Regards,  Gert Vanthienen  
If/when it's merged in it'd be scheduled for the M1 of Akka 2.1 which is slated to be released within the coming weeks, depending on the stability of 2.10.0-M4 and the releases of Akka deps against it.
Adding another set of commits to split up the akka-osgi and akka-osgi-aries modules as requested - I also added the return types to the public API methods and improved the Scaladocs a bit to better explain about the different use cases handled by the `ActorSystemActivator`, `OsgiActorSystemFactory` and the namespace handler.
Gert,  Thanks for the changes. I added some minor coding style related comments. I vote for merging after those have been applied.  Heiko
Heiko,  Thanks for the review - code style changes have been added to the pull request now!  Regards,  Gert
@viktorklang I vote for merging this pull request
Great stuff Gert! I commented on some code-conformity things that needs to be addressed before I can merge it in, functionality-wise everything is great!  Looking forward to merging this in! 
Victor,  Refactored the activator itself to get an API that works for both Java and Scala users of Akka and I think I also addressed your other concerns - be sure to let me know if anything else needs changing.  Regards,  Gert
Excellent! Merging in!
Hi Gert,  The docs didn't compile for OSGi in master so I had to do some emergency fixes to get it to compile, please review my fixes so I haven't broken anything.  Cheers, 
Victor,  Your changes look good to me - even includes a few more nice code style improvements!  Sorry for the inconvenience,  Gert
Thanks for the review! Don't want to mess it up :-)
Still needs a signed CLA
Eric, what is the status on this? Have you signed the CLA? 
Closing due to no response
Hi Eric,  please sign the CLA: typesafe.com/contribute/cla  cheers, 
Still needs a signed CLA
Eric, what is the status on this? Have you signed the CLA? 
Closing due to no response
Sorry guys, I was just cleaning up my GitHub account and I realized I didn't see the notifications about this pull request. I have now signed the CLA, so you might want to reopen the request. Thanks.
The dependency on specs2 has been removed in order to lessen the pain of living close to scala master, it would be great if you could check the plain text documentation and make that as good as possible instead, thanks.
What happened here? I can't see that this is merged to master, but the pull request is closed. Now, I don't want it to be merged to master yet, because we have a regression in jenkins that I would like to sort out first. I hope that is due to a know problem we have, ticket #2137, but I can't be sure until that is fixed.
I merged (or closed) by mistake. Then I tried to revert it by opening it again, but that didn't work so I closed it again. Now you are saying that the merge was reverted anyway, but the pull request is closed. I don't understand. Sorry if I messed up. 
So I made all the JUnit tests that ran with both JUnit and ScalaTest run with JUnit only, since ScalaTest doesn't call the `@Rule` and `@ClassRule` before and after methods correctly, making some tests null pointer if JUnit ran the test before ScalaTest.
LGTM; does this explain the previously mentioned strange invocations with uninitialized classes?
Yes, it explains the strange invocations. Now there is only normal reuse of the static resources, which happens sometimes.
You are fixing this ticket as well: https:www.assembla.com/spaces/akka/simple_planner#/ticket:3201
@patriknw I trust that you take care of your last comment about StressSpec; Im merging this one in since it is a good change in any case.
Ok, I will.  /Patrik  9 maj 2013 kl. 09:56 skrev Roland Kuhn <notifications@github.com>:  @patriknw <https:github.com/patriknw> I trust that you take care of your last comment about StressSpec; Im merging this one in since it is a good change in any case.   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1410#issuecomment-17653590> .
(kitty-note-to-self: ignore 17505423) :cat: Roger! Rebuilding pr-validator-per-commit for 676eb459. :rotating_light: 
PLS REBUILD ALL
(kitty-note-to-self: ignore 17508722) :cat: Roger! Rebuilding pr-validator-per-commit for eb4bd78c, 85a4ebf6, 676eb459. :rotating_light: 
LGTM: great stuff!
I trust you  /Patrik  8 maj 2013 kl. 09:04 skrev Roland Kuhn <notifications@github.com>:  more reviews?   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1406#issuecomment-17589959> .
Ok, I see the point now, LGTM. Do you have benchmark data for the original and the new impl?
We are still working on the benchmarks. Turns out it's not at all trivial to produce reliable numbers. Currently we are in the process of optimizing for simple HTTP request/response benchmarks (see: http:www.techempower.com/benchmarks/) and for these types of load the message dispatch we save with this patch is not in the critical path. I suspect this change only becomes significant for many concurrent connections with a lot of interest settings, which only happens with messages that are larger than the send/receive buffers (not the case in our HTTP ping/pong cases).
Ok, all feedback (so far) incorporated and rebased onto current master.
The two failures are known and external to the PRs code, so Im merging it in. Thanks again, Mathias!
failures were in DelimiterFramingSpec and ClusterSingletonManager
merging to make the kitty a little less unhappy
the failure was in the unrelated "RemoteNodeDeathWatch (fast)" test
Now with more cowbell
I have not tried to follow what this code does, but LGTM
LGTM apart from the small nitpicks
Thanks for noticing! Ive uploaded the paper to http:doc.akka.io/docs/misc/smli_tr-94-29.pdf, it might be a safer bet to link to that location instead.
I'll update this later today to point to the akka mirror.
Ok, updated, rebased, and `push -f`'d - should be good to go.
yup, looking good
The failure was in TestTimeSpec, created ticket 3317
this is much simpler indeed
The failure was `akka.remote.RemoteNodeDeathWatchFast`
LGTM What is the purpose of `prepare`?
It gives the ExecutionContext the chance to do any set-up which might be necessary (e.g. gathering up some values out of ThreadLocals). We do this already for Futures and their callbacks, and the normal ECs just return `this`
ok, thanks for explaining
ah, forgot to mention: Im aware of the missing docs update, will do that tomorrow.
great that you do this!
pushed review fixes: is it good to go?
:+1: but I think you need to merge with master first ;-)
apart from my comment the section in dispatchers.rst it looks good
You need to run the tests before submitting the PR, it doesn't pass :-)
Sorry! Added a patch that fixes it for me. Also learned in the process that you can call become() in initialization. Very nice!
What about it? (not sure what you're getting at)
I think something like this is needed, have a look: https:github.com/akka/akka/pull/538/files
It's semantically different, but I can't think why that semantic couldn't be fine. Looks good
closed due to 538
fixed comments, including the scheduler
Fixed: * Use dedicated cluster scheduler only when default scheduler resolution isn't good enough * Config properties for scheduler * Commented shutdown considerations * Use nanoTime in FixedRateTask * Rewrote test to use latch and assert rate instead
fixed log and wrapped scheduler
Looks ok. Have you tested it? 
Yup, but I can't add any test for it
I know. Good.  :+1:
Also removed all usages of immutable.Stack
Thanks!  Please sign the CLA so I can merge this in : -)  http:typesafe.com/contribute/cla
Done. Please use my contribution for Good, not Evil (I'm just joking :D).  On 12 June 2012 15:20, viktorklang < reply@reply.github.com > wrote:  > Thanks! > > Please sign the CLA so I can merge this in : -) > > http:typesafe.com/contribute/cla > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/535#issuecomment-6270099 >
:+1: and backport to 2.0.3
On hold, awaiting more awesome commits, not ready for merge
This is going to take a bit longer: foundand unfortunately openedsome barrels of worms.
thank God it isn't "Silos of worms"
I think suspend/resume has now assumed the status of "unmaintainable" it's just waay too complex to understand the intricacies involved. Time to take a step back and revise it fully?
Following discussions and white-board sessions (and an emergency session of the EU council), I have split up ActorCell into a cake to separate the different concerns it handles. While doing so, I also fixed the review comments and a few other bugs. A few of the good ol names also bit the dust, to be replaced by awfully descriptive ones. That last one clearly lies in the eye of the beholder, so fire away with the comments.
Hey man,  just finished a run-through of this PR. Lemme know if you have any followup questions!
Now I would say its finished. I found and fixed that bug which caused `BalancingDispatcherModelSpec` not to terminate correctly (which was an actual bug, and also independent of the BalancingDispatcher). There is one test which consistently fails in akka-camel, though, but that was the case even before I started this branch (at least on my machine).
So ready to merge?
nobody gave a +1 yet 
Alright, I'll read through everything tomorrow and give the final verdict :-)
Awesomely done Roland, I really love the new tests!
Want me to merge it in?
Coolio! Have you've signed our CLA?  http:typesafe.com/contribute/cla
Yes, I have signed it just now.
Thanks, great work! :-)
Hey Mike,  I can't find you amongst our CLAs, can you sign it here please: http:www.typesafe.com/contribute/cla
In what situation did you get the send returning false?
Hi Mike,  I've given this some thought and I think your patch only solves half of the problem. The thing is that send returns false when EAGAIN is requested, which essentially means that you should try again later. I've implemented this in the following PR: https:github.com/akka/akka/pull/548
I tested with Akka latest.integration just now on Ubuntu 11.10, and verified that the Akka build had your modifications. I used "zeromq-scala-binding_2.9.1" % "0.0.6", and got this error:  ```` Starting Akka... Running Akka 2.0.2 Deploying file:/home/mslinn/work/testPubSubKernels/subKernel/target/subscriber-dist/deploy/minimal-subscriber-kernel-0.0.2.jar Starting up com.x.pubsub.SubscriberKernel Successfully started Akka [ERROR] [06/21/2012 19:42:11.830] [default-1] [akka:default/user/zeromq/$a] no more frames available while socket.hasReceivedMore==true java.lang.IllegalStateException: no more frames available while socket.hasReceivedMore==true         at akka.zeromq.ConcurrentSocketActor.receiveMessage(ConcurrentSocketActor.scala:200)         at akka.zeromq.ConcurrentSocketActor.akka$zeromq$ConcurrentSocketActor$$doPoll(ConcurrentSocketActor.scala:183)         at akka.zeromq.ConcurrentSocketActor$$anonfun$receive$1.apply(ConcurrentSocketActor.scala:40)         at akka.zeromq.ConcurrentSocketActor$$anonfun$receive$1.apply(ConcurrentSocketActor.scala:39)         at akka.actor.Actor$class.apply(Actor.scala:318)         at akka.zeromq.ConcurrentSocketActor.apply(ConcurrentSocketActor.scala:24)         at akka.actor.ActorCell.invoke(ActorCell.scala:626)         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)         at akka.dispatch.Mailbox.run(Mailbox.scala:179)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)         at java.lang.Thread.run(Thread.java:636) ````  You can find the (short) test publisher kernel and subscriber kernel programs here: https:github.com/mslinn/testPubSubKernels
Hi Mike,  Create a minimal testcase according to the Akka guidelines and open a ticket so I can verify:   http:doc.akka.io/docs/akka/2.0.2/project/issue-tracking.html#issue-tracking
I'd comply except that this problem manifests when run under the akka kernel.
If I cannot automate the test then I cannot know that it won't regress. I see no reason why it shouldn't be possible to reproduce in a normal unit test, the akka-kernel is just executing bytecode...
I need something which reproduces the problem reliably so I can add a _unit_ test against it.
I've committed something that might help, but as I cannot test against it, I'm stabbing in the dark.  Check out the latest master.  --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
Pushed some additional changes. Test with latest master.
After some discussion, my client has decided to have me repurpose the pub/sub code I wrote so it uses Akka remoting instead of ZeroMQ. I won't be spending more time with ZeroMQ unless another client asks me to, or this client changes their mind.
On Tue, Jun 26, 2012 at 11:38 PM, Mike Slinn < reply@reply.github.com > wrote:  > After some discussion, my client has decided to have me repurpose the > pub/sub code I wrote so it uses Akka remoting instead of ZeroMQ. I won't be > spending more time with ZeroMQ unless another client asks me to, or this > client changes their mind. >  So I conclude that the bug is fixed until proven otherwise.   > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/544#issuecomment-6588630 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
If a bug lives in a forest with no-one to bite, does it exist?
I have changed the tests to use less vals of addresses.
The cache revealed a race in TransitionSpec. Fixed with extra, needed, barrier.
This is ready, but I'm holding back merge, waiting for some other pull requests from @jboner and @bantonsson to be merged first, to not put the burden on them to handle the merge conflicts this probably will introduce.
Wouldnt it make sense to include the checks in ActorClassification, since its `publish` method would also throw NPE otherwise?
I think that can mess things up as it assumes that false return values means something else.
No, I meant throwing NPE upon subscribe instead of in publish.
Nice work! Will try using it right away
This will work with aggregate projects, but not with a project with dependencies specified using dependsOn.  Please, take a look at https:github.com/szabolcsberecz/freezing-ninja/blob/master/project/XBuild.scala#L24. Running dist on project "probe" will create a proper distribution package because all the required projects are aggregated by this project. It won't work with project "backend" though because akka-sbt-plugin doesn't track project dependencies.  All this means that you need to have an aggregate project for each project which you want to create a dist package for. Also, the dependencies must be maintained by hand in two places: at the project definitions and in the aggregate project.  sbt-assembly handles project dependencies correctly, and I think it would be a huge win to incorporate this feature.
"sbt-assembly handles project dependencies correctly, and I think it would be a huge win to incorporate this feature."  What is "this feature" in that context?  Please feel free to create a pull request with the suggested improvement, my SBT-fu stretches just about as far as this pull request.
Closing this PR as invalid
"this feature": sbt-assembly finds out the dependencies based on Project.dependsOn, and not the aggregate. By what magic it achieves this, I don't know.  I will give it a try, but I'm not familiar with SBT either.
yes, this looks good!
Fixed all issues (and some other stuff). Check it out: https:github.com/akka/akka/commit/391fed65941c29aa7d139011b0a97fb7c37f768e
I'll kick tests on scalable1 for this branch...
...tests sucessful. Merge +1
Nice. Thanks for verifying.
I'm overwhelmed by the comments! :D
you wanted comments, you got them ;-) apart from that, it looks good :+1:
Addressed comments ;-)
Awesome. Great to have unit tests now. 
good to merge?
Looks good to me
Good to go? 
Looks good. I guess the failure detector interface harmonization is part of #2215.
Ignore that comment... I need to refresh before commenting...
Ok. Changed all tests now to use both FDs. Good to push? 
Yes +1  11 jun 2012 kl. 16:54 skrev Jonas Bonr<reply@reply.github.com>:  > Ok. Changed all tests now to use both FDs. Good to push? >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/530#issuecomment-6246510
I'm loving it!
:-D Just had to make this a pull req so it doesnt go unnoticed. It was introduced waaay back when LocalActorRef had some notion of replicationStorage, if you get my drift 
hehe golden days
Good. Push so I can test it. 
You can already test it, doesn't need to go into master until the reviews is done, just checkout wip-2182-unreachable-patriknw
Sure I can. But I'm lazy.
Laziness shouldn't be a reason to merge things into master ;-)
First virtue of a programmer is Laziness. Second virtue of a programmer is Impatience. According to Larry Wall.  http:www.hhhh.org/wiml/virtues.html :-)
tested on jenkins on scalable1, merging
:-)  I am the Heimdall of akka master, that's the 3rd virtue, hubris.
Damn right you are. :-P
Did I say that this shaves off 1 minute of the ordinary build on MBP and 6 minutes on jenkins.typesafe.com? :-)
badass! Can you merge and remove the branch, Patrik? Thanks!
What is the status?
Sorry for the delay. Didn't had the energy. Job I took was pretty intense. Now it is quieting down.  I've implemented Viktor's suggestions except for the Proxy Class Loader (will add comment in that section)
Should I sync with Master? Thoughts?
I'd hold off until the Scala 2.10 support has been merged into master since it's a lot of changes.  Cheers,  --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
Looking forward to see the changes. Let me know what and when you need from me.
@avrecko So I assume that you are not planning to finish this? If so, we'll close the request as invalid. 
I'd like to finish this. Like I might have mentioned. I am currently focused on non-programming things. I can make an exception and spare some time for this now. Ideally I'd resume this in October as I'd like to take a break from programming for now.  But in any case I need you guys to tell me what to do. Again. Let me know what and when you need from me. That was my comment that apparently goes unnoticed.  I just don't have the energy to keep syncing the rcl code with the akka master code flux.
Awesome reproducer.  LGTM
Great work, guys! LGTM
Increasing duration (it's a stresstest)
yes, not all machines are as fast as a macbook
I wonder how much this will add to the running time on slow jenkins :wink:
(kitty-note-to-self: ignore 18553347) :cat: Synchronaising! :pray:
we can reduce the number of messages, but then it will be less reliable...
Reduced the number of messages and using dilation
(kitty-note-to-self: ignore 18555815) :cat: Synchronaising! :pray:
this test takes 9 seconds on a0 (I happened to have a git clone there). The other a machines should be slightly faster. I recommend that you set the within to 20 seconds when using 2000000 messages.
I changed it to 30 seconds
So incoming heartbeat requests will go to dead letters?
@drewhk yes, the request is sent to the other guys sender (which isn't there until welcome is received).
rebased on top of master, merge failed in the job
(kitty-note-to-self: ignore 18508532) :cat: Synchronaising! :pray:
LGTM apart from some formatting
indentation/braces fixed. Ready?
Thanks for reading this patch directly from my brain and dumping it to github for me :-) (and as such it naturally LGTM)
Yes, well done. The one remark with returning `false` is only relevant for printing the warning message that things were lost, is that correct?
LGTM, I compared with my attempt and I was pretty close, but I never got all pieces correct. Great work!
Yes, but I changed that already, will commit after you finished review.
LGTM  On a side note, should the reference configuration for cluster be in the `configuration.rst` file?
@bantonsson I guess it should, and so should akka-camel. I have added both.
Looks GREAT to me! :-)
No mention of the default resolution in the scheduler docs? We might want to fix that, could you verify? Thanks.
Scheduler.rst contains: The default implementation of Scheduler used by Akka is based on job buckets which are emptied according to a fixed schedule. It does not execute tasks at the exact time, but on every tick, it will run everything that is (over)due. The accuracy of the default Scheduler can be modified by the akka.scheduler.tick-duration configuration property.  and I thought that was still valid without adjustments.
Yes, it is still valid, which made me wonder why we do not mention the default value directly in there. Since I dont think we will change this value very frequently I think it would be good to tell people about it up front.
I think we should stick to the plan to **only** define the default value in reference.conf, and all those default values are easily accessible from the docs.
I don't see the code that completely disables all outbound attempts. Is that the change in the EndpointWriter?
No, that is in EndpointManger L501
but that was not changed, so outbound was not getting through?
This was changed:   else createAndRegisterWritingEndpoint(Some(uid)) ! s   to  else extendedSystem.deadLetters ! s
ah, you are right, thanks for clarifying
No failures so far in 80 runs on the repeat job. Praying...
fingers crossed, it would be an awesome way to end this week
Passed 200 runs on Jenkins. Merging.
not mergeable yet: there is a problem with unidoc and akka-sbt-plugin
now it should work
Now Await free.
Anybody else care to comment?
I dont see any bad stuff right now, but it would take me a while to fully penetrate it; @drewhk your LGTM would be good enough for mine as well.
Ah I actually reviewed it, just forgot to LGTM at the end :)
then it is good to merge
One more thought: can we fix things for Java by somehow determining whether the Creator or UntypedActorFactory is a static class or not? If that is possible, then we could retain the methods non-deprecated and just print warnings (and error out in 2.3) when someone does it wrong.
Don't you need to change the constructor for `FailureDetectorPuppet` to take an `EventStream` as well?
you have to update the documentation in the reference.conf files as well
aside from the minor things LGTM
The failure for the second commit was davyJones
I assume that adding the "INTERNAL API" marker will not break the build, so I will merge to have this in the next nightly build. Then I can update https:github.com/akka/apps/pull/2/ tomorrow.
merging right away, since it broke all builds -- sorry guys!
Hurray for davyJones!
Yeah, I should not have messed with him.
thanks for contributing :-)
Do you mean we should leave it as is? 
In case someone wonder: this fixes the failing test. No failures in 40 runs. but we should of course fix the flushing also.
Great work guys, makes me proud
My spidey-sense is tingling, it's waay too much indirection and we're also introducing a completely new concept. I think we should be focusing on simplifications and unifications instead of fragmentation. 
Yes, the guys in the same room can confirm that I did grimace when writing this code. The problem is that the internals of the routers need a major overhaul and that must be planned for. It takes at least one week to get them in shape.  This does solve a serious problem. Otherwise we will have to answer question on mailing list with "you must start ... first". That kind of answers are failure.  I'm open to alternatives (excluding the major overhaul).  
Okay, seeing what this quest turned into Im inclined to not touch Routers for 2.2 at all, i.e. let them rot. That sounds tough, but I dont think we have much choice: your patch demonstrates one thing, namely that the whole design conflates at least two very different kinds of routers in the same piece of code, requiring you to add all those layers of boxing and ugliness.  What we should do after 2.2 is to start fresh.  We have the basis in good shape, i.e. I dont think we need to work much on RoutedActorRef/Cell, it is only the user-facing API which is broken. Hence we must deprecate the whole thing. Then we will introduce a loosely coupled toolbox of   * routing strategies  * pluggable routee management (which allows for either creation or look-up)  * prefabricated actors as a substrate to replace the current offering  This will allow us to avoid mixing look-up routers (which are about external services with independent life-cycle) and scale-up routers (which are about creating worker actorspossibly on demand).  And since all this will take a lot more time than we have until we want to ship 2.2, Id say shelf it for now. I might be wrong on this last point, but that is what my currently very cluttered assembla view leads me to believe.
Nice! LGTM -- and later we should optimize the hell out of it.
LGTM (it even decompresses the Welcome message!)
yes, the Welcome message contains the full Gossip state
Updated with a minor fix so we don't try to serialize the `constructor`.
LGTM (when the ordering is fixed)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/141/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/141/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/145/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/145/
Seems OK to me. Checked the commit that added that resize() line, I think it is safe to remove it.
I changed `tell` in RoutedActorCell to handle the different cases in a more unified way. Careful review, please.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/147/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/147/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/148/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/148/
Cleanup based on comments.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/151/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/151/
I looked at the last part (tests) also. Looks good.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/159/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/159/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/172/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/172/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/176/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/176/
Bjrn, I think you'll need to fix this
Yes, I already know, and I'm planning to. When I get back to work.
LGTM as soon as the bothOld/bothNew thing is fixed; the UNREACHABLE comment will be a different ticket
@bantonsson I thought of one thing, check all usage of things similar to       cluster.subscribe(self, classOf[MemberEvent])  that expected receive of MemberUnreachable, but now that is not a MemberEvent. note that it not always match exactly on MemberUnreachable, it can be things like      case other: MemberEvent         other events means that it is no longer interesting, such as        MemberJoined, MemberLeft, MemberExited, MemberUnreachable, MemberRemoved  Check samples also. 
Yes, I realized that this friday. There are a couple of places that doesn't match on `UnreachableMember` (The event previously known as `MemberUnreachable`) but expect to get it as part of the other member events, and act on it. 
please add a ticket for back-porting this to release 2.1.1 (i.e. not the current release-2.1 branch)
Done.  https:www.assembla.com/spaces/akka/tickets/2785
All comments have been addressed. Should be picked up by the Kitteh.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/194/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/194/
the bothOld/bothNew replacement LGTM
This needs to go into 2.1. Is that the plan? 
There is a ticket for back porting to 2.1.1  https:www.assembla.com/spaces/akka/tickets/2785 
Why not 2.1? The current behavior is just so wrong. 
That is more of a time and release management question.
I agree with Jonas, we still have the opportunity to cut more RCs and not have Scala have to issue another RC
We have hopefully cut and published the final RC for 2.1 already, which is why it will probably have to go into 2.1.1; we can release that one very soon after 2.1.0.
Well, I disagree with that. We could cut a new one today. This is in my view a critical flaw. User is currently geting spammed with completely uninteresting events. And will have to change his implementation between 2.1 and 2.1.1. Pretty bad. But it is your call. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/198/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/198/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/205/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/205/
What's the status here?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/146/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/146/
Added explanation that names cannot be partially matched.
Never seen this. Compiles fine on my Mac and Win box. I know several others building fine.   Are you running Java 6? That is a req.  I am traveling during the weekend. Can look at it on Monday. 
Hey Jonas,  Ahhhh i didnt realize that - do you need open JDK on mac then? of course the default JDK on mac is only 1.5.... guess I could just download the binaries!  Cheers, tim
No, JDK 6 is available for all 64-bit Intel Macs. Change the default JDK here: Applications->Utilities->Java Preferences
And the compiler warnings are generated when "<arg>-unchecked</arg>" is supplied in the POM, Scala 2.8 will probably rewrite case t : T into case t if t.isInstanceof[T]
@viktor - I'll be damned, you learn something everyday! I didnt know about that...
Hey Tim.  Did you resolve the issues?  If so, could you close the issue? Thanks. 
Yes thanks Jonas - all sorted now :-)
I'm assuming from our previous discussion that we're talking about something like Cassidy? Are there any hard requirements or do we make them up as we go? :)
New Thrift API:      public List<Column> get_slice_by_names(String keyspace, String key, ColumnParent column_parent, List<byte[]> column_names, int consistency_level) throws InvalidRequestException, NotFoundException, TException;      public List<Column> get_slice(String keyspace, String key, ColumnParent column_parent, byte[] start, byte[] finish, boolean is_ascending, int count, int consistency_level) throws InvalidRequestException, NotFoundException, TException;      public Column get_column(String keyspace, String key, ColumnPath column_path, int consistency_level) throws InvalidRequestException, NotFoundException, TException;      public int get_column_count(String keyspace, String key, ColumnParent column_parent, int consistency_level) throws InvalidRequestException, TException;      public void insert(String keyspace, String key, ColumnPath column_path, byte[] value, long timestamp, int consistency_level) throws InvalidRequestException, UnavailableException, TException;      public void batch_insert(String keyspace, BatchMutation batch_mutation, int consistency_level) throws InvalidRequestException, UnavailableException, TException;      public void remove(String keyspace, String key, ColumnPathOrParent column_path_or_parent, long timestamp, int consistency_level) throws InvalidRequestException, UnavailableException, TException;      public List<SuperColumn> get_slice_super(String keyspace, String key, String column_family, byte[] start, byte[] finish, boolean is_ascending, int count, int consistency_level) throws InvalidRequestException, TException;      public List<SuperColumn> get_slice_super_by_names(String keyspace, String key, String column_family, List<byte[]> super_column_names, int consistency_level) throws InvalidRequestException, TException;      public SuperColumn get_super_column(String keyspace, String key, SuperColumnPath super_column_path, int consistency_level) throws InvalidRequestException, NotFoundException, TException;      public void batch_insert_super_column(String keyspace, BatchMutationSuper batch_mutation_super, int consistency_level) throws InvalidRequestException, UnavailableException, TException;      public List<String> get_key_range(String keyspace, String column_family, String start, String finish, int count) throws InvalidRequestException, TException;      public String get_string_property(String property) throws TException;      public List<String> get_string_list_property(String property) throws TException;      public Map<String,Map<String,String>> describe_keyspace(String keyspace) throws NotFoundException, TException;      public CqlResult execute_query(String query) throws TException; 
I have started to hack this. Need to make it work with new thrift API, see prev comment. Do you want to own this task? If so I can just push my branch. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/167/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/167/
Yes, this is a good one: LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/168/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/168/
Update migration guide also. Aside from that :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/169/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/169/
LLMTM == Looks Like Magic To ME :dancer: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/170/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/170/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/177/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/177/
yes, very good!
please back-port to release-2.1 branch
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/171/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/171/
Looks good! If you think that it would be a good idea to create a script for changing the scalaVersion, then create a ticket. Im squarely on the fence.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/173/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/173/
apart from the nitpick: LGTM  could you do the same for `ThreadPoolBuilder.conf_?` and `TypedActor.MethodCall.returns<*>_?` ?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/179/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/179/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/174/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/174/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/180/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/180/
Is she a keeper?
I think so, the design feels right, i would like to take another look at the details. Migration guide is also needed.
Yep, check the details first. I'll write migration guide once we agree this is what we wanna do
Yes, LGTM. Well ship it with 2.2-M1, then, including possibly some of the low-hanging fruit I pointed out. Would you please create tickets if you agree?
LGTM after some additional cleanup
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/196/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/196/
Alright, cleanup done. Want me to write the migration docs and then merge?
yup, sounds good. 
Migration docs added
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/209/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/209/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/175/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/175/
Very nice! Im always pleasantly surprised by how neat those cluster tests are. Good job!
Indeed! And they were invaluable to find bugs in the new remoting.
fixed review comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/178/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/178/
activity every other year implies that Ill close this now ;-)
If you want to run mvn install, then you should check out the sources from github. The 0.5 dist should be used as-is. But it is very old. I suggest checking out the sources. 0.6 is coming in 4 weeks. 
Also, please file issues to assembla.com/spaces/akka instead.
Ok, mvn install fails too. FWIW, I am on OS X Java 1.5.0_19 and Maven 2.2.1 Known issues ?  BTW, clean ~/.m2/repository grows to 37M at the end of mvn scala:doc and mvn install !
The akka-kernel and akka-java-util are build during mvn install.  Have you invoked 'mvn install' from the root? You need Java 6 to build it.   mvn scala:doc fails for me as well. I'll look into it. Thank.  
Updates:  1. mvn install goes through with Java 1.6. yay! And therefore, akka might want to use maven-enforcer-plugin (http:bit.ly/eMYG6) and enforce Java Version. In OS X, Maven has affinity for Java 1.5 unless JAVA_HOME is set explicitly.  2. mvn scala:doc is successful _after_ a successful mvn install (because both the jars become subsequently available in local repo) 
I'll look into the plugin. Thanks. mvn scala:doc do work for me as well.  Thanks for trying Akka out. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/150/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/150/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/149/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/149/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/166/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/166/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/161/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/161/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/165/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/165/
@richdougherty you can merge this
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/162/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/162/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/153/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/153/
It doesn't pass the tests: https:jenkins.akka.io:8498/job/akka-pr-validator/153/console
Correction, it didn't compile.
Note  there is a small race condition possibility in the tests cases. Sometimes if the machine is running really slowly, the log message emitted by the logging system itself will be picked up by the tests (i.e. after the `before` method runs, but before the specs).  The same race condition exists for the SLF4J tests.
@viktorklang that's probably the test cases my build system isn't set up to let me compile/run it with SBT. What command should I issue (and from which directory) to be able to build/run *just* my code and test dependencies?
[error] /localhome/jenkinsakka/workspace/akka-pr-validator/akka-contrib/src/main/scala/akka/contrib/jul/JulEventHandler.scala:89: value getStackTraceDepth is not a member of sun.misc.JavaLangAccess [error]     val depth = access.getStackTraceDepth(throwable) [error]                        ^ [error] /localhome/jenkinsakka/workspace/akka-pr-validator/akka-contrib/src/main/scala/akka/contrib/jul/JulEventHandler.scala:92: value getStackTraceElement is not a member of sun.misc.JavaLangAccess [error]       val frame = access.getStackTraceElement(throwable, i)
Interesting, the error is actually  ``` getStackTraceDepth is not a member of sun.misc.JavaLangAccess ```  I suppose I'll have to find a solution that doesn't use `sun.`...
to run tests, type *test* in the sbt repl
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/154/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/154/
Still doesn't compile. Please hold off pushing until tests to complete locally first.
ok, this is what I expected it's the Specifications vs AkkaSpecs thing. I know how to fix it, but I just need to get my dev environment working.  I don't understand it, but typing `run` within `sbt` seems to behave as expected but `sbt run` did not.  (Bugger, sorry, didn't mean to push that last one)
you shouldn't type run, you should type *test*.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/155/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/155/
ok, that fixes it! (sorry, meant `test` in the last comment)  A big stacktrace is dumped out when we simulate the error. If that causes problems, I could look into it further and provide a logging.properties file which will stop this. It will involve passing a parameter to the jvm.
ok, I'll make that change and also I realised that I wanted the LoggerAdapter as the `log` field in JavaLogging (doh!). So making that change too. Will probably push tomorrow.
@viktorklang I'm not really sure how to do this without the LBQ. I need to be able to pick up the `LogRecord`s at their final delivery point and this is the only way I can think to do it.  Compare it with the SLF4J spec, which actually gathers the output from a `ByteOutputStream`!  https:github.com/akka/akka/blob/master/akka-slf4j/src/test/scala/akka/event/slf4j/Slf4jEventHandlerSpec.scala
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/156/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/156/
@fommil move the adding of the handler into the class constructor and instead of putting into the queue do: ``testActor ! logrecord`` then in the test, instead of pulling the queue, use expectMsg
OK, that was trivial. There is also no console output of the tests.
Cool. Now, did you try to change so that the LoggingAdapter creates the LogRecord and sends it to the EventStream wrapped in a LogEvent (Debug ... )?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/157/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/157/
I only cast a casual glance for today, will look more into it tomorrow. One thing is that we dont do author tags; instead you should put your name into the docs for this feature (hint, hint ;-) ). 
OK, many comments addressed. @viktorklang , I'd like to hear your response to my note above. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/158/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/158/
Sorry, still didnt get around to do a proper line-by-line review, but concerning the strategy:  * `JavaLoggingEventHandler` is desirable, and it is also in-line with the modularity of the logging in Akka * `JavaLoggingAdapter` will probably never be interesting for Akka proper, but I think it makes sense in the contrib area, since it does provide a way to abstract sync/async logging under the Akka faade; might be useful to pull into some projects * `JavaLogging` is so simple and straight-forward that it does not pull its weight; unification does not buy much here
@rkuhn indeed `JavaLoggingEventHandler` is the real meat of this contribution.  Regarding `JavaLoggingAdapter`/`JavaLogging`, these are only useful for unification purposes. They are so small that I would never actually create an OSS project just to distribute them. akka-contrib is perfect for them. My workaround if `JavaLogging` is not accepted is to simply continue using the following trait I have in every project:  ```scala trait J2SELogging {   @transient lazy protected val log = logging.Logger.getLogger(getClass.getName) } ```  but mentally noting that this exposes the `j.u.l` `Logger` and not the Akka Logging API (which really only matters when Exception logging).
Just talked to Patrik and while we agree that the `JavaLoggingAdapter` is not something we ever want to support officially (because it is synchronous) we have decided that it should be merged *on a provisional basis* (imagine voice of Jack Bauer). But before I can actually do that Ill have to ask you to sign the CLA (see CONTRIBUTING.md).
@rkuhn what if I always do the logging in a `Future` using the system `ExecutionContext`, making it asynchronous?
Wrapping blocking things in Futures is not a silver bullet, and especially so for logging since Futures may be run in any order at some unspecified later time. As I said, it may make sense to use the `JavaLoggingAdapter` in certain specific scenarios, its just that we dont want to support it officially with all the bells and whistles, hence `akka-contrib` material. Ill merge it today; could you help me remember that by creating a ticket? (and it would also reduce the effort for back-porting it to release-2.1 if you could squash the commits and re-push this branch, mentioning the ticket number in the commit message) Thanks!
Im doing the squashing and merging now and have created the ticket: https:www.assembla.com/spaces/akka/tickets/2805
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/160/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/160/
I run tests on scalable1 and this failed again, so there is something else that is wrong.
the previous failure on scalable1 was because default await timeout 3 seconds was too short, I have increased the `within` for the test
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/163/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/163/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/197/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/197/
Great stuff Patrik!
pushed today's work
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/202/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/202/
Added some more stuff, especially the interesting join/remove repeat test. This could be the first version of this test, which could go into master, and we can add tickets for more stuff. For example I would like that we add something that tests a large actor tree hierarchy (creating and using many actors). Then we have all the throttling and drop messages part, but that can't be done until we have reliable system messages.  By the way, I'm running this test periodically in moxie cluster. 
great to hear! unfortunately Ill have to defer scrutinizing it until later
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/207/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/207/
Added the test of many actors, in tree structure also. Squashed commits.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/218/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/218/
Im unable to understand each line at this point, but it looks good nevertheless.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/242/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/242/
The last commit looks reasonable, but I dont completely get it. It looks like your change basically makes the heartbeat sender aware of a removed node a little earlier than before, but why should the sender care?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/246/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/246/
Alright, I have been running this (with new remoting) a few times on jenkins.akka.io, and it failure rate is ~50%, so we should not merge it in now. I don't have more time today to investigate the failures. They are here: https:jenkins.akka.io:8498/job/akka-multi-node-repeat2/  Apart from the test the commit contains fixes for 3 issues: * Avoid adding back members when downed in ClusterHeartbeatSender * Avoid duplicate close of ClusterReadView * Add back the publish of AddressTerminated when MemberDowned/Removed   it was lost in merge of "publish on convergence"
Which ones are the failures?   If you find something fishy, please send me, I'll try to investigate.
look at build 111 and thereafter https:jenkins.akka.io:8498/job/akka-multi-node-repeat2/  that job is weird, it shows blue bullets even though it fails, so you have to look at the log
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/271/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/271/
Merging! Remove LargeClusterSpec, since StressSpec can be used for that as well.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/199/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/199/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/200/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/200/
aside from style comments: LGTM
Cleaned up based on comments.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/203/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/203/
So this failure seems weird. I've created a ticket https:www.assembla.com/spaces/akka/tickets/2801  Jenkins has been running the multi-node cluster tests on this branch every 15 minutes during the nigh (35+ times) without a single failure.
PLS REBUILD ALL 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/204/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/204/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/201/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/201/
I am overwhelmed by the interest and amazing feedback
aside from the missing serialVersionUID, LGTM
Alright, added a comment to set SerialVersionUID for 2.1 and 2.2
Note that this PR targes release-2.0
Yeah. FP sounds good. FP to release-2.1 as well?
yes, Im currently debating with myself whether to put it into RC6
If we're having an RC6 I think it should go in. Otehrwise I think it needs to go into 2.1.1
The change looks good, but I have one question. If this whole dance is because `null` is not a valid message, then why do we have that restriction? 
@bantonsson Read the description of the PR
The restriction was documented since before I learned about akka and it never came up (meaning nobody ever asked about it or wanted to have it changed). In Scala pattern matches youll have to explicitly match it or have a wild-card match, otherwise `null` will be unhandled anyway. And I do have a hazy recollection of thinking along the lines of is this okay? ah yes, msg can never be null; but I cannot remember where exactly that was. Plus, I think it is nice to not have the billion dollar mistake in our actor world ;-)  The point is that if we want to change it, then it should be done in a proper way (with ticket and research and so on) instead of as a shot from the hip in response to some other problem (I do realize that this was not what you asked).
So the consensus is that TypedActors pay for nulls, but no one else
@viktorklang Yes, I did that, and then I read some remoting code, and the only limitation I found is that we explicitly disallow `null` as a message, and I'm asking why. The value null can travel just fine over the wire if you let it.
@bantonsson I think using null as a message is a bug, the only reason it poses a problem for TypedActors is that they live in another dimension where Java code returns null at times.
@rkuhn @viktorklang Thanks. I agree that sending `null` is not a good thing.  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/208/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/208/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/211/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/211/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/213/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/213/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/210/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/210/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/215/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/215/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/214/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/214/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/216/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/216/
:+1:  thanks Please sign the Typesafe CLA, http:www.typesafe.com/contribute/cla
All set! Thanks.   On Dec 17, 2012, at 1:10 AM, Patrik Nordwall <notifications@github.com> wrote:  >  thanks > Please sign the Typesafe CLA, http:www.typesafe.com/contribute/cla >  >  > Reply to this email directly or view it on GitHub. > 
verified CLA, all good
Updated after comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/217/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/217/
LGTM; the reason for not being an implicit class is the import?
Yeah, and you can't put it inside a trait, doesn't work...
Great. Thanks.  -- Jonas Bonr  http:scalablesolutions.se http:jonasboner.com http:akka.io http:letitcrash.com  On 28 Mar 2011 16:04, "hseeberger" < reply@reply.github.com> wrote: > Could successfully build. Tested by installing akka-actor into Felix OSGi container. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/78
Thanks, Heiko. I updated to 1.0.1 on the wip-2.9.0 branch yesterday. Thanks for testing it. Will be merged into master soon.  https:github.com/jboner/akka/commit/4548671ef7dd99da512c951f8df2bc0bfd18e7af
Great, thank you.  Sent from my iPhone  On 28.03.2011, at 22:13, pvlugter<reply@reply.github.com> wrote:  > Thanks, Heiko. I updated to 1.0.1 on the wip-2.9.0 branch yesterday. Thanks for testing it. Will be merged into master soon. >  > https:github.com/jboner/akka/commit/4548671ef7dd99da512c951f8df2bc0bfd18e7af >  > --  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/78#issuecomment-927833
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/182/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/182/
Needed an excuse to try out the new GitHub online patch tool :)
shouldnt this be closed?
Yeah, close it
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/184/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/184/
LGTM; I think this might want to be back-ported, could you open another PR against release-2.1? Thanks!
BTW: the jenkins failure was a spurious timeout
Ah right I never even checked what master was on. I'll open a pr against release-2.1   On Fri, Dec 7, 2012 at 5:22 PM, Roland Kuhn <notifications@github.com>wrote:  > LGTM; I think this might want to be back-ported, could you open another PR > against release-2.1? Thanks! > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933#issuecomment-11135888>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
PR Done.   On Fri, Dec 7, 2012 at 5:22 PM, Roland Kuhn <notifications@github.com>wrote:  > LGTM; I think this might want to be back-ported, could you open another PR > against release-2.1? Thanks! > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933#issuecomment-11135888>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/186/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/186/
I would like to see better commit messages.
merged manually after adding more verbose commit message
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/185/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/185/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/187/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/187/
merging manually in order to expand the commit log a little
merged into release-2.1
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/181/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/181/
same as master, LGTM
That's a bit racy to check mailboxIsEmpty inside of the actor itself, I'm not so keen on adding this patch.  On Fri, Jun 3, 2011 at 7:43 AM, chicofranchico < reply@reply.github.com>wrote:  > Hello Hakkers, > > We are contemplating changing our worker infrastructure to akka. For that, > we have performed some tests and that required the possibility to check for > emptiness of an actor mailbox. Our use case currently requires the > processing inside an actor to be performed only when its mailbox is empty. > Since checking for the mailbox size is expensive, we thought this would be a > nice addition to the use case. > > I hope this makes sense and the changes are useful for everyone. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/81 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
Well, the thing is that if I do check mailboxSize all the time wouldnt that also cause a race condition? If, instead of checking for emptiness using this patch, we do on the size, the performance is more than 10x slower than our implementation.
Yes, checking the mailbox size also causes races.  If you need fast size you can always change the backing mailbox to a LinkedBlockingQueue.  new ExecutorBasedEventDrivenDispatcher {   override def createMailbox(actorRef: ActorRef): AnyRef = mailboxType match {     case UnboundedMailbox => new LinkedBlockingQueue with UnboundedMessageQueueSemantics with ExecutableMailbox     case _ => super.createMailbox(actorRef)   } }  On Sat, Jun 4, 2011 at 12:49 AM, chicofranchico < reply@reply.github.com>wrote:  > Well, the thing is that if I do check mailboxSize all the time would that > also cause a race condition? > If instead of checking for emptiness using this patch we do on the size the > performance is more than 10x slower than our implementation. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/81#issuecomment-1300987 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
I need to test that for performance. I'm using akka 2.0-snapshot. Could you tell me please what would be the exact code?  I'm trying the following but it doesn't work:  aRef.dispatcher = new PinnedDispatcher(aRef) {   override def createMailbox(actorRef: ActorRef): AnyRef = mailboxType match   {     case b: UnboundedMailbox => new LinkedBlockingQueue[MessageInvocation] with UnboundedMessageQueueSemantics with ExecutableMailbox     case _ => super.createMailbox(actorRef)   } }  object creation impossible, since method dispatcher in trait ExecutableMailbox of type => akka.dispatch.Dispatcher is not defined 
Try  new PinnedDispatcher(aRef) {   override def createMailbox(actorRef: ActorRef): AnyRef = mailboxType match  {     case b: UnboundedMailbox => new LinkedBlockingQueue[ > > MessageInvocation] with UnboundedMessageQueueSemantics with > ExecutableMailbox {                 def dispatcher = PinnedDispatcher.this   > } >    case _ => super.createMailbox(actorRef) >  } > }
Weird error. Never seen this popup before.  PinnedDispatcher is not an enclosing class
Oh, it's because you're creating an anon inner class.  do:  val self = this override def createMailbox ... {   ...   def dispatcher = self   ... }
oh, right. Thanks a lot for the help. I'm gonna run some benchmarks and get back on that.  I just still think that "size" has the same problems, whatever the problem of "isEmpty". But isEmpty is much more efficient, depending on the mailbox type used. I think if size makes sense, so should isEmpty. And we have a use case for that.  My worry is only because if we need to use the event dispatcher with a thread pool sharing the queue, then the ConcurrentLinkedQueue would make more sense right? Or would I still be able to change the mailbox type without affecting performance for such a dispatcher? Makes sense?
Size on LinkedBlockingQueue is an AtomicInteger.get, on ConcurrentLinkedQueue it's a full iteration of the queue.  --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
Hi Viktor, so I'm still making up a case for the isEmpty feature and maybe if you could consider the microbenchmark below: https:github.com/chicofranchico/akka-microbenchmark It tests the ReceiveTimeout feature that makes use of the mailbox size in the message invocation (invoke) in the actor.  While profiling, the microbenchmark gets stuck in this method when using the receivetimeout feature:  protected[akka] def checkReceiveTimeout() {    cancelReceiveTimeout()    if (receiveTimeout.isDefined && **dispatcher.mailboxSize(this) <= 0**) { Only reschedule if desired and there are currently no more messages to be processed      _futureTimeout = Some(Scheduler.scheduleOnce(this, ReceiveTimeout, receiveTimeout.get, TimeUnit.MILLISECONDS))    }  }  By using the isEmpty feature and default configuration the performance increases by a whopping factor of 173 which is the same as when not using the receive timeout feature on the unpatched version (i.e. without this commit). Please, accept this pull request, we *really* need that extra performance and the flexibility to use any queue. Thank you so much again for taking the time.
Hi again, I've made some changes in both the benchmark description and in the commit comments to become more clear what the problem is exactly. I hope this helps.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/193/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/193/
Changed so that `AddressTerminated` is published when `MemberRemoved` also. Added a comment about `MemberRemoved` and `MemberDowned` in ticket https:www.assembla.com/spaces/akka/simple_planner#/ticket:2788
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/192/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/192/
`min-nr-of-members` LGTM, `memberUpFuture: Future[CurrentClusterState]` would be good to have in the way your last comment suggests (wouldnt call it onX if it returns a Future which obtains its value once X happens, because it would sound a little weird to say `onMemberUp onComplete blah`).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/195/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/195/
Does any of this need to be mentioned in the migration guide?
Better to have too much in the guide than to little. Saves us time. 
Deprecation warnings are emitted by the compiler, and the messages display how to migrate
When I deprecated timerActive_? I mentioned it in the [migration guide](http:doc.akka.io/docs/akka/snapshot/project/migration-guide-2.1.x-2.2.x.html#API_changes_to_FSM_and_TestFSMRef). I guess we should decide whether or not we want to list all deprecations or whether we want to list none at all.
I view the deprecation warning as the migration guide
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/206/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/206/
I am not sure whether it is a good idea to add sbteclipse to individual projects. Maybe it is better to just add it to the global plugins project (~/.sbt/plugins).
Sounds good.  I'll close the request.  Thanks, Heiko.
Looking at the current plugins for Akka, it looks like these are needed for the (offline) build. But sbteclipse is "just" for Eclipse users, therefore it should go into the individual users' local global plugin project.  But as I said, I am not sure. Therefore I would like to hear other opinions.
I agree, better to use global plugins for sbteclipse. Similar with sbt-idea.
He Brendan,  thanks for the great work!  A few question since I'm not completely in touch of the exact semantics of durable mailboxes. How often is something persisted to mongo? Is that on every put/take or can this be configured? I can imagine that in some situation message loss would be acceptable while in others it would not be.  I see that some asynchronous communication is going on. But do you get the guarantee, that if you return from a put message on mailbox, that the newly added item is persisted (so is made durable). Or could it be the new mails are pending somewhere and could get lost on system failure.   On Tue, Jul 12, 2011 at 7:06 PM, bwmcadams < reply@reply.github.com>wrote:  > I neglected to do this before; forgot that Wiki has been replaced with > Sphinx. > > This is documentation for the Durable Mailboxes page about how to use the > new MongoDB Durable Mailboxes. > > I test built Sphinx and everything renders correctly. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/86 >
I'm using the SAFE write semantics, so the enqueue method will ensure that the commit is being persisted to MongoDB.  I am not using fire and forget write at all, as I'm aware of the durability concerns.  The MongoDB interface library is asynchronous, but because I use SAFE semantics, MongoDB will reply to me once it attempts to write the data.  If the write succeeds, the callback is invoked with RIGHT, and I complete the future with a "true" value.  If the write fails for any reason (network down, write timeout, etc) the callback is invoked with LEFT and I complete the future with an Exception which should signal to the layer above that the write failed.   There are more 'durable' semantics available with MongoDB writes which long term may be worth recommending:  - recommending users run with journaling enabled, and set JOURNAL_SAFE instead of safe, which guarantees that mongoDB puts the write into the group commit log (which guarantees crash recovery). - Running with Replica Sets, and ensuring writes replicate out to a certain # of nodes 
On Tue, Jul 12, 2011 at 9:59 PM, bwmcadams < reply@reply.github.com>wrote:  > I'm using the SAFE write semantics, so the enqueue method will ensure that > the commit is being persisted to MongoDB.  I am not using fire and forget > write at all, as I'm aware of the durability concerns. >  I'm not familiair with the 'SAFE' write semantics. Could you give a pointer to some documentation?  > > The MongoDB interface library is asynchronous, but because I use SAFE > semantics, MongoDB will reply to me once it attempts to write the data.  If > the write succeeds, the callback is invoked with RIGHT, and I complete the > future with a "true" value.  If the write fails for any reason (network > down, write timeout, etc) the callback is invoked with LEFT and I complete > the future with an Exception which should signal to the layer above that the > write failed. >  But could it be that the write completed, but you do get an error? E.g. you get the timeout but also at that moment the write completes... Probably not an issue since you can also get that in a normal situation.. you know if you returned that it is durable, but if it has become durable, the return doesn't need to have finished.   > There are more 'durable' semantics available with MongoDB writes which long > term may be worth recommending: > > - recommending users run with journaling enabled, and set JOURNAL_SAFE > instead of safe, which guarantees that mongoDB puts the write into the group > commit log (which guarantees crash recovery). > - Running with Replica Sets, and ensuring writes replicate out to a certain > # of nodes >  One of the things I'm very interested in long term, is how we are going to deal with all kinds ACID like sub-systems inside an actor based application and reason about certain correctness properties. Even reasoning about a business transaction spanning 2 different SQL databases can be extremely challenging.   > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/86#issuecomment-1556770 >
On Tue, Jul 12, 2011 at 3:49 PM, pveentjer < reply@reply.github.com>wrote:  > On Tue, Jul 12, 2011 at 9:59 PM, bwmcadams < > reply@reply.github.com>wrote: > > > I'm using the SAFE write semantics, so the enqueue method will ensure > that > > the commit is being persisted to MongoDB.  I am not using fire and forget > > write at all, as I'm aware of the durability concerns. > > > > I'm not familiair with the 'SAFE' write semantics. Could you give a pointer > to some documentation? > > SAFE implies calling getLastError; In Casbah/Mongo-Java/Hammersmith SAFE also calls it with w: 1 -  http:www.mongodb.org/display/DOCS/getLastError+Command  <http:www.mongodb.org/display/DOCS/getLastError+Command>Essentially, all MongoDB writes are asynchronous regardless of client library.  Invoking getLastError will block until the *last write on that connection *succeeds.  With Hammersmith because of the fun of a non-blocking API, you cannot call getLastError out of band.  Instead, there is an implicit WriteConcern that can be set or explicitly passed to a write operation which says how to handle the write.  If WriteConcern is set, we write a combined packet to MongoDB that is |<write op>|<getLastError>| to ensure we invoke getLastError on the same write op. In which case we'll wait around for the answer to getLastError before invoking the callback with the *result of getLastError* (if you don't set a writeConcern your callback gets called immediately).  WriteConcern is just a wrapper around the values of getLastError.   > > > > The MongoDB interface library is asynchronous, but because I use SAFE > > semantics, MongoDB will reply to me once it attempts to write the data. >  If > > the write succeeds, the callback is invoked with RIGHT, and I complete > the > > future with a "true" value.  If the write fails for any reason (network > > down, write timeout, etc) the callback is invoked with LEFT and I > complete > > the future with an Exception which should signal to the layer above that > the > > write failed. > > > > But could it be that the write completed, but you do get an error? E.g. you > get the timeout but also at that moment the write completes... Probably not > an issue since you can also get that in a normal situation.. you know if > you > returned that it is durable, but if it has become durable, the return > doesn't need to have finished. > > If w is set to > 1, MongoDB will report *any* errors that occur in the write.  HOWEVER --- it *is* possible with the current code that timeouts could be fickle.  We should discuss how to better handle this.  I'd say honestly that Write Timeout should be entirely driven by WriteConcerns' wtimeout value but I don't think Akka allows blocking forever.  At the least, I need to completeWithException if we timeout on the Akka future.  Thoughts?  > > > There are more 'durable' semantics available with MongoDB writes which > long > > term may be worth recommending: > > > > - recommending users run with journaling enabled, and set JOURNAL_SAFE > > instead of safe, which guarantees that mongoDB puts the write into the > group > > commit log (which guarantees crash recovery). > > - Running with Replica Sets, and ensuring writes replicate out to a > certain > > # of nodes > > > > One of the things I'm very interested in long term, is how we are going to > deal with all kinds ACID like sub-systems inside an actor based application > and reason about certain correctness properties. Even reasoning about a > business transaction spanning 2 different SQL databases can be extremely > challenging. > > This is I think a problem all around... all I can really guarantee is that we follow appropriate semantics within MongoDB itself.  Any other transactional type operations are beyond the scope of things that MongoDB does.  One thing important to note - the dequeue semantic that is used, findAndModify, is atomic and guaranteed to be all or nothing.  THat is to say, if we remove the document or modify it we guarantee we don't give it to someone else, etc.  > >
> > This is I think a problem all around... all I can really guarantee is that > we follow appropriate semantics within MongoDB itself.  Any other > transactional type operations are beyond the scope of things that MongoDB > does. > > I have no problem with providing certain guarantees. But what all these nosql solutions do is to push concurrency control back from the database to the developer. The question now is: how can I as developer deal with ACID problems that otherwise would have been taken care of (at the cost of performance/scalability/flexibility etc).   > One thing important to note - the dequeue semantic that is used, > findAndModify, is atomic and guaranteed to be all or nothing.  THat is to > say, if we remove the document or modify it we guarantee we don't give it > to > someone else, etc. >  Very cool. I love atomic nature of some basic operations. Makes it possible to create higher level operations on top of them..   > > > > > > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/86#issuecomment-1557310 >
I think this is a useful addition; would you care to include a patch to akka-docs? Also, do you have signed a CLA for Scala or Akka?
I have signed a CLA for scala under my real name, Christian Krause.  What guidelines are there for akka-docs? What file should I patch - general/event-handler.rst?
Unfortunately Scala and Akka are different things (still), so please contact me at roland.kuhn@typesafe.com for the Akka CLA.  Concerning the docs: yes, you found the right file. Just add one example and a short paragraph for letting people know that the extractor exists. It might be best to open a new pull request afterwards because your addition collides with the only change to that functionality within months just done by Viktor.
I added two examples to general/event-handler.rst, I hope the format is ok, if not, please let me know.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/245/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/245/
I removed completely the Status trait. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/247/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/247/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/248/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/248/
Updated with fixes for reviews, and fixed a threading bug in ThrottlerTransportAdapter
LGETM (good enough), will properly review later
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/252/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/252/
Fixed in master, credit given where credit is due
Cool, thx Viktor!
You're most welcome
I think this is interesting, and a clean way of doing it. Would it be possible to do it in a separate trait, extending Actor, instead of placing it in Actor?
You could separate it but it would add code; you'd have something like, first the separate trait:  ```scala trait ActorWithWhenBecoming extends Actor {   def whenBecoming(behavior: Receive): Receive } ```  and then in actor cell or wherever the behavior stack ends up, a switch on actor type:  ```scala private def wrapBecoming(behavior: Receive): Receive = actor match {   case a: ActorWithWhenBecoming => a.whenBecoming(behavior)   case _ => behavior }  def pushBehavior(behavior: Receive): Unit = behaviorStack.push(wrapBecoming(behavior)) ```  I'm not sure this wins us much, it just adds source and bytecode? Do you see an advantage? 
I'm not saying that it is a bad idea to place it in Actor. Two reasons why I ask. 1. Interesting to see if we miss something in Actor to enable this kind of thing to be written in pure user code. 2. This could potentially be an optional add-on, placed in akka.pattern, to keep Actor minimalistic.  As I understand it from your answer, it is not possible to do in user code, at the moment.   On Tue, May 22, 2012 at 3:03 PM, Havoc Pennington < reply@reply.github.com > wrote:  > You could separate it but it would add code; you'd have something like, > first the separate trait: > > ```scala > trait ActorWithWhenBecoming extends Actor { >  def whenBecoming(behavior: Receive): Receive > } > ``` > > and then in actor cell or wherever the behavior stack ends up, a switch on > actor type: > > ```scala > private def wrapBecoming(behavior: Receive): Receive = actor match { >  case a: ActorWithWhenBecoming => a.whenBecoming(behavior) >  case _ => behavior > } > > def pushBehavior(behavior: Receive): Unit = > behaviorStack.push(wrapBecoming(behavior)) > ``` > > I'm not sure this wins us much, it just adds source and bytecode? Do you > see an advantage? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/467#issuecomment-5846335 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
Right, you can do something like it in user code now but it's messy:  1. you have to "rename" receive in some idiosyncratic way, especially unpleasant in a library since other libs or apps may not use the same convention 2. you can't keep your customizations after a become() (no way to hook in to become(), the only hook is to override receive) 3. people feel compelled to make receive final when doing this (because overriding receive again would break the mixin), but that keeps you from having more than one mixin at once  So `whenBecoming` adds a hook, "let me modify any new behavior the actor gets" and documents a chain-up convention to support mixing in multiple modifications.  I think the nicest thing here is that the mixin is completely transparent to the subtype doing the mixing-in; _all_ people have to do is add `with MyMixin` and not make any other changes, and the mixin will work its magic.  
Thanks for clarification. I like it.
Moved to https:github.com/akka/akka/pull/492 , rebased, with rename back to aroundReceive pending discussion, at Viktor's request. 
good, I'm observing this
Looks really cool guys. I think the resulting impl would be way cleaner if the ExecutionContext would go in the Breaker constructor instead on every method signature.
We would probably want to have java api of this also. Would this work?      def onOpen(callback: Runnable): Unit      withCircuitBreaker[T](body: Callable[Future[T]], executor: ExecutionContext): Future[T]      withSyncCircuitBreaker[T](body: Callable[T]): T
Ok for the suggested Java API am having a type inference compilation error that's not obvious to me.  Adding  ```scala   def withCircuitBreaker[T](body: Callable[Future[T]]): Future[T] = {     withCircuitBreaker(body.call)   } ```  Gives:  ``` [error] F:\projects\akka\akka-actor\src\main\scala\akka\pattern\CircuitBreaker.scala:116: type mismatch; [error]  found   : akka.dispatch.Promise[_ <: T] [error]  required: akka.dispatch.Future[_1(in method withSyncCircuitBreaker)] where type _1(in method withSyncCircuitBreaker) <: T [error]       try Promise.successful(body) catch { [error]       ^ [error] one error found [error] {file:/F:/projects/akka/}akka-actor/compile:compile: Compilation failed [error] Total time: 1 s, completed May 20, 2012 4:23:48 PM ``` I think the clue is in _1(in method withSyncCircuitBreaker) ...  will keep researching.  For reference: ```scala   def withSyncCircuitBreaker[T](body:  T): T = {     import CircuitBreaker.syncExecutionContext       execute the body in caller's thread     implicit val executor = syncExecutionContext     Await.result(withCircuitBreaker(       try Promise.successful(body) catch {     <== Failing here         case NonFatal(t)  Promise.failed(t)       }),       Duration.Zero)   } ```
The pull request for the documentation of the durable mailboxes has been merged to master now. akka-docs/modules/durable-mailbox.rst akka-docs/modules/code/akka/docs/actor/mailbox/DurableMailboxDocSpec.scala  Add description of how to use circuit breaker together with durable mailbox and add it to the example MyMessageQueue in DurableMailboxDocSpec.scala  Thanks
Optimizing performance matters a lot, but only for code where it matters to optimize performance (and don't tell me it matters everywhere because it doesn't).  In all other areas we should optimize for developer speed and productivity. It is not black or white. It is a balance. 
On the Unsafe - is backing off to ARFU a good compromise, or should I being using AR?
@patriknw, will start looking at the docs shortly, haven't forgotten about them.  Want to wrap up the last few items.   @viktoklang, @jboner On Unsafe vs ARFU vs AR, is there a consensus, beyond removing Unsafe?
Leave it as it is, I'll take a stab at it when no one is looking ;-)
Will need to squash this into a single commit very soon.  be9bce8 has the changes of interest.  I tried something a bit different than @patriknw recommended with the documentation in the spirit of this being in patterns instead of confined to durable mailboxes.  The proposal is adding a section to common next to Duration.  I also included a fairly simple graphviz state transition diagram.  If you guys agree this is a good direction, I can essentially link to this section from the durable mailbox section, as well as add the config specifics for the file-based durable mailbox.  If not it should be straightforward to push this down to durable mailboxes.  To generate the graphviz the graphviz `dot` application must be in the path, I used `-D graphviz_dot = "<location>"`, since I was running sphinx_build manually (windows FTL).  I would need some assistance from someone with OSX to see whether any additional work needs done to the Makefile to enable graphviz.  I have one outstanding issue (that I'm aware of) with the code, and that is the use of `withSyncCircuitBreaker` with an asynchronous execution context causes a few tests to fail in the `Closed` state.  Am still troubleshooting this one.
I can try the graphviz build later
Obviously having some trouble with merging in master - clearly i'm doing something very wrong, having to make the same merges over and over again.  At this point I'm worried about a clean merge of the pull into master.  Will see if there's a good way to confirm that the merge would go cleanly by which I mean the changes in the branch apply and none of the files that i haven't touched would change.  Otherwise i'm thinking about moving the commits to a clean branch.  Docs changes are in 9244af0, 5d836f0, de8c17d 
yes, this merge/rebase? looks very strange. I don't know what you did, but the general principle is to not rebase something that has been pushed to a remote branch. Starting a new branch is a good idea.
Replaced with https:github.com/akka/akka/pull/493
Good initiative and docs. But I for sure don't like Specs2. 
Looks good, +1
I removed the branch, so if you want to port this for 2.0.2 you'll have to do it manually on release-2.0 (or gcp)
Viktor, I think you may have merged too eagerly ;-)
I apologize for my trust in your work.
now that I merged `deployOn` into master, could you fix the hard-coded paths when you merge this in?
yes, I will use deployOn
@rkuhn does the deployOn really solve `akka.cluster.node-to-join`? To me it looks like a specific thing for `akka.actor.deployment`, which are not used in those tests.
oh, sorry, deployOn is neither the right tool in this case, nor is it necessary: simply dont configure joining up-front, but instead do it programmatically using      Cluster(system).join(testConductor.getAddressFor(master))
ok, will do
Very nice. Looking good. 
Are you planning on porting all these tests now? Or should we create tickets for each one of them so we can share the work? 
Yes, we can create child tickets to 1948 I want us to review and agree on the overall format/structure first.  /Patrik  23 maj 2012 kl. 17:10 skrev Jonas Bonr<reply@reply.github.com>:  > Are you planning on porting all these tests now? Or should we create tickets for each one of them so we can share the work? >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/470#issuecomment-5875456
Good stuff! +1
Think it looks great so far +1
Yep. Very good. Let's go with this. How far out is the port management? 
moved the barrier to `after`
The test actor systems name should be fixed once http:www.assembla.com/spaces/akka/tickets/2122 goes in, final barrier would probably exclude certain tests (e.g. node goes down, what should it do?), so I think that one line should be explicit where needed.
oh, crap; thanks for catching this! should also go into master
How fitting. Just in time for Jul. :christmas_tree:
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/243/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/243/
This is not all places where akka: is defined. Why not change all of them?
without having looked at the places you allude to: `akka:` stays as denominator of the local transport, so I would expect a few occurrences to stay unharmed.
Hm. I haven't seen the "no protocol loaded" error message after these changes in the cluster tests, so I assume this is sufficient -- and might be over-eager. Without looking deeper into the logic of the tests I cannot determine if the use of "akka" is valid.
ok, it makes sense to keep `akka:` for local, but I noticed it here: https:github.com/akka/akka/blob/master/akka-remote/src/main/resources/reference.conf#L41 https:github.com/akka/akka/blob/master/akka-docs/rst/scala/remoting.rst https:github.com/akka/akka/blob/master/akka-samples/akka-sample-cluster/src/main/resources/application.conf  and that's only examples. I think a manually judged search and replace is needed.
Yes, we had that debate once before with Roland, therefore the period :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/244/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/244/
Conclusion: merge this in, since it fixes test failures, and create a ticket for making sure that the rest is also dealt with (e.g. Patriks examples).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/239/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/239/
Hey Kjell,  I saw that there hadn't been any news on this since last time, I'd really like to get this excellent piece of work into master, can you verify that it works with the latest master before I merge it in?  Thanks!
Hi Victor,  I just verified that it still works with the latest master by merging in the latest master into my repository.
Awesome. Thanks a lot for this, it's looking sharp!
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/238/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/238/
ah, we had not done that on master, great :-) :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/236/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/236/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/237/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/237/
0MQ rocks :-)  I've added some comments to the diffs, please don't hesitate to ask if there's something I can clarify or explain.  Cheers!  
Cool, thanks for the comments -- I appreciate your feedback! I'll go through your comments and push a new version tonight.
Great, looking forward to re-reviewing! :-)
By the way, do you like reviewing patches that incrementally change the code based on your review or should I squash my changes to the original patch? I prefer the former one.
Just create a new pull request (prefer to see all new changes in one diff, makes it easier to see). Thanks!
Sorry for the delay in the review, I'll take a look at everything first thing in the morning.
Viktor, feel free to re-review the changes I've made based on your comments. Thanks!
Just commented latest version, one thread-safety-issue and some suggestions, then all done!
Alright, I think that the issue with the 'socketClosed' is now fixed, though, I didn't fix it in the way you suggested.
In the latest diff I can see that it's still not threadsafe. Tbh I think it's easier (and threadsafe) to just check self.isShutdown
I don't see how using 'self.isShutdown' in 'select' would result into correct behaviour. Can you explain a bit?
The idea is to not reschedule the select if the ConnectionActor is shut down, right?
Yeah, you're right. Pushed a new patch.
Here is what ZeroMQ has to say about the thread safety of ZeroMQ sockets:  "MQ sockets are not thread safe. Applications MUST NOT use a socket from multiple threads except after migrating a socket from one thread to another with a "full fence" memory barrier."
Crap, I wrongly assumed that the Socket would be safe since the Context is.
In the light of this new information, it might be safest to drop the entire selectTask idea and just keep the socket local to the Connection Actor, and then after every inbound message do the receive, and then use a receiveTimeout for the actor (essentially becomes the polling interval) and on ReceiveTimeout do a receive of 0MQ bytes.
And what would this construct look like, approximately? Inbound message -- a message received by the actor? 
Some clarification on creating sockets:  "0MQ 'sockets' are _not_ thread safe. Applications MAY create a socket in one thread with _zmq_socket()_ and then pass it to a _newly created_ thread as part of thread initialization, for example via a structure passed as an argument to _pthread_create()_. Applications MUST NOT otherwise use a socket from multiple threads except after migrating a socket from one thread to another with a "full fence" memory barrier."  
Alright, here's a suggestion that might work, note that I haven't tried to compile it:  https:gist.github.com/1295252
Did it work? (My assumption was that recv(0) means "read without blocking"
I took a glance at the code and it looked good to me. I'll resume working on that tomorrow morning :)
Wicked, can't wait to get it into 1.3!
Alright, the code is starting to look quite concise :)
Great! Looks good now!
Hey Karim, did that help? Let me know if I can assist in some way. Thanks for your hard work!
Yeah, thanks! I'll push a revision of the spec shortly.
Okay, I think there is now some more sense to the test case.
Viktor, any news on this?
Oh, sorry, I've been busy preparing a training. You've signed CLA and everythings green?
CLA signed and looks green to me. No more yo-yo tests since latest commit.
Great! I'll merge it into 1.3 ASAP
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/232/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/232/
yeah, sorry: me too!
and this change should be back-ported to 2.0 and 2.1 as you stated
There seems to be a conflict on 2.1 with something about timers being cancelled that hasn't been backported. what about that?
Is there no use case for knowing current data, as well as next data, in onTermination? I think onTransition and onTermination has some similarities, and in onTransition you have access to current state and next state. Might not be important, so LGTM.
hmm, just checked, it was https:github.com/akka/akka/pull/915, ticket 2689; I had not thought to back-port it because it does not look super-urgent, but for all practical purposes it should be binary compatible, and it does fix something (nothing catastrophic for sure), so why not? Lets add it and say what MIMA says (in case it complains rip out that one new method, which is internal API anyway).
I backported that as well (some surgery required)
Thanks Patrik for demystifying the test, here's a non-wasteful reimpl, will be backported to 2.1 as well
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/229/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/229/
I think it is all good quality improvements and minimizing published api is great. Explicit return types in api is also great as we all know.  What I think we should also do is to take a closer look at the traits that we expose to be extended by user code. Pure interface trait is no problem. I guess we should be careful when we have some implementation in the traits. I'm not sure exactly what we should prepare for in the traits, but I'm raising the question. Minimizing the number of traits that can be extended would be good.  Here is a few:  * ActorLogging * Actor - should we consider using abstract base class? * ActorRefFactory - should we support extension of this and related?   * various ActorRef   * ActorRefProvider   * ActorContext   * TypedActorFactory * ScalaActorSelection * Deploy.Scope * SupervisorStrategyLowPriorityImplicits * FSM * LoggingFSM * IterateeRef * Stash * Listeners * Router   
Great, thanks Patrik. What about the FIXMEs I put in there, that we'll need to take care off before merging in
If we make Actor an abstract base class we will break literally every user code base that exists. 
Really impressive diff. Great job. So many great cleanups. Code is now in much better shape. Thanks.   What makes me a little worried is that there are also massive changes in behavior/algos. Do we have solid test cases for all of these changes? 
Great effort! Lots of good improvements.
all-in-all: the greatest example of following the boy-scout rule Ive seen. Great Work!
I agree. Epic. 
Thanks guys, appreciate the kind words!
looks good +1
Alright, merging this in now, we can always adjust the changes after the fact.
Looks great I think. 
Hmm, looks like you suggest users to write a general purpose ExternalAddressExt. Why don't we provide it in akka, and reduce the amount of documentation needed? This is probably something I need to consider in the durable mailbox and deamon msg pull requests right now. Let's talk about it today.
One reason why no general and simple solution is included is because we don't know enough about the details of clustered refs yet. But I agree that we should fix this ASAP.
Looks good, merge
not yet merging because of open points for discussion
(also on release-2.0)
The ByteStringIterator is not consistent with the behaviour of other iterators:  ``` Welcome to Scala version 2.9.2 (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_31). Type in expressions to have them evaluated. Type :help for more information.  scala> import akka.util.ByteString import akka.util.ByteString  scala> val bs = ByteString("Hello World") bs: akka.util.ByteString = ByteString(72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100)  scala> val bsit = bs.iterator bsit: akka.util.ByteIterator = non-empty iterator  scala> bsit.take(2).toByteString res0: akka.util.ByteString = ByteString(72, 101)  scala> bsit.take(2).toByteString res1: akka.util.ByteString = ByteString()  scala> val list = List(1,2,3,4,5) list: List[Int] = List(1, 2, 3, 4, 5)  scala> val listit = list.iterator listit: Iterator[Int] = non-empty iterator  scala> listit.take(2).toList res2: List[Int] = List(1, 2)  scala> listit.take(2).toList res3: List[Int] = List(3, 4) ```
Some of this applies to a previous pull request which I missed commenting on:  Is there a reason for exposing CompactByteString and ContByteString as return types of the public api? I had placed ByteString1 and ByteStrings within the ByteString object as the actual implementation being used shouldn't be a concern to the user (just like users shouldn't need to deal with Map1, Map2, etc). A user should never be using the CompactByteString object, as the ByteString object should be creating compact ByteStrings. It's fine for the actual implementations to return more precise types, but the abstract ByteString class should just be returning ByteString. A contrived example of where this might create problems:  ``` var bs = ByteString("hello").compact bs ++= ByteString("world")  fails to compile ```  or  ``` (ByteString("hello").compact /: ByteString("world"))(_ :+ _)  also fails ```  Not to mention it can make later changes more sensitive to binary compatibility problems.  Previously a user never had to be aware of the different implementations. If a user wanted to have a compacted ByteString, a call to 'compact' would either return a compacted ByteString, or return itself unchanged if it was already compacted. It wasn't even worth having an 'isCompact' method.  It all seems to clutter the user api without adding any benefit.  All of your other changes seem great so far though. I love the work you have put into all of this.
derekjw  wrote:  > The ByteStringIterator is not consistent with the behaviour of other iterators:  Sorry, but I have to disagree - true, ```ByteString.iterator``` behaves differently from ```List.iterator```. However, I'm not sure all Scala iterators behave like that. They certainly don't have to, as the Scala documentation for Iterator.take (and several other iterator operations) states:  > Reuse: After calling this method, one should discard the iterator it was called on, and use only the iterator that was returned. Using the old iterator is undefined, subject to change, and may result in changes to the new iterator as well.  So the Iterator does not have to be immutable under take, drop. etc. like the ```List.iterator```. For ByteString.iterator I chose to modify the Iterator and return ```this``` to avoid excessive object creation in applications that will frequently drop single Bytes (or similar). So in the case of ```ByteString```, the state of the old Iterator is intentionally well defined after these kinds of operations, and they advertise this by having the return type ```this.type```. I think this behaviour is compatible with the above rules for generic Iterators (stating that you can basically do whatever is best for your specific Iterator :-) ). 
That's why I made certain not to actually call ByteIterator's behaviour wrong, since there isn't a clearly defined correct behaviour for iterators. I'm not too concerned about it either way, I just wanted to point out that this behaviour was different then any I had seen so far, so was surprising. I see the reason for it now, thanks.
derekjw wrote:  > Is there a reason for exposing CompactByteString and ContByteString as return types of the public api? I had placed ByteString1 and ByteStrings within the ByteString object as the actual implementation being used shouldn't be a concern to the user (just like users shouldn't need to deal with Map1, Map2, etc). A user should never be using the CompactByteString object, as the ByteString object should be creating compact ByteStrings. It's fine for the actual   I think the user, in some cases, *does* have to know - or to specify exactly whether he requires a compact or contiguous ```ByteString``` at some point. You are right, in many cases the user will not care. However, in a performance critical application, dealing with high data volumes, the cost of compacting might be a good trade-off compared to the run-time cost incurred by operating on a chunked ByteString (possibly repeatedly), especially for random-access scenarios. Im my application, I do care (in some cases). :-)  This is also where ```isCompact``` and ```isContiguous``` come in - usually, you do not care. But when you do performance optimization, you might want to know if your data is chunked or not, and want to check if ```compact``` will be an O(1) or an O(n) operation at some stage.  Another, unrelated, reason for my proposal of ```CompactByteString``` was that I wanted a serializeable ByteString.  > var bs = ByteString("hello").compact > bs ++= ByteString("world")  fails to compile > or > (ByteString("hello").compact /: ByteString("world"))(_ :+ _)  also fails  Hm, true. However, is that really so bad? As ```compact``` clearly returns a CompactByteString, not a ByteString, the user should not be surprised by that. I would argue that if the user explicitly requested a compact ByteString, like in these examples, then it's actually a good thing that something that will result in a non-compact one will fail to compile. :-)
derekjw wrote:  > That's why I made certain not to actually call ByteIterator's behaviour wrong, since there isn't a clearly defined correct behaviour for iterators. I'm not too concerned about it either way  Ah, ok - thanks! I was worried I might have to change it all again. ;-)  And, by the way, thanks for ByteString in general - it was exactly what I had been looking for for a long, long time!
I think I can live with having the the more specific return types. The example problems I brought up are the same problems I have to deal with when using Some, None, or Nil, so I'm used to those situations.  Don't worry too much about my critical tone while reviewing, I just want to make sure you have justification for decisions like that.  I also had been looking for something like ByteString for a long time, but wasn't happy with any similar implementations. I hope that after it's really been put through it's paces that it might be considered for inclusion into the standard library. It's disappointing that the only space efficient byte storage in the standard library are mutable collections. Improvements like the ones you've added help a lot, thanks. 
Hi Derek,  > Don't worry too much about my critical tone while reviewing, I just want to make sure you have justification for decisions like that.  that's fine - I did ask for a review after all. ;-)  Don't worry - I'm a scientist, I'm used to honest criticism. It's important, I'd do the same in your place - ByteString is a fundamental class, after all, and these are big changes. Just don't mind me defending them with equal fervour. ;-)  I'm glad to get feedback - we need to get this right.  > I also had been looking for something like ByteString for a long time, but wasn't happy with any similar implementations. I hope that after it's really been put through it's paces that it might be considered for inclusion into the standard library. It's disappointing that the only space efficient byte storage in the standard library are mutable collections.  Space efficient and performant - you're so right! Having something like ByteString in the standard library would be really nice.  If you're interested, I have a high-performance generic collection, optimized for primitive types (it uses specialization in the right places to achieve native-array-like performance). It doesn't support chunking and O(1) slicing, though, it's basically a generic version of CompactByteString + ByteArrayIterator (actually, I derived ByteArrayIterator from my specialized generic ArrayIterator).  
Stopped commenting on missing return types after a while ;-)  I've done a lot of work trying to rectify things like that on my binary compatibility branch.  Overall very nice code quality, most common problems:  * Lack of return types * Seemingly large public API (make as much private as possbile to allow for flexibility w/o breaking binary compat) * using null return values to indicate need to implement feels wrong. Make it an abstract class instead and avoid implementing it, or throw an exception, never return null ;-)  Very cool stuff!
Hey Victor,  thanks for your comments - I'm on it! Give me a few day, bit busy atm. :-)
Hey Victor,  a quick question concerning the lack of return types: Do you want explicit return types only for the public classes, or also for all methods of the internal classes like ```ByteString.ByteString1```?. I just ask because the latter didn't have explicit return types for many methods before, either.  Cheers,  Oliver
Check my branch at: https:github.com/akka/akka/tree/wip-2006-binary-compat-%E2%88%9A Return types go on all methods from now on, it has a number of benefits: 1) Documentation for devs == less maintenance costs 2) No risk of accidental change of return type == less maintenant costs 3) No risk of reflective types generated == higher performance 4) Less work for the typer == faster compilation times  It's perfectly fine to use type inference within a method though.
> Return types go on all methods from now on, it has a number of benefits  Just for clarity - that also goes for methods implementing or overriding a method whose return type has already been defined in a parent class / trait? I guess yes, to prevent accidential specialization of the return type?
Yes, precisely because of that reason. It also avoids accidental regressions where the specialized return type is accidentally removed. I also try to enforce specifying "override" even for overriding abstract members/methods since you get a compiler error if you remove that member/method on the parent, so you can clean up properly afterwards.
Hey Victor,  I implemented your comments, fixed a few design flaws and bugs and extended the tests quite a bit.   > Lack of return types  It's fixed now.   > Seemingly large public API  The public API is reduced now to ```ByteString```, ```CompactByteString```, ```ByteStringBuilder``` and ```ByteIterator```. I think we do need all of these.  The ```ByteString``` implementations are inside ```object ByteString```, as before. I did the same for the ```ByteIterator``` implementations now. Is that good enough, or does all that have to become ```private[akka]```?    > using null return values to indicate need to implement feels wrong.  No, it's ugly as hell :-). I only did it because it *had* to give an implementation to override the return value, as these methods were already implemented a super-type. You're right, though - throwing an exception looks a lot cleaner, I changed it accordingly. Anyhow, since all affected classes are sealed, it is guaranteed these dummy implementations will *never* be executed (which is why I made the comments on them normal, not ScalaDoc - they're only relevant to Akka hackers).  Could you give this another review? I'm not planning any further major changes, except maybe a few small performance tweaks and an override for ```ByteString.grouped```. Or should I do that first?  I propose that once as you're happy with the overall state of the code, I'll close this review request and open a proper pull request. That sound good? 
Hi Oscar,  I'll review this ASAP, terribly swamped right now.
Alright Oscar, what's the status here?
> Alright Oscar, what's the status here?  Sorry, Victor - haven't forgotten. I'm abroad on a conference (several, actually). I'm working on it, but can spare only limited time, so it'll take a few more days. Hope that's ok? When would you like to merge it, latest?
No worries man, I'm mainly just making sure that you don't think I'm neglecting you. I'd really love to have it in Akka 2.1-M1 which is due about a week/10 days after Scala 2.10-M4  Have fun at the conferences,  Cheers, 
As you have probably concluded from my comments: I didnt find much to pick on ;-) Great work!
Looking great now, excellent work!  Now lal that's preventing me from merging this in is the lack of docs, could you add .rst docs in IO and ping me when that's ready so I can merge *fingers itching*
Hi Victor,  > Now lal that's preventing me from merging this in is the lack of docs, could you add .rst docs  ok, I extended the ByteString docs in IO (Scala), mainly based on the description of this request. It that roughly what you wanted?  If you're happy with the docs, please don't merge just yet - there's one or two things in the code I still might change. :-)
Ok, lemme know when you think it's ready to go in and I'll review then!
Hey Victor,  > Ok, lemme know when you think it's ready to go in and I'll review then!  ok, with the last commit it's ready now.  I still have some ideas for future performance improvements, but they will not touch the public API and still have to mature in my head for a while anyway. So from my side, ready for merge.
After the .rst docs is changed to pull in the code this is good to go, ping me as soon as it's done and I'll merge it in!  Cheers!  
Hey Victor,  is the documentation OK like this? 
Yes, looks good! Will merge it in ASAP, just hang on :-)
> Will merge it in ASAP, just hang on :-)  Thanks! No hurries on my account - I'm glad it's ok and done. :-)
Crap, and just as I am supposed to merge it in it's no longer automatically mergeable... Could you merge with the current master and re-push so I can merge it? (promise to do it fast this time ;-) )  Thanks!
I'm on it!
> Could you merge with the current master and re-push so I can merge it? (promise to do it fast this time ;-) )  Ok, it's merged. Sorry, I overlooked your refactor commit 94e71b7a, that why I didn't realize things weren't up to date. 
Hey,  have you've signed the CLA?  www.typesafe.com/contribute/cla  Cheers, 
Hello Viktor, I just signed the CLA a couple of seconds ago.  Regards Mirko
Waiting for better times? :-)
WDYM? I have fixed the issues people had. I have not received a GO from anyone.  If you like the patch, merge it else delete it. I'm fine with either. 
Looks good to me, merge and close ticket
Cool. Thanks. I thought it was you that did the merging. E.g. was the gate keeper. 
I just delegated it to you, like a boss :-)
@rkuhn should we backport to 2.0.5 as well?
hmm, that ticket is closed already, and it is on the 2.1.x milestone; something does not add up
Yes, according to your instructions: https:github.com/akka/akka/pull/953
What I meant was that the ticket was closed before all of its pull requests were merged (or closed), which can be confusing when reviewing the github state.  And Im very much missing links from pull requests to tickets, its a bit of a hassle to click through a bunch of slow assembla pages each time, dont know whether we want to do it with greasemonkey or otherwise.
Since there was no merge conflict there is really nothing to review. The only reason for these PRs is for Kitteh-action.
Ideally a ticket should wrap the whole feature development cycle, i.e. it exists before you start working and it is closed when everything relating to the ticket is done; this way we have one authoritative source for what is ongoing, what is planned and what is finished. Ill send a mail to akka-dev so we can discuss this at a more appropriate place.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/227/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/227/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/226/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/226/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/241/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/241/
Is this sufficient for OSGi support? Do we not need to do something for the class loading in DynamicAccess? 
It is a first step. See above: "Perhaps once we have these initial OSGi headers sorted, we can add an akka-osgi module which does the right thing in an OSGi environment (e.g. providing an easy to easy Activator implementation and/or a Blueprint namespace handler to set up the system."
OK. So it is useful as-is, but can be improved? 
Yeah, I think so - I managed to get the Pi calculation sample deployed in Karaf/ServiceMix with these bundles in their current form.
One thing that might be missing (cannot check right now, because traveling): Are the published artifacts already bundles? If not, let's switch to sbtosgi 0.3.0 (still snapshot, but release can be puslished anytime), which takes care of that.
No, they're not - I didn't figure out how to get those generated by default when using 'sbt package' so for now, you have to explicitly run 'sbt osgi-bundle' to get these generated.  I tried the solution you suggested in http:stackoverflow.com/questions/10210992/how-to-use-sbtosgi-to-generate-osgi-bundle-as-part-of-sbt-publish-task, but that did not seem to work for me (probably because of my limited understanding of how sbt actually works).
OK, how should we proceed from here?  I suggest the following: - gertv updates this pull request to work with the latest master and reflect all the issues discussed here except for the last one (publishing artifacts) - Then the Akka team merges this pull request - After that I create a new pull request adding bundle publishing  WDTY?
I think the best thing is to retrofit it onto my https:github.com/akka/akka/pull/456 since I've removed/moved a lot of the embedded libraries.
I think all remarks have already been addressed in my previous commits and I just added a commit to ensure the pull request applies to akka/master again.  If you guys prefer to handle https:github.com/akka/akka/pull/456 first, I'll keep an eye on that pull request and update this one as soon as it gets closed/committed.
That sounds like an excellent plan. Would also be good with a small writeup for the docs so that people know what to expect from the OSGi support.
Let's consider this level of OSGi support "experimental" or "first-step" and wait with documentation until we know that we can really use Akka under OSGi.  One more thing to mention: Viktor said that the config library will no longer be included in the JAR in the future. Interestingly I added OSGi support to config today and Havoc already published a 0.4.0-SNAPSHOT version. In order for this "project" to work Akka must depend on 0.4.0 (or use a range for the ivy dependency, but I would rather not do that).
Alright. When is 0.4.0 scheduled?
My mistake. 0.4.0 is already out there. It has to be 0.4.1 (with OSGi support). I have asked Havoc to release it ASAP-
Excellent work!  Looking forward to this.
I'm looking forward to this, too. Heiko, are you planning to work on some blueprint spec for an "ActorSystem service" or something like that? Would be great.
Updated the pull request to be in sync with latest changes on akka/master again
I wonder if remote-actors can work in OSGi after u construct proper Manifest, since after deserialization akka system needs to find proper ClassLoader that contains definition of messages and actors, from my experiments I observe two options: 1. Providing to AkkaSystem ClassLoader that is capable of resolving classes needed during deserialization, but then for some reason I couldnt use default configurations provided by akka 2. Providing definition of message and actors as OSGi Fragment with akka as a host, and then akka default configuration could be resolved as well as classes during deserialization, only obstacle is limitation for developers, since Fragments cannot have activator and its own classloader.  What do You think about it? Do You have some other experience that you are willing to share
@elyast So far I don't have any relevant experience, but both options make sense to me.
@elyast - I haven't tried using akka-remote in OSGi yet, but I did manage to get some basic examples working - in order to get around the problem you mention in #1, I explicitly created the reference config with the classloader for the ActorSystem class (i.e. the akka-actor bundle) and then used my own bundle's classloader for everything else - I think this should work for use case as well, probably.  Something like:     val reference = ConfigFactory.defaultReference(classOf[ActorSystem].getClassLoader)     val config = ConfigFactory.load(getClass.getClassLoader).withFallback(reference)     val system = ActorSystem("my-system", config, getClass.getClassLoader) 
That sounds good,  I will try that one, so far I am creating test suite using tycho surefire published here: https:github.com/elyast/orbit,  the interesting test is located under: https:github.com/elyast/orbit/blob/master/system.tests/src/test/scala/org/elyast/orbit/system/tests/scala/AkkaTest.scala  I dont really like OSGi fragment since they are very limited, so that would solve my issues with passing whole default reference.  my utlimate goal is to create P2 repository, so u can be able to develop OSGi modules on top of popular scala frameworks using PDE environment.  Best regards Lukasz Jastrzebski  2012/5/25 Gert Vanthienen <reply@reply.github.com>: > @elyast - I haven't tried using akka-remote in OSGi yet, but I did manage to get some basic examples working - in order to get around the problem you mention in #1, I explicitly created the reference config with the classloader for the ActorSystem class (i.e. the akka-actor bundle) and then used my own bundle's classloader for everything else - I think this should work for use case as well, probably. > > Something like: >  val reference = ConfigFactory.defaultReference(classOf[ActorSystem].getClassLoader) >  val config = ConfigFactory.load(getClass.getClassLoader).withFallback(reference) >  val system = ActorSystem("my-system", config, getClass.getClassLoader) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/460#issuecomment-5940184    --  Pozdrawiam ukasz Jastrzbski
Hi there,  When the akka project is osgified, is there any idea to publish it into an p2 repository?  For example with [pde4sbt](https:bitbucket.org/jmhofer/pde4sbt/overview), which is currently not up-to-date.  The reason for this is, that Eclipse RCP applications that are build with tycho could use this repository to easily use akka.  cheers, Muki Seiler
Akka 2.1 will be published to Maven Central
Hi Gert, could you sign the CLA so I can merge this into master?  www.typesafe.com/contribute/cla  Thanks!
@viktorklang - done!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/223/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/223/
yes, nice cleanup: LGTM
LGTM, but this is api change and should be mentioned in migration guide
Right, will add.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/222/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/222/
Should this go into release-2.1?
yes, I think it should
Ok, I'll make it happen
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/225/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/225/
I'll backport to 2.1
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/224/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/224/
I'm assuming we don't backport this since it's not going to be backwards compatible.
:+1: (after the fact)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/219/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/219/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/221/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/221/
LGTM after comments
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/231/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/231/
I like it! :+1:
I haven't had the time to completely review this. If it can wait one more day, then I will look into this tomorrow.
This is not nearly as urgent as stabilizing master, don't worry about it Drew.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/220/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/220/
If everyone's OK with this I'll backport this to release-2.1
:+1: after changes according to discussion
You can merge this
No build kitteh?
hmmm, the build kitteh hasn't run in 5h?? Sounds like a problem to me
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/268/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/268/
I tried this patch in the stress test branch, locally, and it was success. That is a good start, and I would like to do a proper rebase and continue running the stress test on the build server. Please merge.
Very nice Peter!  So what's the semantics, only the online html gets the analytics, and not any offline docs?
Yeah. Two versions of the html docs are created - the online version that gets pushed to the server includes the analytics, the version bundled in the dist doesn't.
Interesting idea, but I feel like it's conflating the model. There has to be a better solution.
What do you mean when you say conflating the model?  I thought of other ideas for example around the behavior stack; you could have a `become()` variant that combines your new behavior `orElse` the previous stack top. Questions about that include:    - probably need to support `new orElse old` and `old orElse new`, so two "modes"   - when does the mixin trait call the `become` variant (in the trait body might work...)   - what to call the `become` variant and the two modes  I think it could work but I haven't tried it. It will be more than the one line of code for `aroundReceive` but not terrible. 
Though, in both the aroundReceive patch and this become() variant idea, it sucks that become() will lose the mixin behavior. I think it would be much more common for the intent to be to keep it (more like how the autoReceiveMessage messages work, they are "before" the behavior stack). So perhaps it could be more like a `preReceive orElse behaviorStack orElse postReceive` where `preReceive` and `postReceive` by default do nothing at all. 
But the options arent that good:  1) make preREceive and postReceive methods that take a message (and return what?) which sort of mixes styles between PFs and methods 2) Use PFs all the way, which will be costly since the number of allocations and the level of indirection will increase, which is bad for performance  Would an ability to apply the behaviorStack until a match is found solve parts of the problem?
+1 for some built-in solution for this.  I have a custom StackableActor trait that does something similar but then all actors must mix that in in order to support stacking of receives transparently.  I suspect others have solved this in a similar way.
You made me think of yet another idea: what if preReceive and postReceive returned an Option[PF], None by default. Then whenever something is added to the behavior stack, chain it with preReceive / postReceive _only_ if they return Some(pf). That way there is no overhead at all (well, some tiny one-time overhead in become, but that's it) if you haven't mixed anything in; and using become() does not lose the mixin behavior, since each new behavior gets wrapped in pre/post.  What I mean is something like: ```scala     def become(behavior: Actor.Receive, discardOld: Boolean = true): Unit = {       if (discardOld) unbecome()        could micro-optimize this a bit more       val chain = preReceive.toSeq ++ Seq(behavior) ++ postReceive.toSeq       actor.pushBehavior(chain reduce { _ orElse _ })     } ```  `chain reduce { _ orElse _ }` would evaluate to simply `behavior` if the pre/post receive PFs are None, so the per-message overhead would be completely unchanged from before unless you use a mixin.  A mixin's implementation of preReceive / postReceive would need to chain up to get the implementations from other mixins.  I think something like applying behaviorStack until a match is found could work, but the trick is that sometimes you want to replace an item in the stack as with current become(). So it seems like you'd need to track, for each item in the stack, whether it should be chained to with orElse or not - and then traverse the whole stack on each message? Not sure.
That's an interesting proposal, what does the Java API look like?
One consideration is that in Java you can only have one "mixin" because there's no multiple inheritance... so we just have a single linear chain of superclasses, if that matters. It may make this whole concept much less useful in Java than in Scala.  It looks like the way become() works now in Java is to use Procedure, but Procedure has no isDefinedAt so can't be used with partial function chaining. Similarly UntypedActor.onReceive has no way to not handle a message other than directly calling unhandled() to throw an unhandled exception. This would mean that no postReceive is ever called in Java right now I guess. And the whole docs section I was patching in this pull doesn't apply to Java.  We could add a japi type that implements PartialFunction perhaps (it would have an abstract isDefinedAt and apply)... could even un-final the override of receive() in UntypedActor and people could directly implement receive() rather than onReceive, if they are willing to code an isDefinedAt...  Then preReceive / postReceive could be overridden directly - there's a japi.Option already and you'd use the new japi.PartialFunction thing. Sure it'd be a little clunky but outside of abstract base classes that want to handle messages, you would never touch it.  Not sure, would have to play around with it in real code and see what it looked like probably.
Above patches illustrate the newer approach. I can rebase and squash if desired.
Over lunch I thought of a hybrid idea between aroundReceive and preReceive/postReceive, which I like more than either:  ```scala     protected def mapBehavior(behavior: Receive): Receive ```  vs. `aroundReceive`, `mapBehavior` is better because it persists across `become` (it wraps the current behavior, not just the default behavior).  vs. `preReceive` and `postReceive`, `mapBehavior` lets you customize how you invoke the behavior. For example, you could transform messages in some way before passing them along, or you could call the behavior multiple times, or whatever. Also, `mapBehavior` is simpler for people to implement (`super.mapBehavior(behavior orElse handler)` rather than `Some(super.postReceive.foldRight(handler)(_ orElse _))`).  Here's the code, squashed vs. master: https:gist.github.com/2727054 I'll also push the unsquashed commit in a minute.  I left the Java API using onPreReceive/onPostReceive for now, because to do mapBehavior in Java without creating an extra indirection by default will require the conversion between japi.PartialProcedure and scala.PartialFunction to avoid "no-op nesting" (i.e. we want `pf.asJava.asScala eq pf`). I think that's probably possible, I'll try to code it if we like this approach, but let me know. 
I moved a rebased/squashed version over to https:github.com/akka/akka/pull/467 so I'll close this one. (wasn't sure github would do something sane if I pushed a non-fast-forward here)  Roland if you and Viktor agree on a name I'm happy to search-and-replace ;-) I'm not 100% happy with any of the names so far but I think they are all at least "OK"  The Java API patch on https:github.com/akka/akka/pull/467 may need some work or maybe you don't even want to do it, let me know your thoughts over there.
Hi Havoc,  I'd really like your input on what I've done for: https:github.com/akka/akka/pull/456  I've essentially pulled out the behaviorStack out of Actor and placed in ActorCell, for easier binary compat in the future.
behaviorStack seems to logically go with ActorCell to me, it looks good. It sharpens up the line where application code is in Actor and akka implementation is in ActorCell. It was weird before that the stack was in Actor but become() was in the cell.  
Thanks Havoc, credit goes to my team for suggesting this excellent refactor
Also nicely solves the issue of using local vars to create the receive
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/262/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/262/
Can you describe a bit more about the problem? Is it because RemoteActorRefProvider is created via reflection from the akka-actor bundle? If that is the problem, what about other things that are created in the same way, such as event-handlers, mailbox-type, serializers, extensions?
hello Patrick,  Thanks for your interest  You described exactly what I'm thinking: the akka-actor bundle does not import any akka package, thus, it may not import other ActorRefProviders. I didn't check other things but it should be possible to let every thing be loaded importing optionally akka.*   No ticket had been created until today, only a discussion on https:groups.google.com/forum/?fromgroups=#!searchin/akka-dev/osgi/akka-dev/onGIJXI3u3A/3ufd4G97HEYJ. New ticket #2864 (tested with akka-2.1.0)
Im not an OSGi guru: will marking akka.* optionally imported be resolved for each requested class name, or will it be content with importing just the contents of akka-actor.jar? Or is this resolved only once the complete bundle set is known, so that e.g. mailbox types in different packages (e.g. akka.contrib) are also found?
Thanks for that point, Roland. After some discussion with my team, and from what I know, the akka.* would be resolve only for Class that are not present in the akka-actor bundle. For others, the akka.* will let OSGi lookup in every bundle exporting akka packages to find the requested class. I've just learned that using the optional resolution with akka.* has the same impact as a dynamic import, which is not great practice as it may lead to bad dependency resolution if two bundle are exporting akka packages differently. The problem comes from On the other hand, listing every akka modules in a classic import statement (such as akka-remote, akka-cluster and so on) could drive to ClassNotDefException if a package is missing.  I would prefer the akka.*(even in DynamicImport) because people are  responsible for exporting akka packages outside of the akka project.  Those "difficulties" come from the OSGi integration and moreover the fact that OSGi does already akka stuff such as the class selection through configuration (as Class.forName) in akka.actor. I let you two links about OSGi that our OSGi guru sent me: http:wiki.osgi.org/wiki/Avoid_Classloader_Hacks http:iocanel.blogspot.ch/2012/06/osgification-good-bad-purist.html I know OSGi may not be a priority and refactoring akka in an OSGi version would be painful but it may be interesting to discuss it.
Couldn't you just write an OSGI-based DynamicAccess implementation? http:doc.akka.io/api/akka/2.1.0/#akka.actor.DynamicAccess
Also, from the Avoid Classloader Hacks link, it recommends "Allow clients to pass a ClassLoader that the framework should use when it performs its lookup by name." which we do, since you can pass in a CL into the ActorSystem constructor, which is used throughout Akka for all DynamicAccess (reflection in default impl).
I don't fully understand this, but what about classes that are located outside of the akka.* package and hat are created in the same way? E.g. event-handlers, mailbox-type, serializers, extensions.
@patriknw The same ClassLoader is used for all reflective loading inside Akka, so I assume that as long as you pass in a ClassLoader which can see the classes then it's all good.
ok, but why is then akka.remote and akka.cluster special? why is the optional imports needed, if the are not needed for other packages?
My guess is that the ClassLoader doesn't have access to loading it otherwise?
@viktorklang I'm not akka expert but this was what I was thinking speaking about "OSGi version" of akka. I think this would be enough. I agree with you on the ClassLoader which does not have access on classes that are not imported.   obvious: the optional resolution or dynamic import instead of simple import only avoid to need a bundle even if you don't need it.
Feel free to create such a DynamicAccess and propose it as a PR.
So, is the conclusion that this should be solved by an OsgiActorSystem which features an OSGi-aware DynamicAccess implementation?
Or simply providing an initial ClassLoader that works
While discussing again before diving into designing some new DynamicAccess, we tested correctly (at least in our use case), the Dynamic-Import strategy (adding akka.* as dynamic import).  Moreover, we thought having two different bundles : akka-actor and akka-osgi may be not useful and that they could be merged to one bundle (only in BND). I'm currently testing it.  
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/291/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/291/
What is the reason for copying `reference.conf` to akka-osgi?
because, from what I've see, resources may be exported from other bundle only if they are in exported package, not on the bundle root and the main akka bundle which creates the actor system needs to import them. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/322/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/322/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/331/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/331/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/331/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/331/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/350/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/350/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/261/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/261/
Hello again Patrick  Really good points and thanks for your interest.   ticket created at #2865 
 concerning the name extendedActorSystemConfig, it could also be replace the existing actorSystemConfig as it is used only at bundle start and is transparent enough (from my point of view). Otherwise, I will think about some better name.
concerning the properties and json point, it's really interesting. I'm going to fix it to let each type of configuration get read. By the way, as the configuration file name is explicit enough, I don't think there may be a mix with another application configuration file, even with those extensions
Thanks for the String interpolation idea, I'm going to use it. (and sorry for the bundle typo, you're right)
regarding extendedActorSystemConfig vs. actorSystemConfig; if the etc directory is an expected location for the configuration files I think it should be folded into the default behavior in actorSystemConfig  this is only called once on startup, right?
yes, it is the expected location for the configurations files and it is called at each bundle.start (as it is called to create the actor system, which is done in the bundle Activator.start method.  Another possibility would also be to modify the BundleDelegatingClassLoader to let it find the relevant resources (just an idea...). This could be nicer 
aside from comments: LGTM
but before we can merge it, youll have to sign the CLA
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/270/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/270/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/273/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/273/
looking good, but before I can press the button you need to sign the [CLA](http:www.typesafe.com/contribute/cla)
done, thanks for that point.
Based on the same idea, but really more sexy at least in my mind, the possibility to create ActorSystem when the corresponding configuration file is provided. OSGi (in its compendium) listen to the configurations files and provide those a services. From this point, akka-osgi would provide an ActorSystem with the right configuration as a service and the application would just listen to the right ActorSystem. This would be more usable and more simple for the user, akka-osgi from this point may just be passive, providing ActorSystems and application bundles would be simpler. a schema is comming
cons:  - OSGi listen to configuration files, those have same synthax as .properties files (but we won't have .conf or .json files in this case) - properties updates are sent to the bundle -> need to manage those properties updates pros:  - more OSGi style - akka may just let the users creates and manage its actorsystem with configuration  - more OSGi Framework will use it than the etc directory (just in OSGi compendium)  so what do you think about that?  based on this: http:felix.apache.org/site/apache-felix-file-install.html a schema to explain what would it be ![configadminservice](https:f.cloud.github.com/assets/1533315/77751/cbba361a-6157-11e2-885e-4c52890f2cfa.png) 
better idea, after discussion with my team leader: let the application bundle take care of the configuration modification (but defined in the akka-osgi module) to handle bundle restart in case of configuration modification (with preRestart hook, for example) ![osgi-service-based](https:f.cloud.github.com/assets/1533315/78471/e5f53b44-6182-11e2-8f97-e5b1d4c86725.jpg) 
in the previous diagram: - application bundle provides one service listener that gets configuration updates and call a service provided by akka-osgi - akka-osgi provide a service that construct an ActorSystem depending on given properties  From this point, the user application may or not restart an actorsystem on configuration changes. Most of the code would be inherited from akka-osgi. 
Thanks Viktor! For the past week or so I've been reviewing all of akka-actor. It has been a pleasure to read the code base. Very well done, and I'm learning a ton of great Scala idioms. :-)   On Sun, Dec 23, 2012 at 1:15 PM, Viktor Klang () <notifications@github.com>wrote:  > Awesomely done! > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/983#issuecomment-11650661>. > >
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/260/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/260/
`BoundedBlockingQueue` is used in `BoundedPriorityMailbox`. `ReentrantReadWriteLock` would be an advantage if there are many many concurrent reader threads. Is that the case here?
The only method which takes the readLock which we use is `isEmpty`, but that is called for every message sent, so it might well be a gain.
ok, but don't you think we should benchmark it to make sure that it's not a perf regression for 1 sender - 1 receiver?
Yes, that would be nice indeed, but I dont think we have time to do it now. Ideally wed have a mailbox test harness which runs different scenarios and shows how a mailbox behaves   * without contention  * with contention  * with backlog / at maximum capacity  but this entails more than just measuring the target actors throughput, it also includes the time it takes to enqueue things.
Also, in the case of a priority queue the big hits will be A) the blocking part and b) the priority part
I agree that a thorough performance tests takes long time, and is wrong priority, but a quick sanity check might be appropriate.  I have run a modified variant of the ordinary `TellThroughputPerformanceSpec` with a `BoundedPriorityMailbox`. Here is the result of 4 test runs, with 4 actor-pairs. Each run consist of 45000000 messages, and takes ~ 14 sec.  Before: 7012058 msg/s 7200094 msg/s 7079586 msg/s 6908532 msg/s  After 6868004 msg/s 6522654 msg/s 6461820 msg/s 6513178 msg/s  Not a huge degradation, and might be worth it if you think it improves scalability for many senders.
Nice that you checked it!
Great, thanks Patrik! Now we need some many-senders benchmark  (otherwise its just a 510% loss)
Thanks for benchmarking the change, Patrik. Please let me know if there's some way that I can assist. I am still trying to become more familiar with the testing side of the Akka code base.    On Wed, Jan 2, 2013 at 3:11 AM, Roland Kuhn <notifications@github.com>wrote:  > Great, thanks Patrik! Now we need some many-senders benchmark  (otherwise > its just a 510% loss) > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/983#issuecomment-11805129>. > >
To complete the sanity check I adjusted the benchmark and added a "fan out", via a RoundRobinRouter.  The idea is that the routees will generate contention in the mailbox of the single Destination.  Results, roundrips/s (each roundtrip generates 3 message sends)  	routees	before	after 	1:		998198	899700 	2:		933109	906934 	4:		939937	782490 	8:		875773	808520 	16:		867681	750070 	32:		879429	766749   As a reference, the same test, but with ordinary mailbox: 1707265 roundtrips/s with 1 routeee, and 1270408 roundtrips/s with 32 routeees
Okay, Patriks measurements bring me to the conclusion that the RWLock does not help at all, it just makes performance worse. So the correct action is to just remove that TODO.  My guess for why this is: we only ever call hasMessages after enqueueing, which means that the ReadLock part does not actually help all that much because all other readers will also be writers. And then the intrinsic overhead of the RWLock makes it more costly than the plain Lock.  @viktorklang, do you agree?
If we don't need reentrancy, perhaps take this for a spin: http:gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/jsr166e/StampedLock.html
due to the reasons outlined above Im a little skeptical; but if someone wants to give it a try 
BoundedBlockingQueue is way more general than just for mailboxes, it's essentially a missing puzzle piece in the JDK
okay, thanks for reminding me of the bigger picture (that had slipped my mind)
Stepping back a bit, it looks like BoundedBlockingQueue is currently only used to wrap a java.util.PriorityQueue in BoundedPriorityMailbox. I understand that BoundedBlockingQueue is meant to be a generic wrapper, but why couldn't we use a java.util.concurrent.PriorityBlockingQueue instead of the current approach?  http:docs.oracle.com/javase/7/docs/api/java/util/concurrent/PriorityBlockingQueue.html     On Thu, Jan 3, 2013 at 6:58 AM, Roland Kuhn <notifications@github.com>wrote:  > okay, thanks for reminding me of the bigger picture (that had slipped my > mind) > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/983#issuecomment-11846229>. > >
Ah, forget what I wrote in my last mail. I realized that PriorityBlockingQueue is missing the "bounded capacity" option like we have with java.util.concurrent.LinkedBlockingQueue. Bummer.   On Thu, Jan 3, 2013 at 12:02 PM, Ryan LeCompte <lecompte@gmail.com> wrote:  > Stepping back a bit, it looks like BoundedBlockingQueue is currently only > used to wrap a java.util.PriorityQueue in BoundedPriorityMailbox. I > understand that BoundedBlockingQueue is meant to be a generic wrapper, but > why couldn't we use a java.util.concurrent.PriorityBlockingQueue instead of > the current approach? > > > http:docs.oracle.com/javase/7/docs/api/java/util/concurrent/PriorityBlockingQueue.html > > > > > On Thu, Jan 3, 2013 at 6:58 AM, Roland Kuhn <notifications@github.com>wrote: > >> okay, thanks for reminding me of the bigger picture (that had slipped my >> mind) >> >>  >> Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/983#issuecomment-11846229>. >> >> > >
closing this pull request: if something nicer comes along it can reference this discussion, but it should be a new PR
Makes sense. Thanks for reviewing guys.    On Jan 8, 2013, at 8:17 AM, Roland Kuhn <notifications@github.com> wrote:  > closing this pull request: if something nicer comes along it can reference this discussion, but it should be a new PR >  >  > Reply to this email directly or view it on GitHub. > 
Shouldn't that TODO be deleted then?
thanks for the reminder - fixed
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/259/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/259/
Since I assume no one will be reviewing this for like 10 days, I'll merge this in now.
I am back on 29-30 part-time, so proceed freely if you catch something.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/266/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/266/
I dont fully grasp it at this point, but I trust you guys. LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/263/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/263/
Good point Patrik! Will do
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/264/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/264/
The failure in the ticket https:www.assembla.com/spaces/akka/simple_planner#/ticket:2801  "0 was not greater than 0"  https:github.com/akka/akka/blob/0d185e297d3cf198937142d644801159504abfd9/akka-cluster/src/multi-jvm/scala/akka/cluster/routing/ClusterRoundRobinRoutedActorSpec.scala#L191  How can that ever be possible? The replies are gathered here: https:github.com/akka/akka/blob/0d185e297d3cf198937142d644801159504abfd9/akka-cluster/src/multi-jvm/scala/akka/cluster/routing/ClusterRoundRobinRoutedActorSpec.scala#L96  How can that Map contain a value that is 0 Can you see anything wrong with that `receiveReplies` method?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/256/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/256/
The changes look good, but dont seem to be related to the problem. Could it not happen that a cluster node has joined and `awaitUp()` has returned but the router has not yet had the chance to add new routees? And then the `"hit"`s go to the old routees first and out of the 10 there are too few which go to the new node?
Previously I suspected something wrong with the `receiveReplies`, but I was wrong about that. Of course it can contain 0 values in the Map. It is as you say, the router picks up the new nodes later, and that was also the reason why I added the additional await on the number of routees in the test, before sending the messages.  So, I would say that this actually is related to the problem, and fixes it .
ah, okay, thanks for clarifying
so what was the underlying problem this patch solves? you said something about a corner case with two routees?
This is from the ticket:  So I can't explain excatly how the scheduling of the different threads handling the actors could behave in this way but by making the routees move a bit slower and the sender a bit faster, I can get the test to have a routerSize of 2 when the backoff starts (which is ok) and always have a pressure of at least 1 which means that the backoff will be 0.5 which is greater than the threshold 0.4.
yes, so we have some guesses but no clear description of why exactly it failed; if this test tests the function of resizers, is it that we dont understand their runtime behavior or is it that we are testing them in a domain where they are simply not fully deterministic?
LGTM, we give it one more chance and if it fails again we scrap it and improve unit tests instead, if they don't cover enough already
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/255/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/255/
@rkuhn so to try to answer your question I think that the domain (scheduling of multiple threads running multiple actors) is not fully deterministic, and you have to try to coax it into behaving the way you want, which makes the test unnecessary brittle. I'm all for rethinking this completely.  And now for something completely different :christmas_tree:
What's the status Sean?
Sorry, rather busy day at work yesterday  sRp On May 10, 2012 3:40 AM, "viktorklang" < reply@reply.github.com> wrote: > > What's the status Sean? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/440#issuecomment-5620569
No worries, let me know when it's ready for prime-time and the CLA is signed so I can move it into master.  Great job!
Signed cla yesterday On May 10, 2012 6:08 AM, "viktorklang" < reply@reply.github.com> wrote:  > No worries, let me know when it's ready for prime-time and the CLA is > signed so I can move it into master. > > Great job! > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/440#issuecomment-5622570 >
Alright, I'm merging in this now.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/250/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/250/
Reliable-cough-Proxy spec... ;-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/251/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/251/
yes, this can well go into 2.1.x
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/249/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/249/
Aside from comment: LGTM!
:+1: After the fact
Have you checked that the strings for the docs are ok? I mean the `preprocessVars`?  And I assume that still using the sonatype resolver is ok for the build anyway.  Otherwise LGTM 
there is a CrossVersion in multi-jvm-testing.rst, checked that?
ouch, that should be a preprocessor thing, will fix
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/254/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/254/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/253/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/253/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/258/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/258/
Great job Patrik!
Good fix and boy scouting. Thank you. 
Found and fixed regression.
Great. Push it. I get the same error in my test now: "Can't find this Member"
LGTM after some optimization
Updated according to review comments.
Umm, looking again, I should probably add a comment why the ordering is important.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/274/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/274/
Added comment describing why the specific ordering is needed, and squashed commits.
PLS REBUILD ALL
@rkuhn Can you look at this one before I merge?
Updated: added scope queries to Address
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/279/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/279/
CallingThreadDispatcherModelSpec failed. I added the failure to #2821.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/277/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/277/
Weren't there a third one using blackhole?
that was perhaps ReliableProxySpec, but that is enabled
Lets see what the KITTEH says ;-)  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/278/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/278/
Let's see what the build servers says :-)
LGTM. I will add the inbound flushing.
I changed the scheduling in the throttler as discussed, please review, I might have misunderstood something
yes, your changes look like what I had in mind; the two comments are more to the pre-existing state of the code, @drewhk can you comment?
I had no time to review this fully, I will continue tomorrow.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/267/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/267/
Alright, I have made the suggested adjustments and will merge after PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/276/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/276/
Closing for now, as setIssueHandshake() does not solve the issue directly.
Checked the netty code, and setIssueHandshake should do the trick:          if (issueHandshake) {             handshake().addListener(new ChannelFutureListener() {                 public void operationComplete(ChannelFuture future) throws Exception {                     if (!future.isSuccess()) {                         Channels.fireExceptionCaught(future.getChannel(), future.getCause());                     } else {                         ctx.sendUpstream(e);                     }                  }             });  Writes should be buffered still, but that is handled by the remoting code. We should give it a try I think.
No, you can't assume that !isSuccess == failure, you're forgetting cancellation
Hmm, it's their code, so I can't fix it... Then I have to do this manually.
If they don't cancel the future, can isCancelled be true?
How do you know if they're ever going to cancel it?
Then it's their bug :) The code I pasted is from SslHandler.
apart from the style comments, LGTM
LGTM, in the @patriknw sense
LGTM  PLS REBUILD ALL
Is the kitteh dead or something?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/269/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/269/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/272/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/272/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/275/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/275/
great, apart from the leader check, but that assert is perhaps not needed by this test (covered by others)
Good. Feel free to merge this one and my other one if you feel they are done. 
great, I can merge them in when my build change has been merged.
Trying to fix the bug during second construction, in doRecreate() if it catches an exception ("case NonFatal(e)"), it uses the `actor` field. But `actor` could be either the failed actor, if the exception is from newActor(), or the new actor, if the exception is from freshActor.postRestart(). So I think it's sort of undefined whether `actor.supervisorStrategy.handleSupervisorFailing(self, children)` for example is called on the old or new actor. Is that a problem?
That's a good point, I'll need to investigate
I did a fair bit of commenting/rearranging the code to try to understand it: https:gist.github.com/2839564 That patch isn't perfect but it makes the state machine and the possible status of `actor` and `behaviorStack` a little more explicit, I think.  Some conclusions:   - I think supervisorStrategy should always be used from the last actor to be successfully constructed. I put a test case in the above patch that tests this, and the patch fixes it.  - for a given `instance` of Actor, I don't think `instance.receive` or become() behaviors originating from `instance` should ever be in the behaviorStack once a recreate or terminate has been received while `actor eq instance`. If `aroundReceive` is added, then the same applies to `instance.aroundReceive`. Also none of these methods on `instance` should be called until `instance` has been constructed.  - there's some code unification possible between create, recreate, and terminate handling. I did some of it in the above patch, there is more possible though probably.  - I believe null checks are pretty unavoidable without a larger overhaul that would keep the `actor` field from ever being null (even pre-create).  - In the patch, I made `actor` null _more_ often so that the recreate case matches the create case; in the patch it's always null when a replacement of the `actor` field is conceptually in progress (pending create, recreate, or terminate). This would then allow `become()` in the `aroundReceive` patch to reliably know whether to use `actor` or not. Also, it makes it harder to accidentally use the "wrong" instance (one that's already been terminated or recreated).  To avoid `actor eq null` I believe you'd have to do something more radical. Could just use Option, if willing to accept its memory overhead, but that's just a safer syntax for null. But to avoid the conceptual Option/null, you'd probably need some kind of "state" object which would virtualize those behaviors that change in the various conceptual states of the ActorCell. For example you might have PreCreateState with a become() method that does one thing, and NormalState with a different become(), and then context.become() would delegate to state.become(), that kind of idea. You'd probably virtualize some of the system messages such as terminate and recreate to delegate to the state. Only NormalState would have an Actor instance, while for example a DeadState might have a supervisorStrategy left over from a former Actor instance, and PreCreateState would have neither actor nor supervisorStrategy. This could be clean and "prove" that actor and supervisorStrategy aren't used incorrectly, but it adds another object and thus some overhead.  Or who knows, maybe there's another solution.  Anyway...  `aroundReceive` is pretty orthogonal to all this.  Given the current state of ActorCell, the null check is IMO the right way to write the become() method, and I don't think there's anything conceptual about `aroundReceive` that's creating a problem ActorCell doesn't already have.  So after a lot of messing around: what I'd suggest is to merge `aroundReceive` with the null check in become() as it is.  And then separately undertake an effort to clarify when `actor` is null if ever, and over what timeline each Actor instance exists and gets used... I can continue to hack on that problem too if you want, but I suspect you'll care so much about it you'd be better off just coding it yourself, but whatever is most helpful ;-)  Whether you like https:gist.github.com/2839564 overall or not I think some of the test cases in there might be useful. 
Nice, I'll go through the gist with Roland on monday/tuesday and we'll get back to you asap
I think this looks very good. I will try the graphviz generation today, and see how the docs looks like when rendered. @viktorklang might want to take a final look. Then I think it's ready to go in. Awesome work, thanks a lot.
graphviz in the docs works fine for me on mac, I have dot installed in path (/usr/local/bin/dot) nice!
the java code sample should be formatted, it contains tabs, should be spaces only, braces in "wrong" location
Not done yet - half-open test is failing, rest are passing.  Need to understand why, but is this the kind of thing you guys had in mind?
This rebase+squash seemed to go better.  I did a push -f, which should be OK as long as nobody else has done work against the branch.
Noticed that I had stomped on some epub settings in conf.py, so undid that.
Looks great. +1 from me.  cc: @viktorklang 
Looks good!  Signed CLA over at: www.typesafe.com/contribute/cla ? (I have goldfish memory on fridays)
@viktorklang - yep CLA signed
Woot! :)  Thanks again for all the mentoring both @viktorklang and @patriknw.  I learned a lot about Akka internals and Scala in general.  Will take a look at assembla and see where else I can help out!  Definitely owe you guys for developing and maintaining such a useful project.
You're most welcome! Thanks for your excellent work!
You are welcome. It was a pleasure. Thanks!  1 jun 2012 kl. 17:31 skrev scullxbones<reply@reply.github.com>:  > Woot! :) >  > Thanks again for all the mentoring both @viktorklang and @patriknw.  I learned a lot about Akka internals and Scala in general.  Will take a look at assembla and see where else I can help out!  Definitely owe you guys for developing and maintaining such a useful project. >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/493#issuecomment-6063231
Awesome. Very nice. Thanks. 
changed so that tags.exclude adds to default excludes, not overrides
fixed everything from feedback
Very cool stuff, really looking forward to get this into the feature-set of the Netty remoting!
Thanks patriknw and viktorklang for your comments, please review again whenever possible.  I had to merge with changes that were made on upstream and therefore re-factored the code.
Thanks! I'll have a look as soon as I can!
Very cool stuff, just a tiny bit of work left before we can add this!
On 2012/05/30 04:30 PM, viktorklang wrote: > Very cool stuff, just a tiny bit of work left before we can add this! > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6011240 I have read about the SecureRandom problem in #2153, should I be using ThreadLocalRandom?
Good that you noticed that. YES! SecureRandom is banned.
We probably need to know how this affects the security of the TLS/SSL given that TLR is not a cryptographically secure random?  Also, since the API requires an instance you'll probably need to create a wrapper that delegates to TLR.current
On 2012/05/31 11:01 AM, patriknw wrote: > Good that you noticed that. YES! SecureRandom is banned. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6030984 What do I do for Java 6 since it is only in Java 7
You do the same for both: akka.jsr166y.ThreadLocalRandom
we have it embedded `akka.jsr166y.ThreadLocalRandom` wrapper is important as V mentions
On 2012/05/31 11:08 AM, patriknw wrote: > we have it embedded `akka.jsr166y.ThreadLocalRandom` > wrapper is important as V mentions > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6031131 perfect, thanks a lot
On 2012/05/31 11:04 AM, viktorklang wrote: > We probably need to know how this affects the security of the TLS/SSL given that TLR is not a cryptographically secure random? > > Also, since the API requires an instance you'll probably need to create a wrapper that delegates to TLR.current > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6031050 I have used the code from this page: http:www.alife.co.uk/nonrandom/ to test 'ThreadLocalRandom' and 'Random' provided by Java 7 and it isn't random. SecureRandom is visibly random.
So you might want to double-check that the SecureRandom won't be used by multiple threads
On 2012/05/31 01:39 PM, viktorklang wrote: > So you might want to double-check that the SecureRandom won't be used by > multiple threads > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6033635 The code doesn't share SecureRandom. It creates and passes it immediately to sslContext.init.
So you guarantee that the SecureRandom isn't indirectly called by multiple threads?
The problem is not multiple threads. It is how it is implemented on linux. See here http:www.assembla.com/spaces/akka/tickets/2153-cluster-usage-of-securerandom-is-slow?comment=111923933#comment:111923933  @jboner can explain more if needed. There might be different SecureRandom implementations. The problematic one was:      SecureRandom.getInstance("SHA1PRNG")  Needs to be investigated and tested thoroughly on linux before usage.
Yeah. SecureRandom (and Random) is fully thread-safe. Wrapping it in a ThreadLocal is only to decrease contention.  The problem is that it blocks on /dev/random on Linux when its entropy pool is empty.  If this is not a bug then I would at least call it extremely sloppy design/implementation in SecureRandom. Why not asynchronously periodically fetch chunks from /dev/random. I don't know what is best to do. 
There's some nice reading here: http:stackoverflow.com/questions/137212/how-to-solve-performance-problem-with-java-securerandom
On 2012/05/31 03:15 PM, viktorklang wrote: > There's some nice reading here: http:stackoverflow.com/questions/137212/how-to-solve-performance-problem-with-java-securerandom > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6035419 that is what I have been reading, thanks
On 2012/05/31 03:04 PM, patriknw wrote: > The problem is not multiple threads. It is how it is implemented on linux. See here http:www.assembla.com/spaces/akka/tickets/2153-cluster-usage-of-securerandom-is-slow?comment=111923933#comment:111923933 > > @jboner can explain more if needed. > There might be different SecureRandom implementations. The problematic one was: > >     SecureRandom.getInstance("SHA1PRNG") > > Needs to be investigated and tested thoroughly on linux before usage. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6035149 I am only using "new SecureRandom()"
On 2012/05/31 03:12 PM, Jonas Bonr wrote: > Yeah. SecureRandom (and Random) is fully thread-safe. Wrapping it in a ThreadLocal is only to decrease contention.  > The problem is that it blocks on /dev/random on Linux when its entropy pool is empty.  > If this is not a bug then I would at least call it extremely sloppy design/implementation in SecureRandom. Why not asynchronously periodically fetch chunks from /dev/random. > I don't know what is best to do. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6035348 If I understand correctly according to this: http:bugs.sun.com/view_bug.do;jsessionid=ff625daf459fdffffffffcd54f1c775299e0?bug_id=6202721 I will need to set -Djava.security.egd=file:/dev/./urandom to force it to be non-blocking on Linux
How do you know that it is good? See javadoc for that constructor. 
if -Djava.security.egd works we must make sure that it is used, so that users don't hit this issue in production (akka will be the first thing to blame)
At the very least the PRNG used needs to be configured in the reference.confuser conf
On 2012/05/31 03:24 PM, patriknw wrote: > How do you know that it is good? > See javadoc for that constructor. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6035659 According to the javadoc for the class:  A caller obtains a SecureRandom instance via the no-argument constructor or one of the <code>getInstance</code> methods: SecureRandom random = new SecureRandom();
On 2012/05/31 03:15 PM, viktorklang wrote: > There's some nice reading here: http:stackoverflow.com/questions/137212/how-to-solve-performance-problem-with-java-securerandom > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6035419 According to these pages: http:docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#Troubleshooting  Slowness of the First JSSE Access  Problem: JSSE seems to stall on the first access.  Cause: JSSE must have a secure source of random numbers. The initialization takes a while.  Solution: Provide an alternate generator of random numbers, or initialize ahead of time when the overhead won't be noticed: SecureRandom sr = new SecureRandom(); sr.nextInt(); SSLContext.init(..., ..., sr); The <java-home>/lib/security/java.security file also provides a way to specify the source of seed data for SecureRandom: see the file for more information.
So what's the status here, what do we do?
On 2012/06/04 11:47 PM, viktorklang wrote: > So what's the status here, what do we do? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6112037 I have completed the changes and will be submitting them later today (if time allows). There is an option now on 3 different random number generators with the default being the default internal Java one which is supposed to cause the least hassles.
Excellent, looking forward to see it!  Cheers, 
On 2012/06/05 01:09 PM, viktorklang wrote: > Excellent, looking forward to see it! > > Cheers, >  > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6122598 I have committed the changes for your review.
I'm missing .rst documentation. Great contribution. Will be very useful.
Aside from the missing functional test I think this is very good! The config change I proposed would be nice to have, meaning I could also add that later.
On 2012/06/07 01:28 PM, Roland Kuhn wrote: > Aside from the missing functional test I think this is very good! The config change I proposed would be nice to have, meaning I could also add that later. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6173733 Thanks
Very nice. :+1:
 On Fri, 15 Jun 2012 14:34:37 +0300, viktorklang wrote: > Excellently done! >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6354463  Thanks. It wouldn't be what it is without everyone's input. Thanks everyone!
Hey Peter,  We're having a lot of issue with the SSL tests. I have commented them out for now and wasted about a day trying to diagnose, we need to solve this.  akka-remote > test-only *Ticket1978*Spec [ERROR] [06/19/2012 10:19:32.647] [remote-sys-2] [ActorSystem(remote-sys)] RemoteServerError@akka:remote-sys@localhost:12346] Error[ javax.net.ssl.SSLException: Algorithm missing:   	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.changeReadCiphers(SSLEngineImpl.java:554) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:1051) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:845) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:721) 	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:607) 	at org.jboss.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:969) 	at org.jboss.netty.handler.ssl.SslHandler.decode(SslHandler.java:670) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:333) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:91) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:373) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:247) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35) 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 	at java.lang.Thread.run(Thread.java:680) Caused by: java.security.NoSuchAlgorithmException: Could not create cipher AES/128 	at com.sun.net.ssl.internal.ssl.CipherBox.<init>(CipherBox.java:99) 	at com.sun.net.ssl.internal.ssl.CipherBox.newCipherBox(CipherBox.java:119) 	at com.sun.net.ssl.internal.ssl.CipherSuite$BulkCipher.newCipher(CipherSuite.java:369) 	at com.sun.net.ssl.internal.ssl.Handshaker.newReadCipher(Handshaker.java:410) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.changeReadCiphers(SSLEngineImpl.java:550) 	... 17 more Caused by: java.security.InvalidKeyException: No installed provider supports this key: (null) 	at javax.crypto.Cipher.a(DashoA13*..) 	at javax.crypto.Cipher.init(DashoA13*..) 	at javax.crypto.Cipher.init(DashoA13*..) 	at com.sun.net.ssl.internal.ssl.CipherBox.<init>(CipherBox.java:88) 	... 21 more ] [ERROR] [06/19/2012 10:19:32.648] [Ticket1978CommunicationSpec-6] [ActorSystem(Ticket1978CommunicationSpec)] RemoteClientError@akka:remote-sys@localhost:12346: Error[javax.net.ssl.SSLException: Received fatal alert: internal_error] [WARN] [06/19/2012 10:19:32.657] [Ticket1978CommunicationSpec-akka.actor.default-dispatcher-3] [akka:remote-sys@localhost:12346/user/echo] received dead letter from Actor[akka:Ticket1978CommunicationSpec/system/testActor1]: ping [ERROR] [06/19/2012 10:19:32.657] [Ticket1978CommunicationSpec-5] [ActorSystem(Ticket1978CommunicationSpec)] RemoteClientError@akka:remote-sys@localhost:12346: Error[java.nio.channels.ClosedChannelException] [WARN] [06/19/2012 10:19:38.357] [remote-sys-akka.actor.default-dispatcher-2] [akka:remote-sys/deadLetters] received dead letter from Actor[akka:remote-sys/remote/Ticket1978CommunicationSpec@localhost:12345/user/looker/child]: postStop
On 2012/06/19 10:22 AM, viktorklang wrote: > Hey Peter, > > We're having a lot of issue with the SSL tests. > I have commented them out for now and wasted about a day trying to diagnose, we need to solve this. > > Hey Viktor  Odd that it doesn't support AES 128-bit encryption. I will see if I can replicate the issue.
Hey Peter,  No, that's not the issue. The test runs X times on my machine and then fails. I think I've nailed it in master though, I'm trying to detect if it can run the tests before trying to run them. The problem was that there was a new AkkaProvider instantiated every time, this has to be reused to avoid the issue. See master.  Cheers, 
Hmmm, tests still fail on our CI server. sigh.
It seems like this is the test that normally acts up: Ticket1978AES128CounterRNGSecureSpec
I'ev wasted 2 days on this now. We need to get this fixed or we have to rip the SSL out again.
Somehow this ends up on the server at the other end:  java.security.InvalidKeyException: No installed provider supports this key: (null) 	at javax.crypto.Cipher.a(DashoA13*..) 	at javax.crypto.Cipher.init(DashoA13*..) 	at javax.crypto.Cipher.init(DashoA13*..) 	at com.sun.net.ssl.internal.ssl.CipherBox.<init>(CipherBox.java:88) 	at com.sun.net.ssl.internal.ssl.CipherBox.newCipherBox(CipherBox.java:119) 	at com.sun.net.ssl.internal.ssl.CipherSuite$BulkCipher.newCipher(CipherSuite.java:369) 	at com.sun.net.ssl.internal.ssl.Handshaker.newReadCipher(Handshaker.java:410) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.changeReadCiphers(SSLEngineImpl.java:550) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:1051) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:845) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:721) 	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:607) 	at org.jboss.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:969) 	at org.jboss.netty.handler.ssl.SslHandler.decode(SslHandler.java:670) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:333) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:91) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:373) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:247) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35) 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 	at java.lang.Thread.run(Thread.java:680)
Could it hae something to do with the keystore?  https:www.servoy.com/forum/viewtopic.php?f=5&t=14332
Just got this at random:  java.security.InvalidKeyException: Secret key expected 	at com.apple.crypto.provider.HmacCore.engineInit(HmacCore.java:87) 	at com.apple.crypto.provider.HmacMD5.engineInit(HmacMD5.java:21) 	at javax.crypto.Mac.a(DashoA13*..) 	at javax.crypto.Mac.init(DashoA13*..) 	at com.sun.net.ssl.internal.ssl.MAC.<init>(MAC.java:94) 	at com.sun.net.ssl.internal.ssl.CipherSuite$MacAlg.newMac(CipherSuite.java:449) 	at com.sun.net.ssl.internal.ssl.Handshaker.newReadMAC(Handshaker.java:448) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.changeReadCiphers(SSLEngineImpl.java:551) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:1051) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:845) 	at com.sun.net.ssl.internal.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:721) 	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:607) 	at org.jboss.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:969) 	at org.jboss.netty.handler.ssl.SslHandler.decode(SslHandler.java:670) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:333) 	at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) 	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) 	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:91) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:373) 	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:247) 	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35) 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 	at java.lang.Thread.run(Thread.java:680)
Opened this: http:stackoverflow.com/questions/11115978/netty-sslhandler-headache
On 2012/06/20 01:52 AM, viktorklang wrote: > Could it hae something to do with the keystore? > > https:www.servoy.com/forum/viewtopic.php?f=5&t=14332 > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6441032 I have attached the working key stores as well as their checksums
On 2012/06/20 11:18 AM, viktorklang wrote: > Attached where? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6448629 It got lost. I have uploaded it to assembla as well.  Can you explain how you do the repeated tests so that I can try to replicate the problem. I have tried testing it on FreeBSD but I keep getting errors for all the tests saying:  Could not run test akka.remote.RemoteCommunicationSpec: org.jboss.netty.channel.ChannelException: Failed to bind to: localhost/127.0.0.1:12345 Caused by: java.net.BindException: Address already in use
Restart SBT to release the socket.  run on this branch: https:github.com/akka/akka/tree/wip-ssl-unbroken-%E2%88%9A  test-only *Ticket1978*Spec
 On Wed, 20 Jun 2012 11:56:40 +0200, viktorklang wrote: > Restart SBT to release the socket. >  > run on this branch: https:github.com/akka/akka/tree/wip-ssl-unbroken-%E2%88%9A >  > test-only *Ticket1978*Spec >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6449298  tested this 25+ times without issue on: win7 64 and freebsd 9.0 amd64  Both have the unlimited strength policy files installed. Will try windows with strong policy files instead
Try without the strong policy files. That needs to not blow up as well, see my modifications to the test.
I get it to fail on Linux and OSX
What I don't see is where it issues the SSL handshake since handler.setIssueHandshake(true) hasn't been set and handshake() isn't called in the code.  "You must make sure not to write a message while the handshake is in progress unless you are renegotiating. You will be notified by the ChannelFuture which is returned by the handshake() method when the handshake process succeeds or fails." - http:netty.io/docs/stable/api/  I have tried handler.setIssueHandshake(true) on server and/or client but it doesn't work either.
 On Wed, 20 Jun 2012 13:53:54 +0200, viktorklang wrote: > Try without the strong policy fiels. That needs to not blow up as well, see my modifications to the test. >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6451836  Your modifications work well. Tested 20x with default 'export' policy files on windows and freebsd without fail. The 3 tests are skipped with the weaker policy files
I think I've nailed down the bug. See my recent commits to that branch. Waiting for our CI to give me an OK/KO now
Alright, I think we've got a winner: https:github.com/akka/akka/pull/554/files
 On Wed, 20 Jun 2012 15:46:09 +0200, viktorklang wrote: > Alright, I think we've got a winner: https:github.com/akka/akka/pull/554/files >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476#issuecomment-6454166  Good news, getting it now
This is scheduled for 2.0.2, and I need to cut the RC, is this ready?
Nope, writing as fast as I can... Can't the doc go in later?
Now I'm done... Somebody read through it...
Not done fixing two issues...
sorry for that
I got the hard exit shutdown to work, with barriers in right place, see HardExitLeaderElectionSpec. I think that would replace the previous LeaderElectionSpec, or do you think we need both?
I don't think we need both.  cluster.shutdown is just a cheat anyway, will be removed (think there is a ticket about that already)
I think it looks great. Like the fact that we test randomness of node addresses and cluster roles.
Very nice test. I love how these multi-node tests becomes much simpler and more readable than the in-memory counterparts. Also that a language and set of patterns for how to write these is emerging. Good job Patrik. 
Thanks, I agree, it's very nice to write tests with this framework.  /Patrik  28 maj 2012 kl. 12:39 skrev Jonas Bonr<reply@reply.github.com>:  > Very nice test. I love how these multi-node tests becomes much simpler and more readable than the in-memory counterparts. Also that a language and set of patterns for how to write these is emerging. Good job Patrik. >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/483#issuecomment-5962813
Hi Patrik, thanks for the review, point taken on all notes - I've updated the pull request.
ping? (which means: if nobody says anything, Ill close this pull req due to inactivity next weekend)
Ping. Last chance. 
Sold to the gentleman with the furry hamster.
Hi guys, no idea how I missed out the pings in this issue (heh now i get it, 8days ago - paris, 3 days ago prague) :-\ Anyway, too bad - especially since a very similar pattern enteres the summer of blog :-)  Well, my fault for not noticing the pings - sorry.  Cheers!
@avrecko Are you closing this one and opening a new one with your rewrites? 
I don't like the name of branch wip-rcl-preview-1. Is a pretty crappy name for branch. It is a mistake to name it like this in the first place. It is intended just to host my "clean" changes i.e. no other code changes for version 1.  I've done the work in wip-rcl-preview-2 branch thinking it should work fine with pull request. I am mistaken.  The preview-2 was synced with master. I've merged it back to wip-rcl-preview-1 but the results are what you see here.  Should have rebased. But still it would not be that clean i.e. only my changes visible.  I see you can rename the pull request but can't point to new branch.  I can make a new "clean" pull request. Alternatively you can see the refacture here with the commit Remote Class Loading with SEDA style netty onward.  Tell me what do you prefer?  For the submodule. I've done it like this  akka-remote akka-remote-tests akka-remote-rcl akka-remote-rcl-tests  Is this ok? Or should it be in the subdirectory? What confuses me is that akka-remote contains code. I don't think  akka-remote/akka-remote-rcl is that good idea. Imho the project either contains code or other projects but not both.  What would work is  akka-remoting/akka-remote akka-remoting/akka-remote-tests akka-remoting/akka-remote-rcl akka-remoting/akka-remote-rcl-tests  Thoughts?
You don't need to have a separate project for the tests (akka-remote-rcl-tests) if you don't have a good reason for it. For akka-actor-tests and akka-remote-tests we have reasons for doing that (testing framework is dogfooding)
Correction: You should not have a separate...
Yeah. It should just be:      akka-remote    akka-remote-rcl
Thanks for the feedback guys. Sorry it took me so long. Been super tired and super busy.  The branch name really bothers me, so I opened a new pull request with included feedback from this thread.
Hmmm, I shouldn't forget to merge this into master as well if this is accepted
Yes, and when you do youll find a JavaPartialFunction which might want it, too. :+1:
liek a bahs
Hey Bjrn,  sure thing, just sign this: http:typesafe.com/contribute/cla  Thanks! 
Excellent, thanks for your contribution!  
Applied the same update to Akka master
Interesting! Maybe I don't understand the background. The main purpose (according to its doc) "This object contains elements which make writing actors and related code more concise" Then the Inbox provides a powerful feature (use with care!) to `select` messages. The select feature is not provided for ordinary actors, is it? Isn't it strange to provide such a feature only in a DSL layer, and not in ordinary actors?
The `select` feature is in fact provided in the form of `Stash`, basically requiring only a default case which calls `stash()` in addition to the normal pattern match. This DSL was asked for by Martin, and I agree that every character saved is beneficial when trying things out in the REPL (e.g. during teaching). But lets discuss a bit more in detail when I get to the office.
I didn't know that select is provided by Stash. Is it documented? I can't see it. You mean that it can be easily implemented using Stash. Perhaps a candidate for documentation?  On Mon, Aug 13, 2012 at 9:18 AM, Roland Kuhn <notifications@github.com>wrote:  > The select feature is in fact provided in the form of Stash, basically > requiring only a default case which calls stash() in addition to the > normal pattern match. This DSL was asked for by Martin, and I agree that > every character saved is beneficial when trying things out in the REPL > (e.g. during teaching). But lets discuss a bit more in detail when I get > to the office. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/607#issuecomment-7684863>. > >
@patriknw Stash is documented, and I think specific documentation for converts from Scala actors should be given in their migration guide.
Since submitter didn't respond a cleanroom impl of the testcase was created.
Shall I merge now with the new testcase?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/828/
@viktorklang ok, added test also
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/828/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/827/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/827/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/825/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/825/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/819/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/819/
Woo hoo...  ``` [INFO] [04/07/2013 23:11:01.261] [akka-osgi-sample-akka.actor.default-dispatcher-9] [akka:akka-osgi-sample/user/$b] Created Hakker atakka:akka-osgi-sample/user/$b [INFO] [04/07/2013 23:11:01.262] [akka-osgi-sample-akka.actor.default-dispatcher-9] [akka:akka-osgi-sample/user/$b] Hakker (TestHakker) takes position(3) ---------------> TestHakker is busy with Thinking. Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.723 sec ``` 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/836/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/836/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/836/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/836/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/838/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/838/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/831/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/831/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/830/
Something like:    public Terminated expectTerminated(FiniteDuration max, ActorRef target, Boolean existenceConfirmed) {     return p.expectTerminated(             target,             max,             (existenceConfirmed == null) ? Option.none() : Option.some(existenceConfirmed));   }
why not `Option.apply(existenceConfirmed)`?
Lemme try :)
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/830/
yes, that is better :+1: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/832/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/832/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/832/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/832/
rebased on top of master
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/839/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/839/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Kitteh, you are no gud today.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/839/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/839/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
This one is still building. The Kitteh keeps picking up old runs when the commit date is in the past. We need to migrate to the new Kitteh.
Ok, I'll create a new PR
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/840/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/840/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Not related failure in CoronerSpec (ticket updated)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/837/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/837/
How about adding an automatic replace in the release script?
Hmm, but that does not match so perfectly, AFAICS: there is no change in scalaVersion coupled to the (temporary) change of Akka SNAPSHOT to release version. We want the right thing to be shown in the nightlies, dont we? So how would an automatism look like?
On Sun, Aug 12, 2012 at 11:38 AM, Roland Kuhn <notifications@github.com>wrote:  > Hmm, but that does not match so perfectly, AFAICS: there is no change in > scalaVersion coupled to the (temporary) change of Akka SNAPSHOT to release > version. We want the right thing to be shown in the nightlies, dont we? So > how would an automatism look like?   Vlugter said they're using a sphinx extension that does this.  Cheers,    >   > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/610#issuecomment-7674103>. > >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - The software stack for applications that scale  Twitter: @viktorklang
Wat iz happen here?
shall automatization be added in this pull req or is that a separate ticket (to be created)?
I suggest adding automation in this release (by the means of find-replace as per the github links in the release script), and then the ticket will be to replace that method with the sphpinx extension.
okay, added automatic scalaVersion pick-up
feels a bit scary to add this to public API, but I guess there is no other way, if we don't want to do more thorough work on the API for RemoteTransport & co +1
It's already public in 2.0.x
do know by chance whether this also affects TreeSet directly?
I don't know. TreeSet is default impl of SortedSet, so I guess so https:issues.scala-lang.org/browse/SI-5986  On Mon, Aug 13, 2012 at 12:27 PM, Roland Kuhn <notifications@github.com>wrote:  > do know by chance whether this also affects TreeSet directly? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/613#issuecomment-7688211>. > >
I'm on my way...  My connection is not helping me though :-(  ``` ceefour@annafi:~/git$ git clone git:github.com/akka/akka.git Cloning into 'akka'... remote: Counting objects: 137605, done. remote: Compressing objects: 100% (34461/34461), done. Receiving objects:  86% (119669/137605), 28.40 MiB | 4 KiB/s     ```  Need to be **very** patient =)) 
Stuck on remote place, I can't download anything... :(  ``` ceefour@annafi:~/git/akka$ sbt compile Getting org.scala-tools.sbt sbt_2.9.1 0.11.3 ... ```
So how can I know if I've fixed the problem?
``` ceefour@goku1:~/tmp/akka$ sbt compile Detected sbt version 0.11.3 Error occurred during initialization of VM Could not reserve enough space for object heap ceefour@goku1:~/tmp/akka$ free -m              total       used       free     shared    buffers     cached Mem:          1511       1435         76          0         33        531 -/+ buffers/cache:        870        640 Swap:          255         15        240 ```  How much RAM does this compilation needs ?  Any prebuilt snapshots ?
This is my sbt launcher:  #!/bin/sh if test -f ~/.sbtconfig; then   . ~/.sbtconfig fi exec java -XX:+CMSClassUnloadingEnabled -XX:ReservedCodeCacheSize=128M -Xss2M -Xms128M -Xmx3G -XX:MaxPermSize=512M -XX:+UseCompressedOops ${SBT_OPTS} -jar `dirname $0`/sbt-launch-0.11.3.jar -Dakka.parallelExecution=true "$@"   Andno, there is ny prebuilt jars since it is not merged into master yet because it isn't verified :-)  If you want I can build the jars locally, just you tell me what info you need to verify it.
Hi Viktor,  A proper OSGi bundle would contain the following in its `META-INF/MANIFEST.MF` :  ``` Bundle-ManifestVersion: 2 Bundle-SymbolicName:  Bundle-Version:  Export-Package: Import-Package: ``` 
Thanks for the launcher tip! I'm using the sbt package from typesafe Ubuntu repository, tweaking the RAM size in /usr/bin/sbt seem to get it going.. (its default is 1536 (MB), no wonder)
I've finished building, seems like this commit didn't fix it yet :( :  ``` ceefour@jannah1:~/tmp/akka$ git branch   master * wip-2370-  ceefour@jannah1:~/tmp/akka$ unzip -p /home/ceefour/tmp/akka/akka-osgi-aries/target/akka-osgi-aries-2.1-SNAPSHOT.jar META-INF/MANIFEST.MF Manifest-Version: 1.0 Implementation-Vendor: Typesafe Inc. Implementation-Title: akka-osgi-aries Implementation-Version: 2.1-SNAPSHOT Implementation-Vendor-Id: com.typesafe.akka Specification-Vendor: Typesafe Inc. Specification-Title: akka-osgi-aries Specification-Version: 2.1-SNAPSHOT ``` 
This is what I get from the akka-osgi-aries jar after running publish-local:   Manifest-Version: 1.0 Bnd-LastModified: 1344955807800 Bundle-ManifestVersion: 2 Bundle-Name: com.typesafe.akka.osgi.aries Bundle-SymbolicName: com.typesafe.akka.osgi.aries Bundle-Version: 2.1.0.SNAPSHOT Created-By: 1.6.0_33 (Apple Inc.) Import-Package: akka.actor;version="[2.1,2.2)",akka.osgi;version="[2.1,2  .2)",com.typesafe.config;version="[0.4.1,0.5)",org.apache.aries.bluepri  nt;version="[0.3,1)",org.apache.aries.blueprint.mutable;version="[0.3,1  )",org.apache.aries.blueprint.reflect;version="[0.3,1)",org.osgi.framew  ork;version="[1.5,2)",org.osgi.service.blueprint.container;version="[1.  0,2)",org.osgi.service.blueprint.reflect;version="[1.0,2)",org.w3c.dom,  scala;version="[2.10,2.11)",scala.collection;version="[2.10,2.11)",scal  a.collection.generic;version="[2.10,2.11)",scala.collection.immutable;v  ersion="[2.10,2.11)",scala.collection.mutable;version="[2.10,2.11)",sca  la.reflect;version="[2.10,2.11)",scala.runtime;version="[2.10,2.11)" Include-Resource: /Users/viktorklang/Documents/workspace/akka/akka/akka-  osgi-aries/src/main/resources,/Users/viktorklang/Documents/workspace/ak  ka/akka/akka-osgi-aries/target/resource_managed/main Private-Package: akka.osgi.aries.blueprint Tool: Bnd-1.50.0   Is that right?
And this is what is generated for akka-osgi:  Manifest-Version: 1.0 Bnd-LastModified: 1344955802512 Bundle-ManifestVersion: 2 Bundle-Name: com.typesafe.akka.osgi Bundle-SymbolicName: com.typesafe.akka.osgi Bundle-Version: 2.1.0.SNAPSHOT Created-By: 1.6.0_33 (Apple Inc.) Export-Package: akka.osgi;uses:="akka.actor,org.osgi.framework,scala.run  time,scala,scala.reflect,scala.collection,scala.collection.immutable,sc  ala.collection.mutable,com.typesafe.config";version="2.1.0.SNAPSHOT" Import-Package: akka.actor;version="[2.1,2.2)",com.typesafe.config;versi  on="[0.4.1,0.5)",org.osgi.framework;version="[1.5,2)",scala;version="[2  .10,2.11)",scala.collection;version="[2.10,2.11)",scala.collection.immu  table;version="[2.10,2.11)",scala.collection.mutable;version="[2.10,2.1  1)",scala.reflect;version="[2.10,2.11)",scala.runtime;version="[2.10,2.  11)" Include-Resource: /Users/viktorklang/Documents/workspace/akka/akka/akka-  osgi/src/main/resources,/Users/viktorklang/Documents/workspace/akka/akk  a/akka-osgi/target/resource_managed/main Private-Package: akka.osgi.impl Tool: Bnd-1.50.0
That looks good!  How come I don't get the same results ?
I wish I had that kind of super power to know what you're doing wrong :-)  Thanks for the confirmation, I'll merge and close this.
Thank you Viktor ! :)
You are most welcome, thanks for verifying the fix
Updated with better solution.
What do you say about scrapping all the query methods on Cluster and make the domain events the only user read API? We still need them for tests and JMX, but then we can place all that in a separate class (ClusterReadView), which can be private[akka]. This ClusterReadView doesn't even have to be active (subscribing) if jmx is disabled.
Perhaps. I like the idea. Can we get all info we need for the cluster admin tool, in terms of querying, from the event stream? All member-state changes yes, but what about 'has-convergence', 'is-available', 'is-singleton', etc. Or are these not needed you think? We still need the management API though. 
I move all existing methods to the new class, at least better separation of conserns
I already use the eventStream to update this state in Cluster so the change trivial
I trust your judgement. Do what you think is best. 
Can I merge this now because Helena is depending on this work and it would be easier to have it in master? I have done the metrics gossip based on this. I'll do the more fine grained domain events before closing the ticket.
Do it On Aug 16, 2012 6:37 PM, "Patrik Nordwall" <notifications@github.com> wrote:  > Can I merge this now because Helena is depending on this work and it would > be easier to have it in master? I have done the metrics gossip based on > this. > I'll do the more fine grained domain events before closing the ticket. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/617#issuecomment-7791171>. > >
ok, if there are any more comments I'll change afterwards
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/803/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/803/
Thanks for the fix, LGTM. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/806/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/806/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/805/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/805/
LGTM (apart from the small tiny things)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/805/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/805/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/808/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/808/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/823/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/823/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/807/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/807/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/811/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/811/
LGTM (but Id like to hear @drewhks opinion as well)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/810/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/810/
Apart from the missing code-block samples it LGTM. There should be one overall sample on `JavaTestKit` and one on each nested class; you can just copy the ones from the docs.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/810/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/810/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/812/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/812/
Given how small this feature is code-wise Im certain that were doing things right. Great work, Patrik!
Thanks for valuable feedback. I have added a commit with the improvements.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/817/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/817/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/821/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/821/
apart from the extension question: LGTM
yes, of course, I did that extension after this, and ... What do you say about the ticket? Can I use https:www.assembla.com/spaces/akka/tickets/1165
yes, that is the right ticket
strange, messed up after re-base, closing this and will add it in #1307 instead
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/813/
Looks awesome, Patrik.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/813/
Grymt bra jobbat! Excellent initiative, Patrik!
Thanks for valuable feedback. I have added a commit with the improvements.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/813/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/813/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/818/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/818/
LGTM - Merge it in. 
Thanks for all LGTM, but it is not ready yet. I will add documentation also. My plan is to complete that during the weekend. You know, this is a hobby initiative, which was not really in the roadmap. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/822/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/822/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/816/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/816/
I think this solution has a way too high cost vs benefit. We should return the Thread at the first convenient place to do so, as it should not add maintenance burden. Adding a systemAppend essentially invalidates the reason of the design of the system messages queue. 
I'll simplify it so we rethrow after processing all system messages. Gets rid of the append code.
Cleaned up the code.
Does that mean that this PR can be closed?
Yep, I'll close it.
Sorry for the delay, I only got around to this review now. Apart from the details commented on above it looks pretty good; the following result from further deliberations with the team:  - please rename the subproject to akka-incubator - the design of things in there should be: one file for the impl, one file for the test, so that anybody who wants to can simply copy a pattern into their project if they do not want to depend on experimental features - in this case I would collapse it into a TimerBasedThrottler class & companion, where the specific messages are in the latter  This might be the start of something very cool :-) (I plan on adding a few patterns in there, when time permits)
I think we decided to name it akka-experimental?
@patriknw Can you confirm that it is called akka-experimental? 
I will verify with the other guys. The naming convention that makes sense is akka-actor-experimental or akka-actor-pattern-experimental.  /Patrik  24 sep 2012 kl. 16:42 skrev hbf <notifications@github.com>:  > @patriknw Can you confirm that it is called akka-experimental? >  >  > Reply to this email directly or view it on GitHub. > 
just opened a pull req (#746) for the contrib area, please merge on top of that one (possibly waiting a few hours until it is merged into master)
Ok, @rkuhn, cool. Will do so.
@hbf in case you haven't noticed, the new contrib area has been merged to master. Use the new Reliable Proxy Pattern as template. http:doc.akka.io/docs/akka/snapshot/contrib/index.html
@patriknw Thanks, I actually did but have tons of work here. It's on my schedule for this weekend.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/27/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/27/
Sorry for the delay I submitted the code as a new pull request (https:github.com/akka/akka/pull/835).
:+1: I guess
@bjornharrtell, have you signed the CLA?
great, I created a ticket for this: https:www.assembla.com/spaces/akka/tickets/2412 we will merge when we have verified some more, thanks
This just ran the cluster tests without problems. Merging.
seems like you based your work off of the wrong commit: I have just updated the version to 2.0.4-SNAPSHOT, so you can rebase your commit on top of release-2.0. Then I'd probably just push -f to this pull request.
Hmm, strange.. don't know what happend. A rebase release-2.0 says patch-2 is up to date already.
Ah.. I must have accidentally branched from releasing-2.0.3. Will try to work out the git magic to fix, I need to learn how to do that... if I fail I'll just delete this branch/pull request and redo the oneliner change.
Should be ok now?
Thinking some more about the api usage. Wouldn't it be more convenient with      def getChild(name: String): Option[ActorRef]
or more scala style      def child(name: String): Option[ActorRef]
Unfortunately it would be more than just an optimization to push it into ChildrenContainer: getByName returns None if it finds a ChildNameReserved, but in this case we dont want to return `false`, because this method is meant to signify that creating such a child would succeed.  On the other hand, if were looking at the case where a child is externally created (which should currently not happen to normal user actors, but who knows), that might happen at any point in time, basically rendering a non-reserving method useless. But that would optimize for a case we currently don't allow.  I think the request which was behind the creation of this ticket is to use the ChildrenContainer as a local actor registry, hence I'd go with Patrik's suggestion for now.
WDYM more than just an optimization.  I assumed that since you can't create a child of another actor, then the actor itself wouldn't find a `ChildNameReserved` in its own `ChildrenCOntainer`, hence creating such a child would succeed.  The real question here is if we want something nice with an `Option` in the API that can't be optimized or something that can?
Looking at the use case I have in mind:      if (childExists(name)) context.actorFor(name) ! msg     else context.actorOf(Props(...), name) ! msg  so basically in the yes case it is quite likely that people will want to use the existing child for something. Classical Java solution would be to model no as `null`, but that is not nice. 
ok, then I'll go with `child(name: String): Option[ActorRef]` so you can do  ``` context.child(name).getOrElse(context.actorOf(Props(...), name) ! msg ``` 
please also add `getChild(name: String)` to UntypedActorContext. Do we want to use null there? I know of no precedent 
`null` is the classical java way will have to do for now...
fixes according to review
+1 apart from the scaladoc err
fixes according to review comments
should this possibly include a link to a Java SSL FAQ/Howto, saying that we only enable people to use it but didn't invent it and thus will probably not be able to help much with SSL specific questions?
Sounds like a good idea. Do you have any specific FAQ URL in mind, or should I just link to http:docs.oracle.com/javase/7/docs/technotes/guides/security/
linking to some Oracle docs is probably not the worst thing to do, could also more directly go to http:docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html
fixed link to Oracle doc
Excellent migration guide. Very good job Patrik. 
I did an additional diff of reference.conf files and only thing changed was log-remote-lifecycle-events, so I added a note about that also.
Please note that the akka jobs on jenkins.typesafe.com and scalable1 needs to be updated to use the 0.12 sbt launcher before merging this or things might break. 
Launcher has now been updated on jenkins and scalable1, so everything should work fine.
looks good to me, as a stepping stone towards the next Scala / Akka milestone
apart from that UnstartedTimeoutMs should be a Duration this looks good
LGTM: this puts the logic pieces again where they belong, I think
Good catch Patrik. Thanks. :+1:
I have not followed the problems so I don't have any valuable comments here.
The problem was that ChildTerminatedwhile enqueued firstmay be processed after Terminated, leading to a non-unique child name error. If the UID is not taken into account, this code might remove the wrong child, since Terminated as a normal message might be processed long after the child has been created anew.  Wrt. the check-then-act, I think that the performance gain is very probably not worth the spreading of UID knowledge into the ChildrenContainer. If this turns out to be wrong, we can still fix it later.
Sounds like a good idea to clarify that.
OK done, I thought it would continue on the previous warning block.. github rst rendering doesn't tell :/
alright, it looks like black magic, but if it solves the problem it solves the problem
you mean I should clarify more by which mechanism the barrier works?
So basically what you are saying is that by sending this message to the parent after the Supervise system message you guarantee that it will see the Supervise before processing any "real" messages from the child since we process all system messages after every normal message.  LGTM
@bantonsson yes, that is exactly what this is supposed to achieve
A little more description int the NullMessage comment wouldn't hurt.
yes, that would be good, "someone" might refactor away it otherwise :-)
awesome that you fixed it!!
thanks, but Jenkins has just been so kind to supply me with another one  aargh!
ok, if you say so
yeah, that code makes one a bit uneasy. but: LGTM
This is ready for final review. I will add rst docs also, if you think this is the right thing.
I reverted the RouterContext
I will add possibility to lookup routees also, not only remote deploy. We have that for ordinary routers.
Whenever you have some spare cycles this is ready for review.
I'll have a look asap, but most likely on monday. :_(
all review comments fixed
yes, good one
All comments addressed
Looks good, but one piece is missing. You should also update the migration guide!
Added migration guide
Look good, thanks Raymond!
Fixed as in review comments.
Closed, new PR will be opened for this
Looks  great!
you're too quick: SmallestMailboxRouter has the same problem
sorry for not verifying earlier
Perhaps time to make it DRY?
alright, I didn't look close enough on that one, thought it handled it already since it's using deadletters somehow I'll fix that also
Thanks Viktor. The commit is updated now.
LGTM, I'll let Roland give his verdict before merging.
Alright, so functionality-wise this one is a keeper, only one thing missing: updating the docs, can you do that as a part of this PR as well?
Yep, will update the PR later tonight.
Excellent! Thanks Gerolf!
My thought for the Cluster DeathWatch was to send the NodeUnreachable to the root-actor which would then send it to all its children, which would send it to all its children and so on. Node death should be infrequent enough for this not to become a problem. WDYT?
Yes, I thought about that, and that is absolutely an alternative. I see + and - with both. How many remote watchers do we expect? Many would result in many subscribers, which might slow down the event bus? Propagation through the tree would require check in each and every actor? Propagating through the three can result in stuck msg if an actor is doing long processing?   In that case it would be a system msg, I guess?  What do we expect if watch is added (immediately/long) after node unreachable? Should it receive Terminated?
Problem is that it needs to be idempotent, which essentially means that it needs to go through the RemoteActorRefProvider.... lets talk on monday.
Adjusted some things according to today's design discussions.   Left todo (known so far): * publish AddressTerminated via ClusterActorRefProvider, which will also   * mark connection as "unusable"   * close connection * verify/implement watch of already broken connection should generate Terminated (via deadLetter) * verify/implement watch of unknown path should generate Terminated (via deadLetter)
This is ready for final review.  As discussed over lunch I'l skip the "connection quarantine" part, since that is not implemented in current transport. Will probably be easier to support that in new actor based remoting. 
aside from comments: LGTM!
Shall I merge or will someone else review this?
I like it!
I was doing something like that in my own projects, although with the possibility to start up multiple quasi-main actors.
(kitty-note-to-self: ignore 17228909) :cat: Roger! Rebuilding pr-validator-per-commit for 651f699. :rotating_light: 
(kitty-note-to-self: ignore 17324202) :cat: Synchronaising! :pray:
The failures were very weird ones in cluster code, so Im merging this anyway.
I will probably wait for more eyes here
otherwise it LGTM
This seems merge-safe
(kitty-note-to-self: ignore 17215145) :cat: Synchronaising! :pray:
PLS REBUILD ALL
(kitty-note-to-self: ignore 17220466) :cat: Roger! Rebuilding pr-validator-per-commit for d34996ae. :rotating_light: 
(kitty-note-to-self: ignore 17324167) :cat: Synchronaising! :pray:
(kitty-note-to-self: ignore 17330050) :cat: Roger! Rebuilding pr-validator-per-commit for d34996a. :rotating_light: 
(kitty-note-to-self: ignore 17333103) :cat: Synchronaising! :pray:
So build 69 succeeded but since I used only the short sha d34996a, the Kitteh doesn't seem to set the status correctly.  Merging. 
I like this
its either this or special-casing `SelectionPath` in the remote protocolwhich sounds rather hacky
PLS REBUILD ALL
(kitty-note-to-self: ignore 17221419) :cat: Roger! Rebuilding pr-validator-per-commit for dbac4808. :rotating_light: 
previous failures:  	akka.testkit.CallingThreadDispatcherModelSpec 	akka.remote.router.RoundRobinRoutedRemoteActor  PLS REBUILD/pr-validator-per-commit@dbac480
(kitty-note-to-self: ignore 17324022) :cat: Roger! Rebuilding pr-validator-per-commit for dbac480. :rotating_light: 
(kitty-note-to-self: ignore 17335054) :cat: Roger! Rebuilding pr-validator-per-commit for dbac480. :rotating_light: 
(kitty-note-to-self: ignore 17337038) :cat: Synchronaising! :pray:
The failures are unrelated, and I have only successful build runs since (not reported by kitteh)
LGTM; though one thought remains: your solution of adding an AbstractStash to get around the single-inheritance problem is probably the only viable one, but there must be a better name prefix than Abstract for this purpose. Unrestricted perhaps?
Yes, I'm not too pleased with `Abstract` either. I think that `Unrestricted` is a better name.
(kitty-note-to-self: ignore 17215164) :cat: Synchronaising! :pray:
Doesn't look good. The actor system could not shutdown. https:jenkins.akka.io:8498/job/akka-cluster-stress1/27/consoleText https:jenkins.akka.io:8498/job/akka-cluster-stress1/28/consoleText
I see the shutdown problem without the patch also, but the exception logging is still there with the patch as you see in above links to build 27 and 28.
This is not ready yet.
Thanks, the change looks good. But youll have to sign the CLA before it can be merged (see CONTRIBUTING.md); there are 47 more superfluous occurrences of must or should, may I interest you in removing those as well?
I signed the CLA online.  I will take a look at removing those other occurrences.  If you already have the output of your grep it would make it easier for me to eliminate those superfluous occurrences.  If not I will do so on my own.  I will submit a separate ticket for the additional occurrences.  Regards,  Mike Krumlauf  From: Roland Kuhn [mailto:notifications@github.com] Sent: Sunday, April 28, 2013 1:19 PM To: akka/akka Cc: Mike Krumlauf Subject: Re: [akka] #3272 - Eliminate redundant "should" in DataflowSpec test output (#1375)   Thanks, the change looks good. But youll have to sign the CLA before it can be merged (see CONTRIBUTING.md); there are 47 more superfluous occurrences of must or should, may I interest you in removing those as well?   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1375#issuecomment-17137795>.  
Thanks! All I did so far was the following (which might or might not be complete): ~~~ bash egrep -r '"(should|must)' `gfind -type d -name src` ~~~
(kitty-note-to-self: ignore 17215130) :cat: Synchronaising! :pray:
Something doesn't look right - I don't think these extra merges should be there (see previous comment).  If somehow this is messed up we can close this pull request without accepting the commit and start over.
you can also rebase on current master and use the `-f` flag for pushing, thereby replacing the contents of the pull request (thats how we usually do it)
I have verifed that mjkrumlauf has signed the CLA. I will merge this and fix other occurrences in another PR. Thanks for contributing, @mjkrumlauf
The name change is good, but as you say it's not the cause of the failure.  Barrier names don't have to be unique, it just makes it a lot easier to debug when things go wrong. Here we have several barriers since previous "actors-started-3" so there is no mixup.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/927/
Mixup no, but will they wait for each other, or since the "actors-started-3" was finished before they pass through unsychronized?
no, the new barrier with same name will be used
Yes, and there can only be one barrier in progress at any given time. So there is no room for somebody getting an old barrier.
Ok, then this will not fix anything.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/927/
 but it LGTM anyway
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/937/
can you please add the requirements on the system actors, such as RemoteWatcher?
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/937/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
So the `SerializationCompatibilitySpec` failed. I assume that it's ok to change it for things that aren't dot releases.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/940/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/940/
LGTM (apart from the default arg)
PLS REBUILD ALL
Since the Kitteh is taking a nap, this has been verified by triggering the validator by hand:  https:jenkins.akka.io:8498/job/akka-pr-validator/941/
Note that the failure "unannounced disconnect" is fatal in the way that it is an exception (ClientLost) that restarts the barrier coordinator and thereafter nothing can continue.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/935/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/935/
I think this is the right approach. LGTM after name change and separation of messages.
OK, then I will complete this.
Ready for final review
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/939/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/939/
There is no versioned snapshot I presume... Anyway, I am happy that my build will work correctly again :)
Nope, non versioned snapshot. He only published a version so that we could use it.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/934/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/934/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/932/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/932/
This should be merged before other pull requests, since it generates most failures. Please review asap.
please review afterwards if you like
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/933/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/933/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
The failure is the one fixed by #1365 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/931/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/931/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/930/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/930/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
The failure was in ClusterDeathWatchSpec, ticket 3255
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/929/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/929/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
The failure was in ClusterDeathWatchSpec, ticket 3255
Changed based on feedback from @viktorklang 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/936/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/936/
looking good so far
Updated with tests and Scaladoc for the adapter
Old kitteh sad GO, so I merge.
The PR validator only runs on JDK6, so it won't test the fix properly. Verified locally on both JDK7 and JDK6.
(kitty-note-to-self: ignore 17216054) :cat: Synchronaising! :pray:
So the failure was in the RemoteNodeDeathWatchSpec pull #1387. Could I please get some eyes on this one and that one so we can start merging and get rid of failures?
(kitty-note-to-self: ignore 17322838) :cat: Roger! Rebuilding pr-validator-per-commit for 5fd1955. :rotating_light: 
Thanks for pointing this out (and for thinking about both languages); would you please amend and re-push (i.e. with "force" to keep the history clean)?
Good catch, LGTM No more places? docs code? cluster? samples?
I skimmed through the usages of `shutdown()` before and we weren't creating that many actor systems except for the cluster StressSpec. I'll add that one as well even though it has never failed.
please also add all the Java tests which spawn systems, check it and merge when confident
Created a JUnit managed `AkkaJUnitActorSystemResource` and changed actor system shutdown to use convenience methods in all tests except documentation tests that demonstrate normal usage.
I see no changes to `.rst`? Is description of AkkaJUnitActorSystemResource covered by examples? Any need to explain it further? I guess that is our recommended way to write junit tests now.
Very nice! Aside from the doc question: LGTM
So akka-testkit doesn't have a dependency on either ScalaTest or JUnit, so the `AkkaJUnitActorSystemResource` is not part of the normal akka-testkit.
LGTM: nice cleanup!
One more magic thing removed.  LGTM
@bantonsson do you know whats up with SerializationCompatibilitySpec?
Nope, no idea about `SerializationCompatibilitySpec`. Seems fishy. 
So it probably has something to do with you removing the tell method from `ActorRef` which somehow changed the serialization of the `FakeActorRef` in the test. That test is so brittle.
It would be better to test the actual serialization, i.e. that it can read bytes and create obj from it. That only verifies compatibility in one direction though.
oh crap, Ill figure this out, then
I removed FakeActorRef, @bantonsson is that an option? It sure makes the test green 
@rkuhn yes, I think that it is an option to remove the `FakeActorRef`. We should really create integration tests instead that uses the stable version and the head version together.
apart from not understanding the Exiting thing it LGTM
(kitty-note-to-self: ignore 17214813) :cat: Synchronaising! :pray:
the build bot fails to comment about the failure because the http request to get the console text fails:  ``` javax.net.ssl.SSLException: hostname in certificate didn't match: <jenkins.akka.io> != <*.typesafe.com> OR <*.typesafe.com> OR <typesafe.com>         at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:228) <snip>         at dispatch.classic.Http.apply(Http.scala:21)         at backend.PullRequestCommenter$$anonfun$receive$1.applyOrElse(PullRequestCommenter.scala:51) ```  I'll make it more robust and deploy a new version.
previous failure was in RemoteNodeDeathWatchFast
(kitty-note-to-self: ignore 17322956) :cat: Roger! Rebuilding pr-validator-per-commit for 3308039. :rotating_light: 
wow, thats a pleasant surprise :-) Im currently running the whole test suite since our build kitteh is on vacation, will comment if I find something else
Cool, thanks. If desired, I would love to remove additional deprecation warnings. There are some more low hanging fruit, where language feature imports are missing and some Props with creators, where anonymous actors are passed in. But that takes some more effort, as I would first have to understand what these annonymous actors do, to give them appropriate names.
Unfortunately I didnt get around to it today, but Ill write a blog post explaining a bit more of the background for the Props change and also some future perspective: well try to reintroduce quite similar syntaxmacro-basedto keep the anonymous actor syntax but make it safe, which is why Im not sure yet that we will want to convert them.  That being said: I find it totally awesome that you did this clean-up. You made my weekend, thanks!
You are most welcome. That is the least I can return for all the amazing work you guys do all the time!
it can be merged once the KITTEH has been repaired and signed off on this
(kitty-note-to-self: ignore 17215223) :cat: Synchronaising! :pray:
okay, the last commit was verified correctly (the failure was a known deficiency in the OSGi sample application test run); with the new kitty well have to squash more eagerly, I think, to avoid long build times due to multiple commits, Ill see whether CONTRIBUTING.md needs to be updated.
Again: thanks a lot, Dario!
You're welcome ;-)
LGTM: we can optimize garbage creation laterafter having measured that it is worth convoluting the code
failure was "buffer overflow"
PLS REBUILD ALL
(kitty-note-to-self: ignore 17386163) :cat: Roger! Rebuilding pr-validator-per-commit for a9394d9f. :rotating_light: 
It is still referring to build 89. I merge this oneliner anyway.
any more opinions? @drewhk ?
Hmm, I would not put this in the Transport API. Is there any reason to log it there and not in Remoting?
ok, I will move it to a separate extension
I have moved it to an extension, please take another look, @drewhk 
updated as discussed, and added to docs
(kitty-note-to-self: ignore 17215231) :cat: Synchronaising! :pray:
previous failure was      [error] Failed tests:      [error] 	akka.cluster.Stress     [error] 	akka.cluster.routing.ClusterRoundRobinRoutedActor 
(kitty-note-to-self: ignore 17322881) :cat: Roger! Rebuilding pr-validator-per-commit for 2b4787f. :rotating_light: 
(kitty-note-to-self: ignore 17324865) :cat: Synchronaising! :pray:
the build failures are fixed, and kitty has verified one was a misplaced barrier another was due to parallel multi-node builds I merge this to wip-2787-cluster-stress-patriknw branch now
This is nice. LGTM
LGTM (apart from that doc thing)
renamed to `isOlderThan` and added the doc comment
then by all means: mrs (can someone translate that to sknska?)
ok, I want to merge this after #1381, because I think it can be merge conflicts and it will be easier to fix that in this branch (and kitty will have less to do)
I don't like the Array for several reasons. An alternative would be to use Vector and put PatternHolder (implementing equals/hashCode) as elements for wildcards.
switched to immutable.IndexedSeq and PatternHolder, vastly reducing the hand-crafted code (to nil)
ah, forgot some docs 
excellent, thanks LGTM after some minor adjustments
(kitty-note-to-self: ignore 17228867) :cat: Roger! Rebuilding pr-validator-per-commit for 7ea3044. :rotating_light: 
(kitty-note-to-self: ignore 17322852) :cat: Synchronaising! :pray:
(kitty-note-to-self: ignore 17335285) :cat: Roger! Rebuilding pr-validator-per-commit for 7ea3044. :rotating_light: 
(kitty-note-to-self: ignore 17414079) :cat: Roger! Rebuilding pr-validator-per-commit for 7ea3044a. :rotating_light: 
build was successful, but kitty did not bother to comment
Looks Good To Me, we will merge when we have collected a few LGTM
Ah, capisce. Molto grazie.
LGTM  Awesome speedup
LGTM, nice job!
(kitty-note-to-self: ignore 24386495) :cat: Synchronaising! :pray:
(kitty-note-to-self: ignore 24401600) :cat: Synchronaising! :pray:
(kitty-note-to-self: ignore 24297628) :cat: Synchronaising! :pray:
LGTM: nice and clean
Regarding deprecation. This feature doesn't add any value to 2.2, without the reachability feature, so I don't think it should be backported, and just adding a deprecation warning without an alternative doesn't make sense. So I guess I for 2.3 I have to remove the `require` and have a fallback for that meaning `auto-down-unreachable-after=0s`. OK?
yes, you are right of course, deprecation in 2.3 it is.
Rebased and changed to deprecation for 2.3
(kitty-note-to-self: ignore 24571933) :cat: Synchronaising! :pray:
@drewhk Could you please try this out on windows? You need protoc (v2.5.0) on the path and then run `akka-remote/protobuf-generate` inside sbt.
Yep, will do.
Windows guy approves, LGTM
(kitty-note-to-self: ignore 24386374) :cat: Synchronaising! :pray:
I have added `-Dakka.cluster.assert=on` to jenkins jobs
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/876/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/876/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
This will complicate things when using stackable traits with actors. For example, given an actor A with a stackable trait S (where `class A extends Actor` and `trait S extends Actor`), I could do so far       actorOf(Props(new A with S))  Now a I have to explicitly define a new class X      class X extends A with S  to get       actorOf(Props[X])  working, because      actorOf[A with S]  will raise an `ActorInitializationException`. Or am I missing something?   If the extra `class X extends A with S` is really needed then I'd be highly in favor of not deprecating `Props.withCreator` but rather offering it as an alternative to the new API and documting the issues that may arise.
The goal is to remove API which equates to a gun pointed squarely at your foot: of course you can point it somewhere useful if you know how to operate it, but most users wont. In this sense Im sorry to say that we really do have to remove those methods.  However: the plan is to wait until macros become a viable option to include in core Akka packages, at which point we can reintroduce syntax which looks quite like the one you want but is safe. The idea is that you give a thunk of code as before to a macro which will then convert that to the new way of doing things (basically writing `Props(clazz, args )` for you), and I dont see a reason why this macro could not create that anonymous class which is needed to make things work in the stacked traits case.  The old methods will remain availablealbeit deprecated to warn visibly about the problemsuntil that safer solution has been implemented. Does that sound good enough?  PS: you could also play with this if you want it earlier by creating an `IndirectActorProducer` which can transform a TypeTag into a class generated on the fly and instantiate that, but youd still have to wait until Eugene has allowed TypeTag serialization to escape from his brain into the world at large.
> The old methods will remain availablealbeit deprecated to warn visibly about the problemsuntil that safer solution has been implemented. Does that sound good enough?  As long as the deprecated methods aren't removed until there's an alternative solution available for dealing with anonymous classes and stackable modifications, I'm fine with it. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/878/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/878/
LGTM, I might take a look at the generated (full) documentation. Perhaps we should mention "that deprecated methods aren't removed until there's an alternative solution" in the migration guide to not annoy users that change this all over their code base and then a source compatible solution pops back.
@patriknw I think that being explicit about no removal until alternative solution would be an excellent addition.
thanks for the suggestions!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/880/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/880/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/883/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/883/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/884/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/884/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Cluster StressSpec failed.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/886/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/886/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
pls suspend reviewing, I am merging the props serialization changes
Updated, sorry for the intermission
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/887/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/887/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/888/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/888/
While the kitteh was successful, ClusterStressSpec sometimes fail on my machine. I have some ideas, talk about it tomorrow.
Updated resend strategy to a more economic one
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/888/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/888/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/892/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/892/
Great work, Endre!
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/903/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/903/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
akka.cluster.UnreachableNodeJoinsAgain failed. @patriknw is this something known? Cluster stresstest survived though.
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/905/
Build was successful. In you go.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/891/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/891/
I reviewed this yesterday, but never placed the LGTM stamp here, so here it is: LGTM! Very nice docs and samples.
rebasing was not entirely trivial (clashed with the WriteFile feature), could be a good idea to take a look at TcpConnection.scala again
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/902/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/902/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/871/
I think the wording of the "must not..." is confusing (apart from the fact that it shouldn't have must first). What are we testing here really?  Wasn't the purpose of the first "test" message to ensure that the it had been forwarded to the child and that when we stop the child we make sure that we don't get back the "child green" to see that we really stopped and recreated the child of the supervisor? 
I mean then you would have to add some ack that the message had been forwarded before triggering the latch.
I asked Roland what this test tests, we weren't completely sure either. I hoped someone enlightens me here :) We can ack though the forwarding of that message and then it will be lost -- if this is something we want to test.
I created a version that acks the forwarding of the first "test" message. All we need to decide if we need it or not.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/871/
I think that we should have the ack of the forwarding of the test message to make sure that we actually stop and start the child.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/872/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/872/
apart from the err msg, LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/873/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/873/
Good idea, LGTM
sounds like a good idea
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/874/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/874/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/875/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/875/
LGTM! I like the "instant unwatch" feature.
LGTM  Yes, akka-persistence is a much better solution, well spotted!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/864/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/864/
Conclusion from mlist: https:groups.google.com/d/msg/akka-dev/d0sXyAr8eBU/86RYR_aHDLIJ I will implement the Welcome message and UID as discussed.
This is ready for final review. Massive number of changes, but most of it is trivial. I will squash before merge. Here is the updated task list: * Disallow join requests when already part of a cluster * Remove wipe state when joining, since join can only be   performed from empty state * Welcome message instead of gossip as reply to join, only accept Welcome from the member that we are trying to join with * Create uid in RARP * Use uid in cluster Member identifier, UniqueAddress * Use uid in vclock identifier * Ignore gossips to wrong destination (uid), and from unknown (and unreachable) members * Make sure received gossip contains selfAddress * Test join of fresh node with same host:port * Remove JoinTwoClustersSpec * Retry unsucessful join request * Make Member apply and constructor is private[akka]
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/870/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/870/
apart from placement of the actual UID: LGTM
Adjusted according to comments. 3 last commits. I will squash and merge when you have looked at those minor changes.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/889/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/889/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
It's a real failure. The problem is in the test. I will fix it.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/889/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/889/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/894/
Great job, especially the boy-scouting! Squash and merge.
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/894/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/866/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/866/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Fixed, missed committing the reference.conf
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/867/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/867/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/869/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/869/
LGTM  Awesome bug-hunting.
LGTM, after minor things
(kitty-note-to-self: ignore 24591871) :cat: Synchronaising! :pray:
@krasserm please squash and adjust the commit comment to our encoding:      =per #3615 Fix InvalidActorNameException in persistence tests
Is the commit message encoding documented somewhere? Just found http:doc.akka.io/docs/akka/2.2.1/dev/developer-guidelines.html which doesn't explain the `=per` etc ....
@patriknw I mean, I know that the 3 letter abbreviation is a module, but I'm not sure about `=`, `!`, `+` ...
https:github.com/akka/akka/blob/master/CONTRIBUTING.md  I will merge this when pr-validator is done
oh dear, how could I miss that. There's even a link at the top of the doc I was looking at. Sorry for keeping you busy with that ... Will adhere to it in upcoming commits. Thanks in advance for review and merging.
It would be interesting to see how much this changes convergence times (hopefully only a slight increase) and unnecessary gossips (hopefully drastic decrease). 
faulure was: unable to connect to github.com
yes, I will run the cluster Stress test on this next week to make sure that it works in practice. 
Probably the default setting needs to be re-tuned, too.
why do you think 0.8 should change? in what direction? tests will tell, but I'm not really interested in spending a lot of time finding the optimal values now, this ticket is more about avoiding the shoot down game
No, 0.8 seems to be a nice first shot, I expected something like 0.1 that is why I thought that we need to increase it -- not necessarily optimizing, just to avoid effectively disabling talkback.
(I meant biased gossip, not talkback of course)
failure was in ChannelSpec, created ticket
LGTM  Will be interesting to see the results.
Failed experiment, attached info in ticket
LGTM  I have a felling that the merge in Reachable can be simplified at the algorithm level. Will open a separate ticket.
(kitty-note-to-self: ignore 24502178) :cat: Synchronaising! :pray:
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/863/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/863/
Merging this since jenkins.typesafe.com will fail consistently without it (It hasn't got mvn installed or it's not in the path).
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/858/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/858/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
JavaTestkitSpec failed, should be fixed in master but I guess the kitteh is being stupid again.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/862/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/862/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Ok, this now contains an improved solution for toning down the logging of DeathPactExceptions. The UDP side is now also covered.  I also fixed a problem with Johannes' latest "half-closed" feature that only surfaced under OS/X and cleaned up some small stuff.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/868/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/868/
so, shall I merge and make the `publish` change afterwards, or do you want to do that, Mathias?
Roland, sure, go ahead.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/855/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/855/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Note that failures were on jenkins.akka.io (not slow jenkins that skips TimingTests). The adjustments of the timeouts might help anyway, but I'm not convinced that it must be marked as TimingTest.
Good point, will see if I can rewrite things to not require uninterrupted sleeps.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/855/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/855/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
The test failure is for the fix to JavaTestKit, so ignore that for now
great solution! LGTM (still not sure you need to tag it is timing test)
rebased on top of new master with fixed JavaTestkitSpec
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/859/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/859/
Second thought, it is a small assumption on CallingThreadDispatcher that makes this non-racy. watch sends a message to the testActor, and it is only because of CallingThreadDispatcher that it is guaranteed to be delivered before the stop is finished.  I don't think you should check existenceConfirmed in such a test.
WDYM? The Watch system message has to be deliviered before the Terminate system message?
the `testActor` always uses the CTD, so I think were fine (what Patrik means is that your `watch` sends the `Watch` indirectly)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/857/
yes, your previous test also received Terminated (I guess), but with wrong existenceConfirmed. Do whatever. It LGTM
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/857/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/851/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/851/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I like it!
I have tested RemoteWatcher thoroughly.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/879/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/879/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/882/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/882/
Added the cluster aware specialization of the RemoteWatcher.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/885/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/885/
I have read the implementationwhich looks goodbut not studied the tests; I trust you when you say that youve been thorough ;-)
I have updated the docs in 2a0c59d. I have a question there.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/885/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/885/
Added a warning about actorFor in the Watching Remote Actors section: 3fb9d1b
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/893/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/893/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
After kitty verification I will merge this. I have added the UID to the heartbeat messages, and dummy api in RARP for quarantining, which will be implemented in ticket 2594.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/897/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/897/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/854/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/854/
Documentation still missing.  We didn't yet finally decide on which strategy to use for scheduling (possibly blocking) file IO operations. @rkuhn commented in the previous PR:  > The configurable dispatcher to run the actual sending on is one way to do it, another could be to wrap the transferTo call in `blocking`.  What is `blocking`? Or, should we use a different dispatcher per default, e.g. a ThreadPool with a low fixed size number of threads (e.g. 4)? The advantage of using several threads would be that it might be worthwhile even with just one disk to initiate disk reads as early as possible so that the OS would be able to deliver data more efficiently from its IO queue.  
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/848/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/848/
LGTM: the chosen solution allows tuning by setting a different dispatcher, which I think is fine. It should be documented that these tasks may block threads and that no `scala.concurrent.blocking` marker is used; the latter is deliberate in order to control maximum parallelism on these tasks.
Cool! Just a side question, will this be exposed in some way in Spray to serve static content? 
Yes, that's the plan. See spray/spray#269
I incorporated the suggested changes:   - `file-io-transferTo-limit` (default: 512KiB) to prevent long blocking file IO calls when network is too fast  - don't lose direct buffers on exceptions  - forward Exceptions caught during the ex-actor `transferTo` call to the actor for proper handling
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/848/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/848/
Great work, thanks!
Yay, thanks for the comments.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/850/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/850/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/850/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/845/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/847/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/847/
(kitty-note-to-self: ignore 25014046) :cat: Synchronaising! :pray:
Great work, Patrik, I like what I see! This should cover all shortcomings I remember, e.g. users can add routees by sending a message and the whole thing works equally well in a normal actor.  Theres just one thing I dont understand right now: why does a Nozzle need a SupervisorStrategy?
yeah, for the Nozzle it doesn't make sense to have supervisorStrategy
Re-implemented all remaining routers. Next step is to consolidate the `routing2` package with old `routing` and deprecate and remove old stuff.
Regarding the naming of the router that sends to paths I want to find a name that associates with something that spread the messages, typically for scaling out on other nodes.  An alternative to `Nozzle` might be `Fan`. "fan out" is typically what it does.  Was also thinking of "farming out", but `Farm` doesn't give the right association.
New stuff in correct packages, old api deprecated, and old api for custom routers removed. Kept duplicate of all old tests.  Next step will be to add the nicer props factories.
That failure (969) was unrelated to these changes, created ticket.
Now is a good point for initial review if you have not looked at this before.  Added proposal of the nicer props factories; b7d2228 When you have reviewed and we agree that this is the way, or we decide for something else, I will add the same props factories to other routers and update more tests.  After that the final step will be to rewrite the documentation.
Head is spinning but LGTM.  Regarding the name `Nozzle` or `Fan`, they just don't feel righ. Isn't it more appropriate to call it a `Group` or similar. The routees are a group of named paths as opposed to an anonymous pool.
I am through half of it. Sorry for having more questions than actual comments, but this is huge :) Incredible work Patrik!
@bantonsson I kind of like the simplicity of the name `Group` (and `Pool`)
I finished the second round of my review. LGTM
Yes, still looking great! and +1 for s/Nozzle/Group/g
LGTM! (I like the refactoring!)
Yes, this looks very good; one question: why not rely on the serialization infrastructure already present in an ActorSystem?
@rkuhn main reason for this is that snapshots can get very large (MB - GB range) and converting them to a byte array in memory would roughly double memory consumption. Nevertheless, we could implement a default snapshot serializer that delegates to the existing serialization infrastructure which should work well for all apps with small to medium size snapshots. I'll add this to my todo list.
we could also enhance the infrastructure to allow streaming in general  Regards,   Dr. Roland Kuhn  Akka Tech Lead  Typesafe  Reactive apps on the JVM  twitter: @rolandkuhn
This would be even better. Shall we follow a two-step approach here? First a delegating solution to the existing infrastructure, then an extension of the existing infrastructure to allow streaming? I think the first step is just minor effort, not sure about the second step though ...
sounds good, please create tickets
LGTM, very good change
Isn't this a band-aid mitigating the fact that the lifecycle events call each other but are overridable (as opposed to being called in the right order by the environment)?
@drewhk, yes the lifecycle callbacks are a bit messy, but you would still need the separate callbacks for the _around_ so as not to mess with the user actor behavior.
Yes, you are right. LGTM.
To persist my reasoning for recommending this pull request: stackable overrides are possible in Scala using `abstract override`, which is not applicable in this case, and if it were then we would still have the problem that nearly nobody understands that feature properly and it does not work at all with Java. Therefore we have now an overridable interceptor which exposes the desired functionality and is compatible with the whole user base.  LGTM
(kitty-note-to-self: ignore 24824141) :cat: Synchronaising! :pray:
(kitty-note-to-self: ignore 24857315) :cat: Synchronaising! :pray:
I don't know what is wrong with the kitty, she is building this over and over again, once per hour, all successful. 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/925/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/925/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/925/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/925/
please put ticket number in commit comment; otherwise LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/926/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/926/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/910/
Anyone objects? I give 10 minutes ;)
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/910/
only looked at it briefly, but as far as I can see it is good
Could not take a very deep look, but otherwise LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/912/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/912/
I have to trust you/Spray on how the SSL decoding/encoding should work.  LGTM
apart for some minor adjustments to the config: LGTM
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/913/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/913/
Although Kitteh is happy, there was a bug. I will neeed a second run
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/913/
There is some unexpected interplay with clustering and it fails in ClusterDeathWatchSpec. Do not merge
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/914/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/914/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I know that technically the heading mark-up _could_ be shortened, but Im not doing that now
I love the change, but the index.html page only contains two headings (the PDF looks fine)  * Java Documentation * Scala Documentation  Should we link to the sub pages from the site?  Otherwise LGTM
will change the site later
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/919/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/919/
ok, looks innocent. Please run cluster tests on it. 
yep that is what I am doing 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/920/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/920/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/921/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/921/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
ClusterDeathWatchSpec ... ticket commented
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/922/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/922/
FYI: It seems the latest version of the netty workaround is here  https:github.com/netty/netty/blob/master/transport/src/main/java/io/netty/channel/nio/NioEventLoop.java
One thing: our default setting is an infinite `select-timeout`, which ends up not actually running the `guardedTimedSelect`. Does the epoll bug only manifest itself when select is called with a timeout? If not the current solution in this patch is insufficient.
The (new) netty logic seems to be something like this:   * calculate a deadline for the given timeout  * during the deadline call `select(timeout)` in a loop with the timeout value set according to the deadline and current time  * count how often `selected == 0` occurred during this loop  * if the count goes over a threshold (default = 512 times), rebuild the selector
Comparing with the netty solution I find theirs a bit superior. They do not require the addition of an instance field that is written to even in the case of a regular "unsuspicious" selection result. We could change the bug detection logic to spin on the `select(timeout)` call for as long as the original timeout deadline has not yet passed. If we spin more often than the threshold we declare the selector dead.
On a related note: We just talked about the selection infrastructure a bit more, thinking about remaining perf improvement potential. Why do we actually offer the `select-timeout` setting? Currently we cannot see one reason why I should want to set a non-infinite value for this setting. If there really isn't one, we should remove it and simplify. Once this is done there might optimization potential in manually managing the select/wakeup interaction by switching to selectNow (and avoid the wakeup call) as long as there are already other tasks scheduled on the selector dispatcher.
The reason for the AkkaSelector is to avoid complecting the current logic. However, if we decide we are not going to support select(timeout) then we won't need it afaict.
`akka.io.select-timeout` is configurable, so we should better have code somewhere which uses `select(timeout)` ;-) [code here](https:github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/io/SelectionHandler.scala#L179)
The question is if there's still good reason to make it configurable.
Optimized the implementation of Wakeup by trying to do so idempotently without issuing a syscall each time. There's a bit of terse code in there, but it's for the sake of performance.
I still think we should re-evaluate whether we really want to introduce all this complexity just for allowing the user to configure a time-bounded select. After some more thinking we remembered the reason we included it in the first place: for enabling sharing of a single selector thread between multiple selectors. Is this use case really worth this effort? When exactly would I want to share a single thread between multiple selectors?  I think disallowing time-bounded selects would help us in keeping complexity low and make use of one advantage we have over the netty architecture: netty apears to use its IO thread also for other (non-selector-related) tasks and therefore has to support time-bounded selects. Since we don't we don't.  Also, we are currently experimenting with an optimization that would allow interestOps for a SelectionKey to be set directly from the connection actors. This is what netty was doing up to version 3.5. They needed to remove this optimization due to the Epoll bug workaround, which requires key migration from one selector to another (as done by Viktor here), which is hard to do if interestOps setting is not centralized. We don't have benchmarking numbers yet, but I'd feel uneasy about loosing out on optimization potential because of a feature that noone needs.
(source for the netty detail: https:github.com/netty/netty/pull/789)
calling `interestOps` from many different threads may or may not be beneficial, at least it is not immediately clear to me; we should benchmark both alternatives
yes, we are currently on that. (This is the change we are testing: https:github.com/spray/spray/commit/fa170d525b861cbb92103bb71f73d25b0eab9d34)
Ok, benchmarking shows that moving `setInterestOps` off the selector thread doesn't help. Apparently the selector has only one lock that *all* keys synchronize on. Having the connection actors manage ops themselves reduces the number of actor message sent to the selector actor but at the same time creates contention on the single lock, causing throughput to actually drop by about 30% in our tests. So this is not a good idea.  Still, getting rid of the hashmap in the selector can (and IMHO should) still be done. This has no dependency on this PR however. I'll open a separate PR for that change.
Ok, I won't do more on this PR until someone decides what happens with select(timeout)
Bounded or non-blocking `select` are not really advertised yet, and first and foremost we do not have a single test case for them; hence they should go. The use-case is running Akka IO on a smartphone where one thread is used for everything, including the select loop. If we want to introduce that later we can just resurrect the code from the 2.2-M3 tag. Or we could then make it an alternative facility (TcpAlt or something).  Viktor, do what you like most! ;-) (and please keep a comment in the code that `interestOps` are ONLY to be called within the loop, possibly pointing to this discussion)
Alright, what do you guys think about the new commit?
apart from the small fix it LGTM
(kitty-note-to-self: ignore 16716300) :cat: Roger! Rebuilding pr-validator-per-commit for 50172337, 4a590b73. :rotating_light: 
Ummm, is the kitteh dead or something?
So the kitteh works a bit different now. It normally uses the issues to mark the state of the PR and it doesn't comment in the PR when it starts/finishes a job. It has picked up this PR and scheduled builds.
Ah, but doesn't that mean that the author doesn't get a notification by email if it failed? (as it did with the PRs (which was awesome btw))
Yes, I miss the emails as well. We'll have to see what it does on failures. It commented on this pull anyway.
PLS REBUILD ALL
(kitty-note-to-self: ignore 17198718) :cat: Roger! Rebuilding pr-validator-per-commit for 3d4e26a3, 96a6deb7. :rotating_light: 
PLS REBUILD ALL  previous failed with connect timeout in IODocSpec; why the kitty did not complete the determination is unclear to me
(kitty-note-to-self: ignore 17211400) :cat: Roger! Rebuilding pr-validator-per-commit for 3d4e26a3, 96a6deb7. :rotating_light: 
so it seems that rebuilding does not really happen after a previous unrecognized failure 
(kitty-note-to-self: ignore 17215070) :cat: Synchronaising! :pray:
so, what does this Synchronaising mean? it didnt start new jobs and it didnt change the status of the PR  (i.e. the Details link above still points to the outdated failure)
So it's meant to make the Kitteh reread the jobs from the PR and Jenkins and try to set the status accordingly. But for some reason, which haven't been nailed down yet, it doesn't do anything with our PRs even though the log files on buildbot indicates that it finds the jenkins job status.
okay, thanks for the update
this needs some merge love 
After you have merged this, Roland, I'll put in another PR building on this, which brings another set of improvements.  I wanted to wait until this get's merged because it touches a lot of the same code...
Since Viktor is on vacation I took the liberty to rebase and squash, well see what the kitty thinks about it now
@sirthias youre good to go!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/923/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/923/
Looks good, but it should also be mentioned in the last section of logging.rst
+1 for documentation  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/923/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/923/
Changed to UTC and added test.
BUT you have still not updated logging.rst with the new mdc value
yes, needz moar docks
very good :+1: 
sorry for the nitpicking, apart from which it LGTM
PLS REBUILD ALL
(kitty-note-to-self: ignore 17198788) :cat: Roger! Rebuilding pr-validator-per-commit for 7f84dbf4. :rotating_light: 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/924/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/924/
Thanks for this fix!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/909/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/909/
Any thoughts on this? Can it go in?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/906/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/906/
I will merge this.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/908/
LGTM: good catch! (and you are probably right about the re-joining, including its brokenness)
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/908/
Awesome catch. I think you're spot on in the analysis. LGTM.
alright, then I merge
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/899/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/899/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Looks like you missed a spot in `StatsSample.scala`. Otherwise LGTM.
fixed cluster samples also
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/900/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/900/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Last failure was in UdpIntegrationSpec, ticket 3245 created.
LGTM  bra jobbigt ;-)
I added the `Scala API` marker
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/898/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/898/
Yes, this is the old pain point; but if Id have a good idea it would be fixed by now.  LGTM
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/907/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/907/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/901/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/901/
LGTM very nice feature!
Great work, Bjrn!  There are two things: naming things, describing overriding rules and off-by-one errors.  The terminology is currently something like:  * mailbox is the thing which knows how to run an actor; non-configurable * message queue is the thing which holds the messages; fully configurable * mailbox type is the configurator, which mostly concerns itself with the message queue type  In that light Id say that the term mailbox queue should be extroduced ;-)  The other thing is that there should be a numbered list of the override priorities in the docs.
Update according to comments.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/901/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/901/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/904/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/904/
It's not immediately obvious [to me] what the goal of this change is, can you elaborate?
Check out [the ticket description](https:www.assembla.com/spaces/akka/tickets/3581-io--add-tcp-compoundwrite#/activity/ticket:), I hope it nicely explains the motivation.
Ah, thanks for the pointer. Just being curious, but what other solutions where viable?
I see at least three alternatives to this currently proposed solution:  1. Don't change anything     This would require all applications building on top of akka-io to actively manage writes combining memory with disk data (or non-continous disk data, e.g. from several files) themselves. This can be done with explicit ACKs that trigger the sending of the next chunk after having been received by the application.    In spray this would require us to write a new pipeline stage that does this.    Apart from hurting latency *and* throughput (not a trade-off!) this solution increases the logic and complexity on the user side of akka-io and requires everyone to implement this same stuff again.  2. Fuse `Tcp.Write` with `Tcp.WriteFile`     This could be done by extending `ByteString` to also allow off-heap (i.e. disk-based) content. ByteStrings already support compounding (i.e. copy-less "concatenation"), so theoretically a simple `Tcp.Write(ByteString, ack)` would suffice in akka-io.    However, I'm not sure ByteStrings really should be able to model off-heap data. IMO this would overload the ByteString abstraction. Another disadvantage over the solution proposed with this patch is that you wouldn't be able to request intermediate ACKs (e.g. after the second of 3 sub-writes has been completed).  3. Model `Tcp.CompoundWrite` differently     The currently proposed model for compound writes certainly isn't the only one conceivable. One alternative could be something like:         case class Write(data: ByteString, ack: Event, next: WriteCommand) extends WriteCommand        case class WriteFile(data: ByteString, ack: Event, next: WriteCommand) extends WriteCommand        case object EmptyWrite extends WriteCommand     However, somehow the proposed model is currently the one we like best.
Couldn't it be solved by sending a message for each using NACK and then do the last write with ACK?
The problem is that if I have 2 writes that are ready to be sent I cannot sent them right after each other to the IO layer, as the second one will usually immediately come back with a NACK. I need to delay the second write until the IO layer has written out the first. The only way I can do this reliably is via ACK.
My concern: Sending huge messages is IMO a bad idea (especially if you need to send it over the network) since it requires full retransmission if it arrives broken. Also, it creates a huge entity that can only get GCed once it has been written to the sink.  If we take a step back, would it make sense to have some kind of transmitter/transfer actor spawned that will deal with the transfer in one go? 
If huge messages are a concern that we should prevent them. However, this patch doesn't introduce huge messages. Huge message are already possible with the existing API (since we can write one ByteString and a ByteString can have any size and even be a compound one consisting of many sub-ByteStrings).  I don't think the IO layer should enforce any limit on message size. It's up to the user to decide whether and how she wants to allow and deal with "big messages". Also, we already allow the passing of potentially huge off-heap data chunks to the IO layer with `Tcp.WriteFile`.  This patch merely smoothens out the API where it isn't yet as properly designed as it should be. It doesn't add anything that you cannot already do without the patch. It simply removes complexity and unnecessary overhead.  > If we take a step back, would it make sense to have some kind of transmitter/transfer actor spawned that will deal with the transfer in one go?  What would that buy us over the solution proposed with this patch? Apart from added overhead and complexity I don't see any benefit.  
It's also about fairness, sending a massive IO message means that no other writer gets through until the massive message is all over the wire, right?  Re-reading the PR I'm less concerned, however there is one glaring omission  there is no Java API for chaining the writes.
> It's also about fairness, sending a massive IO message means that no other writer gets through until the massive message is all over the wire, right?  Not sure what exactly you mean. A "massive IO message" will be spoon-fed to the kernel, either in chunks that are no bigger than  - the socket's send buffer (in the case of `Tcp.Write`, i.e. in-memory ByteStrings) or - the configured `file-io-transferTo-limit` (in the case of a `Tcp.WriteFile`)  So the connections `TcpConnection` actor will not block the thread for too long thereby allowing other connection actors to run. Of course, one "massive IO message" might take some time to go out and will therefore prevent other data from being able to be sent across *this one connection*. However, that is what I'd expect as a user, no?  > Re-reading the PR I'm less concerned, however there is one glaring omission  there is no Java API for chaining the writes.  Yes, that is very true and needs to be fixed. I'll do it today.
>> It's also about fairness, sending a massive IO message means that no other writer gets through until the massive message is all over the wire, right?  >Not sure what exactly you mean. A "massive IO message" will be spoon-fed to the kernel, either in chunks that are no bigger than  >- the socket's send buffer (in the case of `Tcp.Write`, i.e. in-memory ByteStrings) or >- the configured `file-io-transferTo-limit` (in the case of a `Tcp.WriteFile`)  >So the connections `TcpConnection` actor will not block the thread for too long thereby allowing other connection >actors to run. Of course, one "massive IO message" might take some time to go out and will therefore prevent other >data from being able to be sent across *this one connection*. However, that is what I'd expect as a user, no?  If you have multiple Actors sending writes to the same endpoint, if they all write many, smaller, writes, they have the chance of interleaving, but if you only send one huge write it till need to be digested by the network before the other messages get processed. Does that make sense?  (So it's not only about the TCPConnectionActor starving other TCPConnectionActors much is it is writers to the TCPConnectionActor starving the writes of other writers to the _same_ TcpConnectionActor. (Tuning fairness of the TCPConnectionActors can be done with the dispatcher throughput setting.  >> Re-reading the PR I'm less concerned, however there is one glaring omission  there is no Java API for chaining the writes.  >Yes, that is very true and needs to be fixed. I'll do it today.  Thanks!
> If you have multiple Actors sending writes to the same endpoint, if they all write many, smaller, writes, they have the chance of interleaving, but if you only send one huge write it till need to be digested by the network before the other messages get processed. Does that make sense?  The use case you are sketching out is unlikely. If you have several actors sending write commands to the same `TcpConnection` actor in an uncoordinated fashion you will very quickly have NACKs coming back since the IO layer doesn't accept more than one pending write. Additionally the order in which their writes get written to the connection is basically undetermined. So, in the very large majority of cases you will have some kind of coordination between writes, i.e. one actor serializing the write commands, establishing order and managing flow control. For efficiency in such a use case the ability to group writes (that are in memory already anyway) into one single command that the IO layer can then transfer to the send buffer as quickly as possible is exactly what this patch provides.
Consider me convinced now then :-)
I'll get the update with the missing Java API in ASAP.
One thing though: I'm not sure I like "CompactWriteCommand" as a name. What's compact about it? (It is not using compaction (like ByteStrings))
I was looking for something that is not "compound", i.e. combined from others. Maybe `SimpleWriteCommand`?
Atomic would be a good name but unfortunately it has already a defined meaning in the concurrency context. Elementary maybe?
I was thinking about `AtomicWriteCommand` as well, but since it is not really "indivisible" (the `ByteString` in `Tcp.Write` might even be compounded) it might not quite fit. I don't have a strong preference though...
Ok, I'll go for `SimpleWriteCommand`. Good enough.
Ok, renamed `CompactWriteCommand` to `SimpleWriteCommand` and added the Java API for write command compounding.
LGTM  Maybe add example use cases to the documentation? 
LGTM (once docs are added)
Ok, rebased onto current master and added section to IO docs. (Sorry for the delay...)
Thinking about the API a bit more: people will want to append, not prepend, in most cases. How can we make that more intuitive?
Classic solution: provide a builder. Or have a 
(Sorry about the mis-comment and the accidental close, no idea how that happened...)  > Thinking about the API a bit more: people will want to append, not prepend, in most cases. How can we make that more intuitive?  I think we have at least these 3 options:  1. Add an (inefficent) general `append` logic. 2. Provide a mutable builder for compound writes. 3. Add a `CompoundWrite.apply(writes: Seq[WriteCommand]): CompoundWrite` overload.  Keeping the tail-based design of `Tcp.CompoundWrite` probably makes sense since it keeps the implementation simple, fast and memory efficient.
New doc chapter updated according to comments and duplicated on the java side.
Id favor option 3 (`apply(Seq)` and `create(java.lang.Iterable)`), then people can build their sequences however they like. A small doc snippet showing that off would then be good to also fix that part of the API against accidental improvements ;-)
Ok, expanded the API with some more construction helpers enabling the convenient building of `WriteCommand` instances from `Iterable[WriteCommand]` and `JIterable[WriteCommand]`.
Great, thanks, Roland! We'd also need a backport to 2.2 for spray. What'd be the best way to get this done?
given the ! marker that should not be possible: 2.2.x needs to stay binary compatible and source compatible
Well, with two small changes we can turn the `!` into a `+`:  1. Turn `WriteCommand` back into a `trait`. 2. Move the `ack` and `wantsAck` members from `SimpleWriteCommand` back up into the `WriteCommand` class.  The `CompoundWrite` class would then receive two hardcoded members      def ack = Tcp.NoAck     def wantsAck = false  and we should be fine.  Without this patch in place for Akka 2.2 spray 1.2 (running against Akka 2.2) would have a significant disadvantage over spray 1.0 and spray 1.1, which we'd really like to avoid.
Sounds like a plan. Please try it out on release-2.2 and run MiMa to verify: well need a separate pull request for that anyway.
Ok, will do.
Alright looks great :)  You'll need to squash the commits into 1 and make sure the commit message adheres to the convention listed here: https:github.com/akka/akka/blob/master/CONTRIBUTING.md  Also, have you signed the CLA? (Don't worry, that's a one-time thing, which lowers the bar for the next contribution as well :))  Let me know if you have any questions and we'll help you out.  Cheers! 
Okay, CLA signed.  The squash is a more interesting problem, and shows my newbietude with GitHub.  (While I've been using it for a year, it's a one-man project, so I haven't had to deal with most of the interesting real-world situations yet.)  I did my edits in my own master branch without even thinking about it; the squash instructions presume that you're editing on a private branch first, squashing *that*, and then committing to your master before the pull request.  It's not clear how one cleans up undesired commits from the master.  Any suggestions for fixing this without major hassle?  The change in question is small enough that the easiest solution may well be to kill my fork and start over, which is fine (probably would only take 15 minutes), but I'm taking this as a learning opportunity...
And reading a bit more carefully -- do I correctly infer that I should open a ticket, and refer to that in the commit comment?  (SOP for code, but I hadn't thought about it for the documentation tweak.)
Hi @jducoeur, Thanks a lot for contributing. We try to track everything with tickets (sometimes we are pragmatic though). I created a ticket for you: https:www.assembla.com/spaces/akka/simple_planner#/ticket:3596  That means that the commit message should be:      =doc #3596 Add paragraph about Agent locality  There are several ways to squash, but in this case it is probably easiest to do: git reset <commit id before your first commit> git add . git commit git push -f origin <your branch>  That will update this pull request.
BUILDLOG?  Just checking why our PR-validator hasn't commented correctly on this PR. 
(kitty-note-to-self: ignore 24153503) pr-validator-per-commit:   - b4ee54cd: [success](https:jenkins.akka.io:8498/job/pr-validator-per-commit/746/): Took 46 min., [pending](https:jenkins.akka.io:8498/job/pr-validator-per-commit/746/): started.   - 49b617de: [success](https:jenkins.akka.io:8498/job/pr-validator-per-commit/749/): Took 46 min., [pending](https:jenkins.akka.io:8498/job/pr-validator-per-commit/749/): started.   - e7caa624: pending: queued. 
PLS SYNCH  Still checking the PR-validator
(kitty-note-to-self: ignore 24153810) :cat: Synchronaising! :pray:
Hmm.  Having difficulty fixing the repo, so I'm going to drop this fork and restart...
Thanks for the review, Viktor, during this change I somehow felt that I was about to lose sight of the way, so I wanted confirmation that I am not completely in the woods. Will finish along the lines outlined above during the next days.
You're very much welcome, I like this direction. Great work Roland, looking forward to see it take shape.
Good job Roland. 
BUILDLOG?  Wondering what the kitteh is up to.
(kitty-note-to-self: ignore 23939546) pr-validator-per-commit:   - 67baeaa7: pending: queued.   - 149d0278:  
(kitty-note-to-self: ignore 23944408) :cat: Synchronaising! :pray:
Thank you for your contribution.  Would you mind signing our CLA?  http:www.typesafe.com/contribute/cla
Done. Thanks David              Gesendet:Montag, 09. September 2013 um 10:02 UhrVon:"Bjrn Antonsson" <notifications@github.com>An:akka/akka <akka@noreply.github.com>Cc:"David Schmitz" <David-Schmitz@gmx.net>Betreff:Re: [akka] ramp up of outdated aries dependencies (#1711)                   Thank you for your contribution.  Would you mind signing our CLA?  http:www.typesafe.com/contribute/cla  Reply to this email directly or view it on GitHub.
LGTM  We have a commit message policy described here: https:github.com/akka/akka/blob/master/CONTRIBUTING.md#creating-commits-and-writing-commit-messages  Would you mind changing the main commit message to something like `!osg Updating Aries dependencies`, with the two versions as sub messages?
Alright? Or do I have to change my git history?
Yes, you have to change the commit with e.g. `git rebase -i HEAD~1` and then reword the commit message and force push the change.
Done. I hope everything is now as needed I will definitly have to dive a bit more into git normally I am using mercurial most of the time.
Hi, something went wrong with your rebase so you have 3 commits instead of just one. I have applied the patch in your name against master and opened a new pull request #1718 
are you running this in a repeat job, with the temporary old vs. new impl check activated?
As it says in the pull request description, I ran it during the night with a temporary assert comparison enabled. 
LGTM, sorry for not reading
LGTM, after running it in a repeat job for a while
It has been running the multi node tests on repeat the whole night without any failures.
The kitteh missed this one for some reason. Ill have to investigate.  PLS SYNCH 
(kitty-note-to-self: ignore 23932342) :cat: Synchronaising! :pray:
updated, my first patch was not correct
These silly bugs are there to remind us of the fallacies of distributed systems ;) Really nice catch, looking obvious after being found.
Looks even better :wink: 
@drewhk I have added the QuarantinedEvent as discussed yesterday: ad121de
Finally found the bug I had introduced. Pruning of the reachability table is in place, so the only missing piece is update of documentation.
failure was not caused by me: https:www.assembla.com/spaces/akka/tickets/3587
Do you think Int would be enough for the version fields?
Great work, Patrik! I have not read every single line (especially not the collateral damage in the other tests) but what I have read looks good. There are some things which might want to be optimized (e.g. changing reachability and then sorting through all of it again when publishingunless I overlooked something), but bug-freedom is certainly a very good first step :-)
I don't understand what you mean by the optimized publishing. It is handled by two different actors with different responsibilities. A published event can also be generated from a received gossip, not only a change made by that node.
Updated documentation: a92b72e
Looks like the arrow heads between `unreachable` and `joining` are bigger than the others.
ah, I picked the wrong arrow head, thanks
Added one more test, as suggested by @rkuhn: c8979de
Very cool. This looks awesome.  Is there a separate ticket for optimizing the merge of the reachability?  One thing that struck me after reading the code. If we remove all nodes that think that node A is unreachable, then won't it magically become reachable again? Hopefully some new observer will mark it as unreachable again. Might cause a bit of hysteresis though.
Removing all codes removes the evidence of unreachability, which sounds as it should be; nothing good will become of trusting the parting words of a leaving stranger ;-)
It still might be that the node is unreachable. I'm just saying that it's something to keep in mind.
@bantonsson I have added ReachabilityPerfSpec in 36228fa to verify performance of merge. Improved some obvious things. Do you think these results are good enough or do you have any suggestions of how to improve?      [info] ReachabilityPerfSpec:     [info] Reachability merge of size 1000     [info] - must do a warm up run, 10000 times (2 seconds, 527 milliseconds)     [info] - must merge with same versions, 10000 times (812 milliseconds)     [info] - must merge with all older versions, 10000 times (1 second, 73 milliseconds)     [info] - must merge with all newer versions, 10000 times (2 seconds, 378 milliseconds)     [info] - must merge with half nodes unreachable, 10000 times (3 seconds, 276 milliseconds)     [info] - must merge with half nodes unreachable opposite 10000 times (2 seconds, 553 milliseconds)
The numbers seem fine, but I'm a bit uncertain as to how they are affected by the test reusing the same `reachability` instances which will then reuse the `table` cache.  How costly is that to create?  During normal execution the `table` cache would have to be build for the deserialized message  coming in over the wire. 
@bantonsson I have added more tests and optimized further: 218eb8f I kept the table, but you the aggregated status is not computed all in one go, which was important. Thanks.
So, won't the _aggregated status_ for all members be computed in one fell swoop by anything that uses `leader` or `isLeader` in `Gossip`? Don't we do that for every gossip we publish? 
good catch, and also in `diffReachable` I will fix
@bantonsson I changed strategy. Now I produce all cached values at once, and all aggregated status are computed in a one pass through the records.  On my machine population of the full cache takes (avg of 10000 iterations): 250 nodes, 625 records => 0.37 ms 500 nodes, 1250 records => 0.75 ms 1000 nodes, 2500 records => 1.65 ms
that means that I will squash and merge, any objections @drewhk ?
(kitty-note-to-self: ignore 24232289) :cat: Synchronaising! :pray:
Yes, I have not reviewed every part of it, but overall looks good! Merge!
great job Martin!
Looks really promising!  - In general: please use more descriptive names in most of the places -- it helps reviewing  - you should test it with the serialize-messages and serialize-creators options on, and marking messages with NoSerializationVerificationNeeded and Props with Deploy.local when needed  There are quite a lot of things we might want to think about:  - journal format needs to be documented eventually (at least for "official" plugins)  - we should make sure that migrating saved snapshots and/or journals between system updates is possible -- not necessarily automagically  - apart from adding snapshot support, markers might be useful, too. -- some rollback functionality might be possible even.  - using journals "externally" -- for example for debugging purposes, taking state data from a live system and launching a dev system using the journals. 
@drewhk thanks a lot for your feedback. I'm on vacation the next two days. Will add my comments when I'm back.  Cheers, Martin
Happy Holiday! :)
> - you should test it with the serialize-messages and serialize-creators options on, and marking messages with NoSerializationVerificationNeeded and Props with Deploy.local when needed  Will do.  > - journal format needs to be documented eventually (at least for "official" plugins)  +1, when akka-persistence is not any longer marked as experimental.  > - we should make sure that migrating saved snapshots and/or journals between system updates is possible -- not necessarily automagically  +1, when akka-persistence is not any longer marked as experimental.  > - apart from adding snapshot support, markers might be useful, too. -- some rollback functionality might be possible even.  +1, separate ticket  > - using journals "externally" -- for example for debugging purposes, taking state data from a live system and launching a dev system using the journals.  +1, separate ticket
@krasserm when you add the changes, please do that separate commits to make it possible for us to review the changes without going through everything again. When review is done you can squash everything into one commit.
@patriknw you mean a single second commit or every little change in a separate commit (as I'm currently [doing locally](https:github.com/eligosource/akka/commits/wip-3574-akka-persistence-prototype-krasserm-feedback))? I was planning to make a single second commit ...
well, it would be rather inconvenient for us to review each change as separate commit, and that will also trigger pr-validator on each commit, so I suggest one or a few commits containing the changes. My main point was to not squash with the first commit until review is done, so that we don't have to go through everything again.
aside from the life cycle hook options this looks very good
LGTM  Very nice
Thanks for your feedback guys. Everything should be covered by the 2nd update now.
LGTM, I vote for merging this today!
Great, shall I also squash everything into a single commit when adding `++ experimentalSettings` to `AkkaBuild.scala`?
yes, but wait until we have some more approvals
@patriknw ok, please let me know when you think it's ready to push
@krasserm Endre will review the changes also
LGTM, if we make a decision about the usage of Enumeration. I'm a bit scared about the resolution stuff, but I don't have any better idea. I opt for merging and we probably discover how it can be made painless after having some experience with the API.
@drewhk the resolution stuff is only needed for reliable delivery e.g. to resume a failed conversation after a JVM crash for example (see [conversation recovery example](https:github.com/eligosource/akka/blob/wip-3574-akka-persistence-prototype-krasserm/akka-samples/akka-sample-persistence/src/main/scala/sample/persistence/ConversationRecoveryExample.scala)); many application won't need that probably. We also discussed *persistent* actor references some time ago (i.e. actor references that keep their uid across incarnation) but decided against them, so sender reference resolution is the only alternative to support reliable delivery via channels. The only thing that users must know whether to use `Resolve.Sender` or `Resolve.Deliver()` is where the sender reference of a replayed message is used when sending via a channel.  
I have read up to Channelneed to run nowand it looks really good. Ill continue review later; for the Message rename Im beginning to like Registered, because it works both while sending and receiving.
Oops, wrong button ...
Based on the recent comments, it looks like there will be another (bigger) round of changes + review. In addition to that, I'm currently also working on enabling processors to support failures during replays (which requires to ignore the remaining replayed messages of the current recovery, restart the processor after the last replayed message and run another recovery after restart). I have that already working locally but want to redesign the `Processor` a bit to make the processor behaviors and the state transitions more explicit. If there are no objections I'd like to add this feature as well in the next commit.   Thoughts?
that is fine with me
Regarding the renamings, the core concept of akka-persistence is to persist messages instead of current state. Therefore having a `Persistent` wrapper for messages i.e. (`Persistent(message: Any, sequenceNr: Long)` instead of `Message(payload: Any, sequenceNr: Long )`) would make that very clear and explicit. `Registered` is a bit overloaded IMO, with no clear relation to persistence. Furthermore, I more and more tend to rename `Processor` to `PersistentActor`. WDYT?
Yes, that certainly is also a good alternative. Registered mail is something which probably only few non-native English speakers are intuitively familiar with, but it would have fit together with Envelope so nicely ;-)
I'm for sure not a native English speaker so I vote for Persistent.
Well, technically, to avoid confusion with persistent collection, "durable" is perhaps more appropriate. But considering the module-name is "persistence" Persistent might be the way to go.
LGTM! (the G means Great) You also included that one bug I needed to find to make my Saturday enjoyable, so all is fine :-)
@rkuhn hah, test passed :) will push soon
Thanks all for reviewing.  The latest commit should now cover all the raised issues in the recent discussions. Furthermore, the new processor implementation now makes the processor states and transitions more explicit. In addition to that I added a feature that allows processors to fail with replayed messages during recovery (i.e. allow failures for messages that have previously been processed successfully).
The state machine looks a lot cleaner like this. Now would be a good time to squash and merge.
The squashed commit also contains the latest discussed changes.
Thanks for merging. When will the artifacts be published to the Akka snapshot repo? When will the user and API docs be published?
The machine which did these jobs is currently not running AFAIK, it could take a few days to fix this; Ill let Patrik decide how exactly to get things published for your purposes.
Great to see this merged. If things are not automatically published as it should I will take care of it on Monday.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/769/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/769/
Ack! Commented on the wrong PR, please go through the comments I made here, they should mostly apply to this PR: https:github.com/akka/akka/pull/679
I see 2 problems in generate_config_with_secure_cookie.sh 1) include and comment about akka-reference.conf is wrong 2) property path of `secure-cookie` and `require-cookie` is wrong  The samples/start script is not up to date. Why do we have that script? We only have one microkernel sample, and it is documented to be started with `bin/akka`.
True. So the question is, should we delete the start script? And, should we keep the generate_config_with_secure_cookie.sh? We might as well just add a link in the docs on the best practices of generating such a cookie?
Remove the samples/start script. The cookie script is still useful, if updated.
aside from the missing `"`: :+1: 
You're right, it should be a quoted string. fixed!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/771/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/771/
Hi,  I have reviews all your comments and am almost done with the corrections. I was a bit confused by some of them as they do not seem to match the commit I have pushed. Don't know what happened...  I will try to update the branch during the weekend.
What assembla ticket is this?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/795/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/795/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/796/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/796/
(kitty-note-to-self: ignore 17215110) :cat: Synchronaising! :pray:
Thanks. Have you signed the Akka CLA?
@jboner No, I didn't. How can I do so?
We have the same problem in Future.scala. Shall I fix both?
Right Roland, I didn't see that one. I think it would be better as these are the sort of things that can create real headaches to Eclipse and, as a consequence, to the Scala IDE plugin (e.g., wrong errors being reported in the editor...)
okay, I fixed both cases, lets see how much of Eclipses misbehavior can attributed to this issue ;-)
One thing I'm sure you'll get is hyperlinking to work correctly when a user links the sources to the akka-actor binaries. That alone is enough to justify the change ;)
This should be a compiler flag (enforce source location and package name) so we can know which ones are fcuked
Yep, good idea!
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/772/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/772/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/773/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/773/
PLS REBUILD ALL
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/774/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/774/
Alright, this should be good to go now, waiting for kitteh approval!
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/790/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/790/
It does (or is supposed to) support /a/b/c/actor and on/off as of a couple days ago, but didn't originally.
should I be publishing a jar so you don't need to copy in all the sources, or is that to avoid dependencies?
We can't have external dependencies in akka-actor. Therefore I have embedded the code. You can continue work in your repository and we will copy it. The long term goal is to push it down to scala library.  On Fri, Nov 18, 2011 at 3:25 PM, Havoc Pennington < reply@reply.github.com > wrote:  > should I be publishing a jar so you don't need to copy in all the sources, > or is that to avoid dependencies? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116#issuecomment-2789590 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Added some comments :-)  Great work Patrik! (& Havoc!)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/775/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/775/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/776/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/776/
The docs probably need to be updated, otherwise LGTM. Great work!
Actually there is no documentation yet for the case "outgoing connection could not be established". But from reading the current docs I would assume that I receive a `CommandFailed` event if the Connect command fails.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/764/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/764/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/781/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/781/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
LGTM, thanks Mathias!
`override` is now added ...
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/763/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/763/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/763/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/763/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Can someone reproduce this locally? Something special on the Jenkins box?
I will try to run it tomorrow on my machine. Just ping me so I don't forget :)
LGTM.  The second Kitteh failure is just bad reporting from the Kitteh. They both point to 763 but the second one should be 765.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/765/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/765/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/770/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/770/
I +1 all of Patriks comments. Apart from that LGTM.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/770/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/770/
Alright, fixed the issues we've talked about here and added the test(s).
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/792/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/792/
Well, its beginning to turn around in circles ;-) So, lets have a look at the basic constituents:   - a factory, which can be instantiated without constructor arguments  - a key, which will typically be in a static field  - the extension object itself  The user may choose to use the same object for more than one of these purposes. What else do we want? I liked the idea of requiring a certain interface so that e.g. help or configuration assistance may be printable from the REPL or generate better error messages etc. This could be on the extension object (the most obvious choice, I believe) or on the key, but then it would need the actual extension object to do its job, because the key shall be global; the factory is just a one-off registration thing.  Comparison:   - before your change the extension was the factory (with init() for compensating the no-constructor-arg deficit) and produced the key  - with this change the factory produces the key which produces the extension  So, the initialization is possibly a bit cleaner with your change, but I would suggest to   - rename ExtensionProvider to ExtensionKeyProvider  - rename Extension to ExtensionKey  - add interface Extension for the actual extension object  - sprinkle some doc-sauce on it.
Good improvements. Great job Viktor. Is getObjectFor Java friendly?
Thanks Patrik, the getObjectFor only works for Scala, the Java way of _creating_ your own extensions is like this: https:github.com/jboner/akka/commit/603a8ed034f0489cb1330e81d1159640b9432e16#commitcomment-745865  I don't expect this will be very common, so I don't think it'll be a problem.
Should I merge it in?
youve got my smiley: :-)
Do it!  On Fri, Nov 25, 2011 at 11:19 AM, Roland Kuhn < reply@reply.github.com > wrote:  > youve got my smiley: :-) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/123#issuecomment-2873169 >
Already done :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/767/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/767/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/768/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/768/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/768/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/768/
I'm glad you like it! :)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/798/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/798/
3:31AM in Sweden!!!!   On Thu, Apr 4, 2013 at 6:30 PM, Viktor Klang () <notifications@github.com>wrote:  > hehe: http:t.co/wDqnzDjmgx > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1297#issuecomment-15934682> > . >
The Klang never sleeps ;)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/801/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/801/
Fixed and rebased.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/801/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/801/
PLS REBUILD ALL
Squashed and rebased. Awaiting Kitteh.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/865/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/865/
Can I press the green button? Only thumbs up from @patriknw and the Kitteh.
Just awaiting kitteh-validation
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/788/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/788/
Code looks great, some documentation needed.
Thanks for feedback. Fixed those (some as FIXME). Merging this into master now.
Wicked, great work.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/799/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/799/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/802/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/802/
This is a cherry pick with some resolved conflicts. The Kitteh approves, so I'll go ahead and merge.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/777/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/777/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
It breaks `SerializationCompatibilitySpec`.  Otherwise LGTM. 
I think the doc tests should be updated to not use `isTerminated`. Any internal usages that should be updated? Any tests that should be updated?
A note in the migration guide would be nice.
I think the deprecation and the refactoring should be kept separate as one is changing user API and one is internal implementaiton details.
Also, SerializationCompatibilitySpec seems broken and needs fixing.
I still think the doc tests should be updated to not use isTerminated, since that is user API. Create another ticket if you don't want to change the internals.
True, I'll clean up the doc tests
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/789/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/789/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
PLS REBUILD ALL
(race condition between push and start of kitteh)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/791/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/791/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Alright, should be good to go now!
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/794/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/794/
Only some minor comments. Great to have the samples flying again!
Thanks for your comments. I'll amend accordingly and push updates asap.
When all comments are handled, please merge this pull request into master.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/785/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/785/
The patch itself LGTM, but is this always going to be a win? Im thinking about frame-length encoded data where this will just result in more message sends an no clear benefit (unless the frames are very small). Do you have benchmarks?
With this patch we still read all available bytes up to the configured `direct-buffer-size` (default: 128 KB) in one go. This means that in all cases where less than 128 KB are ready to be read the behavior is unchanged. If more than 128 KB are ready we used to aggregate all of them (up to the `max-received-message-size` limit, which has a default of `unlimited`) before sending out the first message. With the patch we now send one message per 128 KB chunk.  The benefits we see are increased concurrency (as the application layer can already work on the first chunks while subsequent ones are still read from the kernel read buffer) and more determinism as the application can be sure to always receive compact ByteStrings, which have better character-level iteration performance. This also better suits the character of the TCP layer as a low-level implementation.  One thing that we should still improve is to replace the `max-received-message-size` setting with a `receive-throughput` setting (with a default of maybe `10`), which, in analogy to the one on the UDP side, limits the number of chunks read and sent off as messages before returning to the event loop.
Okay, thanks for the explanation, I see where my reasoning went astray earlier.
One more thought: I think this patch should enable us to change      case class Received(data: ByteString) extends Event  to      case class Received(data: CompactByteString) extends Event  This should yield quite a significant performance benefit for byte-level iteration, since the JVM will happily inline `CompactByteString.apply` (because this method is only implemented by one class (`ByteString1C`)) whereas `ByteString.apply` will not be inlined if the `ByteString1` and `ByteStrings` classes are also loaded, which will frequently be the case (at least that is my current understanding of Hotspot's inlining behavior).
@sirthias, inlining by HotSpot -server is more sophisticated than that. It will inline as long as only one class type is used in that call site. If you use two types in that call site, then it will still use an optimised call path (but a bit slower). More than 2 and it's considered a megamorphic call site and performance goes down significantly.
I'm not so sure that changing signatures alone will make such a big difference. The most important thing wrt hotspot is IMO if the call-site is actually polymorphic or not. If it is not (and with this PR it shouldn't be) hotspot will notice by counting invocations and be able to generate a faster invocation or inline the target.
@ijuma, thanks for this clarification. Makes sense. Still, I think changing the signature to the more specific ByteString type would be good as it better signals to the API user what data the TCP layer will produce.
Fair enough. It also enforces a monomorphic call sites.
Depends on whether you see it as an implementation detail or not. Apart from that I'm not so sure if you should invite anyone to program against subclasses of `ByteString` in general.
Im with @jrudolph on this one: by exposing the more specific subtype we limit the implementation to always honor this contract, which is not 100% obvious to me at this point.
+1, ByteString is more future proof.
Alright, guys, flash in the pan. Let's keep the `ByteString` then.
Wow, thats an impressive piece of work, thanks! I skimmed the diff and didnt find anything obvious, but maybe I was biased because the things you mention in the commit message are very clear bugs with their fixes ;-)
Thanks, as usual, memory leaks are obvious once you have found them, which isn't always that easy.  On Fri, Dec 2, 2011 at 10:12 AM, Roland Kuhn < reply@reply.github.com > wrote:  > Wow, thats an impressive piece of work, thanks! I skimmed the diff and > didnt find anything obvious, but maybe I was biased because the things you > mention in the commit message are very clear bugs with their fixes ;-) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/134#issuecomment-2987906 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Great work, I'm done commenting :-)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/783/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/783/
Overall looks great, it's just completely void of docs, could you please add some documentation to the case-classes etc?  Thanks!
Yep will do   
Added some docs and removed the options that made no sense within akka
What the status of this guys, if I need to cut an RC3 of 1.3 should this go in or not?
There is one tiny fix in there I'd like to see make it into 1.3  in the testkit a rename of a private var from end to _end in this commit d92ef8c
Alright, so it's only that commit that should make it into 1.3?
I think the rest are more complete in @kro 's pull request #115
Ok, so I'll close this, merge in 115 and the aforementioned commit for RC3
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/780/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/780/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I'd vote for per-connection configuration of the "linger" timeout, i.e. the time within which the handler for the connection has to actively close the connection after having received a `PeerClosed` event. During this time the connection still accepts `Write` commands (but will of course not generate `Received` events anymore). If the timeout expires before the connection has been actively closed the `TcpConnection` should close it itself.  One global (per ActorySystem) setting for this appears to be not flexible enough, since I might have HTTP connections for which this facility is not needed as well as other ones (e.g. for Akka Remoting). For a per-connection setting I'd vote for extending the existing `options` Traversable, which would have to be changed to allow for non-socket-options.  Also, we could make the connection immediately "auto-closing" if the "linger" timeout is undefined, thereby preventing accidental resource leaking.
Or have make it immediately auto-closing if the timeout is 0 and never auto-close if the timeout is undefined.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/782/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/782/
A timeout for automatically closing after `PeerClosed` seems over-engineered to me: either I want the immediate close semantics we had before, or I want to control closing myself. To me this reads like the handler should inform the connection actor what it wants, i.e. in the `Register` message and with a simple boolean.
I like the idea of configuring this in the `Register` message. If the default is "auto-close on PeerClosed" I'm also fine with no timeout. Then we can also leave the `PeerClosed` event as is (i.e. implementing `ConnectionClosed`).
I added the parameter as discussed and added a bit of documentation. How do we support the Java side of the new `Register.keepOpenOnPeerClosed` parameter? Another overload for ``TcpMessage.register`` with an additional boolean parameter?
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/814/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/814/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Rebased to merge cleanly.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/815/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/815/
Thanks, Johannes! Yes, the Java side should be an overloaded `register` method, and please update `java/io.rst` accordingly, then Ill merge it in.
I added the Java overload.
thanks! but now youll have to rebase to make the KITTEH happy 
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/815/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/815/
PLS REBUILD ALL
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/841/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/841/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/778/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/778/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
I think "fully functional" might be a bit of an overstatement - "largely functional" seems a little more accurate. Many pipeline stages are going to have to be mutable in order to do any real work. So, pushing a command or an event through the pipeline is going to cause all sorts of side effects. I'd think that a fully functional design would require that all pipeline stages be immutable. So, if they needed to maintain state, instead of storing it in a mutable structure, they'd return a new version of themselves to be inserted into the pipeline in their place (or something else that doesn't involve side effects). That model would probably be harder to work with (especially for imperative programmers like myself) and might be slower, so, I'm not saying that that model would be better, but, it would be more functional.
LGTM, only minor comments
@DaGenix wrt. fully functional: yes, the PR description is not correct, I changed it. Although I must say that according to my tests they actually are fully functional ;-) (I really dislike that overloaded term, isnt there a better one?) Your reasoning is correct, and I think while reifying side-effects sounds like a good idea in theory it is not going to work out in this case because it is too inefficient.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/834/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/834/ <br> ![sad kitty](http:cdn.memegenerator.net/instances/100x/31464013.jpg)
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/835/
jenkins job akka-pr-validator: Success - https:jenkins.akka.io/job/akka-pr-validator/835/
This is good!
Different kinds of configuration for different tests (e.g. round-robin vs. random router) means I can't have a single conf for all tests running on a certain machine, right?
@vigdorchik I don't see the whole picture of what needs to be configured, so I'm not sure what to recommend. I think the things things corresponding to  val node1Config val node2Config can easily be specified as system properties (-D flags) when starting the jvm on the specific node. System properties are used first.  The you have properties like /app/service-hello.remote.nodes = ["localhost:9991"] Was this what we talked about, that should be in zookeeper?  There is also substitions. I'm not sure if that could be useful for this. See https:github.com/havocp/config/blob/master/HOCON.md
Aside from my minor comments I think this is great!
Shall I merge this, or will you do it Jonas?
You can merge it. But are the remote tests running now?  -- Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner  On Dec 9, 2011 9:51 AM, "patriknw" < reply@reply.github.com> wrote:  > Shall I merge this, or will you do it Jonas? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/135#issuecomment-3076701 >
True, I thought Roland said it was fixed now, but that was probably not in master. I'll wait for the tests.  On Fri, Dec 9, 2011 at 10:16 AM, Jonas Bonr < reply@reply.github.com > wrote:  > You can merge it. But are the remote tests running now? > > -- > Jonas Bonr > CTO > Typesafe - Enterprise-Grade Scala from the Experts > Phone: +46 733 777 123 > Twitter: @jboner >  On Dec 9, 2011 9:51 AM, "patriknw" < > reply@reply.github.com> > wrote: > > > Shall I merge this, or will you do it Jonas? > > > > --- > > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/pull/135#issuecomment-3076701 > > > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/135#issuecomment-3076855 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Cool. Thanks. Do it when they run.  On Fri, Dec 9, 2011 at 10:48 AM, patriknw <reply@reply.github.com> wrote: > True, I thought Roland said it was fixed now, but that was probably not in > master. I'll wait for the tests. > > On Fri, Dec 9, 2011 at 10:16 AM, Jonas Bonr < > reply@reply.github.com >> wrote: > >> You can merge it. But are the remote tests running now? >> >> -- >> Jonas Bonr >> CTO >> Typesafe - Enterprise-Grade Scala from the Experts >> Phone: +46 733 777 123 >> Twitter: @jboner >> On Dec 9, 2011 9:51 AM, "patriknw" < >> reply@reply.github.com> >> wrote: >> >> > Shall I merge this, or will you do it Jonas? >> > >> > --- >> > Reply to this email directly or view it on GitHub: >> > https:github.com/jboner/akka/pull/135#issuecomment-3076701 >> > >> >> --- >> Reply to this email directly or view it on GitHub: >> https:github.com/jboner/akka/pull/135#issuecomment-3076855 >> > > > > -- > > Patrik Nordwall > Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts > Twitter: @patriknw > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/135#issuecomment-3077088    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
I will do that today, am still verifying first with my simplified same-JVM setup.  On Dec 9, 2011, at 10:48 , patriknw wrote:  > True, I thought Roland said it was fixed now, but that was probably not in > master. I'll wait for the tests. >  > On Fri, Dec 9, 2011 at 10:16 AM, Jonas Bonr < > reply@reply.github.com >> wrote: >  >> You can merge it. But are the remote tests running now? >>  >> -- >> Jonas Bonr >> CTO >> Typesafe - Enterprise-Grade Scala from the Experts >> Phone: +46 733 777 123 >> Twitter: @jboner >> On Dec 9, 2011 9:51 AM, "patriknw" < >> reply@reply.github.com> >> wrote: >>  >>> Shall I merge this, or will you do it Jonas? >>>  >>> --- >>> Reply to this email directly or view it on GitHub: >>> https:github.com/jboner/akka/pull/135#issuecomment-3076701 >>>  >>  >> --- >> Reply to this email directly or view it on GitHub: >> https:github.com/jboner/akka/pull/135#issuecomment-3076855 >>  >  >  >  > --  >  > Patrik Nordwall > Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts > Twitter: @patriknw >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/135#issuecomment-3077088  -- [scala-debate on 2009/10/2] Viktor Klang: When will the days of numerical overflow be gone? Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January 2038
What's the status here?
This one have been ready for ages. Was initially waiting for Roland to enable multi-jvm tests. Should be merged in. 
Yes, that looks good. Care to document it in the rst?
committed docs, check it out
Are we mergeable? ;)
jep, ya got ma smiley :-)
Any more review of this, or shall I merge it to master?
I didnt have time to read the actual docs, but the code changes look good :-)
Merging this to master now. Feel free to comment afterwards if you see anything that should be improved.
Wicked. It is all coming together so nicely. I'm so very proud of this. Akka 2.0 (and even more 2.1) will really be the way actor programming is meant to be.  
It is done: routers work locally and with remote children deployment, all tests green. Speak now or forever hold your silence ;-)
I haven't inspected every line of change, but I like what I have seen.
okay, guys: I have fixed all review comments brought up so far, and Id like to merge this real soon, so Ill start the merge after lunch unless someone objects.
Perhaps I've just had too much cold medicine, but couldn't we remove the dispatcher field from DefaultPromise by moving it into the MessageDispatcher class? I gave it a try and the tests pass: https:gist.github.com/1469887  
Wouldn't matter, it still needs to close over its parent so the field would still be there, or?
Ah yes, used javap to check the results and it does add a field.
Btw, I added some more tests to the FutureSpec.
Looks all good to me, great changes in there. We might want to add helper methods in testkit that will automatically use the default timeout for blocking methods, something like:  def result[T](future: Future[T]): T = Await.result(future, timeout.duration)  since testing is a perfectly valid use of those methods (although it might be possible to use specs2 to make nonblocking tests... something to try out sometime).  Other then that, nothing jumps out at me. Great work!
Yes, I noticed you implemented the pending tests I added. I keep forgetting about those, thanks!
Wicked! Great job Viktor!
I definitely need reviews here after last nights crazy hakking
Its a huge chunk, but what Ive seen looks good. Youll encounter some conflicts in core code, though.
Jonas, I agree with Viktor here: special-casing just those routers does not gain much, compare      .withRouter[RoundRobin]     .withRouter(RoundRobin(2))  The second one has much more information, and if you leave out the number, you will have to set one in config unless you like ConfigurationExceptions. I think that is already good enough, no?
Sure. I have what Viktor wanted here now I think but I can just delete it all:    def withRouter[T <: RouterConfig: ClassManifest] = {     val c = implicitly[ClassManifest[T]].erasure     val error = () => throw new IllegalArgumentException("Class [" + c.getName + "] is not a valid router - has to subclass RouterConfig and have a no-arg constructor")     if (c isAssignableFrom classOf[RouterConfig]) {       val routerConfigClass = c.asInstanceOf[Class[_ <: RouterConfig]]       val factory = try { routerConfigClass.getDeclaredConstructor(Array.empty[Class[AnyRef]]: _*) } catch { case e: NoSuchMethodException => error()}       val router = factory.newInstance(Array.empty[AnyRef])       copy(routerConfig = routerConfig)     } else error()   }   On Wed, Dec 14, 2011 at 3:51 PM, Roland Kuhn <reply@reply.github.com> wrote: > Jonas, I agree with Viktor here: special-casing just those routers does not gain much, compare > >  .withRouter[RoundRobin] >  .withRouter(RoundRobin(2)) > > The second one has much more information, and if you leave out the number, you will have to set one in config unless you like ConfigurationExceptions. I think that is already good enough, no? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/155#issuecomment-3140468    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
    import ReflectiveAccess._     val c = implicitly[ClassManifest[T]].erasure     createInstance(c, noParams, noArgs) match {       case Left(t) => throw new IllegalArgumentException("Class [" + c.getName + "] is not a valid router - has to subclass RouterConfig and have a no-arg constructor", t)       case Right(r) => r     }
We should be able to bring this back in a better way once proper Scala reflection is available, but I still think that saving two characters might not be worth the complexity.  Anyway, this branch has a lot more changes than this method addition, so might I suggest that you leave out that one piece and apply the rest?
Ok. Deleted it. 
Just did that. Ready for merge.    --   Jonas BonrCTO   Typesafe (http:www.typesafe.com/) - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner (http:twitter.com/jboner)     On Wednesday 14 December 2011 at 16:16, Roland Kuhn wrote:  > We should be able to bring this back in a better way once proper Scala reflection is available, but I still think that saving two characters might not be worth the complexity. >   > Anyway, this branch has a lot more changes than this method addition, so might I suggest that you leave out that one piece and apply the rest? >   > ---   > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/155#issuecomment-3140836 >   >  
what about removing the whole directory structure? would be a bit misleading to people browsing github to leave it in.
I didn't do that because it contains transactors. We have similar with other stuff, such as spring, camel.  /Patrik  14 dec 2011 kl. 16:57 skrev Roland Kuhn<reply@reply.github.com>:  > what about removing the whole directory structure? would be a bit misleading to people browsing github to leave it in. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/154#issuecomment-3141733
Okay, so we should do it later.
Have made changes based on comments. I think it's good enough now to go in the download and be used for apps.
Excellent work Pete, as always. +1 :-)
Cool. Will merge now.
It's the Kraken!!!
Shouldn't we merge this in now? +1 :-)
yup, go ahead
This is a actually a chapter that would be more readable if split into Java/Scala, since it's more sample than text. WDYT?
Split it up as we've done for the other parts. We don't have time to invent new ways of solving this for M1. Open a ticket to unify the docs for 2.0
I have split it into java and scala. Created ticket #1493 Revise documentation structure.
You've got my simley and +1 :-)
Very good. :+1:
:+1: Also I need the changed transition spec or else my branch fails now that I wipe the state when a node joins a cluster.
Alright, then I merge this now. I have create a ticket for the related issue I found: http:www.assembla.com/spaces/akka/tickets/2290
Oops, I wasn't thinking and force pushed an amended commit. Do you mean something along the lines of this?
Sorry, I probably should have actually looked at the Duration API.
Sorry for the delay on this! Could you please sign the CLA so I can merge it in?  http:typesafe.com/contribute/cla/  Thanks!
I just signed it.
Great! Thank you very much for this contribution!
Actor failures are logged for good reason, and your change has nothing to do with failures to send to remote actors. Also, your change breaks actor semantics, which is an absolute no-go. ActorCell is not the place to start hakking 
I touched 2 lines. One line changes the logging level; it surely does not change semantics. That change is the more important one (to me).
Touching two lines within that specific file has the potential to wreak havoc: why have you commented out that other line?  Also, if that first line is so important to you, you should give some motivation for it. And frankly, I find the title of this pull request a bit misleading, because the discussion was just a monolog: the title impliesor at least strongly hints atthat consensus had been reached, be that concerning the existence of a problem or a proposed solution.
The title was not intended to imply anything beyond providing a point of reference, so the motivation would be clear.  The main purpose is to control the verbosity of logging. I expect that changing the verbosity from Error to Warning would not disrupt the integrity of the Akka product.
aside from minor comments it looks good to me. +1 :)
Cool. Fixing issues now. 
Pushed updates. Merging into master now. Thanks for review. 
Looks good. TypedActor is lookin' nice and purty.
Alright, all commented things fixed and pushed
Looks great. Only a few minor comments.
also rewrote ActorRef ScalaDoc
good, minor nitpicky comment
What you are doing is essentially making all the cluster code single-threaded.  Before, this was done in parallel:  * the gossip management - backed by N number of actors and a round-robin router (where N was configurable) * the leader tasks * unreachable node reaper  Now all of this is done sequentially - task by task. This also means that one task can stop progress for other tasks. F.e. gossip management vs leader actions etc.   Since collisions and retries in a CAS operation is *very* rare - the parallelism was almost "free" - fully lockless. Funneling everything through an actor is essentially the same thing as taking a big giant lock on 'this'.  How can we be sure that this is not going to turn into a scalability/performance bottleneck in the future, when we are running lots and lots of nodes? It might be ok, it just feels weird to go from fully lock-less/concurrent to single threaded without knowing what we need. I would feel better about it if we had test suites that could verify this on 1000 nodes.   
I don't think you are right, everything, except the side-effecting (such as notifying listeners and sending gossip back) was done in the CAS scope, so it was serialized anyway, with the risk of contention and doing the same thing several times when actually trying to do things in parallel.
Since no time consuming operations, only minimal state shuffling, is done in the "CAS scope", the risk for collisions and retry is very rare, so believe the "normal" execution path is close to "free".   I do believe that a concurrent, (mostly) collision free, CAS operation is orders of magnitude faster single threaded code that has to go through 2 queues and 4 handoffs to be executed.   If you can show that collisions is the norm rather than the exception, then I would agree that making it single-threaded is a good idea. 
the bottleneck for this will always be remoting, a local actor can probably handle 1 million messages per second  Previously the messages were received by actors also, so I don't see the difference. Well the difference is that previously there were a pool of actors, that were serialized by the CAS. Why have pool of actors for something that isn't possible to do in parallel? 
Ok. Patrik and I discussed and he convinced me that it will be good enough to use actors all the way down.  Actors: I want to believe - http:is.gd/Lo6WL4
Haha, excellent! I'll look at the PR ASAP.  But I am not surprised since Patrik is one hell of a guy
I still know that CAS is order of magnitude faster than our actors, but I got convinced that it doesn't matter in this case, actors are good enough. Need to take the leap of faith. 
Yup, if it doesn't work out I know who'll revert it ;-)
LOL. But I'm sure it will work. Patrik is just too much 'one hell of a guy' to be wrong. I'll remember that. Yet another guy to be scared of in this office. :-P
hehe, true! :D
thx guys, I feel inspired by your awesomeness
small note: it looks like LargeClusterSpec runs better with this, when there are merge conflicts, less time to convergence, less needed messages  probably due to less conflicts due to things not accidentally "reordered" :-)
thx, I'll do one more mechanical change, splitting this gigantic file into several smaller files
I synced this with master locally and pushed to master.
Is this ready to be merged or?
yes, but I have based the refactoring branch on this branch, so I'm not sure in what order I should merge in stuff, any suggestion? should I merge the refactoring into this and then merge to master? or should I merge this to master first and then merge master with refactoring  perhaps it doesn't matter
It's probably easier to merge this first into master, then sync the other branch with master, then merge that one.
Hi Peter, can you synchronize with the current master so it can be automatically merged?
I have rebased the branch.  On Wed, 20 Jun 2012 23:51:07 +0200, viktorklang wrote: > Hi Peter, can you synchronize with the current master so it can be automatically merged? >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/555#issuecomment-6468381
Wow, that is an amazingly bad bug. And they don't even intend to fix it?
it's just marked as duplicate but I can't find the thing it's supposed to be a duplicate of. this isn't all that pretty but it does work.   At least it's documented :) I think their logic is this:  You should follow some rules to write happy multithreaded code with MQ:    * You MUST NOT access the same data from multiple threads. Using classic MT techniques like mutexes are an anti-pattern in MQ applications. The only exception to this is a MQ context object, which is threadsafe. You MUST create a MQ context for your process, and pass that to all threads that you want to connect via inproc sockets.     * You MAY treat threads as separate tasks, with their own context, but these threads cannot communicate over inproc. However they will be easier to break into standalone processes afterwards.   * *You MUST NOT share MQ sockets between threads. MQ sockets are not threadsafe. Technically it's possible to do this, but it demands semaphores, locks, or mutexes. This will make your application slow and fragile. The only place where it's remotely sane to share sockets between threads are in language bindings that need to do magic like garbage collection on sockets.*
"MQ sockets are not threadsafe. " there's a big difference between race conditions and unsafe publication.
I would like to see the commit message to be revised to indicate which bugs were fixed. Also, test cases for those bugs. Also, placing them in separate commits would make it read better. And, why those extra empty lines? 
Anything new here Ivan?
I don't know how I would test this with akka dispatchers. It's not something that is reproduced easily, i could possibly reproduce the issue using threads to show it is there, the error only showed up every 5-10 test runs. I think the commit message can be revised when the code is being merged in but I can't change a commit message of a published commit or can I?  The loop is one chunk of work and has its own commit, it's one fix for all the issues previously discussed.  the extra lines are purely for readability.  
Ok, I'll merge into 1.3
Alright, pushed updates.
+1 :-) Now I'm happy with it.
Awesome. Thanks Roland.
Jonas, you told me not to work, remember? ;-)  (Im just trying to stay under the radar while reading mail)
Sorry. I thought it was Roland since we discussed it. Thanks Viktor. 
I decided to remove the karaf testing stuff as even the maven build broke. Sticking to plain PaxExam I created a working sample build [here](https:github.com/muuki88/sbt-paxexam-example).  However my akka build doesn't work as the scala compiler complains about missing classes:  ```bash [info] Compiling 4 Scala sources to /home/muki/Development/git/akka/akka-osgi-tests/target/test-classes... [error] /home/muki/Development/git/akka/akka-osgi-tests/src/test/scala/akka/osgi/test/LocalActorTest.scala:25: object duration is not a member of package scala.concurrent.util [error] import scala.concurrent.util.duration._ [error]                              ^ [error] /home/muki/Development/git/akka/akka-osgi-tests/src/test/scala/akka/osgi/test/LocalActorTest.scala:82: value seconds is not a member of Int [error]  Note: implicit value timeout is not applicable here because it comes after the application point and it lacks an explicit result type [error]     implicit val timeout = Timeout(5 seconds) [error]                                      ^ [error] /home/muki/Development/git/akka/akka-osgi-tests/src/test/scala/akka/osgi/test/LocalActorTest.scala:83: could not find implicit value for parameter timeout: akka.util.Timeout [error]     val result = Await.result(pong ? Ping, timeout.duration) must be(Pong) [error]                                    ^ [error] three errors found ```  It seems that my project isn't build with 2.10.0-RC1, but I couldn't figured it out why. I tried to add extra settings, without success.
Duration and friends moved into scala.concurrent.duration in RC1, you'll have to update your tests accordingly.
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/54/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/54/
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/55/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/55/
Noticed this while cleaning up pull requests: what is the status? The latest build failure looks like a forgotten comma in AkkaBuild.scala, which should not be hard to fix.  Im trying to assess what exactly this test suite covers (not an OSGi expert) and what it would take to pull it over the finish line.
PaxExam doesn't seem to work pretty well with sbt. I wasn't able to fix this and the guys on the paxExam mailinglist couldn't help me either. :(
So, is your conclusion that this branch is a failed experiment after all? In that case we should close it.  Thanks for the quick response!
I'm afraid yes. I would be really nice to provide integration tests with osgi. However paxExam is itself not stable enough to be used. Sorry :(
no problem, thanks a lot anyway!
A few notes I forgot:  * The tests run currently with Pax Exam and Maven. I think its not too difficult to port them to sbt. * There is no osgified scala library in maven (I found none), so I used the scala-ide update site * To get the test running I had to use some deprecated methods (`felix().version(..)`) . The documentation was a bit unclear about what to do instead 
It would be awesome if you could port the test execution from Maven to sbt.
What would be nice in addition is to have an akka-osgi-tests subproject for Akka that verifies all Akka bundles (all that are selected to be osgi-ified) that we can execute as a task in our release script.
I will add all osgified akka bundles after I switched  to sbt successfully. There's one important thing left. Currently the akka projects already published to Maven are being tested.
Yeah, I think it would be beneficial to not have to do a local publish before release just to run the test. is it possible to obtain the artifacts/bundles that are created by the OSGi plugin
Yeah. It's possible to provision bundles from files. Hopefully pax exam does resolve relative paths. Than it's just building the bundle and executing the test with a relative path from the project root.
I think it would be possible to construct absolute paths using the sbt setting `baseDirectory`.
I need help with sbt as there are dependency conflicts and I have no idea how to fix them. I will be away for the weekend no hurry. See  commit message below for details  ## Commit Message formatted Changed osgi test framework to Karaf. Using [paxexam-karaf](https:github.com/openengsb/labs-paxexam-karaf/wiki)  ### Dependency conflicts  Still not running due to dependency conflict on _commons-io:commons-io:2.0.1_ and  _org.apache.commons:commons-io:1.3.2_ **Error:**  ```java  {file:/home/muki/Development/workspace_play/akka/}akka-osgi-tests/*:update:  sbt.ResolveException: unresolved dependency: org.apache.commons#commons-io;1.3.2: java.text.ParseException:  inconsistent module descriptor file found in 'http:repo1.maven.org/maven2/org/apache/commons/commons-io/1.3.2/commons-io-1.3.2.pom':  bad organisation: expected='org.apache.commons' found='commons-io'; ``` [pom.xml](https:github.com/openengsb/labs-paxexam-karaf/blob/master/pom.xml) which declares dependencies.  ### Wrong formatted versions (solved)  The `scalatest.org` bundle exports it's dependencies in an unproper way, so karaf can't deploy the bundle. The following exception is thrown  ```java  karaf@root> 2012-08-03 09:54:36,676 | ERROR | .pax/exam/deploy | fileinstall                      | 6 - org.apache.felix.fileinstall - 3.2.4 | Failed to install artifact: /home/muki/.pax/exam/deploy/abac942f-8cef-45b9-aff2-6a896987415b_1.9-2.10.0-M5-B2.jar org.osgi.framework.BundleException: Could not create bundle object. 	at org.apache.felix.framework.Felix.installBundle(Felix.java:2592)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.Felix.installBundle(Felix.java:2443)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.BundleContextImpl.installBundle(BundleContextImpl.java:129)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.fileinstall.internal.DirectoryWatcher.installOrUpdateBundle(DirectoryWatcher.java:1027)[6:org.apache.felix.fileinstall:3.2.4] 	at org.apache.felix.fileinstall.internal.DirectoryWatcher.install(DirectoryWatcher.java:941)[6:org.apache.felix.fileinstall:3.2.4] 	at org.apache.felix.fileinstall.internal.DirectoryWatcher.install(DirectoryWatcher.java:854)[6:org.apache.felix.fileinstall:3.2.4] 	at org.apache.felix.fileinstall.internal.DirectoryWatcher.process(DirectoryWatcher.java:483)[6:org.apache.felix.fileinstall:3.2.4] 	at org.apache.felix.fileinstall.internal.DirectoryWatcher.run(DirectoryWatcher.java:291)[6:org.apache.felix.fileinstall:3.2.4] Caused by: java.lang.NumberFormatException: For input string: "0-M5" 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)[:1.7.0_04] 	at java.lang.Integer.parseInt(Integer.java:492)[:1.7.0_04] 	at java.lang.Integer.parseInt(Integer.java:527)[:1.7.0_04] 	at org.osgi.framework.Version.<init>(Version.java:133)[karaf.jar:2.2.8] 	at org.apache.felix.framework.util.VersionRange.parse(VersionRange.java:92)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.util.manifestparser.ManifestParser.normalizeImportClauses(ManifestParser.java:268)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.util.manifestparser.ManifestParser.<init>(ManifestParser.java:145)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.ModuleImpl.<init>(ModuleImpl.java:243)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.BundleImpl.createModule(BundleImpl.java:1153)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.BundleImpl.<init>(BundleImpl.java:79)[org.apache.felix.framework-3.0.9.jar:] 	at org.apache.felix.framework.Felix.installBundle(Felix.java:2535)[org.apache.felix.framework-3.0.9.jar:] 	... 7 more  ```  I created a custom `scalatest.jar` which runs and upload it to my server. I think this problem will disappear, when scala 2.10 is out in a final version.
I'm not able to resolve the dependency conflicts thrown by sbt.  I found this [thread](http:stackoverflow.com/questions/8650088/sbt-wont-resolve-fakehttpserver-dependency-because-of-bad-commons-pom-file) on stackoverflow, but no matter what I excluded, I always got "bad organisation name".  The osgi project depends on commons-io  ```scala    lazy val osgi = Project(     id = "akka-osgi",     base = file("akka-osgi"),     dependencies = Seq(actor),     settings = defaultSettings ++ OSGi.osgi ++ Seq(       libraryDependencies ++= Dependencies.osgi,       parallelExecution in Test := false     )   ) ... val osgi = Seq(osgiCore,Test.logback, Test.commonsIo, Test.pojosr, Test.tinybundles, Test.scalatest, Test.junit) ```  which seems to result in a conflict. And karaf and pax got me the closest working with sbt. Everything runs fine with Maven.
What is the status on this? 
Hi,  I spent one day trying to resolved the conflicts sbt throws, but without success. As I'm not really familiar with sbt I could use a hand in this. The maven build works.   ### Dependency Conflict  ```bash  {file:/home/muki/Development/workspace_play/akka/}akka-osgi-tests/*:update:  sbt.ResolveException: unresolved dependency: org.apache.commons#commons-io;1.3.2: java.text.ParseException:  inconsistent module descriptor file found in 'http:repo1.maven.org/maven2/org/apache/commons/commons-io/1.3.2/commons-io-1.3.2.pom':  bad organisation: expected='org.apache.commons' found='commons-io'; ```
Can you post the question on either akka or sbt mailing lists (or both) to see if you can get any help?
Of course. Thanks for the hint. [Thread on Akka userlist](https:groups.google.com/forum/?fromgroups=#!topic/akka-user/nqSn6teoX6Y)
Open an [ticket on sbt](https:github.com/harrah/xsbt/issues/547)
It seems like the ivy resolver does not like the child not to share the parents groupid (see: http:repo1.maven.org/maven2/org/apache/commons/commons-io/1.3.2/commons-io-1.3.2.pom)
Thanks. I'll posted this on the https:github.com/harrah/xsbt/issues/547
Started jenkins job akka-pr-validator at https:jenkins.akka.io/job/akka-pr-validator/28/
jenkins job akka-pr-validator: Failed - https:jenkins.akka.io/job/akka-pr-validator/28/
I think we should consider _not_ doing this. It's a performance optimization that may not be worth the extra complexity
I guess it would be difficult to write a test which verifies that batching is actually taking place?  Apart from that it does not look obviously broken to me, but I cannot give a definitive answer without spending a lot more time studying the whole infrastructure which is not part of this pull req.
That's a fair point. I see if I can come up with a general test for BatchingExecutor.
yes, with those tests it LGTM
Is this ready to roll?
Been offline for a while. Yes, this one is ready to go. Henrik  On Sun, Dec 25, 2011 at 16:12, viktorklang < reply@reply.github.com > wrote:  > Is this ready to roll? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/184#issuecomment-3270952 >
Great, thanks. Merged and ticket closed.
This is definitely going the right way
Alright, I think I'm done with everything. If you have time to look and give me go ahead, I can merge it in, otherwise you do it yourself tomorrow.
Great, I'll have a look at it when Sandra's gone to bed!
Looks great to me!
Excellent work Raymond! After some finishing touches this will be ready to merge in!
Alright, lemme know when you're done and I'll just skim through it before I merge.
Finished review, should be good to go!
Wicked! Awesomely done Raymond!
you're welcome :-)
Great stuff, +1 merge!
This looks real clean, good stuff! The only thing that worries me is that there are quite some devs out there who will be reluctant to use something they dont understand, so I think this needs really carefully written docs picking people up at the border of imperative-land; then, it could actually be a show-case for FP in the mainstream.
@rkuhn One of the reasons I went this direction is because the iteratees are completely optional. All of the basic IO functionality is performed by sending messages to/receiving from the IOManager. What the iteratees give you is a way to deal with chunks of bytes that may not contain enough information to parse, or contain too much information. Developers are free to handle this any way they choose (typically ByteBuffers I would assume).  So that is an important part of the docs that I haven't yet written, the basic use of the IOManager without the use of iteratees (along with docs for the ByteString class). The iteratee implementation is also very basic, but a developer can easily switch to (one of the many) Scalaz iteratees (again, due to the iteratees being optional and separate from the actual IO operations).
Cool, what about docs?
Hi Derek, could you rebase this on the current master so it can be automatically merged? Thanks!
Nevermind, it was an easy merge
Hey Mike, thanks for the contribution, before I can accept any contributions from you I need you to sign the Akka CLA. In order to send that to you I need your email addres.
actually, actorOf requires a Props argument which would mean      system.actorOf(new Props(MyActor.class), "name");
Uncompiled code examples FTL
sad but true (black album)
Blackened - ...and justice for all
My email address is mjkrumlauf at acm dot org
Victor,  Before I sign and return the Akka CLA I need to make sure there are no legal issues.  In the past there were some restrictions on contributing to projects outside the company so I want to make sure I don't get myself (and Akka) in trouble.  Thanks,  Mike Krumlauf   On Wed, Dec 28, 2011 at 09:21, viktorklang < reply@reply.github.com > wrote:  > Hey Mike, thanks for the contribution, before I can accept any > contributions from you I need you to sign the Akka CLA. > In order to send that to you I need your email addres. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/195#issuecomment-3291140 >
Hi Mike,  sounds sensible. The CLA is an Apache V2 CLA if that helps.  Cheers, 
Akka CLA submitted.
Going through the pull requests I noticed that even if legal issues are sorted, this pull request is still wrong: see my comment on Dec 28. Care to change it to Props?
Does this look good now?
yes, looks good.
I'm afraid there might be more than the TODOs that needs it. What about (not exhaustive): Address DeadLetter Broadcast DefaultResizer Failed PoisonPill Kill Terminated  
Alright, sprinkled some more SerialVersionUIDs
What exactly does this buy us over just noticing when we break stuff and then retro-actively fixing that broken class to use the old auto-generated SerialVersionUID? Or do you fear that the auto-generation algorithm may change, in which case well have to cover way more classes, I suppose?
Generated serialversionuid is very fragile for changes. Even adding a *method* will change the generated serialversionuid.  From Javadoc:  If a serializable class does not explicitly declare a serialVersionUID, then the serialization runtime will calculate a default serialVersionUID value for that class based on various aspects of the class, as described in the Java(TM) Object Serialization Specification. However, it is strongly recommended that all serializable classes explicitly declare serialVersionUID values, since the default serialVersionUID computation is highly sensitive to class details that may vary depending on compiler implementations, and can thus result in unexpected InvalidClassExceptions during deserialization. Therefore, to guarantee a consistent serialVersionUID value across different java compiler implementations, a serializable class must declare an explicit serialVersionUID value. It is also strongly advised that explicit serialVersionUID declarations use the private modifier where possible, since such declarations apply only to the immediately declaring class--serialVersionUID fields are not useful as inherited members.
Yes, agreed that we need to fix the UID in the face of added methods, but my point was that that can also be done once the method is being added (and detected by some MigrationManager-like tool).
That could be done, but I think it is easier/better to add it up-front at the places we know of to avoid breaking compatibility by accident. The annotation is also a good indication that this class must be handled with care when doing changes.  We don't have that tool. I think it would be better to add test that fail if we break ser compatibility. See the tests that @havocp wrote for config.    /Patrik  1 aug 2012 kl. 12:17 skrev Roland Kuhn<reply@reply.github.com>:  > Yes, agreed that we need to fix the UID in the face of added methods, but my point was that that can also be done once the method is being added (and detected by some MigrationManager-like tool). >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/585#issuecomment-7423555
:+1: I agree that this is better than the current state, regardless of whether well find more places later.
For the config tests, see https:github.com/typesafehub/config/blob/master/config/src/test/scala/com/typesafe/config/impl/TestUtils.scala#L220 and https:github.com/typesafehub/config/blob/master/config/src/test/scala/com/typesafe/config/impl/ConfigValueTest.scala#L51 for example.  btw, if you want to support serialization compat between different versions, I'd recommend NOT using the default Java serializer...     - it exposes all your implementation details, so even if you carefully made those private to let them evolve while keeping ABI, you can't evolve while keeping serialization compatible   - it's grossly space-inefficient (the serialization is huge)  The custom serializer I wrote for ConfigValue is incredibly lame, but it's better than the default serialization on these fronts... https:github.com/typesafehub/config/blob/master/config/src/main/java/com/typesafe/config/impl/SerializedConfigValue.java With the default serialization it was basically impossible to make any changes to the library. 
Hey,  you'll have to sign the Typesafe CLA before I can merge your PRs in:  www.typesafe.com/contribute/cla  Thank you!  
Done.  On Tue, Jul 31, 2012 at 8:26 AM, Viktor Klang () <reply@reply.github.com> wrote: > Hey, > > you'll have to sign the Typesafe CLA before I can merge your PRs in: > > www.typesafe.com/contribute/cla > > Thank you! > >  > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/580#issuecomment-7396751    --  Derek Mahar 1.514.316.6736 Home 1.514.316.7348 Mobile 1.514.931.6222 #7754 Work 102-1365 boul. Ren-Lvesque Est Montral QC H2L 2M1 Canada
is that the preferred order? it looks kind of backwards to me, without knowing what the preferred shutdown sequence is supposed to be
orders is LtR, close channel, shutdown channel factory, release external resources. What seems wrong about it?
as I said I don't know what order is expected, but since factory is part of channel I would expect the factory to be shutdown before the channel, but if you know this is correct I trust you
:-) It's a child-parent relationship, many channels share the same factory, therefor the child ought to be closed before the parent is :)
Good one, I am a bit worried though, that if we add a field to Terminated there are numerous places we need to review/modify to maintain consistency. Wdyt?
Now we're 2*N, can't we do a one-pass?
We're also doing this comparison twice
alright, do you prefer to do the removal inside the above loop with `watching -=` or as a for-yield and remove all afterwards with `watching --=`  doing the sendSystemMessage inside `filterNot` feels wrong
then we would add the same field in `DeathWatchNotification` so I don't think that is a big problem
A foldLeft? or a collect?
Hmm, can we avoid having to build a new Set just to drop it from the old? foldLeft?
but foldLeft would build up a completely new set, wouldn't it? here it is normally a few elements that are to be removed
Ok, I'm convinced :)
thanks for feedback
the compiler will save us (which is why I try my best to defend the code base against those evil default arguments)
can also be written as `if` ;-)
Yeah, you're right, I'll fix!
these braces are unnecessary
this method needs to stay deprecated
if this method is not deprecated then well also need one with signature `withCreator(clazz, args...)`
please remove empty line
Impossible to solve without Await?
Of course not. Sorry about that. Will fix.
Ok, this can work, since we don't guarantee ordering of associations anyway.
No braces needed here (as there is none above)
Does not help, netty already caught the exception. Or should I rethrow fatals?
Same as below
Yep, reminder of a refactor. Thanks!
perhaps do: val channel = connection.getChannel at the top here and use that throughout?
Why the extra spacing?
Same question here.
If there is a multi-line case statement I usually add spaces after and before it. I remove it  if that is not the preferred way.
should this really be `new Exception`? use something better, at least `RuntimeException` or `AkkaException`
No extra line is the preferred way.
this was copied from the original ClientHandler exceptionCaught code, so I fix it there as well.
Even after the multi-line case statement?
Yes, no extra line there either.
The in does not parse for me, should be deleted?  lowest resolution is ambiguous: low res means coarse, but seconds is actually the finest resolution allowed
This is a bit unclear: isnt the problematic case that in which the server will close the inbound connection after having taken some time to process a request and then it is unable to re-establish because passive connection was the only thing to get past the firewall?
What if `currentChannel` throws? I think this should stay inside the `try {}`.
So if netty decided to catch a fatal exception, we rethrow it on a different thread?
Ok, I'll remove "in" (it's wrong), and change "lowest resolution" to "finest resolution"
No, the problematic case is when you set read-timeout to nonzero but no reads will never happen guaranteeing the connection to close after read-timeout seconds.
Yes, this is what we agreed on with Viktor. Do you think it's problematic?
A Fatal is a Fatal...
so, hypothetically, if netty catches ThreadDeath on the I/O worker, our thread pool stops working?
So if Netty catches and suppresses ThreadDeath and puts it into a Future, we open a ticket
good, just wanted to be clear about it ;-)
So you tell me there is no Santa?
There are actually exceptionCaught invocations where the cause is null?
Shouldn't this also disconnect to avoid Reset by Pierre?
I think it makes sense to use RST in case of exceptions, it is faster than a complete disconnect.
I don't know, I just used the existing code. I suspect that there can't be such a case.
So, shold it stay? Should I remove?
Is it OK then?
This seems like debug residue, should be removed.
Because it returns a future. In the onConnect method of the handler we   need to be notified somehow that the handshake is done.
You want to ensure that the ssl call has been completed before the sslDone is marked as completed?
I suspect that this code can be written without complecting statusPromise with sslDone, isn't what you want to do is to defer statusPromise until sslDone is successful?
Hmmm... Kinda. There is unfortunately some side effecting inside   initOutbound which I want to skip if ssl has failed. But of course that   does not mean that there is no possibility for refactoring, but I have to   think about it.
We should really do that since the current solution is very brittle.
Looking at the code I think it's doable.
This section contains 3 code diff snippets, but the above description is only related to the last sample.
What do you propose?
just move the description down, above the relevant code snippet
Isn't this code a duplicate of a similar existing function? If so, can it be refactored to a common place?
the reason was BC: this is targeted only at 2.1
Ok, then that is a great comment to include.
So if I just pull out the common code in an companion object to RemoteClient, then that wouldn't break BC, right?
Tried it and it seems to break BC since I add a new companion object. Can't find a nice existing object to stick the code in, so I'll just add a comment.
I guess this line can go, then.
or no, wait, I get it now
if handling of adapters is generically applied at transport load time, then it should be documented together with transport-class for enabled-transports above
this will go away when splitting out UDP, right?
Yes, there are some parallel universes on my machine :)
Yes. Best bet is probably to include the same documentation at all of the occurences.
why change? I don't say that 8.0 was a proven value, but... ?
Would change this to `Timeout after which the ...`
I wanted to be slightly more sensitive than the cluster failure detector.   The rationale was that I might be able to recover a connection before the   clustering FD fires.
Dunno, it was there -- I haven't touched it. I was expecting some   clarification about this from you :)
Same here `... which ..`
shouldn't empty string be default?
Why the repition of `ssl`? I think we have used `enabled` in other places We most often use `on`/`off` instead of `true`/`false`
> Why the repition of `ssl`?  Because it is the same driver as the TCP one, with ssl enabled. I don't   want to split TCP and SSL+TCP to separate drivers.  > I think we have used `enabled` in other places > We most often use `on`/`off` instead of `true`/`false`  True!
I have no clue. I don't think I have written it. Please rewrite.
ah, so we are going to have two different configurations, I'm not sure what kind of implications that will have, but I guess we can consider it when fixing the ticket for migrating the FD
ok, just to clarify, I ment that `netty.ssl.enabled = on` should be enough 
That would be confusing, because then the tcp driver config will need to   have netty.tcp.enabled = false. The content of the transport sections are passed to the driver, so if you   enable the transport akka.remote.netty.tcp, then the contents of that   section are used to confiure the driver.
I think we will need different configurations, as the implications are   different. On FD trigger the remoting will try to reconnnect, but the   clustering marks the node as Unreachable (and it might DOWN it as well).   So I think different sensitivity settings make sense (the weight of the   actions are different).
I'll try to figure out what does that setting mean then :)
this needs more thought: it will not work as you intend due to the exponential behaviour of this value
No, I am aware of the exponential behavior, and it conveys my intention :)   Transport should be an order of magnitude more sensitive to fire   reconnects than clustering is to mark a node as Unreachable. Also, 10^-7   is already low.
no, the behavior is opposite of what you expect: the difference in sensibility between 7 and 8 is very small, the value rises fast (not sure about exponential, but at least some large power).
What do you mean by the difference is small? In absolute value, it is   small of course, but that does not matter! What matters is the expected number of fires for a given CDF, which will   be exactly 10 times as much with a value of 7 than with a value of 8 (if   the probability model is appropriate).  -math.log10(1.0 - cumulativeDistributionFunction(timeDiff, mean,   stdDeviation))  This is a logarithm of a tail probability.
Have you looked at the graphs in the documentation? It takes basically no time to climb from 7 to 8 
The graph shows heartbeat inter-arrival-times. I talk about   expectd-time-to-FD-fire. They are completely different.
no, the graphs show how the phi value develops over time, so you can see when it reaches 7 and 8, respectively, and those points in time will usually be rather close, unless the std-deviation is HUGE
I am not talking about how quickly it reaches 8 after 7, but _how often_ 8   is reached relative to 7. I repeat, those are different things.
Hmm, I see your point though. But the reason for this is that a normal   distribution does not describe real world latency distributions very well.   Let's discuss this later.
ah, now I understand, it makes sense
Why is this the correct place to call this?
Because we know that the actor will not ever be able to receive those messages, because it will be suspended and then terminated. Also the children must have the Unwatch() enqueued before the Terminate() so that it has an effect. I think this is correct, and the KITTEH seems to agree.
the comment might want to mention that this is only important for our descendants, though
note that `finishTerminate` is only called in else of this if https:github.com/RickLatrine/akka/blob/4e92e82c27b3413cf2ec5eace13741e645f463c1/akka-actor/src/main/scala/akka/actor/dungeon/FaultHandling.scala#L152  This change will always `unwatchWatchedActors`. Is that difference taken into consideration? 
as far as I can see, finishTerminate is called in terminate() and in: handleChildTerminated (when status becomes ChildrenContainer.Termination) handleInvokeFailure (as an emergency stop)  From this perspective we could include the unwatch in finishTerminate, too.
Do you feel "confident and lucky"? ;-)
yes, this is why it is called the dungeon: `finishTerminate` is always called, but it depends on whether we need to wait for termination of children whether we do it right away or only upon receiving the last `ChildTerminated` message.
this is in much worse shape than the rest of the collections library  (just sayin)  might prefer a Vector
if we have a GetNext outstanding across a restart, well now have two, right?
ah, no, you let them die: good
if a node joins an existing cluster and becomes leader due to its address, wouldnt this code be wrong?
so MemberDowned is always published before LeaderChanged?
would be fun to print `sender` and `previousLeader` to see that they match 
what happens if the child dies?
what about allowing the child to send handOverData at any time and storing it, so that when we need to recreate the child we can pass that data in?
thanks, changed to Vector
That is normally the case, but I don't want to depende on that. Do you see that it does depende on that order? MemberDowned is also handled in BecomingLeader
which `sender`? LeaderChanged is sent from current cluster node, via event bus, and in this case via the LeaderChangedBuffer
you pushed code while I was reviewing which leads to displaced review comments :-(  I meant TakeOverFromMe below.
and this comment was supposed to be on MemberDowned
I don't know exactly which scenario you refer to, but if I got HandOverInProgress, I cancel the retry timer and then I expect a MemberDowned if the previous singleton dies before sending HandOverDone
that's an interesting idea, I will explore
line offset +14 strikes again: this is supposed to be on `when(Leader)`
move down by 14 lines 
ok, sure, I'll add log of that
The child is restarted, it's not supposed to die (terminate) until the termination message is sent to it. Do you foresee a problem with that?
well, the child can always call `context stop self`, so what should that mean?
Do you think this name is good? Should it be ClusterSingletonManager, and the child is the actual singleton?
yes, that would be better; also SingletonException gave me pause ;-)
that would not be a very well behaving singleton, should be documented I already watch the child, so I can add some extra logging of unhandled Terminate(`child`)
second thought, better to take care of the `Terminate` in Leader and WasLeader states also, to not become stuck in HandingOver because of premature termination
GetNext won't happen here?
Why is this AnyRef and not Option[Any] as below? To remain more general at this level?
Can NonLeaderData(None) happen? If not, why?
that would be wrong, the purpose is to deliver one message, and then buffer until next GetNext deliverNext behavior is used when there are no buffered messages
Why do we care about this if we just stop() in the next line?
Ok, thanks for the clarification.
it can be None initially when CurrentClusterState is empty (not really a member yet), I'll see if I can stay in `Start` until there is a leader and don't use an option in NonLeaderData
true, can be removed
this is Java api, and then `null` is used
 only one outstanding _GetNext_ request is allowed 
traits need that? ugh 
What shall happen in case `WasLeader -> BecomingLeader` (currently only the first case triggers). I also recommend keeping these blocks close to the states involved.
thanks, I had them as separate `onTransition` at first, but collapsed them, but that was clearly a mistake
I still think it would make sense to take and stash `handOverData` during the entire lifetime of the singleton: if we propagate these data at all, then we should also propagate it where possible.
one space too many ;-)
yes, I explored that idea and didn't see a relevant use case for it, so I kept it simple, but it's not a big thing, I'll add it
ah, I turned off auto format, thanks
Shouldn't take a system, should take a Scheduler, and then make the ExecutionContext implicit parameter. Then make callTimeout and resetTimeout Durations in the constructor, which makes it possible to remove timeUnit.
With the above change this is no longer a public member either.
This should be encoded in the Open state and not in the breaker itself.
the try-catch block is pointless since you rethrow everything. just remove it
use an akka.util.Deadline instead: https:github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/util/Duration.scala#L259
This should be encoded in the Open state and not in the breaker
I think it's the caller who should decide _how_ something gets executed. I'd remove the Future {} and the Await.result.
I'd also remove the "FSM" from the name, as that's implementation detail leaking through.
I'd probably remove all these methods and added a way to register async callbacks to the transitions.
This does not verify whether the transition is legal or not.
This does not verify whether the transition is legal or not.
This does not verify whether the transition is legal or not.
This does not verify whether the transition is legal or not.
This was in place to get timeout behavior on the call, which I was thinking was necessary when sticking to Nygard's version of the circuit breaker.  So if the call exceeds the call timeout that is specified, that can be considered a failure.    I can certainly remove, and just consider exceptions to be failures?  Or are you suggesting I should find some other timeout mechanism?
The timeout can be considered after the call returns. It will not fail fast of current call, but the failure counter is increased. Using Future with Await here is not an option, since it will consume additional thread resource.
all these should be Durations
Sure thing, Patrik.  The durable mailbox implementations would then be responsible for their own timeout handling - which i'm understanding Viktor's point to be as well "caller who should decide".  Thanks for the feedback!
If there's a good place to look for examples here, that would be helpful to me.  I'm thinking you're looking for a signature along the lines of:  ```scala  def onOpen(func: () => Unit)   or  def onOpen[T](func: () => T)  ```
Definitely - thanks!  I also need to make them config options, will need to dig a little deeper in that to see the best way so that the durable queue subclasses are kept dry.
I think you should have a separate CircuitBreakerHalfOpenException. perhaps extends CircuitBreakerOpenException
should be  extends RuntimeException
ouch.  that's embarassing ... will do.
    def onOpen[T](func: () => T)  is preferrable due to type inference.
You're most welcome, keep hAkking :-)
This isn't thread safe
This should be private
Isn't this method heavily duplicated from the HalfOpen.onCall?
What is this used for?
What is this used for?
Put these in config
This should not use the default dispatcher, but the dispatcher of the Actor, right?
How does this handle asynchronous backends?
These two companion objects were put in place to get rid of strange errors during compilation.  I was able to duplicate with both idea and sbt.  I tried removing them and recompiling, and the issue is gone.
Turns out I needed to remove this when I got rid of the Future, that's all it was used by.
That's a good question - the integrating point "withCircuitBreaker" is written synchronously.  Here's the sig of the mailbox queue:  ```scala trait MessageQueue {   def enqueue(receiver: ActorRef, handle: Envelope): Unit    def dequeue(): Envelope    def numberOfMessages: Int    def hasMessages: Boolean    def cleanUp(owner: ActorContext, deadLetters: MessageQueue): Unit } ```  So beyond enqueue and cleanUp it's a synchronous interface.  I'm thinking a circuit breaker is not needed on the cleanUp method since it happens before disposal.  I'll need to think a bit on a different signature to support injecting success&error callbacks for enqueue.  Of course, am open to suggestions.
 I don't really like the interaction with the error handling on this particular call, the thrown exceptions from the circuit breaker never make it back to the caller.
How about moving these into akka.util?
You are relying on a hairy bit of overload resolution here: the case class has an `apply` which would also work with zero args. Is this really necessary?
It works just fine, it's a very common Scala idiom
I would write `WildcardTree.empty` here in any case
This is a bit half-baked, isnt it? Either do it fully with nice indentation or leave it on one line (in which case you might just not override at all).
Given that it does not make sense to add a deployment to `/user` himself I think the top-level element which you create is quite useless (or Im stupid ;-) ).
Removed. Was only used for debugging.
top-level is initial slash.
I'm honestly not sure how the 0.7.0-M1 version got there, but I can guess: the development version is 0.7.0-SNAPSHOT. But when Josh suggested cutting a new version this time, he suggested the 0.6.1 (the previous stable was 0.6.0). I can talk to him about changing it to 0.7.0, if you'd feel better about it. ;~)
I just wanted to make sure that it wasn't accidental.
Ah, right. This should include everything that was in 0.7.0-M1 and seems to pass the basic test of running the various sphinx:* tasks.
Very nice!  Have you verified that the methods above are inlined? (so no closures generated etc?)
Id like to not repeat the mistake of offering variants without message: lets force users to be clear about what they want.
We should decide whether we want to keep `akka.ConfigurationException` (currently 69 uses), which means not using plain `require` here, or to completely remove our own exception type and only use `require`.
Yes, no requiring without a message please...
I don't see exactly what the `ConfigurationException` buys us over `IllegalArgumentException`. Sure it's more precise in some sense, but is the distinction necessary?
exactly my point
technically naturals do not include zero 
Ah, so iterators were reused in the previous code. Does this new code has any noticeable performance penalty?
IF you want performance you should use LARS, this is just for legacy HWT action
It was just a question out of pure curiosity :)
These override of hashCode and equals are not needed in **final** class HashedWheelTimeout. Same thing is the default implementation in java.lang.Object
Well, since this is something that is required at the "business" level I think it's being worth to spend a "final" modifier (not relying on whether the class is or remains final), and I am not a believer in relying on defaults. That was my reasoning for being explicit. (Also, remember that this is Java)
Why is the event stream an option here? If you don't use it then why have it as an argument?
Since we test failure detectors in isolation without an actorsystem around, it makes sense to leave it optional, so if you instantiate through this constructor you can decide whether you want to use it or not. This particular FD does not use the stream ever, but that might change.
Yes, and then we might change this failure detector? Why do it now?
One constructor parameter will be unused anyway, either this, or the one in the auxiliary constructor. 
So the auxiliary constructor is part of the API for a pluggable failure detector and can't be removed.  Just thought this change to be unnecessary since we don't use the EventStream and don't know if we will. 
It was used by Patrik before since he logged things from the FD. I can simply remove it, although I don't see too much difference.
It's no big thing for me, I was just wondering why it was added without being used.
My idea was if Patrik wants to log again, he can just add the logger. But it is neutral for me.
no, that looks funny, remove the parameter
btw, In the future, if someone will want to use an EventStream here, it has to be optional, since it is not always instantiated inside a system.
FailureDetectors are reusable and therefore public everywhere. But you are right in that it needs documentation...
So we are expecting users to load their own FailureDetectors from Config?
I'm on Viktor's side. Every public api we add locks us down. In Swedish it is called "bjrntjnst".
Then we should set every FD related API to be private[akka]
"Why not?" is not an ideal question when it comes to API design as it always ends up like a bazillion methods that you need to support, the user must find, understand and choose between. Just my 2
No, "why not" was not an argument, but a request for argument against including it, which I can comment on. We made FD API public, since it has many possible uses. We can lock it down of course, I am neutral about the issue, but I find it strange that we keep it open, and we lock down only a reflective loader utility.
Inclusions should be based on "arguments _for_" not "there's no arguments _against_". User API is about value vs cost, and without arguing the value, there's no point arguing the cost. I'm not talking about making the FD API private, I'm talking about FailureDetectorLoader, while there is value in letting advanced users be able to construct and plug in alternative implementations, I see no need for them to use their own loading of them from config, as that's already provided by the config setting for the default FD. Makes sense?
We want the failure detectors to be pluggable, and even extendable, but the construction of them from config is a rather internal thing. It is not a big deal for me.
Ok, I am convinced :)
I mean I make only the loader private
Distributed consensus ftw
hmm, Im inclined to suggest `SendToAllButSelf` instead, since that is equally usable from Java and Scala
please reformat these to be more readable:  ~~~ scala @SerialVersionUID(1L) case class Send(...) @SerialVersionUID(1L) case class SendToAll(...) ... ~~~
"Inclined", is that a decision? 
no, it's more like an invitation for everyone to join the discussion
I agree with you Roland, it needs to work from Java. 
I agree with the name suggestion, since it has nothing to do with the `sender` of the message, but the location of the mediator  AFAIK it is not necessary to use a new message class. Isn't this alright?      case class SendToAll(path: String, msg: Any, allButSelf: Boolean = false) {       def this(path: String, msg: Any) = this(path, msg, allButSelf = false)     }
Shouldn't this be a couple of lines up?
That's better of course. Thanks. 
Don't know if it matters. @patriknw?
I'll go with that Patrik. 
It may be that Java guys are comfortable with  ~~~ java new SendToAll("hello", 42, true); ~~~  but I would prefer  ~~~ java new SendToAllButSelf("hello", 42); ~~~  for immediate understandability.
This is in the test, and tell is a very conscious decision.
Umm, can you explain in a few words how this solves the problem? I am not sure that I understand.
To clarify: What makes the facade ready to the second node before "done-2", but not for the others? Why does adding a new barrier solve this (apart from increasing the delay somewhat - but then why is it working for the second node)?
no, that is not the solution, that's just boy scouting we normally use unique names of the barriers, to simplify debugging, and this tests had already one "done-2"
Ok, so the solution is the change from node(third) / "user" / ... --> user/...? 
yes  In the log there was:      [JVM-Node2] [WARN] [10/17/2012 16:11:57.604] [StatsSampleSingleMasterSpec-akka.actor.default-dispatcher-3] [akka:StatsSampleSingleMasterSpec@scalable2-03:46158/user/statsFacade] received dead letter from Actor[akka:StatsSampleSingleMasterSpec/system/testActor1]: StatsJob(this is the text that will be analyzed)   and the test verifies with:         eventually the service should be ok,        worker nodes might not be up yet       awaitCond {         facade ! StatsJob("this is the text that will be analyzed")         expectMsgPF() {           case unavailble: JobFailed  false           case StatsResult(meanWordLength)              meanWordLength must be(3.875 plusOrMinus 0.001)             true         }       }  so the test expect to eventually receive a StatsResult, but since the StatsJob (the request) is lost there will be nothing by timeout  this is a sample, so I don't want to complicate it too much, therefore I use a **local** facade as entry point
I start to get it :) So by the time the local facade starts processing messages, the workers are already available?
this should be in `finally` block as it was before, because handleSupervisorFailing is not trusted code (potentially)
I dont think we should, lets have a closer look tomorrow
line breaks, please.
what about re-throwing InterruptedException?
I think this is harder to read than the original.  What is the advantage?
This is a fair point Patrik, I had originally written the match in systemInvoke to use the new isFatal method, but abandoned that when Roland suggested the cleaner extractor match. I'll revert my change. (Although it's faster ;-) )
only throws InterruptedException or Fatal now
this needs the same treatment as the corresponding `catch` in `systemInvoke`.
Why not use JavaConverters as in the other cases? (same for CamelMessage)
It was too bulky, I resorted to using a very scoped implicit conversion instead
the key of the entry isn't used, so why not use container.values?
singleton isn't very self describing when imported like this, perhaps rename it to `singletonCollection`
It's a `singletonSet`  OTOH one could argue that the JDK name is FUBAR
File needs a bit of docs
good, shouldn't be published
just curious: does this make a difference?
It will probably change the order on the class path while running the testkit tests, but it has no effect and is not intentional. Just residue from the first rewrite. 
It should of course say call these methods and not override them. Will fix.
that they are in early access mode, which also means that they are not covered by commercial support.
Could add a link to Assembla for the Coltrane milestone
Or/and to the Roadmap doc. 
added links to assembla and roadmap
now that the bug is nailed, we might want to factor this code out, probably having Recreation and Creation extend a helper trait with `enqueue()` and `dequeueAll()` methods and then reducing this match to two cases again (and the others below).
Yes, I was about to ask how you would make tha look nicer since inheriting case classes isn't the way to go ;)
why wrap this inside NonFatal, this should be enough      case i: InstantiationException =>
shouldn't this be DRY:ed with above?
`var todo` is scary, but I guess you guys know what you are doing here
`ne null`, but that doesn't matter
no need for the extra level of nesting here any more      if (actor == null) {     } else if (isNormal) {     } else {       
I agree with Patrik reuse the same code for both and check instanceOf[InstantiationException] to determine the rethrow
These give me the shivers
So this doesn't really wait for the other system to shut down properly does it? How does it make things better (Apart from removing the nested `runOn(first)`?
that is true, but what I could see in the log was that `first` was shutdown before the `"third-crashed"` barrier was completely received on `second`. There was a `BarrierResult("third-crashed", true)` in deadLetters at `second`. I guess it has something to do with stopping the controller too early. This will wait here until the shutdown is confirmed by a ClientDisconnectedException.
Culdn't you run `shutdown(second)` on second before `awaitTermination` instead?
shutdown command must be done from controller (first)
but  do you not at least try to run that on `second` now?
what do you mean? what shall I try on second?
sorry, I was confused: should have looked at some more context around the diff
I'd prefer an algorithm that instead of creating strings and caching them compares the Addresses first (falling back to the selfAddress) and then compares the path-names.
Do I understand you correctly if I would pushing this down to a real toString method in `ConsistentActorRef`? not keeping the string in a `val`
Or avoid Strings altogether and provide a function from (T, Int) => Int to ConsistentHash.create (then you can hash the address and the path name completely without having to create any new strings, just use Murmurhash and `extend`. Wdyt?
I don't think we should redesign the ConsistenHash now, it uses toString of the ring nodes, and appends a counter for the virtual nodes. Keep it simple.
Oh, I'm not talking about a redesign, the default (current) one would supply a function which does exactly what it does right now. But for this particular case (ConsistentActorRef) we should simply make a method of the following shape: def consistentHashActorRef(defaultAddress: Address): (ActorRef, Int) => Int  Wdyt?
as _a_ seed node.
here as well
I expect this to be subject for discussion, as usual, so I didn't spend any time on it up front. We could go totally thread local if you like.
Why not just a one liner: ring.rangeImpl(Some(hash), None).headOption.getOrElse(ring.head)._2 ? 
Why different order of args in this factory method compared to the constructor? 
Because names communicate, and this is not a trivial line
Should this not just be a case class? Why would you want to create your own envelope? 
What happens if you send this message to another kind of router? Nothing? Error? 
You mean 'updateConsistentHash'? Misspelling. 
Why use String.format here and not the {} substitution? 
You must impl consistentHashKey from ConsistentHashable also, so it can't be case class, if not using a function param, but that doesn't work for java. I think pure interface has most flexibility.
Nothing special, but RouterEnvelope on the other hand always unwraps before sending to destination. Broadcast is a RouterEnvelope.
That is plain wrong. Thanks!
Why can't you do:      case class ConsistentHashableEnvelope(message: Any, hash: Any)  Same as we do in Broadcast(..).  Seems tedious to have to define an envelope for each new message you want to hash. 
We use one-liners even when it exceeds 120 chars?
We don't use any unnecessary curlies. Newline is ok. 
Ah, I didn't think of it so, but that is of course better. I'll change 
Sure. Just looked weird. 
But I can change it to a comment :-)
Why is this needed? Isn't rangeImpl's lower bound inclusive?
Isn't there a MurmurHash in Scala libs? In the nightlies there is a scala.util.hashing.MurmurHash3. Is ours customized somehow?
Performance, perhaps, I kept that because the original impl did something like that and also the article http:www.lexemetech.com/2007/11/consistent-hashing.html  I'd be happy to remove it if we don't think it add any value
We had a ticket about migrating to that but V ended up embedding this one, so I guess there is a reason.
Ah nextClockwise is a def! Yes, it must be for performance then. Though I am not convinced that this saves much...
Isn't the name replica a bit confusing? No data is replicated, just more than one intervals mapped to a node on the ring. WDYT?
Yeah, there is #2226, and it was closed as invalid. The comment does not explain why. 
I think it can make a difference when adding or removing nodes, as rangeImpl is probably a lot more costly than get
Isn't removing and adding via :- and :+? They only call nodeHashFor and not nodeFor
True, then I see no reason for it
this pretty much is a constant, so it should be declared like one:      final val DefaultReplicas = 10  (observe the missing type ascription, which is part of the deal)
yes, my bad
Did you see this? 
thanks for pointing out the typo, but the expected discussion was more around synchronized
Sure. But typos shouldn't be ignored regardless. 
no, no, I'll go through all comments and fix all of them
why move this up here? or did Rex write all your code ;-)
since always one drives the other you could also make a non-blocking implementation (CAS-loop for updating the routees and the winner sets the hash)
if the first is true and the second is false, then one should be set to the other, no?
boyscout bonus points!
I blame the computer :-) no, it should of course go together with the MurmurHash class, I think I move it to a separate file, to avoid license ambiguities, also makes sense since we use MurmurHash from other place also.
I agree, suggestion of better name? virtual nodes? virtualNodesFactor
true, that would avoid subsequent equals checks. Like this?        if (currentRoutees ne consistentHashRoutees) {         if (currentRoutees == consistentHashRoutees) {            other instance, same content, no need to re-hash           consistentHashRoutees = currentRoutees         } else {           consistentHashRoutees = currentRoutees           consistentHash = ConsistentHash(currentRoutees, replicas)         }       }
Virtual nodes sounds good to me.
yup, could keep them together in a Tuple2 also  This has the down side that several competing threads would do the same hashing work, this is on the sender side so it's probably not usual to have many threads using the same actorRef  I think it would be perfect if first that detects could do the work and other continue with stale data. Only case where I can think of where it could perhaps be bad is when creating the routees initially, but that is done from constructor anyway, isn't it? For this we could use `ReentrantLock.tryLock()` Is that good in the normal case, when nothing changes, i.e. quick op?
Locking always writes, lock-less will almost always read, hence less cache line bouncing (Id assume). When you see a discrepancy between the two lists of routees, try to fix that (and only that), and if you were the successful one, update the hasher; if you were the loser, you might as well go on with stale data, I guess (i.e. just read consistentHash).
sounds good, thx
How important is it that we see the updates and that they are in synch (consistentHashRoutees and consistentHash match up?)  If they need to be in sync, then put them into a Tuple in an Atomic, and get them in the beginning, and if discrepancy between routees calculate and try to CAS, if fail just use what you calculated or reread.
I think that this is a performance optimization. Maybe I'm missing something but don't you insert the node with the hashFor as key in the TreeMap during `:+`? And calling `getOrElse` seems much less expensive than `rangeImpl`.
my initial idea was to keep them in sync, consistentHashRoutees is just another representation of what is in consistentHash, but I think the ignore on CAS failure works fine
no, it's not used in `:+`, it's only used for "messages" and it should be extremely rare that they have the same hash as the nodes
Check, I see. Not very likely.  Still, unbounded `rangeImpl` for every message lookup... Tried to find something better, but I just can't. A get range of size N would have been nice.
Instinct: I'm not sure this is the best option. It means that you need to control message implementations to be able to use them for consistent hashing. Same issue as with putting equals, hashCode etc in Object. I propose to seed the Router with either a partial function from Any => Any or a total function from  Any => Option[Any] so that you don't need to wrap messages to be able to have them hashed.
Can we make the "ring" a "nodeRing" then?
I'm not sure I like that, because here it's the data that drives the routing logic, and therefore it should be in the message and not in the definition of the router. Other routers implement the routing based the router's state (e.g. round robin) and that is different.
This doesn't seem cheap
This line doesn't do anything
That's how it was in redis client, note that this is only done when nodes are changed, very rare. Suggestion?
I think this is a non-issue, I'm more scared about `nodeFor`.
If it's very rarely used, then it's fine.
I don't think "nextClockwise" needs to be a method. Also, try to make it return a single value (not a tuple).
Don't we always put defaults in config?
In the context of the use case it shouldn't be a big Map, aprox 100 nodes. One could perhaps do something with takeWhile or dropWhile, which could save some tree rebalancing. Should I spend time on it? In that case I would benchmark it in the context of the router.  /Patrik  13 sep 2012 kl. 15:14 skrev Bjrn Antonsson <notifications@github.com>:  > In akka-actor/src/main/scala/akka/routing/ConsistentHash.scala: >  > >    } > >   > > -  private def hashFor(bytes: Array[Byte]): Long = { > > +  /** > > +   * Factory method to create a ConsistentHash > > +   * JAVA API > > +   */ > > +  def create[T](nodes: java.lang.Iterable[T], replicas: Int) = { > > +    import scala.collection.JavaConverters._ > > +    apply(nodes.asScala, replicas) > > +  } > > + > > +  private def nodeHashFor(node: Any, replica: Int): Int = { > > +    hashFor((node + ":" + replica).getBytes("UTF-8")) > I think this is a non-issue, I'm more scared about nodeFor. >  >  > Reply to this email directly or view it on GitHub. > 
Previous comment was as an alternative to rangeImpl, if that wasn't obvious.  /Patrik  13 sep 2012 kl. 15:14 skrev Bjrn Antonsson <notifications@github.com>:  > In akka-actor/src/main/scala/akka/routing/ConsistentHash.scala: >  > >    } > >   > > -  private def hashFor(bytes: Array[Byte]): Long = { > > +  /** > > +   * Factory method to create a ConsistentHash > > +   * JAVA API > > +   */ > > +  def create[T](nodes: java.lang.Iterable[T], replicas: Int) = { > > +    import scala.collection.JavaConverters._ > > +    apply(nodes.asScala, replicas) > > +  } > > + > > +  private def nodeHashFor(node: Any, replica: Int): Int = { > > +    hashFor((node + ":" + replica).getBytes("UTF-8")) > I think this is a non-issue, I'm more scared about nodeFor. >  >  > Reply to this email directly or view it on GitHub. > 
Possible to remove a bit of duplication:      if (currentRoutees ne consistentHashRoutees) {         val rehash = consistentHashRoutees != currentRoutees         consistentHashRoutees = currentRoutees         if (rehash)           consistentHash = ConsistentHash(currentRoutees, replicas)       }
I think this match points out a flaw in the current serialization. Can you add a Serializer mapping that serializes byte arrays just by identity? And Strings should also use Serialization, right?
well, everything would work fine without those lines, see this as a special case optimization to not having to go the extra steps into serializer (defaulting to java serializer, uh)  a special byte array serializer could be useful for other things, but what is the configured class name for such thingy?      scala> classOf[Array[Byte]].getName     res16: java.lang.String = [B
I see now that this `abs` business is totally unnecessary, negative Ints shouldn't be discriminated 
    scala> Class.forName("[B")     res3: java.lang.Class[_] = class [B          scala> res3.isArray     res4: Boolean = true
this wasn't configurable, only with api, didn't thought it was important to have it as config, but now I have moved it to router config, and removed this constant
Aren't we just interested in the first match with hash >= the message hash for less than 100 nodes?   `ring.find({case (h, n) => h >= hash }).getOrElse(ring.head)` 
That would be linear in the size of the ring. The TreeMap implementaion in theory should handle subrange queries in the log(n) order... Maybe benchmark?
Ticket added: https:www.assembla.com/spaces/akka/tickets/2499-byte-array-serializer
The `TreeMap` rangImpl isn't a lookup as far as I can see. It seems to do alot more than looking up stuff.  All I'm saying is that insertion/removal is not common, and the lookup we want is specialized.  Might just be better of with an array and linear find.
Yes, true, because it has to maintain red-black property for the resulting   tree, I think. Log(n) still, but it materializes only for large trees.  > Might just be better of with an array and linear find.  Even better: sorted array and binary search.
yes, it might be better, but remember that consistent hashing will "always" be used with remote routees, so we are not spending time on the bottleneck. If we think it is important I can take a stab at it, or I can create a separate performance ticket.  On Thu, Sep 13, 2012 at 6:10 PM, drewhk <notifications@github.com> wrote:  > In akka-actor/src/main/scala/akka/routing/ConsistentHash.scala: > > >      } > > +    ring.getOrElse(hash, nextClockwise) > > Yes, true, because it has to maintain red-black property for the resulting > tree, I think. Log(n) still, but it materializes only for large trees. >  Might just be better of with an array and linear find. > Even better: sorted array and binary search. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/698/files#r1599242>. > >
it can't be correct to not keep them together, do you mind if I place them in a Tuple2 instead, or did I misunderstand the suggestion that one drives the other
I think they should be the other way around. It's the `consistentHash` that should be in the atomic since you don't want to overwrite it with a stale `consistentHash`. Overwriting the `consistentHashRoutees` with stale values will just trigger a new update.  I would just make it a Tuple. Cleaner I think.
Yes, Tuple please.  Just realized that there is a race when removing nodes otherwise, that could make you get a removed node from the `consistentHash`.
Ignore that comment. There is no race, but go with the Tuple ;)
this import is not needed
this should be `expectMsgType[ActorRef]`
expectMsgType is nice!  On Fri, Sep 14, 2012 at 5:14 PM, Roland Kuhn <notifications@github.com>wrote:  > In > akka-actor-tests/src/test/scala/akka/routing/ConsistentHashingRouterSpec.scala: > > > +@org.junit.runner.RunWith(classOf[org.scalatest.junit.JUnitRunner]) > > +class ConsistentHashingRouterSpec extends AkkaSpec(ConsistentHashingRouterSpec.config) with DefaultTimeout with ImplicitSender { > > +  import akka.routing.ConsistentHashingRouterSpec._ > > +  implicit val ec = system.dispatcher > > + > > +  val router1 = system.actorOf(Props[Echo].withRouter(ConsistentHashingRouter()), "router1") > > + > > +  "consistent hashing router" must { > > +    "create routees from configuration" in { > > +      val currentRoutees = Await.result(router1 ? CurrentRoutees, remaining).asInstanceOf[RouterRoutees] > > +      currentRoutees.routees.size must be(3) > > +    } > > + > > +    "select destination based on consistentHashKey of the message" in { > > +      router1 ! Msg("a", "A") > > +      val destinationA = expectMsgPF(remaining) { case ref: ActorRef => ref } > > this should be expectMsgType[ActorRef] > > -- > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/698/files#r1608480>. > >
Perhaps add tests for the individual settings (and not only the resulting values)?
only two? is this enough?
Yeah, max here seems way too low. Should be 8?
This is for the test conductor sockets that are used only for the test conductor commands.
Ah, ok, I see: the default netty pool-size-max is 8.
Yes, it's better to be explicit. Changed all of these.
Added a new test case for individual settings. 
you need to update the scaladoc also I wonder if someone relies on what is documented? Could it change ordering expectations?
oh, thanks; I removed it because it never guaranteed anything really, since context.watch() is asynchronous (in the non-deadlock cases)
We don't use the return value, so could use `transform` instead of `transformAndGet`.
Using `ref.single.update(newValue)` could be more efficient. (I know eliminating a read dependency can help in a DB transaction, but not sure about in our STM implementation.)
If we're considering pushing this out in 2.1.x then we might want to keep this method in order to minimise API changes. (But could deprecate it.)
Nono, we can't put this into 2.1 anyway
this might want to have some comment explaining what it is supposed to be good for
Come on  - a semicolon? For real?
Lol  I confess, I experimented having it as a one-liner. Residue must go!
how does `suspend` interact with `throughtput`?
I see below that is supposed to interrupt batches, but a test would be nice
The test below makes sure that the EC is loaded with all the tasks before starting to process, so that it will resubmit after "throughput" number of tasks processed. suspension aborts the current batch, otherwise it wouldn't suspend?
What does this line? Why?
It adds 10 tasks to do nothing. We do this to simulate that other stuff gets added in between.
Yeah, forgot to bring this up, should one be able to have different batch sizes per Agent? And if not, what should the default be? I chose 10 at random.
I think this might be premature, just use "-". I'd be surprised if it doesn't return "this" if it doesn't contain the instance, it is an immutable collection.
Same comment as above
Yes, you are right. I'll correct that.
Id make the comment the value (yes, Typesafe Config supports it, use `getBytes` then)
if we read `remainingLimit`, then well also want to return MoreDataWaiting, no?
Yes, good catch. I fixed that.
The previous version called (set-subject) if subject.path.uid == ActorCell.undefinedUid && set(subject) This does not, however. Was this the intention? If yes, can you explain? :)
So if `subject.path.uid != ActorCell.undefinedUid` we always removed both `subject` and `UndiefinedUidActorRef`, in the else case the filter would remove `subject` anyway.
What are the long-term effects of this, memory wise?
What do you mean?
Create an actor with 10, 100, 1000, 10000, 100000 children, how much memory does each approach take? (TreeSet vs HashSet)
ah, so you did not mean long-term but how efficient are the implementations; I should guess that a tree has more overhead than an array of buckets  starting YourKit now
It is a hashtrie actually, no? So not just a simple array of buckets. Still, I would guess TreeSet takes more space.
True, it was a bit mis-worded. I was thinking along the lines of "as time passes and you have more kids"...
well, actors may sometimes also be people, but the children metaphor only takes you so far ;-)
Also, you wouldn't want to see kids in a HaschMap...
Yes, mutable.HashSet uses hash-table and immutable.HashSet uses hash-trie
so, actually: TreeSet has an overhead of 32b per entry (one tree node), whereas immutable.HashSet occupies 41b (on average at 1000000 entries; same result for 1000 entries)
You mean Bytes?
of course, what else?
The lower-case "b" was confusing
nope: this is not an Ordering, try out `Int.MinValue compareTo Int.MaxValue` to see what I mean
emptyActorRefMap does not seem to be used; in that case it should be removed
Hmm, I see where this is coming from, I just wanted to remark that this enlarges every actor by 8 bytes.
You're right. I'll change it.
you are changing it to `if (a < b) -1 else if (a == b) 0 else 1`, I guess
Oh yes. I'm not going back to boxing, and waiting for the auto-unboxing optimization to be enabled by default in HotSpot.
Only if the actor is the only one using those props ;)  Yes, it might be premature, but It also stops every Actor creation by this props to allocate a `Constructor[]` and `Constructor`, and explode the arguments into an `Object[]` only to do the exact same thing again right after.
yes, I know; well notice in case we need to optimize differently, I think it is good for now; I just want to spell things out
I like red code
I'm trying to understand, this is for inbound association, and you remove the quarantine immediately. Is that correct? I thought "talk to the hand" was the purpose of quarantining (e.g. undelivered system messages). 
this means restart, right? and after some attempts it will be stopped
Yes, but in case of quarantining there will be no outbound writers created, so no restarts. All existing writers are stopped, too. 
When we get to this line we already determined that the uid for the inbound connection is not quarantined, so we must lift it in fact. Also gate should be lifted, too, since we just communicated with the guy.
got it, a few lines up: `if (endpoints.isQuarantined(handle.remoteAddress, handle.handshakeInfo.uid))`
Thanks. Sometimes you just want to click on a _more context_ button and keep the diff.
This should be pulled out of the trait since it is a constant, put in some object.
onRouteDefinition closes over "this" in: (rd)  getRouteDefinitionHandler(rd) and now you're exposing "this" to another Actor.
can "getRouteDefinitionHandler" return different things throughout the lifecycle of the Consumer? (i.e. why (rd)  getRouteDefinitionHandler(rd) and not getRouteDefinitionHandler.apply _)
This will mess up message ordering.
I'm still not on board with this impl, the code here should most probably be implemented as a match though:      camelObjects match {        case Some((endpoint, processor)) => produce(endpoint, processor, transformOutgoingMessage(msg), if (oneway) ExchangePattern.InOnly else ExchangePattern.InOut)        case None => messages = messages + (sender -> msg)     }
what happens if activationTracker or registry dies or throws exception?
These should be in a companion object like usual.
"cause" does not need to be a val.  Why is this an Exception and not an AkkaException or similar?
Same comments as above.
prefer vars to immutable datastructures over vals to mutable ones.
What if these terminate?
So a consumer cannot also be a producer? (values get overwritten in the map)
actors.remove(actorRef).foreach(_.apply(actorRef))  No need for the if block or anything  But mixing the produces and consumers in the same map is wrong, they deserve their own maps, then you do not need to store any functions either, just use 2 immutable sets, one for consumers and one for producers.
if this throws then you've already added it to camelObjects...
why toString twice?
Wasn't there a todo to make this ocnfigurable?
timeout should probably be a Timeout instead of a Duration, wdyt?
same comment here as above.
find 1 problem ;-)
Should also verify that there is 1 activation and 1 deactivation per ref, and that all refs were activated and deactivated.
Future.sequence(activations.toList) map activationsPromise.success
Why Vectors and not Sets?
10 seconds seem a bit arbitrary, shouldn't it be configured per test?
no it should not, so i'll use getRouteDefinition.apply _. The idea of using functions instead of methods was to limit mutability and closing over the actor instance.
Ok I'll do that then. whatever floats yr boat ;-)
default supervision strategy restart, so they both get restarted, and lose state, so that's not good. I'll add a Resume strategy for all NonFatal errors. Also add a watch on them and create new ones when they die.
I'll do an AkkaException. 
ok will change.
yes that is not possible (or should not be possible). Producer overrides receive to send everything it receives to an endpoint, so an Actor that is a consumer and a producer would not make sense. Is there some type magic I can add to enforce this? 
yep, and it will be deregistered through supervisor later. (now that I look at it again it is not exactly right yet) when It throws, supervisor stops the registrar instead of the producer being registered, which is not good.   So what should happen is, it throws, supervisor stops the actorRef in the ActivationException, through Terminated it then deregisters the actorRef. All clean again.
I'll take it out, it's not necessary
Now that you mention it, I think there was a ticket for it, we talked about it and removed the ticket. I'll add a FIXME
Yes, better. BTW, Why does Await still use duration? 
yes that could be hard :)
ah yes nice
yeah it's arbitrary.   I'll add an implicit parameter since most of the time you need one of those anyway in the tests. 
cause I'm craaazy like that! ;-) I'll change it.
Beer? ;-) no idea really... will fix 
Ok I'll do that.
Well, we should at least know about it right, and restart them? I could let the Registry watch them and create fresh ones when that happens?
Some options:   1. I don't keep the messages when the producer is not ready yet, they just disappear.  2. I throw an ActivationException when the producer receives a message and is not ready yet.  3. I mess up the ordering ;-)  4. like three, but without messing it up, how about if I call produce directly here and pass in the originalSender from the messages.  Let me know your favorite of these 4  
I wanted to prevent exactly that, closing over "this". I wanted to basically separate the function from the Actor. So the Actor should just be able to return a function, the user should be able to provide a custom function, and I only want that function to be contained in the message, without a reference to the Actor. Any way you can see how to do that? 
Not really possible to enforce, what you could do is test with serialization for local sends turned on, then you'll see that you can't serialize the Actor (afaik).
Oooooor:   use Stash and stash all incoming messages until CPO is received, then unstash them all? It does however require that camel actors use the DequeBasedMailbox...  Thoughts?
Messing up ordering is the worst of them since it breaks the contract of the message sends.
I don't know whaat the right semantics should be, I just want to know that failures are handled :-)
Would be easy to put a section in the camel config that maps the keys to FQCNs:  akka {   camel {     conversions {        "file" = "java.io.InputStream",        "pigdog" = "org.awesome.Pigdog",             }   } }   And then just load that from the config when the Extension is created.
Timeout is for ask
Weird if the test passes ;-)
Yeah the message ordering is extremely important, wasnt a serious option ;-)   The stash option sounds like a good idea, are there any serious drawbacks to the DequeBasedMailbox?  What if the user configures a different dispatcher and mailbox for the actor, then it will break right, how to prevent that? throw an exception if they try? Does that mean you also can't use durable mailboxes in combination with a Producer?   
:-) thanks, at least you are making me think which is great. 
Ah man, nothing more awesome than PigDog :D cool, will do that as well then.
The other option is to spawn a child that sends the messages. So buffer until CPO is received, when CPO is received spawn a child with the CPO info, then send all the queued messages to the child, then all new messages that come in go straight to the child. No stash needed, just a queue (which of course needs to be bounded in some sense to avoid OOMEs). See any issues with that?
If the extension you have a reference to ExtendedActorSystem, from that you can do:      extendedActorSystem.dynamicAccess.getClassFor[AnyRef](fqcn).get (to get the Class loaded in the right ClassLoader)
You might not want to do plain ".get" as you'll want to escalate it as a configuration problem, see how the other extensions to it (they use recover with rethrow of wrapped exception)
noooooo, if the JVM is reused between test runs this is terrible.
My SSD wants to live :(
hmm, on *NIX you can just copy the first 100MB of /dev/zero 
Yes, I understand that this is not a feasible solution for running tests all the time. Any idea how we could do this platform independent? The first step could be just not to create such a big file and delete it instantly. However, that wouldn't solve the SSD problem, of course.
why remove the nice filesystem override? I quite liked the possibility of quickly changing things without having to re-bundle
The config value comes from the ActorSystemActivator, it may be overrided by the user to let him defines its own configuration (as we discuss on Monday with Raman).  You may, in your class extending ActorSystemActivator override the getActorSystemConfig such as  ``override def getActorSystemConfig(context: BundleContext) = ConfigFactory.parseFile(new File("./etc/akka.conf"))`` Of course, it's tricky to test in the osgi sample as it we need to override something that is not yet in dependencies...
this should avoid the error produce in jenkins  ``` [error] /localhome/jenkinsakka/workspace/akka-pr-validator/akka-osgi/src/main/scala/akka/osgi/ActorSystemActivator.scala:90: not found: type Config [error]   def getActorSystemConfiguration(context: BundleContext): Config = ConfigFactory.empty [error]                                                            ^ [error] /localhome/jenkinsakka/workspace/akka-pr-validator/akka-osgi/src/main/scala/akka/osgi/ActorSystemActivator.scala:90: not found: value ConfigFactory [error]   def getActorSystemConfiguration(context: BundleContext): Config = ConfigFactory.empty [error]                                                                     ^ [error] two errors found ``` Do I need to change this import position?
order (trttl, gremlin) ?
Now that you mention: as it stands now, the applied-adapters config   section accepts adapters from lowest layer to highest, while the protocol   scheme is exactly the opposite order. Should I change that config section   as well?
I have not yet dived into it, but looking at this comment Id answer: the config file order should match the toString order you see in logs, configs, etc.
I agree, I will change it.
Nice! Cleaning up the ThreadLocal.
aha, when I looked at your first fix I got scared, because I associated `gc` with `java.lang.System.gc()`
Just a quick thought. Won't this remove the ThreadLocal for ever actor unregistration? Shouldn't it be done in shutdown instead?
dont hit me with a bat, please, but this allocates one ThreadLocal per CallingThreadMailbox (i.e. per actor using CTD); is that so bad that it MUST be change in the RFC sense? or can we leave it at SHOULD level?
Oh. Missed that tiny detail. Hey it's for testing right. It's a SHOULD in my book.
okay, thanks for the confirmation
more test in this file has the same issue, as far as I can see amazing
oh, wow are you sure this is the only one?
I wouldn't use `match` for this
I can't see those. Did I miss some?  I looked through them again and the other ones should shut down by themselves or are shut down explicitly? 
would it make sense to have a similar check in `addInhabitants`?
As far as I can see from the code (I've been over it several times by now), and the fact that the counts add up.  I also ran tests with all registrations and unregistrations logged and matched up the paths and classes.
so this is the cause of the problem? then my guess why we see this now, and not in 2.0, is the new message passing for TerminationHook
So, I first had the fix in addInhabitants and tried to forcefully shut down the actor system in some way, but that just left a lot of debris.  Maybe I should just move the reportFailure to addInhabitants instead.
yes, you are right, they shutdown as part of the test
yes, I was thinking about the reportFailure, to be able to see it as soon as possible if it can't be a forced invariant you should leave the reportFailure here as well
Yes, this and the `actor.start` call in the RemoteDaemon are the important ones.  The reason that we don't see it in 2.0 is that a lot of how the internal versions actorOf have changed completely and inhabitants balance up in 2.0.
Very good analysis! Id prefer adding the `.start()` on the `rootGuardian` directly instead of unrolling it here, though.
Id prefer to blow up if `< 0` because then something is really wrong (as in the error you found above).
ah, I should not read a diff out-of-order
ok, thanks for clarifying
after reading it in-order I think the check should be in `addInhabitants` and print aggressively (although I think it should fail in a way which fails our tests); thenwhen the cunning plan of 2944 is in placewe can just Thread.stop the whole thing, which should get peoples attention ;-)
I agree with Patrik
Can we put these in one method so we don't need to add/remove from multiple places and that the tests for TestSerializer and JavaSerializer are always testing the same system messages?
but why repeat the the config here if it's supposed to be in reference conf?
Can we reduce the boilerplate with foreach?
Foreach? Define the sys msg classes in one val and use at both places?
reference.conf is always loaded by default, so yo don't need all this. AkkaSpec.testConf, or AkkaSpec without any special ActorSystem parameter should be fine.
great that you added this kind of test, that is something we should do more when we take serialization compatibility more seriously
what is the unit?
I think Patrik fixed this differently in another PR
Not keeping a reference to the child?
Why the Some(...)?
Should this switch to use the same approach for "ClusterEnvironment" as https:github.com/akka/akka/pull/675/files ?
Why this rename?
Looks like a no-op to me
latestGossip :+= collector.sample
1 Loc > 4 :-)      def gossip(): Unit = selectRandomNode(nodes.toIndexedSeq filterNot (_ == selfAddress)) foreach gossipTo
Is there semantics required here for ready to be at the front and initialized at the back of the sequence?
Point is, if that semantics is not needed, then the partition and collect can be collapsed into a single "map".
Equivalent to (but runs only one pass):      nodes flatMap { n => if (n.address == address) Some(n.metrics) else None }
It's just a number or window (Int) to grade the weighting, I could set it to 10 or 49
No need for it really.
Good catch, this used to be a val as an Option. Fixed.
decay 0 == boom
Immutable set of constants should be lifted into companion object
and i'll update the comment while I'm on a roll ;)
yes, how did i miss that!! updated.
This is a constant as well, could be a "val".
No need for braces
case n: BigInt => case n: BigDecimal =>
make absolute and move to top of file
Use scala.util.Try instead of Either[Throwable, X]
Yes, much cleaner. 
toSeq.head?      Just use: y.invoke(sigar.get).asInstanceOf[Array[Double]](0)
Rewrite to be a one-pass operation
instead of the match, just do a "map"
Same comments as above
Make into Try
Seemed nice. But with refactor to https:github.com/akka/akka/pull/675/files style it was removed.
use:     Try(call) getOrElse fallback Otherwise exceptions thrown by the callback will be outside of the Try
I'd probably log the exception at debug level
seems inconsistent to have _latestStats and clusterMetrics, use _ or not, but consistently :-)
We usually put these between the package declaration and the code imports, to easily see which extra features are enabled quickly
BigInt(0).longValue() ??? What ever happened to 0L?
    val committed = available.collectFirst { case ("heap-memory-committed", b)  b }   Do the same with the ones above, a bit easier on the old eyes :-)
Really? Expose jmx at the top of ClusterMetricsCollector?
yes, did not realize the lack of full import. thx
ah, did not realize. thanks!
I don't see the value of exposing the exception. We want the instance, with or without SIGAR.
agreed. I dislike underscores but didn't want to touch Patrik's code. I'll see if he minds :)
Actually, I just thought of one - it is possible to have the jar on the classpath but not have the os-specific lib installed properly or the jar if you don't know what you're doing. The differences may be helpful to a user during setup.  I'll log it.
What do you think about the log info - keep it as is?
Yes, good. I'd prefer to add a package declaraton for akka.cluster in a separate pull request if that's ok? And do the necessary refactors there.
This is solved in https:github.com/akka/akka/pull/675 which is not merged to master yet. Probably that will go in first and then you will adjust. I will guide you to what changes are needed when it's time to merge with that.   /Patrik  8 sep 2012 kl. 01:26 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/Cluster.scala: >  > > @@ -149,7 +149,9 @@ class Cluster(val system: ExtendedActorSystem, val failureDetector: FailureDetec > >    /** > >     * INTERNAL API > >     */ > > -  private[cluster] val clusterCore: ActorRef = { > > +  private[cluster] lazy val clusterCore: ActorRef = { > He had sent me that and I pulled it in. In Master it is different https:github.com/akka/akka/blob/master/akka-cluster/src/main/scala/akka/cluster/Cluster.scala But I know after the pull the NPE's went away. I'll look into it. >  >  > Reply to this email directly or view it on GitHub. > 
you could mention boundaries, eg. > 0
yes, when that's in and merged
    def gossip(): Unit = selectRandomNode((nodes - selfAddress).toIndexedSeq) foreach gossipTo
the reason for the `_` here is the name conflict with `def latestStats`
`nodes` should probably be called `nodeMetrics` or `metrics` here?
I don't think Viktor ment a package file, but just the order of the imports, i.e. place the language import at the top
that barrier is already done in awaitClusterUp
why this schedule and latch? I think all this can be written with one awaitCond
those multi-node tests are step-by-step, so you can rely on that previous step has been done, i.e. you don''t need awaitClusterUp here
I would like to see an additional check on that the metrics are changed, i.e. store away the nodeBetrics in a val, and then awaitCond until metrics have changed for all nodes
what is this? looks like wrong constructor parameters to Address, use one of the Address apply
perhaps place those in separate         "xxx " taggedAs LongRunningTest in {
remove this, it will be in MultiNodeClusterSpec, I have added it (in one of my outstanding PR)
Actually I can not use the same strategy:  private[cluster] class ClusterNodeMetricsCollector extends Actor with ClusterMetricsCollector with ActorLogging {   val cluster = Cluster(context.system)   [JVM-Node2] Caused by: 68d04775-ea08-4f16-ba95-06344f2819efakka.actor.InvalidActorNameException: actor name cluster is not unique!  ClusterCoreDaemon creates it with: val cluster = Cluster(context.system)  We have to expose it somehow for internal child actors.  
Yes, that was what I said.  /Patrik  8 sep 2012 kl. 15:26 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/ClusterMetricsCollector.scala: >  > > + > > +  /** > > +   * Samples the metrics for the node and updates metrics statistics. > > +   */ > > +  def collect(): Unit > > + > > +} > > + > > +/** > > + * Metric monitoring of the self-node intended for load-balancing / workload distribution. > > + * > > + * INTERNAL API. > > + * > > + * @author Helena Edelson > > + */ > > +private[cluster] class ClusterNodeMetricsCollector(val environment: ClusterEnvironment) extends Actor with ClusterMetricsCollector with ActorLogging { > Actually I can not change to this strategy yet, cluster actor name is not unique ;-) >  > private[cluster] class ClusterNodeMetricsCollector extends Actor with ClusterMetricsCollector with ActorLogging { >  > val cluster = Cluster(context.system) >  > So we can refactor it when Patrik's PR is in master. >  >  > Reply to this email directly or view it on GitHub. > 
PR-675-merge: here you will use settings.MetricsEnabled
PR-675-merge: here you will remove environment constructor param, and use Cluster(context.system)
PR-675-merge: here you will remove `with AccrualFailureDetectorStrategy`
PR-675-merge: here you will remove lazy and the comment
I've reduced these 11 lines to 2 :) for the next PR
Great, you don't need to create new PR, just push to this one.  /Patrik  10 sep 2012 kl. 17:22 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/ClusterMetricsCollector.scala: >  > > +    val available = data.metrics filter (_.isDefined) > > + > > +    val previous = this previous data.address > > +    val inGossip = metricKeys(previous) > > +    val (peers: Set[Metric], noPeers: Set[Metric]) = available partition (a  inGossip contains a.name) > > + > > +    val updated: Set[NodeMetrics] = if (peers.nonEmpty) { > > +      val mergedMetrics = peers flatMap (m1  previous.collect { case m2 if m1 same m2  m1 :+ m2 }) > > +      nodes collect { > > +        case a if a.address == data.address  a copy (metrics = mergedMetrics) > > +        case a if a.address != data.address  a > > +      } > > +    } else { > > +      val (initialize, ready) = noPeers partition (_.trendable) > > +      val initialized = initialize collect { case a  a copy (average = Some(DataStream(decay, a.value get, newTimestamp, newTimestamp))) } > > +      val merged = ready ++ initialized > I've reduced these 11 lines to 2 :) for the next PR >  >  > Reply to this email directly or view it on GitHub. > 
SIGAR returns 3 while jmx returns 1. There is no requirement to return 1 or 3, and one minutes is good. I'll leave as is for now and can modify this if requested, preferably after the PR is accepted to minimize changes now.
Yes, use 1.  14 sep 2012 kl. 20:04 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/ClusterMetricsCollector.scala: >  > > +  /** > > +   * Samples and collects new data points. > > +   * > > +   * @return [[akka.cluster.NodeMetrics]] > > +   */ > > +  def sample: NodeMetrics = NodeMetrics(address, newTimestamp, Set(cpuCombined, totalCores, > > +    systemLoadAverage, used, committed, max, processors, networkMaxRx, networkMaxTx)) > > + > > +  /** > > +   * (SIGAR / JMX) Returns the OS-specific average system load on the CPUs in the system, for the past 1 minute. > > +   * On some systems the JMX OS system load average may not be available, in which case a -1 is returned. > > +   * Hyperic SIGAR provides more precise values, thus, if the library is on the classpath, it is the default. > > +   */ > > +  def systemLoadAverage: Metric = { > > +    val n = wrap(LoadAverage fold (x  jSystemLoadAverage, > > +      y  y.invoke(sigar.get).asInstanceOf[Array[Double]].toSeq.head), jSystemLoadAverage) > SIGAR returns 3 while jmx returns 1. There is no requirement to return 1 or 3, and one minutes is good. > I'll leave as is for now and can modify this if requested, preferably after the PR is accepted to minimize changes now. >  >  > Reply to this email directly or view it on GitHub. > 
I remember now that I made the decision in not following Patrik's naming to make it very clear that the view returns metrics for the cluster, thus 'clusterMetrics' vs '_metrics' vs a singular nodes metrics. I'll keep it as is unless there's an uproar.
My spidey-sense tingles here. execute _early_??!
Single-shot tasks use roundUp() to ensure not early execution; the same goes for the initial delay of recurring tasks. Using that for the recurring invocations means that the minimum delay you can get between executions is 2*tickDuration, unless we were to check the clock() where we are in the tick when calling `schedule()`. Having the maxFrequency be 0.5 / TickDuration would be unintuitive and looking at the clock() costs time (quite a lot on Java 6; but thats what the old scheduler does).  So, which poison?
BTW: the worst case is to fire slighty less than 1 TickDuration early, but that happens only when the Scheduler is running at capacity (i.e. when dequeuing the tasks for one tick takes nearly the whole tick).
So the schedule early thing, isn't that more likely to be observable right after the `initialDelay` since you calculate the next delay from the original `initialDelay` and not from the rounded up `initialDelay`?  The old timer seems to keep track of when the current `tick` should have finished as well as the absolute time that a task should executed and if it picks up a timeout that's in the wrong bucket then it reschedules it.  Couldn't that be done for this scheduler as well? Not only keeping track of rounds for a task but also the absolute time that it should execute?
This `implicit` does not actually do anything, does it?
forgot to verify `lastSender == testActor`?
Yes it does, "forward" takes an implicit ActorContext.
Good one, will fix tmro.
check that the `secure-cookie` in this section is correct
`has been evolved to`  `has evolved into`
`listen ports`  `listening ports`
`a previous remote address`  `a remote address of`
`has been changed to`  `have been changed to`
~~`only`~~  Typo: `formely`  `formerly`.  I'd also italicise or quote `connections`.
`triggering`  `that triggered`
`reflects that`  `reflects the fact that`
This explanation kind of confuses me. Isn't a "connection-like" association just a connection? What about it isn't a connection?  (I realise it's not a network-layer connection, but it does seem like an Akka-layer one.)
`see the documentation of the remoting for detail.`  `see the remoting documentation for more detail.`  A link would be helpful too.
`transports`  `transport`
~~`The remoting subsystem of `~~ (maybe?)
     SPI. Implementations of the Transport trait can be loaded by         SPI. Transport implementations must extend this trait.      Transports can be loaded by
`Example for`  `An example of`
`Example for`  `An example of`
`possible decorate`  `possible to decorate`
Same comments as in migration guide.
`To intercept when`  `To be notified when`
`To intercept when`  `To be notified when`
`and the cause Throwable`  `and the Throwable cause`
`To intercept when`  `To be notified when`
`To intercept when`  `To be notified when`
`and the cause Throwable`  `and the Throwable cause`
    SSL can be used for the remote transport by enabling the transport ``akka.remote.netty.ssl``     via adding it to the ``enabled-transport`` configuration section. See description of the settings     in the :ref:`remoting-scala-configuration`.        SSL can be used as the remote transport by adding ``akka.remote.netty.ssl``     to the ``enabled-transport`` configuration section. See a description of the settings     in the :ref:`remoting-scala-configuration` section.  Is SSL used *as* the remote transport or is it used *for* it? Or in other words, is SSL the remote transport or does it enhance an existing transport? 
No, it is the remote transport. Currently SSL is not a transport adapter.
`Member nodes are identified with their address, in format`  `Member nodes are identified by their address, with the format`
`Member nodes are identified with their address, in format`  `Member nodes are identified by their address, with the format`
` for the 2.2 release, and others are pluggable.`  ` for the 2.2 release; other transports are pluggable.`
`host & port`  `host and port` (a bit more formal)
`This change results in various necessary changes in`  `This has required some changes to`
Mention (somewhere) what the planned forward- and backward-compatibility of the new protocol will be? I guess that's what you're hinting at when you mention future-proofing.
+1 That should go
Ah buggah, yes, I fixed it for the cause case but not the (my) no_cause case.  When you gonna push? :)
Already pushed :-)
Hm, can't find it. uri?
"bytes" is mutable.
True, but ```ByteStringCompact``` is ```private[akka]```. It was (and still is) done the same way for ```ByteString1```:  ```scala   private[akka] object ByteString1 {     def apply(bytes: Array[Byte]) = new ByteString1(bytes)   }    /**    * An unfragmented ByteString.    */   final class ByteString1 private (private val bytes: Array[Byte], private val startIndex: Int, val length: Int) extends ByteString { ```  The clone happens in the public object:  ```scala object CompactByteString {   def apply(bytes: Array[Byte]): CompactByteString = ByteString.ByteStringCompact(bytes.clone) ```  like is was done before in ```object ByteString```. 
Remove this method and put "UTF-8" as the default value of encoding.
Sort of confusing with ByteStringCompact and CompactByteString.
This docs doesn't really explain anything-
Optimize the case where bytes.isEmpty?
I did it this way (two apply(String) methods), because it was (and still is) done like this in ```object ByteString```. Should I change it in both ByteStringCompact and ByteString to keep them consistent?
Yes, I'm not entirely happy with ByteStringCompact and CompactByteString. Proposal: I'll rename the internal class from ByteStringCompact to ByteStringCompactImpl or (maybe better?) ByteStringStrict. Would that be acceptable?
Yes, change both. Overloading in Scala is tricky, and often not needed.
If it's just internal, just put the impl inside the ComactByteString companion object?
Strike ByteStringStrict - it's still not a strict collection, behaviour-like, just compact as it is.
Oops, indeed. Sorry, I will improve the docs!
I wanted to ByteStringCompact next to ByteString1 and ByteStrings, to keep the three implementations together, and so that ByteStringCompact has access to things which are private[ByteString] (though currently, the critical stuff is all private[akka]. 
Ok, I renamed it to ByteString1C now. :-)
Sorry, can't - some tests don't compile when I combine the apply(String) methods. Lot's of errors like ``` [error] /home/oli/Data/Source/Scala/extern/akka/akka-docs/scala/code/akka/docs/io/HTTPServer.scala:199: could not find implicit value for parameter num: Integral[java.lang.String] [error]   val connection = ByteString("Connection: ") ``` 
no need to do anything in the other `case ShutdownAndFlush`? previously that did something via `postStop`
ah, ok, I see. 
No, since that happens before listen is called, which starts up the transports
should we yield false if this fails?
is it okay to shut down transports and endpoints in parallel?
and when receiving `()` it sends the final `systemGuardian ! TerminationHookDone` -- looks good
I would prefer something more visible than `()` message, but that is an implementation detail
but this `andThen` change did not really change anything, unless Im mistaken
Isn't this sequential between flushStatus and shutdownStatus since I create the future of the second inside the for clause?
oh, forget it, my bad; maybe add a comment?
and I assume that you have tested (manually) to verify that system.awaitTermination doesn't return before all this is completed (with extra sleeps)
so that nobody optimizes it
Well, andThen is more sequential. I don't think anything apart from Unit is needed for RARP, I think all the error reporting has to be done until this point.
Yes, I logged all phases of shutdown with sleeps everywhere. It took some time until I got it right.
Is that one special in some way? If we care about that one, then we should care about the ones below as well I guess?  I think this is just best effort.
yes, I think we should
Patrik is right, but I have to make sure that we keep going on even if some steps fail.
This was somehow reverted at some point, since I split these once
yes, it is only for the logging
I think this baby has got to go
Nice catch dawg
add FIXME, needs to be changed manually for the final release
In migration guide section 'Scheduler Dispatcher' there is a `Duration.parse`, please fix that also
why fully qualified name here? why not import as we normally do?
You're right, I'll fix 
So basically you're using the synchronization of the lazy initialization to ensure that all users wait for the value to be initialized? I mean it's not that this has to be done after something else right?
yes, thats right the problem was that Helena was adding another cluster actor, which use subscribe and sometimes that call was done (from another thread) before this val was initialized. The ugly part is that we expose this (as ClusterEnvironment) to the cluster actors (in constructor of ClusterDaemon) before the constructor of Cluster has finished. Since it's only for the internal cluster actors I think it's alright, or do you have a better suggestion of how to do it?
It shouldn't be possible to get a reference to an Extension instance until it has been initialized. Can you minimize it?
no, that is not possible, this is only an issue for the internal actors, created by the Cluster
Shouldn't they use Cluster(context.system)....
nope, for testing purpose we create special Cluster instance, not the extension
Sounds like something we should avoid, creating brittle code to satisfy testability seems wrong.
creating untestable code usually also hints at some design flaw (just a general observation, have not thought about the specific case)
This resolves the NPE for subscribe/unsubscribe. Thanks Patrik.  
I'll see if I can use ActorSystem.registerExtension to plug in the Cluster subclass for testing purpose. If that is possible the internal actors can use Cluster(system).
I tried some experiments with registerExtension of a test instance of the Cluster extension. I thought it would work if I'd override equals and hashCode to make the two ExtensionId instances be treated as the same id, but that doesn't work because we use ConcurrentIdentityHashMap for the extensions in ActorSystem.  Why do we use ConcurrentIdentityHashMap and not ConcurrentHashMap for the extensions?  Please enlighten me if there is another way of doing it.   
hmmm, don't know. what is the cluster-instance-replacement adding?
It's adding pluggability of failure detector and seed nodes. I have started refactoring which will remove the need for a test Cluster extension instance, so problem will be solved in another way. 
why local methods?
might want to cache Cluster(context.system) in a val
Should be added to the ConfigSpec
no, ClusterMessage are the ones that are sent over the wire, and needs to be protobuffed
reduce clutter further down, for example `selfAddress` is referred to 44 times
I do that in ClusterCoreDaemon, where it matters, this actor is a short lived actor that only calls that a few times. Performance doesn't motivate it, but I can sure do it if you think it's more dry
it's already in ClusterConfigSpec note that this setting existed before also
You could do:        import cluster._
This came from @viktorklang 's change to `Try`. Should it be NonFatal(e) or is that already taken care of in `recover`?
rather not, because there would be confusing name clashes, for example `def join` exists in both places
I could of course change to `import cluster.selfAddress`. That would make sense.
CAtch all is fine here, Try handles NonFatal :-)
great, otherwise it was a good catch Jonas
So I get a bonus point anyway? 
yes, 10000 reviewer-awake-dimes to you
How do they know what timestamp to use?
they click on the link above, and pick one http:repo.typesafe.com/typesafe/snapshots/com/typesafe/akka/akka-actor_@binVersion@/
Ah, the context to that link was not present in the diff. thanks
you're sending the Actor instance to another Actor and not the ref, big no-no
A bit ad-hocy, and id happily overwrites the previous values if it receives more of these messages, is that intentional?
If you can't have an endpoint without a processor, they shouldn't be separate vars.
2 closures created when isEmpty has already guarded this is a bit too wasteful.
oops didn't think about that. it's the ConsumerConfig info I need to pass on I'll separate that out. 
ok I'll just use get
Good one, I'll guard for more than once and create a case class for holding the endpoint and processor
this is not all places in cluster sample, search for `akka.remoting.transports.tcp.port`
Where? I don't find it.
And I actually run the sample... 
Ah there are conflicting changes. Have you merged something? I rebased this not too long ago.
intentionally not calling super.preRestart?
def start(actor:  Actor, name: String)(implicit system: ActorSystem): ActorRef =   Await.result(CamelExtension(system).activationFutureFor(system.actorOf(Props(actor), name = name))(10 seconds, system.dispatcher), 10 seconds)
no, will change
If someone calls endpoint or processor in the constructor then they'll get null...
crap. yes. maybe they shouldn't? :)  I had some intermittent issues with just using a val, and a lazy val 'timed out' the concurrent tests. I'll check what can be done. 
What's the purpose of making it lazy if it is forced in preStart if not earlier anyway?
So that if someone in a constructor calls it, it will get a value. but that doesn't really make sense, cause if that would work OK I might as well have done it as a val.  But, I'm rewriting the activation right now in a separate ticket, and came to the conclusion that it is better to not expose this internal endpoint and sendprocessor. If the user wants an endpoint, they can do camelContext.getEndpoint(endpointUri) and  if they want a sendProcessor they can get one doing just: new SendProcessor(endpoint) with that created endpoint.  I don't know why I didn't think of that earlier, maybe cause I'm rewriting that piece now.    so in this case (and this version) I will just make them private. 
and then it shouldn't be lazy either right?
Alright, let me know when things are ready for review, I've got an insane amount of code to review
OK. good luck :-)
Here it is:  https:github.com/akka/akka/pull/677   On Thu, Sep 6, 2012 at 2:29 PM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/main/scala/akka/camel/Producer.scala: > > > @@ -15,9 +15,17 @@ import org.apache.camel.processor.SendProcessor > >   * > >   * @author Martin Krasser > >   */ > > -trait ProducerSupport extends CamelSupport { this: Actor  > > - > > -  protected[this] lazy val (endpoint: Endpoint, processor: SendProcessor) = camel.registerProducer(self, endpointUri) > > +trait ProducerSupport extends Actor with CamelSupport { > > +  private[this] lazy val (_endpoint, _processor) = camel.registerProducer(self, endpointUri) > > +  protected[this] def endpoint = _endpoint > > +  protected[this] def processor = _processor > > + > > +  override def preStart() { > > +    super.preStart() > > +    make sure registerProducer is called > > +    endpoint > > +    processor > > Alright, let me know when things are ready for review, I've got an insane > amount of code to review > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/672/files#r1543573>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
`shutdown` -> `shut down`
`shutdown` -> `shut down`
ok, replaced those, in scaladoc also
ok, replaced those, in scaladoc also
what is the java api for this?
what is the java api for this?
what is the java api for this?
    def getByteBuffers: java.lang.Iterable[ByteBuffer] 
Would have preferred to have the same method for both, but I guess we're screwed.
Added getByteBuffers and added test
Why is this not a lazy val? (Sorry if this was already discussed)
We want to keep resident memory usage down.
Ah, true, it would be retained as long as the ByteString lives. 
ok, so here we used the 2 seconds timeout, isn't that used at other places in the test, which could cause the same problem
it looks like there is a mix of timeout.duration and remaining in this test file, should it be like that?
Do we mean `SingleExpectDefaultTimeout` when we say `remaining` here? I can't find a `within`. It looks confusing to me.
Switched to DefaultTimeout.timeout for them all.
this is fine, since you only use remaining, which without `within` is akka.test.single-expect-default =  = 3s (dilated) 
I think that is confusing, I think remaining should always be the offset from the test start - timeout if there is no surrounding within block
why not `remaining`?  `timeout.duration` is akka.test.default-timeout = 5s (no dilation) `remaining` without `within` is akka.test.single-expect-default =  = 3s (dilated)  I'm not sure there is still a mix of this timeout.duration and await remaining, but I guess it would be easy to introduce that mix again, which would be very confusing, especially due to the dilation
This is the timeout for the internal ask-calls inside the actor, it'd be very confusing if they weren't fixed.
At other places, like `expectMsg` it makes perfectly sense as it is. I think you should use an explicit within if you care about it.      "be able to call Future-returning methods non-blockingly" in within (3 seconds) {
it's also confusing to have the await timeout shorter than this ask timeout, isn't it?
yes, Patriks suggestion achieves what Viktor actually wants.  Viktor, without that `within` marker TestKit cannot possibly know when the test started, hence it also cannot give the meaning to `remaining` which you think it should.
No, not really, the ask-timeout is just the longevity of the PromiseActorRef and the subsequent fulillment of TimeoutException inside the Future, the reader (the guy with Await) still is in control for how long he wants to wait, and it doesn't make sense to wait longer than the promise lives anyways)
Then I suggest having ``remaining`` throwing an exception if used outside of a within block
And will have to add ``within(timeout.duration)`` for each and every test. yay for boilerplate.
WDYM? If you want to limit how long something takes, then yes, youll have to say so. I dont see how this could be different? What would be the semantics? How would it work?
Have an implicit within for each test where the default value is the ``timeout`` of the test (in this case DefaultTimeout), all ``remaining`` calls would be relative to the closes ``within`` block, and at the top it'd be the default test timeout.  But I think ``remaining`` should be throwing exceptions if used outside of ``within`` blocks if it isn't relative at all in that case.
I dont think it would make sense to have a default test timeout (because it would have to be insanely long, like minutes or so; tests are very different), and the SingleExpectDefaultTimeout was chosen as a reasonable we expect a response right now thing which survives mild machine troubles (but as we know fails sometimes). Making it much larger does not gain much.  Also: how exactly did you envision your implicit within to work? Write our own test framework? (this is not meant inflammatory, you know)  Anyway, we dont have time to rework our test suite right now, so lets add a `within` here and be done with it. We can revisit this when 2.2 is out.
I'll do that, as I said, but I think we should change ``remaining``to throw an exception if used outside of ``within`` and then add an ``expectTimeout`` method that returns ``SingleExpectDefaultTimeout`` because it is absolutely hideous. Then we can see how many tests call ``remaining`` when they shouldn't and we can change them to use ``expectTimeout``.  Deal?
do we have a volunteer? ;-)
ok, I thought it was more interesting for the test to see the real ask-timeout, than the await-timeout, but I don't know the details of the test. Also, I hope you see my point in that mixing those you can trigger different timeouts in different environments due to the dilation factor.
Yes, absolutely. an AskTimeoutException IS-A TimeoutException so as long as you catch TimeoutException it's all the same.
Yes, of course, I suspect we have a few tests that are broken due to this false assumption. You do have to agree that something called ``remaining`` that isn't relative unless in a specific context is a recipe for problems. (someone moves a within block or else)
BUT I won't do that as a part of this pull request as it is vastly more disruptive.
of course, I was thinking more along the lines of making a proper ticket for it 
Break this out into a private method to stay DRY?
we have a ticket for moving this to a macro since it is on the hot path; not sure about macros for akka-actor, though, since that would introduce a new subproject.    Ciao,  Roland  On 27 jan 2013, at 15:55, Viktor Klang () <notifications@github.com> wrote:  > In akka-actor/src/main/scala/akka/actor/dungeon/Dispatch.scala: >  > > @@ -98,21 +107,30 @@ private[akka] trait Dispatch { this: ActorCell  > >    final def stop(): Unit = > >      try dispatcher.systemDispatch(this, Terminate()) > >      catch { > > -      case e @ (_: InterruptedException | NonFatal(_))  > > +      case e: InterruptedException  > > +        system.eventStream.publish(Error(e, self.path.toString, clazz(actor), "interrupted during message send")) > Break this out into a private method to stay DRY? >  >  > Reply to this email directly or view it on GitHub. > 
Interrupted-exceptions are on the hot path? wat?
I think we are talking about different levels of DRYness.
Maybe something like this to DRY it up:  ```scala   import scala.util.control.Exception    private def handleInterruption: Exception.Catcher[Unit] = {     case e: InterruptedException        system.eventStream.publish(Error(e, self.path.toString, clazz(actor), "interrupted during message send"))       Thread.currentThread.interrupt()     case NonFatal(e)        system.eventStream.publish(Error(e, self.path.toString, clazz(actor), "swallowing exception during message send"))   }     and then:    try dispatcher.systemDispatch(this, Recreate(cause)) catch handleInterruption ```
(may not be as performant if this is a hot path, though)
I was more thinking along the lines of:      private def dealWithInterruption(e: InterruptedException, msg: String): Unit = {       system.eventStream.publish(Error(e, self.path.toString, clazz(actor), msg))       Thread.currentThread.interrupt()     }  case e: InterruptedException => dealwithInterruption(e, "interrupted during message send")  To make sure that the interrupt is restored everywhere.
+1 for factoring out the duplicated code
after checking the byte-code for Ryans Catcher variant I think that is the best option right now
Nice! I learned the Exception.Catcher trick from the spray guys.   On Jan 29, 2013, at 2:57 AM, Roland Kuhn <notifications@github.com> wrote:  > In akka-actor/src/main/scala/akka/actor/dungeon/Dispatch.scala: >  > > @@ -98,21 +107,30 @@ private[akka] trait Dispatch { this: ActorCell  > >    final def stop(): Unit = > >      try dispatcher.systemDispatch(this, Terminate()) > >      catch { > > -      case e @ (_: InterruptedException | NonFatal(_))  > > +      case e: InterruptedException  > > +        system.eventStream.publish(Error(e, self.path.toString, clazz(actor), "interrupted during message send")) > after checking the byte-code for Ryans Catcher variant I think that is the best option right now >  >  > Reply to this email directly or view it on GitHub. > 
its a small world, indeed ;-)
might not be needed?
I had 30 seconds timeout last friday and therefore I set the within timeout to 45 seconds on release-2.1 branch. I wrote a comment about that, which you might consider to add:       the long within timeout is because some networks have long timeout for "cannot resolve unknownhost"     "receive Terminated when watched node is unknown host" in within (45 seconds){ 
Oh, thanks, I forgot to ask you about your timeout experiences.
I think this needs a bit of documentation outlining what it's purpose is etc, so if the need ever disappear we know that it isn't needed anymore. 
given that they deserialize the same, why not reuse `DeadLetterActorRef.serialized`?
Thanks for the explanation in Flowdock. How about: "The length of time to gate an address whose name lookup has failed. No connection attempts will be made to an address while it remains gated. Any messages sent to a gated address will be directed to dead letters instead. Name lookups are costly, and the time to recovery is typically large, therefore this setting should be a value in the order of seconds or minutes."
This seems to only work on ZMQ >= 3.0
Correction: 2.2: https:github.com/zeromq/jzmq/pull/109/files
This has no place in the README
Why DEBUG on one line and "DEBUG" on the next?
Use ThreadLocalRandom instead.
Why isn't this defined below "maxMessageSize"?
counter = if (counter >= 2999) 0 else counter + 1
Why this line?
renove this line or add comment about why there is no more code
currentTime == System.currentTimeMillis and should not be used to measure elapsed time as it isn't accurate. Use nanoTime
Should include a comment/output on how to terminate
I suggest "Array()"
Same comment as before
Why this line?
Don't define messages inside the class, they are not static inner classes
why this line?
Why is this defined here?
Why this line?
Why this line?
needs to output how to terminate
should be immutable.Seq
how do you deal with overflow?
Why are these done as messages?
No need to make this into a method, leave it as it was
why add braces?
make it a one-liner
no need for this val, just use recvTimeout
Why the "Value"-suffix?
Remove "Value"-suffix, is not used anywhere else in the codebase
No need for braces
default should be from the config not in code
default should be from the config not in code
why have you removed mima?
This file should IMO not have been checked in
Why is this removed?
Why is this added?
Frame does not exist in master IIRC
socketType as an Int is a bit too loose of a type, would make sense to type it and validate.
Any reason to use blocking here?  If supervisor is None, you won't get any sensible error message, right? I suggest passing the "supervisor" actor into the constructor as to not conflate supervision with services. Makes sense?
What is the return type of this method?
I'd suggest switching to either Vector or a mutable append data structure to avoid reverse on line 28
I recommend:      case null => None     case bytes: Array[Byte] => Some(bytes)     case _ => None
What is the purpose of this synchronize-statement?
Is it intended to be able to send multiple Start messages to the same actor during its lifetime? Otherwise I suggest putting that logic in preStart
Why is the field private?
Can the actor get the Start message multiple times? Is there a reason that the context (supervisor) isn't providing the Sockets in the constructor?
You're using shared mutable state between the context and this (registering the sockets with direct calls instead of with messages etc)  This should be possible to model with Actors so that there's a poller-actor etc.
There doesn't seem to be a way to configure which dispatcher that spawned actor will use, is this intentional?
This is unsafe since this is a captured self reference from the parent Actor, you should store away the self-reference and capture the stored reference instead. (I'm assuming the != null check is there due to NPE in the code)
Why the synchronized block here, which is the owning monitor? (And who is the contending party)
What is the purpose of this? To do Thread.yield()?  I think the receiveMessages method should be reimplemented like Derek has done for IOActor, here's a link to how it's implemented:  https:github.com/jboner/akka/blob/wip-923-derekjw/akka-actor/src/main/scala/akka/actor/IO.scala#L463
Why is this a lazy val?
Why is this a lazy val?
Any particular reason that this is an Array (mutable)?
Any particular reason that this is an Array (mutable)?
Can this be sent/processed multiple times during the lifecycle of this actor?
This could be a case class, or?
I suggest implementing the postStop callback at another step or through a trait instead of blacking it out. (because it's hard to know why)
Can this be received multiple times during the actors lifecycle?
This should be implemented as IOActor to avoid hogging a thread. (also it's not configurable to choose which dispatcher will be used for the spawn)
If there is an error in the calls above, the socket will never be closed.
I think you meant to say !
I think you meant to say !
I think you meant to say !
I think you meant to say !
I think you meant to say !
Please use TestKit (It already has awaitCond etc)
Please use TestProbe from TestKit for this
So, if no blocking is used here, then how 'remoteSocket' would be provided from the context actor ('sup'), or were you thinking that the actor would process messages that provide the created socket?
The idiomatic actor style would be that the supervisor creates its children and passes to them (either via constructor or first message) all information that they need to perform their task.
Alright, I'll revise the code accordingly.
Is Context threadsafe?
Scala's Vector is usually quite a bit more performant and supports append, so use List only if you need the prepend semantics.
Switch to "!listener.isShutdown" (isRunning is removed in master) What should happen if the listener is shut down?
Instead of calling back into the Actor, I think you should send it messages (otherwise you have a race) Also, there is no check for the current status of the actor (what happens if it's dead?)
Nice! Would the code be clearer if the run-method would be named differently?
this can be shortened to: listener foreach { _ ! Connected }
Yes, from the ZeroMQ documentation:  "A MQ context is thread safe and may be shared among as many application threads as necessary, without any additional locking required on the part of the caller."  http:api.zeromq.org/2-1:zmq
Two strategies: either the run-select loop is terminated or nothing. I don't have a strong opinion about that, may be nothing?
"receiveFrames" can be replaced with the following: private def receiveFrames: Seq[Frame] = {   var frames = Vector.empty[Frame]   while(socket.hasReceiveMore) receiveBytes foreach { bytes => frames :+= Frame(bytes) }   frames }
Can you elaborate this note a bit...?
If performance is important here I think I'd recommend not using options and use a cached empty byte array as an EOF marker:  val noBytes = Array[Byte]()  @inline private final def receiveBytes(): Array[Byte] = socket.recv(0) match {   case null => noBytes   case bytes: Array[Byte] if bytes.length > 0 => bytes   case _ => noBytes }  Then you can just match on it:  receiveBytes() match {   case `noBytes` => done   case bytes => Frame(bytes) } 
Nice touch!  Are you sure you want prepend semantics on the requests?
Nice with the SocketType!
You don't need the "val" keyword when it's a case-class. (protip)
Nice! Might be nice to be able to specify a supervisor to the actor?
Might want to make that configurable, Akka has a really nice Duration type in akka.util.duration
I was thinking that ZeroMQ would supply one, down the road, i.e. all ConcurrentSocketActors would have a supervisor when created using #newSocket. Bad style, you think?
That won't work as hasReceiveMore is false, if the message does not comprise of multiple message frames.
Instead of directly calling into the ConcurrentSocketActor through methods like connect, bind etc, you should send messages to the actor.
If the listener is shut down it will never come back up again, so should it still keep running?
So you need to call recv(0) _before_ hasReceiveMore?  If so:      private def receiveFrames: Seq[Frame] =       receiveBytes() match {         case `noBytes` => Vector.empty         case someBytes =>           var frames = Vector(Frame(someBytes))           while(socket.hasReceiveMore) receiveBytes() match {             case `noBytes` =>              case someBytes => frames :+= Frame(someBytes)           }         frames }
I think it might be better to have def newSocket return Props: (for the 2.0 version)  def newSocketProps(context: Context, socketType: SocketType, listener: Option[ActorRef] = None, deserializer: Deserializer = new ZMQMessageDeserializer) = Props(new ConcurrentSocketActor(context, socketType, listener, deserializer)  Then instead of taking the dispatcher into the constructor, you simple get it from the context (inside ConnectionActor: context.dispatcher)  Then when you want to create a connection you do:  actorOf(ZeroMQ.newSocketProps(...).withDispatcher(myowncustomdispatcher).withSupervisor(myownSupervisor))
Yeah, probably the right thing to do at that point is to stop, as the messages would be otherwise lost.
I don't see where is the race as requests are processed synchronously in #select, no?
Cool, #receiveFrames looks much better now!
How about for the 1.3 release? Support for a user-defined supervisor?
An interesting solution would also be to export it as an Iterator and fold it:      val i = new Iterator[Frame] {       var bytes = receiveBytes()       def hasNext = bytes ne noBytes       def next = bytes match {                           case `noBytes`   => throw new IllegalStateException("EOF")                           case someBytes =>                              bytes = if (socket.hasReceiveMore) receiveBytes() else noBytes                              Frame(someBytes)                        }       } }  Then to collect all the avalable frames:      (Vector[Frame]() /: i)(_ :+ _)
But you're calling out to the methods of the Actor, which is a violation of the ActorModel, since the Actor could be shut down or it's fields might be invisible from a JMM perspective, why not put the connect, bind etc as methods inside the select-function? (or are they used internally in the actor as well?=
No, they are only invoked from #select and should not be invoked from any other site, either inside or outside the actor.
Alright, good, so move them into the select function so that they're only reachable from within it.  Great work Karim!
For the 1.3 release I think it's better to create the actor inside the newSocket method and pass the supervisor into it and have it set prior to starting it:  def newSocket(..., supervisor: ActorRef): ActorRef = {   val a = actorOf(...)   supervisor.link(a)   a.start }
Thanks for pointing that out. I think your previous suggestion is much clearer :)
This is not thread safe. Isn't there an isClosed-method on Socket?
Not threadsafe here either
Not here either
Since this method is only ever used in receiveFrames, you might as well move the method definition into receiveFrames, wdyt?
No, there is not one. What if the if-statement were to be removed from #postStop?
Thanks, those two methods (receiveFrames, receiveBytes) were moved to 'select'.
Switch to checking self.isShutdown and remove the "var isClosed"
An idea,  is there any reason that these ops are done on the thread of the selectTask and not inside the ConnectionActor itself? (Or was it like that before, I don't recall?)  Would it be possible to do the receiving of bytes Only in selectTask, and then to the rest in ConnectionActor, then you can capture at the start of the selectTask:  private val selectTask = { () =>  +    def connect(endpoint: String) {   becomes:  private val selectTask = {    val self = this.self () =>  Then you do not need to check self == null anywhere, and you do not need to do select(), but you can do:  if (!self.isShutdown) self.dispatcher.dispatchTask(this)  at the end of selectTask.   Thoughts?
This will generate a MatchError if frames.length <= 0, is this intentional?
Great, you can simplify it a bit by:      receiveFrames() match {       case Seq() =>       case frames => notifyListener(params.deserializer(frames))     }
`Seq#length` is O(n). 
Jason has a valid point, change it to something like:  val i = frames.iterator while(i.hasNext) sendBytes(i.next.payload, if (i.hasNext) JZMQ.SNDMORE else 0 )  Thoughts?
Should be : index.valueIterator("s1").toSet must be === Set(1) i.e. drop (assert)
Good work!  One thing that needs to be added is parallel access to Index, (since the entire point of it is to be threadsafe), can you add that  Cheers!
Ok, that syntax is more intuitive. 
Thanks, I will try to write a test for parallel access as well.
You can probably get away with:       val index = new Index[Int, Int](100, _ compareTo _)
You can probably just use ".sum" instead of ".fold(0)(_ + _)"
Coolio, does it work with the latest master?
Yes, I thought it was ok to use Scala actors since they are part of the standard library. Of course Akka actors could be used to archive the same thing: spawn {   ...  do stuff } One reason to not use Akka actors could be that the Akka actors may be dependent on the Index class (in the future). If the test of the Index class is dependent on Akka actors, then the test is testing something with the help of the thing that shall be tested. This is probably very far fetched so I will change it to Akka actors instead.
I can check that before I push a new version of the code.
I'd suggest using: akka.dispatch.Future instead  Future {  ... }
Do we need this indirection?      listens onFailure addressesPromise.tryFailure
the reason I added the ListensFailure was for the `pipeTo`, but perhaps we don't care about unhandled
wouldn't it be better to store the listeners as `Runnable` instead of `()  _` and then it wouldn't be needed to create `new Runnable`
good point, changing the API now (from Callable[T] with documented ignored return value to Runnable)
Awesomely done Patrik!
reg.failureDetector(address) collect { case p: FailureDetectorPuppet => p }
Are we sure that you want to reuse the same config?
I'm not sure. First I started without that, but it was a bit too much duplication to feel good. On the other hand it will be easier for user to see full config in the documentation, since we include it there. I'm on the fence.
this is racy
no dot at the end of line here
My settings are more sensitive than your settings were in the cluster   module. I started a discussion on that, I thought that the outcome was   that it makes sense to have it that way. WDYT?
that is why I override threshold and max-sample-size
Ah, I missed that one.
"a constructor with a com.typesafe.config.Config parameter"
"How often keep-alive heartbeat messages should be sent to each connection."
I liked the old look better
ok, change it
ok, changed it back
Avoid combining default constructor parameter values with auxiliary constructors.
Might want to use Mapper instead of JFunction
Returns a _new_ CamelMessage with the given ...
I think this is redundant, just do: message.attachments get name
Isn't this just liability, just use: message.withAttachments(message.attachments ++ myAttachments)
Not documented, and isn't it a bit weird to have mutability mied with immutability?
This implementation seems like there should already exist such a utility function amongst the dependencies, try to find one and use that. (DRY)
So what happens if you have an attachment which is a file on disk, does this force that file into memory? If so, then the usefulness of this could be questionable.
If this throws then the files won't be deleted (below=
I'm not a huge fan of adding dependencies (even for tests), can we avoid these?
Make this so that it cannot use a different version than the camel version we use (use a val)
ok didnt know that one yet cool
ok. I just took across the way headers was done by the dude before me. Will bring the amount of similar methods down there too.
It's needed in apache Camel land, but it should actually be private. will change. 
Yeah, didnt know if I shoudl pull in a dep instead of handrolling this once. Is it ok to depend on scala.tools.nsc or not ? otherwise ill find something else. 
yes this does go into mem. The other way I though of is to keep some type of filestore in the camel extension, write an attachment to file if it is big (configurable what big means), or always write it to file, and keep some kind of immutable 'handle' to the attachment (basically a string url) that is stored on a temp location in a file. Not sure when it should then be deleted though. could make a time based / scheduled cleanup of temp, or make the user responsible (like a delete method on the handle). The Attachment would then be something like Attachment(name, contentType, handle). what do you think?  
I needed these to test the attachments with jetty. I don't see other existing deps that have http ability. I could use 'good' ol' HttpUrlConnection from the java stdlib. the camelJetty is a necessity if you want to test jetty with attachments, but using the HttpUrlConnection will remove httpclient and mime. let me know what you think.
It is a bit contrary to your previous DRY comment though ;-)
What problem is this trying to solve?
No, I meant, couldn't you use straight camel to test Akka Camel or somesuch?
The above comment/idea? that you would not be able to handle huge files if they can't fit in memory. But it's a lot of complication for possibly an edge-case. Thought it would be a good idea to do the simplest possible first, but maybe the ByteString is too simple.  The main problem with how camel does it internally is that it is all mutable (uses javax.activation.DataHandler) So the solution needs to be immutable of course. so it's or keeping the entire thing immutable in mem, or having an immutable ref to the thing on disk. I'll build something and see how it goes with the file approach.
Hmmm, don't put any work into the file-based approach, it might end up wrong as well. So, just to backtrack a bit, what are attachments and why are they currently a problem, and in what way?
Apache Camel Messages consist of headers, a body, and attachments. Attachments were never implemented in version 1.x of akka-camel, or as part of the rewrite so far. I picked it up because there was an old ticket from Martin.  http:camel.apache.org/maven/current/camel-core/apidocs/org/apache/camel/Message.html  the attachments in the apache camel version (mutable version) of the message uses DataHandler as type for the attachments. These things are mutable so we can't just take them over to the akka.camel.CamelMessage class.
Well, I don't think we should eagerly suck them into memory either. What are the alternatives?
Well, I did do that before and everything worked, for it to not work with a real component (jetty), and fixing it based on the problems found. I can look into that a bit more. The ticket was specific to camel jetty though: https:www.assembla.com/spaces/akka/tickets/1214 I'll investigate if I can test it without jetty and the http deps.
If you don't want to be eager, then the actual attachment can't be part of the CamelMessage case class. we could add 'attachmentRefs' to the CamelMessage class, like refs or handles to the attachments and add some methods on CamelMessage to get the actual data for the ref, which will recreate the bytes from the ref. something like camelMessage.materializeAttachment(attachmentRef) (or a far better name, not my strongpoint) Problem is, when I tried in a previous version to just only take over the 'id' information of the handler and stream the data later, the data already disappeared. But that could be solvable.. I'll get back to the drawing board.
When does the data disappear?
Yeah I gotta look into that, it was in a test where I just used the datahandler stuff instead of the bytestring. I guess it might be somewhere inside camel..
I should also unsubscribe in unwatch when there is no non-local watchers left
This is waaay too expensive, put the call outside of the for-comprehencion
Make this a 1-liner and move it into the if-clause below (since it isn't used in this scope.
ok, will do, maintainAddressTerminatedSubscription has an optimization check on a, so it wasn't waay, just way
existenceConfirmed true or false here?
false. Because we could have been watching a RemoteActorRef that had never resolved before the other node went down.
thanks, I have removed the fixme and added your comment
add FIXMEs for when we go final?
Yes. A FIXME should probably be added to the dependencies in AkkaBuild as well.
Added tick https:www.assembla.com/spaces/akka/tickets/2684 to 2.1 instead to remove all cross CrossVersion.full when we go to Scala 2.10.0 final.
"In Scala" in the Java rst?
Yes, the ``Future`` is in Scala now. Should we try to hide from people that Akka is written in Scala?
No it's fine.
Then clarify it to say "In the Scala Standard Library, a Future..."
Will a Java programmer be able to relate to Scala Collections?
That's a really good question. Do you have a better analogy? Is it important since what it gives you is explained in the sentence after?
Yes, that is better.
You can use this instead if you want:         node(first).address
`node().address` can only be used in the jvm which runs that node, so if I want secondAddress from first node that will not work, so I think I stick to always retrieving the address from the conductor
ah, sorry for my stupid comment, now I see what you mean. node is a utility in MultiNodeSpec hmm I think something need to change name here if that is named node, we should probably use something else in the tests (I named the cluster node, after your tests) I can change that to cluster.
I thought this would be solved with Rolands in-mem db/registry. Is it implemented? 
Do we need to tag these as LongRunningTest still? Would be nice if all MultiNode tests were automatically tagged. 
Right. Saw your comment now. Good. 
I don't think that this one should be tagged LongRunningTest (since it takes about 3 seconds to run), but I do think that we should tag some of the tests when they do take a long time. I can imagine that provoking the failure detector in weird ways will need actual wall clock time to pass.  Maybe the multi-node tests should be tagged with something automatically so you can exclude all of them easily. 
ok, removed LongRunning from this test
oops, got it wrong
What is the unit of the average? Percent?
Is there a null object for MetricsGossip so we don't create a new empty one?
so decay = -1 == boom?
So timestamp and startTime is defined to be in millis? I couldn't see that in the scaladoc for those fields.
Hmmm, "sameAs" would sound and look a bit nicer, wdyt?
case _ => would work quite well here
Quite a few functions allocated here for every call, makes sense to optimize it
I'm not a huge fan of option.get even if it is guarded by isDefined, how about a middle ground?      value match {       case s @Some(v) if defined(v) => s       case _ => None     }
Or of not super performance is needed:      value filter defined
Use named parameter when using boolean constants in parameter lists?
Should be documented that it creates a new return value
Should be documented that it creates a new return value
Should be documented that it creates a new return value
Needs some docs
Probably costs as much to do:  for(i <- v.inbound.averageValue; o <- v.outbound.averageValue) yield ((i,o))
This should be enforced by the type system instead
Should also be enforced by type system
What about java.lang.Integer, Long etc?
Nil.iterator creates a new iterator every call... yes...
Why @volatile (docs)
U sure you want to close over the outer here and not just do context.system here?
Is this the outer ref update? If so I think it deserves some more bells and whistles
why not context.system.deadLetters?
is this used?
No comment describing why?
look on the line above, also checked when reading the config
ok, this is called pretty often, so it should be rather optimized, I'll change to the `match`
ok, it's the `dummy` overloading clash
it's guarded in HeapMemory I'm not sure what real value those `unapply` methods have. I would rather stick it into `HeapMemory`
by having different `Metric` classes?
I thought those were covered  	scala> val x: Any = new java.lang.Integer(13) 	x: Any = 13  	scala> x match {case _:Int => println("Int")} 	Int  	scala> val y: Any = 13 	y: Any = 13  	scala> y match {case _:Int => println("Int")} 	Int
context.system is fine, thx
this is not inside the actor
ok, I'll add a guard, akka is efficient but running on 0 bytes max heap is in your dreams ;-)
yes, if there are no metrics for a node
It's unfortunately system dependent, but on *nixes you get this http:en.wikipedia.org/wiki/Load_(computing) (for the last minute). Then you have to use it together with number of total cores for it to make sense.  
Is this "N time period" well-known? I personally prefer a plain old alpha \elem (0, 1) scaling value
What does "ewma" mean?
Isn't it weird that it works with java.lang.Integer but not RichInt?
Also, the name DataStream is somewhat confusing. This is just a derived signal from an underlying signal.   Something different: Is that much work to generalize this stuff, so different kind of filters might be applied? Windowed average, windowed max, etc.
I agree on the confusing name. What do you suggest? `DataFilter`? It's used inside `Metric` It wouldn't be that difficult to make it generic, pluggable, and public. Then one would like to configure it per metric type. Not sure how important it is. Adds some complexity. Create a ticket.
Also, this will mean that the actual decay-rate will be dependent on this value, and the collect-interval parameter. It would be nicer to have something similar to half-life here (or any other metric expressed in time units).
yes, I have no opinion, in that case, should I name it `smoothing-factor`?
In IntelliJ you can configure inspections to warn against this.
my previous comment was about alpha, Wikipedia: "The half-life of the weights (the interval over which the weights decrease by a factor of two) is approximately N/2.8854 (within 1% if N > 5)." I don't know if that helps people to decide.
Yes, but even that is problematic, as the actual half life will be   dependent on the sampling interval.
Is it possible to set different decay-rate for different metrics?
How will you collect this?
that must be wrong name, it should not be latency, more like NetworkThroughput or NetworkIO
no, it's the same for all, but that is a possible improvement, as mentioned in the comments about DataStream
Latency would be useful though, but it does not fit in this framework   because it has to be measured from the outside.
we have a ticket for that https:www.assembla.com/spaces/akka/tickets/2639-create-network-latency-load-balancing-router#/activity/ticket: I guess that it could be measured by ping-pong messages
Im with Endre here: the configured value should be the half-life (not the decay rate; thatwhile pleasing the physicistwill be less intuitive to the average programmer), and from that and the collection interval the \alpha value can be computed.
What if the capacity is 0.01 for one node (giving 0.01 minimum capacity), and 1 for 100 others? This would be a huge IndexedSeq.
This should be probably in some trait instead. One responsibility is getting the capacity values, while another one is assigning weights. WDYT?
ok, let's see if I understand. The configured value should be `smoothing-half-life-duration` in unit seconds.  halfLife = smoothing-half-life-duration / collect-interval N = 2.8854 * halfLife alfa = 2 / (1 + N) 
ok, as long as it's still possible to to implement subclasses in java also
 > ok, let's see if I understand. > The configured value should be `smoothing-half-life-duration` in unit   > seconds. > halfLife = smoothing-half-life-duration / collect-interval > N = 2.8854 * halfLife > alfa = 2 / (1 + N)  This seems to be correct, but wait for Roland's answer :) Just a final   nitpicking, because I am a soulless bastard: add comment that it takes   about 4 half-life to drop below 10% contribution, and 7 to drop below 1%.
Its more like      val decayRate = 1.4427 / halfLife     val alpha = math.exp(-decayRate * collectInterval)     avg = alpha * avg + (1 - alpha) * input  The 2/(N+1) approximation does not look convincing in our case.
this use of alpha is contrary to the definition above (i.e. the one we discussed); just to keep in mind
fyi, I have a better solution for this, HeapMemory will be plain  `case class HeapMemory(used: Long, committed: Long, max: Option[Long])`  and I will remove those unapply things
When continuing the the refactoring of the metrics I have got the feeling that optional metric value is wrong. Either there is a valid Metric with a value, or there is no metric at all. I'm thinking of changing       case class Metric private (name: String, value: Option[Number]  to      case class Metric private (name: String, value: Number  WDYT?  The `Metric` is created with `apply` in companion, that handles check of valid number etc. Would it be wrong/confusing to still use `apply`, since it would return `Option[Metric]`?
Alright, so what you're saying is that you want to fold in both "there is no such Metric" into "there is no value for this Metric"?  IF that makes sense in all cases then it sounds like a simplification.
'ewma' is Exponentially weighted moving average, I had this documented explicitly in my original work, and 'DataStream' is for the streaming metric data over the 'duration' or lifetime of the stream. Each trendable metric (one that is defined (i.e. not jmx -1 on an OS, or available if user has sigar, or one that is not already an average (system load average) has a datastream. I have this documented
this was documented in several places of my original PR - some metrics we want to collect may not be available on the particular OS and more if they do not use sigar, and those fields should be explained per metric in the collector methods and HeapMem() etc fields.
Ok, I was unsure about the "w" :)
@rkuhn I'm trying this out, your definition:      val decayRate = 1.4427 / halfLife     val alpha = math.exp(-decayRate * collectInterval)     avg = alpha * avg + (1 - alpha) * input  Your alpha = 1.0 -    ( in the old sense) Do you mind if I continue with the old definition of alpha, since that is what is documented at wikipedia? http:en.wikipedia.org/wiki/Moving_average#Exponential_moving_average Reason is simply to be able to point to the description. Otherwise, do you have a better pointer?  In other words      val decayRate = 1.4427 / halfLife     val alpha = 1 - math.exp(-decayRate * collectInterval)     avg = alpha * input + (1 - alpha) * avg
Of course you are right, your definition of  is the established one. And since I got that wrong, I correctly assumed that I did more mistakes yesterday: the factor in the decayRate calculation needs to be inverted (i.e. 0.69315 instead of the 1.4427). Sorry about the confusion!
@drewhk this has the advantage of being O(1) when sending, but I agree that it's too memory hungry. What do you think about using something similar to what we used in `ConsistentHash`? An array of same size as the original routees Vector and with the accumulated weights as elements. Then using `binarySearch` to find the right bucket.
Should this really be enabled by default?
Please add to the docs what smaller value will have as effect or vice versa
A "normal" user will have no clue what the docs actually means, how does the half-life relate to the specified duration?
Or it's just confusing to have it named "decay-half-life-duration" if we just mean "decay-half-life"
why parens here and not below?
I'd remove the default param and made a no-args apply method on MetricsGossip to return the empty instance, because right now if you do MetricsGossip() you get a new instance every time.
Yes, I will go through all empty when merging/fixing the immutable stuff.   14 nov 2012 kl. 18:20 skrev "Viktor Klang ()" <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/ClusterMetricsCollector.scala:  >   * >   * @param nodes metrics per node >   */ > -private[cluster] case class MetricsGossip(rateOfDecay: Int, nodes: Set[NodeMetrics] = Set.empty) { > +private[cluster] case class MetricsGossip(nodes: Set[NodeMetrics] = Set.empty) {  I'd remove the default param and made a no-args apply method on MetricsGossip to return the empty instance, because right now if you do MetricsGossip() you get a new instance every time.   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/858/files#r2127026>.
The plan was to include a reference to http:en.wikipedia.org/wiki/Moving_average#Exponential_moving_average It also mentions half-life, which is I think is a pretty well know thing for exponential decay, otherwise wikipedia is our friend http:en.wikipedia.org/wiki/Exponential_decay#Half-life  I leave it open to @rkuhn and @drewhk to decide what is the best name of this. The reason I added `duration` was that I thought of the plain `half-life` to be the relative to the interval between observations, i.e. `collect-interval`, but here we wanted it to be configured in wall clock duration.
This method seems _very_ expensive both allocation-wise and complexity wise, IMO needs an overhaul.
Don't copy unless you have to
why this line here?
umm, seems wasteful, why not just a type match and return the value itself?
How are these accessed from Java?
Why this line here?
why extractor + allocation instead of type check and return value?
Document that it creates a new instance every call
expensive, why the flatten?
Should be documented that it creates a new instance every call.
Is this really needed?
Because the HeapMemory.unapply returns an tuple in this to make it look like an ordinary case class extractor when used from Scala. This was what I talked to @rkuhn about yesterday.
maxOption is used in the yield, and this line "converts" it to the right type Is there a disadvantage to write this inside the `for` compared to doing it in the `yield`?
damn java compatibility, I have to move the up one level, and remove the nice `Fields` grouping (the idea was to support `import Cpu.Fields._`)
I can only agree, I'll take a stab at it
@viktorklang which type would you match? The extractor does more than just a type match 
?? then it wouldn't be very optional
because all return an Option
yes, it's possible to define other impl in config, and then it must gave constructor with ActorSystem parameter, and I wanted this one to follow the same rules. I can add a comment about that reason.
To me that code is a WTF since it looks analogous to:      something match {       case Some(x) => Some(x)       case      }
Sorry man :(
The disadvantage is that we don't do it like that anywhere else, so I wanted to know if this new idiom carries its weight. Since it's not traversing a sequence but only flatMapping options you might as well do it in the yield IMO.
Well, unless the compiler says no, in which case it would be nice to know what breaks it.
my bad, missed the part where it's OK for it to be none :/
    s"cpuCombined must be between [0.0 - 1.0], was [$x]"
 metrics are 
well, what I can do is to move the code around, so that this more explicit extract method does the real job and the tuple variant calls this one. This method was originally only for Java API, but it might make sense to use from Scala as well in some cases, and then this should return `Option[HeapMemory]` instead of `null`. Option is pretty java friendly anyway, and used inside HeapMemory also. WDYT? 
Id call it `moving-average-half-life` and document as (replacing last sentence): The relevance of each data sample is halved for every passing half-life duration, i.e. after 4 times the half-life, a data samples relevance is reduced to 6% of its original relevance. The initial relevance of a data sample is given by `1  0.5 ^ (collect-interval / half-life)`.
tried it, but it only makes the scala api more awkward, since the tuple construction feels wasteful. I'd rather have a nice scala api, since that is what we use in the router like this:      override def capacity(nodeMetrics: Set[NodeMetrics]): Map[Address, Double] = {       nodeMetrics.collect {         case HeapMemory(address, _, used, committed, max) 
Yes it should, otherwise it will be surprising that the feature doesn't work. We want least surprise. Can be turned off as optimization if not needed.
case object vs case class, but I have made the default MixMetricsSelector a case object as well (using an extra base class)
for the non-physicists it might make sense to add a comment that the constant is `log(2)` (or make it a named compiler constant)
which only works for `value==xn`; but you are right, it might make sense if there are values which are always zero.
it depends on the circumstances whether that is relevant, but introducing a name like that in the for-comprehension leads to allocating an extra tuple to be passed into the yield part, IIRC (if this is a hot spot, you should consult the byte-code)
no, in which way is that weird? jl.Integer is final, is it not? and RichInt is not the boxed representation of an Int
the better question would be where RichInt can actually come from? Is this not a little over-designed?
this looks like it might be useful when dealing with discovering libraries with native components, could this go into ReflectiveAccess?
this formatting looks a bit surprising (i.e. different first impression from true parenthesis structure); Id suggest keeping all of `createInstanceFor` on the first line and start the next with `.recover`.
Id name this `escalateStrategy`
this comment is misleading, probably better to just say that this is the only place from where `weightedRoutees` is updated
this method can be used from user code, but if you think it's not possible to create/pass in a RichX I don't mind removing. It was there originally, but then things were not java.lang.Number
`refs` is a field, `weights` is (hopefully) not, how about making one of them `private[this] val` and the other `_weights`?
this is a rather circumspect way of saying that the local nodes weight is replaced by the (rounded down) average, no?
ah, now I see: we dont necessarily have stats for all addresses
What part of it? I'm not sure it's worth reusing.
something like def verifyMethod(...) which catches the non-NonFatal stuff; its not that important, was just an impression I had
correct, that makes a good comment :-)
"Something less harmless" - I think this is a typo ;)
Will there be a weighted version of this?
= this ?
`private[this] val refs` and `refs` generates same byte code as far as I can see. It doesn't look like we use `private[this]` in many other places in akka.  `private val` generates something else (less efficient).  The naming convention with `_weights` because it's only used in constructor is not something I like, because that is seen from the outside (named parameters), but if that is how we always do it I will follow it.
as I mentioned somewhere else, I have created a real `case object MixMetricsSelector`, in addition to the `case class MixMetricsSelector`
Yes, I had an idea of that but I don't know how far I should go. I wanted to keep it rather simple as a first implementation, but if think that is useful I can give it a try.
is this mutable?
Why is MixMetricsSelector both a case class and a case object and the case object neither extends the class nor returns a cached default value. Is MixMetricsSelector case-class mutable? I vote for removing the "case object MixMetricsSelector" since it doesn't do anything AFAICT. Or have I missed something?
I don't remember where I read it, but I think Paul Phillips said that mapValues was horribly slow.
no need to use zipWithIndex if you're going to use a sum variable anyway, just use another var for the index.
Why does this need to be an inner method?
would be nice if this was somehow dependent on the config on the metrics collection interval
i is not used
You commented on `MixMetricsSlector()` not being `MixMetricsSlector`.  The way I have implemented it now (not pushed) is as follows. The purpose is to have the object MixMetricsSelector for the default instance, and the case class for custom instance, e.g. MixMetricsSlector(Vector(HeapMetricsSelector, MyOwnSelector))          /** 	 * Singleton instance of the default MixMetricsSelector, which uses [akka.cluster.routing.HeapMetricsSelector], 	 * [akka.cluster.routing.CpuMetricsSelector], and [akka.cluster.routing.SystemLoadAverageMetricsSelector] 	 */ 	@SerialVersionUID(1L) 	object MixMetricsSelector extends MixMetricsSelectorBase( 	  Vector(HeapMetricsSelector, CpuMetricsSelector, SystemLoadAverageMetricsSelector)) {  	  /** 	   * Java API: get the default singleton instance 	   */ 	  def getInstance = MixMetricsSelector 	}  	/** 	 * MetricsSelector that combines other selectors and aggregates their capacity 	 * values. By default it uses [akka.cluster.routing.HeapMetricsSelector], 	 * [akka.cluster.routing.CpuMetricsSelector], and [akka.cluster.routing.SystemLoadAverageMetricsSelector] 	 */ 	@SerialVersionUID(1L) 	case class MixMetricsSelector( 	  selectors: immutable.IndexedSeq[CapacityMetricsSelector]) 	  extends MixMetricsSelectorBase(selectors)  	/** 	 * Base class for MetricsSelector that combines other selectors and aggregates their capacity. 	 */ 	@SerialVersionUID(1L) 	abstract class MixMetricsSelectorBase(selectors: immutable.IndexedSeq[CapacityMetricsSelector]) 	  extends CapacityMetricsSelector {
it doesn't have to, but it's only used here, close connection to the binarySearch, I can make it private if you prefer that
ok, it returns a map view, which might be the problem. Is there anything smarter `map`?      scala> Map(1 -> "aa", 2 -> "bb").map{case (k, v) => (k -> v.toUpperCase)}.toMap     res12: scala.collection.immutable.Map[Int,java.lang.String] = Map(1 -> AA, 2 -> BB)
no, what makes you ask that?
I have changed to `immutable.IndexedSeq` everywhere in my local changes when I merged with the immutable stuff
Map.map already returns a Map, so no need for toMap at the end
:-) thought was some tuples, great 
I'll change another usage in `Serialization` also
IF this is renamed in config it needs renamin' here as well
No need for toMap at the end here, you can add tuple2-Seqs to Maps
You can do Failure(new RuntimeException) instead of Try(throw )
of course, I will push my changes
use foldLeft instead of for + var
Why matches inside a total function instead of a partial function with a fail catch-all case?
no need for braces
no need for braces
You can use recover + get instead of the patmat
AFAIK Akka doesn't do any load balancing of nodes yet. Do you mean that it's primarily used for load-balanced routers?
Shouldn't we pull in the version from somewhere else instead of risking this diverging from what we actually use?
It's better to make it a val and create a method: createDeployer that is overridable in the subclass
Is this really something we want to publish in our samples?
Same here as with the Java example
Why two different matches, looks confusing
  PlEASE STOP SENDING ME EMAILS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Best Regards, Alicia Harrison Marketing Manager PH 310-669-8800 Palos Verdes Footwear, INC Volatile,Very Volatile,Volatile Kids,Volatile Kicks, Volatile Handbags,Sbicca,Sbicca Vintage,Grazie   On November 15, 2012 at 3:38 PM "Viktor Klang ()" <notifications@github.com> wrote:   >  >  In > akka-samples/akka-sample-cluster/src/main/scala/sample/cluster/factorial/FactorialSample.scala: >  >  > +   subscribe to ClusterMetricsChanged >  > +   re-subscribe when restart >  > +  override def preStart(): Unit = >  > +    Cluster(context.system).subscribe(self, >  > classOf[ClusterMetricsChanged]) >  > +  override def postStop(): Unit = >  > +    Cluster(context.system).unsubscribe(self) >  > + >  > +  def receive = { >  > +    case ClusterMetricsChanged(nodeMetrics)  >  > +      nodeMetrics.filter(_.address == selfAddress) foreach { n  >  > +        n match { >  > +          case HeapMemory(address, timestamp, used, committed, max)  >  > +            log.info("Used heap: {} MB", used.doubleValue / 1024 / 1024) >  > +          case _   no heap info >  > +        } >  > +        n match { >  >  Why two different matches, looks confusing >  >   >  Reply to this email directly or view it on GitHub > <https:github.com/akka/akka/pull/858/files#r2144298> . > 
ok, fixed replacement tag for sigar version
yes, that's better
I think it should go a little something like this:  class MixMetricsSelector(...)  case object MixMetricsSelector extends MixMetricsSelector(...) {   def apply() = this   def apply ...   def unapply ...   def getInstance = this }
What about a negative double? Like -1.0 for an undefined metric? (floating point numbers returned in Right(...) if I see correctly)
Why an abs here and nowhere else? What if it was -1 which is considered undefined above?
good catch, the negative check was not done for double, that is important because loadAverage might return that on windows I'll add test and fix it.
what do you mean? what `abs`?
You already removed that, I was reviewing the stuff before your last   commit. I refreshed my browser now :) Just ignore my comment.
What will be the capacity for nodes where the metric is undefined? Do you have a default value when you call capacity and check for an Address?
Ok, I see the answer later down.
Is this a safe thing to do?
"corresponding nodes" or "the corresponding node"?
As it is used now it's safe, 0 indicates that it's empty, and things will be routed to deadletters. I can add another `isEmpty` method to make it more clear
Why List and not Seq?
What does this return in case the props do not have a router?
Nice! Please comment that this prevents props and context to be closed over below.
I don't understand the question, createRoute is invoked from RoutedActorRef, so it's a router already
Alright, that was not evident from this diff :-)
This shouldn't be done here, ActorCell should be agnostic about dispatcher implementations.
lookup will instantiate a new dispatcher for PinnedDispatcher etc, so this won't work. Put this logic where routers are created.
Ok. Will fix. 
Move it to RoutedActorRef? 
Hmmm, might work. As I said earlier, it's tricky :-)
I think you can remove the test, covered by "grow as needed under pressure"
Good. Will do. 
That pool is pretty cool
use: _routees = _routees diff abandonedRoutees
Fraction in what way? Is 0.0d safe to use as a fixed magic value?
What happens if it's < 1
What happens if it's less than lowerBound?
What happens if it's negative?
What does this mean? What should I set it to? What happens if negative?
What happens if negative? What happens if > 1.0d?
is 0.0d a safe magic value? what happens if < 0 or > 1.0d?
What happens if 0? What happens if Infinity?
Don't schdule if abandon isEmpty Run now if stopDelay is 0?
Very good docs!
I'll spray some input validation checks to it!
no problem with >1.0 For example 1.3 would increase with 130% current size 5 5 * 1.3 => 7
fraction of the current pool size. I'll add some sample to the description.  0.0 is no problem if you don't compare with == In this compare is done with > 0.0
you get what you ask for, Infinity is Infinity
othewise it is a bug in the scheduler
good point, changed
This name is not so easy to understand, I'd suggest "messagesPerResize"
Is it fine to have the same ref more than once?
So you cannot share the same RouterConfig between multiple Actors?
People might find that a bit counter intuitive
yes! thanks I totally agree
well, I think its up to the Resizer to behave and create new instances, which our DefaultResizer does nothing will break when adding, but when removing (_routees diff) it will only remove the first one, which is a problem Is it worth checking for duplicates?
I'd probably try to get rid of the repetition here:  routees count {         case a: LocalActorRef            val cell = a.underlying           pressureThreshold match {             case      i if i < 1 => cell.mailbox.isScheduled && cell.currentMessage != null             case                1 => cell.mailbox.isScheduled && cell.currentMessage != null a.underlying.mailbox.hasMessages             case threshold => cell.mailbox.numberOfMessages >= threshold         }         case _  false       }
good that you bring it up, I thought about that, I can move the state and this resize method to the RoutedActorRef instead
Atleast it's symmetrical now, add adds all the given refs, remove removes all the given refs.
ok  On Wed, Jan 11, 2012 at 10:23 AM, viktorklang < reply@reply.github.com > wrote:  > > +    } else if (pressureThreshold > 1) { > > +      routees count { > > +        case a: LocalActorRef => a.underlying.mailbox.numberOfMessages > >= pressureThreshold > > +        case x                => false > > +      } > > +    } else { > > +      routees count { > > +        case a: LocalActorRef => > > +          val cell = a.underlying > > +          a.underlying.mailbox.isScheduled && cell.currentMessage != > null > > +        case x => > > +          false > > +      } > > +    } > > +  } > > + > > I'd probably try to get rid of the repetition here: > > routees count { >        case a: LocalActorRef => >          val cell = a.underlying >          pressureThreshold match { >            case      i if i < 1 => cell.mailbox.isScheduled && > cell.currentMessage != null >            case                1 => cell.mailbox.isScheduled && > cell.currentMessage != null a.underlying.mailbox.hasMessages >            case threshold => cell.mailbox.numberOfMessages >= threshold >        } >        case _ => false >       } > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/206/files#r342714 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
as I said, diff doesn't scala> IndexedSeq("a", "b", "a", "c") diff IndexedSeq("a", "c") res319: IndexedSeq[java.lang.String] = Vector(b, a)
That was exactly my point: IndexedSeq("a") ++ IndexedSeq("a","a","b") == IndexedSeq("a", "a","a","b") IndexedSeq("a","a","b") diff IndexedSeq("a","a") == IndexedSeq("b")
Don't put code in the .rt docs, it's a maintenance nightmare. Please use the same approach used by the other docs, where compiled and tested code is automatically hoisted into the docs.
Sure - it moved it into a file next to HTTPServer.scala. Uhm, how do I trigger compilation of the example? I tried action "compile" in sub-project "akka-docs", but that didn't seem to compile the example code.  Is it ok if it's just compiled to check it (everything in abstract classes), but does not really run and do something? Otherwise this will need test data, etc., making the example a lot more complex.
It should be compiled when you run "test:compile" in the parent project.  You don't need to make it a verified test if it's too much work, but remember that it is an example that should demonstrate something that does work.
Ok, it's done. :-)
to avoid the intermediate list it should probably be: bytestrings.map(_.iterator)(collection.breakOut)
Nice, thanks! I wan't too happy about the toList, either - hadn't thought about breakOut. I'll put that in.
Use explicit return type
Use explicit return type
Use explicit return type
Use explicit return type
Is this guaranteed to cover all cases?
make "bytestrings" private
Use explicit return type
Use explicit return type
Use explicit return type, and make it a one liner
Use explicit return type, and make it a one liner
Use explicit return type
What's the difference between contiguous and compact?
Use explicit return type
Why is it a public class?
no braces in cases please
Why is this public?
Use explicit return type
Use explicit return type
Use explicit return type
Use explicit return type
Why is this public?
Why is this public?
make into one liner
Use explicit return type
Shouldn't this throw an NotYetImplementedException?
don't return null, it's broken, throw exceptions
indexWhere and indexOf has a lot of code duplication
why is this needed?
Why is this public?
Why is this public?
Why is this public?
Why is this public?
FWIW, this implementation of InputStream.read is incorrect. Read should return the unsigned equivalent (0-255), or -1 in the case no byte is available. See http:stackoverflow.com/questions/4332264/wrapping-a-bytebuffer-with-an-inputstream/6603018#6603018
Thanks a lot! Should have read the InputStream doc more carefully - will fix it.
It should, yes.
Never use "should" ;-)
I removed contiguous again.
It's the companion to the class ```ByteIterator``` (which has to be public, of course). I guess it could be ```private[akka]```, in principle - would that increase binary compatibility? Or should I make some of the things inside private (this was not done for the ```ByteString``` object in the past)?
It's private, now.
```ByteIterator``` has to be public, so people can get it as an explicit return type of ```ByteString.iterator```.  They would get a generic Iterator reference instead, which forces boxing/unboxing on next(). Also, the special features  of ```ByteIterator``` like ```getInt```, ```asInputStream```, etc. would not be available.
Yep, that looks neater (though I made sure it can never be called). Done.
True  (though I made sure it can never be called). Done.
It's there for symmetry reasons. If people write binary decoding code using lot's of ```getInt```, ```getShort```, etc. a ```getByte``` will be more readable in the middle of that than next().
It's in ```object ByteIterator``` now, like it was done for the ```ByteString``` implementations in the past. Is that enough for binary compatibility, or should it also be ```private[akka]```?.
See comment on ```ByteArrayIterator```.
You did agree to make it public a while back. ;-)  It's there so one can pass around compact ByteStrings explicitly and serialize them.
It needs to be, so one can use it's features like ```putInt```, ```asOutputStream```, etc. that a generic builder does not have.
Ok, as far as I can see it does, and testing agrees. ;-)
These are all public to JAva code
Ah, yes - indeed!
Ooops ... *cough* *cough*, uhm, that was a remnant from my original implementation. It's gone now.
Is there any tests missing here compared to the public API exposed in the implementation(s)?
ByteStringBuilder isn't completely covered yet by testing, though the new parts are. With ByteIterator and ByteSeq, a lot is covered explicitely, some methods only implicitely, and with some, I have to check - I'm on it.  Concerning the implementations: They don't add any public methods to the public interfaces of ByteString and ByteIterator (there were two or three, but they're gone now).
Great, let me know when you've checked it
Ok, the tests are complete. They cover the full public API of ```ByteString```, ```ByteStringBuilder``` and ```ByteIterator``` now, except the parts that are just inherited (not overridden). Hope I didn't overlook anything.
Import ByteOrder on the line above?
hmm, I actually find that kind-a nice 
would prefer      val (xs, from, until) = slice     likeVecIt(xs)({ _.slice(from, until).toSeq }, strict = false)
might want to lose some braces
could this benefit from being lazy?
I thought about it - but will it really safe memory when asOutputStream is not used? I don't know how heavy a basic OutputStream instance is. On the other hand, if it's really lightweight, might a def even better than a val? Experts?
Hm, i have that in many place. If you really care, i can change them all - it's all testing code, though.
As a compromise, I will remove the braces around ```ByteOrder```. ;-)
hmm, less mental parsing overhead vs. effort of the change: thats why I formulated as would prefer. Your call.
since you cache it unconditionally I assumed that it would be heavy, but in fact its just an Object, i.e. really lightweight, so I think it could well be a `def`.
I'll do that.
top level classes are also fine, perhaps mention that in the error message they are also static, but that is probably not known by all users
What's wrong with collectFirst?
smth like:      collectFirst { case c: Class[_] if ac.isAssignableFrom c && c != ac => c } getOrElse ac
Oh, I see. So this was the problem... 
problem was: `Props(new StashQueueReportingActor)`
Yep, that is what I wanted to comment on
It's not published for sbt 0.11.3, or is it somewhere?
ah, okay. Just wanted to know the reason and whether there should be a FIXME in there somewhere 
I have created a separate ticket for it, #2097
would it make sense to also check `!isSuspended`?
someone broke you indentation 
this punctuation inflation is not necessary: `router ! (1, received1)` should work the same way, no?
ref.routees collect { case l: LocalActorRef => l }
I'd replace the entire thing above with:  local.find(a  !isProcessingMessage(a) && !hasMessages(a)) orElse local.find(a  !hasMessages(a)) orElse local.sortBy(a  numberOfMessages(a)).headOption getOrElse ref.routees(random.get.nextInt(ref.routees.size))
Excellent docs above
Why is this an Iterable of Strings and not an Iterable of ActorPath?
This is Java, what happens if null?
thx, fixed  On Wed, Jan 11, 2012 at 7:31 PM, viktorklang < reply@reply.github.com > wrote:  > > +   * It will always return 0 for remote actors. > > +   * Method is exposed to subclasses to be able to implement custom > > +   * routers based on mailbox and actor internal state. > > +   */ > > +  protected def numberOfMessages(a: ActorRef): Int = a match { > > +    case x: LocalActorRef  x.underlying.mailbox.numberOfMessages > > +    case _                 0 > > +  } > > + > > +  def createRoute(props: Props, context: ActorContext): Route = { > > +    val ref = context.self.asInstanceOf[RoutedActorRef] > > +    createAndRegisterRoutees(props, context, nrOfInstances, routees) > > + > > +    def getNext(): ActorRef = { > > +       non-local actors mailbox size is unknown, so consider them > lowest priority > > +      val local: IndexedSeq[LocalActorRef] = for (a  ref.routees if > a.isInstanceOf[LocalActorRef]) yield a.asInstanceOf[LocalActorRef] > > ref.routees collect { case l: LocalActorRef => l } > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/209/files#r344231 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The modern software stack for applications that scale Twitter: @patriknw
ok, I have removed the vals, but want comments so some curlies and newlines are left  On Wed, Jan 11, 2012 at 7:34 PM, viktorklang < reply@reply.github.com > wrote:  > > +       non-local actors mailbox size is unknown, so consider them > lowest priority > > +      val local: IndexedSeq[LocalActorRef] = for (a  ref.routees if > a.isInstanceOf[LocalActorRef]) yield a.asInstanceOf[LocalActorRef] > > +       anyone not processing message and with empty mailbox > > +      val idle = local.find(a  !isProcessingMessage(a) && > !hasMessages(a)) > > +      idle getOrElse { > > +         anyone with empty mailbox > > +        val emptyMailbox = local.find(a  !hasMessages(a)) > > +        emptyMailbox getOrElse { > > +           sort on mailbox size > > +          local.sortBy(a  numberOfMessages(a)).headOption getOrElse { > > +             no locals, just pick one, random > > +            ref.routees(random.get.nextInt(ref.routees.size)) > > +          } > > +        } > > +      } > > +    } > > I'd replace the entire thing above with: > > local.find(a  !isProcessingMessage(a) && !hasMessages(a)) orElse > local.find(a  !hasMessages(a)) orElse local.sortBy(a  > numberOfMessages(a)).headOption getOrElse > ref.routees(random.get.nextInt(ref.routees.size)) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/209/files#r344257 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The modern software stack for applications that scale Twitter: @patriknw
In the end it does actorFor so it should be fine with ActorPath. Will try to change that in separate refactoring.  On Wed, Jan 11, 2012 at 7:36 PM, viktorklang < reply@reply.github.com > wrote:  > > + * Please note that providing both 'nrOfInstances' and 'routees' does > not make logical sense as this means > > + * that the random router should both create new actors and use the > 'routees' actor(s). > > + * In this case the 'nrOfInstances' will be ignored and the 'routees' > will be used. > > + * <br> > > + * <b>The</b> configuration parameter trumps the constructor arguments. > This means that > > + * if you provide either 'nrOfInstances' or 'routees' to during > instantiation they will > > + * be ignored if the 'nrOfInstances' is defined in the configuration > file for the actor being used. > > + */ > > +case class RemoteSmallestMailboxRouter(nrOfInstances: Int, routees: > Iterable[String], override val resizer: Option[Resizer] = None) > > +  extends RemoteRouterConfig with SmallestMailboxLike { > > + > > +  /** > > +   * Constructor that sets the routees to be used. > > +   * Java API > > +   */ > > +  def this(n: Int, t: java.lang.Iterable[String]) = this(n, t.asScala) > > Why is this an Iterable of Strings and not an Iterable of ActorPath? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/209/files#r344268 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The modern software stack for applications that scale Twitter: @patriknw
it will be some NPE later I could add null check but we don't really have that kind of validations in other places, and I don't think we should. If you supply null to an constructor like this you get what you ask for, i.e. trouble.  On Wed, Jan 11, 2012 at 7:36 PM, viktorklang < reply@reply.github.com > wrote:  > > + * be ignored if the 'nrOfInstances' is defined in the configuration > file for the actor being used. > > + */ > > +case class RemoteSmallestMailboxRouter(nrOfInstances: Int, routees: > Iterable[String], override val resizer: Option[Resizer] = None) > > +  extends RemoteRouterConfig with SmallestMailboxLike { > > + > > +  /** > > +   * Constructor that sets the routees to be used. > > +   * Java API > > +   */ > > +  def this(n: Int, t: java.lang.Iterable[String]) = this(n, t.asScala) > > + > > +  /** > > +   * Constructor that sets the resizer to be used. > > +   * Java API > > +   */ > > +  def this(resizer: Resizer) = this(0, Nil, Some(resizer)) > > This is Java, what happens if null? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/209/files#r344271 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The modern software stack for applications that scale Twitter: @patriknw
The question here is not what works but what is the better API. An ActorPath cannot be obtained from a String in one line of Java, but the other direction is possible. Given that constructing routers will not be the bottle-neck in any real application, I'd vote for the easier route.
  11 jan 2012 kl. 22:06 skrev Roland Kuhn<reply@reply.github.com>:  >> + * Please note that providing both 'nrOfInstances' and 'routees' does not make logical sense as this means >> + * that the random router should both create new actors and use the 'routees' actor(s). >> + * In this case the 'nrOfInstances' will be ignored and the 'routees' will be used. >> + * <br> >> + * <b>The</b> configuration parameter trumps the constructor arguments. This means that >> + * if you provide either 'nrOfInstances' or 'routees' to during instantiation they will >> + * be ignored if the 'nrOfInstances' is defined in the configuration file for the actor being used. >> + */ >> +case class RemoteSmallestMailboxRouter(nrOfInstances: Int, routees: Iterable[String], override val resizer: Option[Resizer] = None) >> +  extends RemoteRouterConfig with SmallestMailboxLike { >> + >> +  /** >> +   * Constructor that sets the routees to be used. >> +   * Java API >> +   */ >> +  def this(n: Int, t: java.lang.Iterable[String]) = this(n, t.asScala) >  > The question here is not what works but what is the better API. > An ActorPath cannot be obtained from a String in one line of Java, but the other direction is possible. But why don't we have a ActorPath.fromSting then?  If we don't change to ActorPath we must anyway improve the docs of it. Now its not clear what the String is.  > Given that constructing routers will not be the bottle-neck in any real application, I'd vote for the easier route. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/209/files#r344888
On Thu, Jan 12, 2012 at 8:17 AM, Patrik Nordwall <patrik.nordwall@gmail.com>wrote:  > > > 11 jan 2012 kl. 22:06 skrev Roland Kuhn< > reply@reply.github.com > >: > > >> + * Please note that providing both 'nrOfInstances' and 'routees' does > not make logical sense as this means > >> + * that the random router should both create new actors and use the > 'routees' actor(s). > >> + * In this case the 'nrOfInstances' will be ignored and the 'routees' > will be used. > >> + * <br> > >> + * <b>The</b> configuration parameter trumps the constructor > arguments. This means that > >> + * if you provide either 'nrOfInstances' or 'routees' to during > instantiation they will > >> + * be ignored if the 'nrOfInstances' is defined in the configuration > file for the actor being used. > >> + */ > >> +case class RemoteSmallestMailboxRouter(nrOfInstances: Int, routees: > Iterable[String], override val resizer: Option[Resizer] = None) > >> +  extends RemoteRouterConfig with SmallestMailboxLike { > >> + > >> +  /** > >> +   * Constructor that sets the routees to be used. > >> +   * Java API > >> +   */ > >> +  def this(n: Int, t: java.lang.Iterable[String]) = this(n, t.asScala) > > > > The question here is not what works but what is the better API. > > An ActorPath cannot be obtained from a String in one line of Java, but > the other direction is possible. > But why don't we have a ActorPath.fromSting then? > > If we don't change to ActorPath we must anyway improve the docs of it. Now > its not clear what the String is. >  It's the same in ActorRefProvider, so I'll not change, just improve the docs.   > > > Given that constructing routers will not be the bottle-neck in any real > application, I'd vote for the easier route. > > > > --- > > Reply to this email directly or view it on GitHub: > > https:github.com/jboner/akka/pull/209/files#r344888 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
thx, it's not necessary
yes, it does, I have added to the first filter to skip supspended
It's scalariform. Only happens for "align parameters" setting, which we use. I filed a bug: https:github.com/mdr/scalariform/issues/44
Switch to ARFU or akka.util.Unsafe
Move this one out to a private companion object for reuse
switch this to: import CircuitBreaker.syncExecutionContext
How about putting the ExecutionContext into the creation of the breaker?
have CircuitBreakerHalfOpen extend AtomicBoolean
Here we have something that might be (at least) confusing. We are in Closed and some call triggers transition to Open. Another call started in Closed and is running in parallel and when that is sucessful it will set the failureCount to 0 even though we are in Open.
We asume that breaker round trip time is longer than duration of calls, which I think is a practical assumption, which simplifies the implementation a lot. Just saying.
have this extend AtomicInteger
yes, if possible for the different ExecutionContext used for withSyncCircuitBreaker. Perhaps the sync/async mode should be decided on creation of the circuit breaker. Mixing is perhaps only confusing?
I think the they originate from the       listeners = new CopyOnWriteArrayList[()  _]  What is the type for a (callback:  Unit) parameter?
?  just do:  CircuitBreakerClosed.addListener(()  callback)
is reading from an AtomicReference that bad? I can't imagine contention here. When changing state you have more serious problems.
I don't like suboptimal solutions when the optimal solution is known and only 5-6 lines of code away
sorry, forget what I said, *braces* can be removed
Collapse the imports using brace-notation
I don't like micro optimizing things that is obviously not a bottleneck, but I will not argue more. You know we have a AtomicReference in Dispatcher?
Yup, that's on my to-do list
I think we shall use `with NoStackTrace` for this, since it is thrown a lot when Open
Preference for ARFU or Unsafe?  Never used either, so will need to research
I prefer Unsafe since it performs about 5-15% better than ARFU in Scala
This is true - I don't see it affecting functionality but it could be confusing.  Should we guard with a current state check here?
currentFailureCount should perhaps not be public? if we change that to private[akka] we don't need to care about this case  On Sun, May 20, 2012 at 8:13 PM, scullxbones < reply@reply.github.com > wrote:  > > +    def enter()(implicit executor: ExecutionContext): Unit = { > > +      _enter() > > +      notifyTransitionListeners() > > +    } > > + > > +    def _enter()(implicit executor: ExecutionContext): Unit > > +  } > > + > > +  private object CircuitBreakerClosed extends CircuitBreakerState { > > +    val failureCount = new AtomicInteger(0) > > + > > +    override def invoke[T](body: => Future[T])(implicit executor: > ExecutionContext): Future[T] = { > > +      callThrough(body) > > +    } > > + > > +    def callSucceeds()(implicit executor: ExecutionContext): Unit = > failureCount.set(0) > > This is true - I don't see it affecting functionality but it could be > confusing.  Should we guard with a current state check here? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/466/files#r850604 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
It is very weird to argue for switching to Unsafe here. I'm with Patrik.  1. It is not needed in this place 2. If you still think it is - prove it with a real-world benchmark - D. Knuth applies  3. It adds complexity to the code, e.g. unnecessary baggage, cognitive overhead in understanding the code etc.  We can't go around and optimize every singel AR to the verbosity of Unsafe. 
1. By the same reasoning I'll refute the claim that "it's not needed here" by requesting a benchmark that shows that it's not needed here. 2. See 1 3. Anyone who is not comfortable with Unsafe and is working within the Akka codebase should definitely spend the required time to learn it.  I'm OK with leaving it as a FIXME, but please, "optimize when you have a performance problem" is not a design philosophy that I will ever agree to. Design needs to be done with performance in mind, otherwise you'll both give users performance problems AND you'll risk having to break binary compatibility to change the design to be performant.
documentation code should go into scala/java doc spec classes, see (almost) all other documentation
`BytesString(bytes)` will do the right thing
DEBUG vs "DEBUG"?
why is the kernel dependency needed?
please dont shadow the standard `akka.actor.ReceiveTimeout` message type
either use `.length` or compare to `Duration.Zero`
+1 on comparing against Duration.Zero
Rate of what?
print how to terminate
val text = m.frames.head.utf8String
text != currentMessage
doesn't say what time unit
doesn't say at what rate.
update this text
Write doc comment for this
I'd put this check in the middle, and move the Sub | Pull ... to the bottom
How can it be a ReqRepPoll if it only polls for Rep?
Nevermind, I see that it's handled differently.
I find this weird. Why do we only recur on mode == Poll?
What happens if the timeout is negative?
What happens if the timeout is negative?
Should this check that it's really the `target` that was terminated?
result success true
result failure (new ... )
Do context stop self here, otherwise you'll close over system needlessly
Isn't the system dispatcher used by default? (if there's an implicit system in scope)
How is this used from Java?
ok, should never happen, but I have added that check now
ah, rusty, thx
Famous last words :-)
with Patterns.scala I'm not sure if we should have separate Scala and Java api:s for these things like this.
Double the maintenance cost though :/
On Sun, Jan 1, 2012 at 9:38 PM, viktorklang < reply@reply.github.com > wrote:  > > +import akka.actor.Actor > > +import akka.actor.ActorRef > > +import akka.actor.ActorSystem > > +import akka.actor.ActorTimeoutException > > +import akka.actor.PoisonPill > > +import akka.actor.Props > > +import akka.actor.ReceiveTimeout > > +import akka.actor.Terminated > > +import akka.dispatch.Future > > +import akka.dispatch.Promise > > +import akka.util.Duration > > + > > +/** > > + * Akka patterns that provide solutions to commonly occurring problems. > > + */ > > +package object pattern { > > Double the maintenance cost though :/ >  Well, we need different signatures anyway to take advantage of implicit parameter for scala, anyway. I don't mind skipping package object and putting everything in object Pattern if we think that is better for the other patterns also - ask, pipeTo.   > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/200/files#r321716 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
What should this do if it times out?
What should these two do if they time out?
when will this fail and use BatchingExecutor as fallback?
this must be `case Terminated(a) if a.path == target.path`, otherwise it will not work with remote target ActorRef acquired with actorFor
isn't it already terminated? why send Unwatch?
Is this change correct? If the target replies with something before terminating the PromiseActorRef will be completed with something else than Terminated.
Since this is calling into a private scala entity, this prevents havoc if that internal thing changes in a micro-release of scala.
Me and Roland has already discussed this, I'll revert the change :)
That would be a bug IMO. There's _tons_ of code that does Terminated(`foo`) and if it won't match then we have to fix it.
I see, good
ok, sorry, so many emails
That was part of my uid pull request. https:github.com/akka/akka/pull/1244 I changed such occurrences in akka code. For user code I have described the problem in the migration guide. It is not possible to fix. Equals and hashCode cannot be based on wildcard matching, it would violate the contract of equals. This specific issue was discussed here: https:groups.google.com/d/msg/akka-dev/bXpaIEQ_CdE/XI1YZFnP5jQJ
ok, but doesn't that pollute deadLetters for the normal case, when it is terminated?
I think this is actually a huge, silent, killer issue. So I don't really see any point in keeping actorFor if we're going to completely break its semantics in respect to the other code. Also, Terminated.unapply should "do the right thing". And what does it mean to do: context watch context.actorFor(...) ?
No, from DeadLetterActorRef:        override protected def specialHandle(msg: Any): Boolean = msg match {         case w: Watch            if (w.watchee != this && w.watcher != this)             w.watcher ! Terminated(w.watchee)(existenceConfirmed = false, addressTerminated = false)           true         case w: Unwatch   true  Just ignore         case NullMessage  true         case _            false       }
Let us talk about it with @rkuhn later, until then, compare Terminated via the path. context watch context.actorFor(...) behaves exactly as before. The difference is that equals in ActorRef takes the uid into account, which must be considered when handling the `Terminated` message. 
Should probably be implicit though
On `ActorSystem` it's not implicit right now, which is why I haven't marked it implicit in `ActorRefFactory`. Should it be implicit in `ActorSystem` as well?
Right now there's an implicit conversion from AS to MD:        private[akka] object MessageDispatcher {              implicit def defaultDispatcher(implicit system: ActorSystem): MessageDispatcher = system.dispatcher     } 
Ok, so ActorSystem has a non-implicit `dispatcher` but an implicit conversion to `MessageDispatcher` while ActorContext has an implicit `dispatcher` but no implicit conversion. Assuming that this was done for a reason and makes sense the way it is, would the best solution for `ActorRefFactory` really be to follow the model of its ActorContext child? I.e. simply have an implicit dispatcher and no implicit conversion? 
This is wrong! The previous was a plain ask, i.e. reacting to a reply. gracefulStop reacts only to termination and not to a reply.
this semantic change needs proper documentation; Im not even sure that it is desired.
You might want to re-read the implementation of `askWithDeathCompletion`
as I said: gracefulStop is only interested in the Terminated (and I think it should stay this way) while we want to get the ACK here even if the actor does not die; the code you deleted below did that. 
I'll restore gracefulStop, but askWithDeathCompletion _will not_ remain as a copy-paste version of it.
this can overflow, just use `size` if that is greater 10000. 
On Sun, Jan 8, 2012 at 6:52 PM, Roland Kuhn < reply@reply.github.com > wrote:  > > > >      def getNext(): ActorRef = { > > -      ref.routees(next.getAndIncrement % ref.routees.size) > > +      def size = ref.routees.size > > + > > +      @tailrec > > +      def reduce(n: Int) { > > +        val safetyValue = size * 100000 > > this can overflow, just use `size` if that is greater 10000. >  I didn't found it realistic to use more than 21474 routees, but I can sure add that extra check for size > 10000   > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/204/files#r335330 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Should it really be 10000 at one place and 100000 in the other?
What if the collections' size is Integer.MAX_VALUE?
yes, because 10000 * 100000 = 1000000000 is safetly below Int.MaxValue (2147483647)
The assumption is that number of routees is far below MaxValue, which is a realistic assumption.
I think it's probably cleaner just to switch to AtomicLong and go with abs on it?
what if `fin.close()` throws `IOException`? 
This is done during startup, so if closing the file fails, we definitely should abort ;)
I'm more concerned about that the real cause from IOException from `trustStore.load` is lost and it will be hard to debug
Alright, so you're for dropping the try-finally?
no, catch-ignore `try trustStore.load(fin, pwd) finally Try(fin.close())`
This file should be placed in src/main/scala/akka/pattern/package.scala package object pattern {
Is this really true? Shouldn't we delegate this to the implementation of MinimalActorRef?
Document as Akka internal
This is not even on the table, just remove the comments :-)
Does this need to be final to be inlined or not?
dont think so: compiler knows which exact method is invoked, no subclassing possible.
This should just contain Java API, all Scala parts should be available with `import akka.patterns.ask`. I know overloading is evil, but in this case it looks very elegant.
perhaps describe what `MPSC` stands for
looks like there is some inconsistency in the naming, `MPSC` vs `Node`
move `size` and `isEmpty` to  `AbstractNodeQueue`?
would it be possible to not expose `Node` to the public? `peek` return `T` instead?
Because then you can't separate a null entry from empty. The alternative is to prohibit to add null, which would add a branch for each insertion.
Yeah, so the reason I gave it this name is because this is visible in the configuration and "Node" says just about nothing. But I agree that MPSC isn't ideal either, so do you have any suggestions?
Because count would be uglier in Java. :-) But I get your point, it might be just safer to avoid the wheel to be reinvented (poorly).
Do we need to support null values?
ah, I see; `next.value = null`
You mean: Change peek and document that we do not support adding nulls and not add the branching check for nullness?
Yes, if we don't null it out we're going to hold onto an already consumed entry
SingleConsumerMailbox, SingleConsumerMessageQueue, AbstractSingleConsumerQueue
I think that is fine. We check for null messages at a higher level. It would be beneficial to not expose Node. You can still have a private peekNode for poll.
We can't really get away from exposing Node atm:  public abstract class AbstractNodeQueue<T> extends AtomicReference<AbstractNodeQueue.Node<T>>   That's if we move it into a field instead, but the problem is that there is no getAndSet on Unsafe, so it cannot use LOCK XCHG. So I'll move in "isEmpty" into AbstractNodeQueue and add "count" (to signal that it's not constant-time)
ok, thanks for clarifying. You could define peek as private to not make the api more ugly than necessary. The returned Node can't be used for anything else than looping over next. Not even the value is accessible.
Not sure that assigning of null will help GC to collect node faster. Possible need to ask GC experts (like Gil Tene) to weigh all procs and cons. 
This is not about collecting the node faster. It is about not holding on to the consumed value of type T if the node is in the tail position and the queue is empty. 
Good point, but then comments are misleading (https:github.com/akka/akka/blob/master/akka-actor/src/main/java/akka/dispatch/AbstractNodeQueue.java#L56):  next.value = null;  Null out the value so that we can GC it early  Also using the null value as a marker for an empty queue lead to failure of following test:   "XXX" in {     val queue = new AbstractNodeQueue[AnyRef]() {}     queue.add(null)     queue.peek() must_== null     queue.count() must_== 1     queue.isEmpty must_== false   }
Adding ``null`` wont happen, we just don't want to pay for the nullcheck.
probably better to remove `@deprecated` here and make sure that all methods on OldRemoteSupport are appropriately annotated
should mention that the `GlobalActorSystem` needs to be stopped manually in order to voluntarily exit the JVM
Think it's good to say which version this guide is meant for.
``actorSystem.stop(actorref)`` or ``actorContext.stop(actorref)``
use applyOrElse ;-)
Because it otherwise exposes protected types at a public scope (see -Xlint)
Because all of them should go to self?
I thought one could ommit the wildcard but now I see that that would only work for case objects like in the ```CurrentRoutees``` case.  Thx anyway.
call to super.postStop() ?
Which begs the question: can we really realistically put anything in Actor.postStop()? Probably nobody out there calls super when overriding because it is known to be a NOP. So, while I think it does not hurt too much, I also think that it will always be redundant.
does this mean that if someone already overrides postStop in their actor, this Stash postStop will most likely not be called, and oops, we have changed semantics, since it used to be done from preRestart? A case for migration guide and docs.  Can't it be done in both preRestart and postStop? It doesn't hurt to do it twice.
I think it just needs to be documented in those traits which modify the defaults, so that everyone deriving from Stash will know that they need to call super. Should be bold italic and really visible.
ok, and then also in the migration guide
I'll bring back the old preRestart and add the postStop callback and mention it in the migration guide.
This shouldn't be like this, see: https:github.com/viktorklang/scala/commit/089805510ee79c389ff2a870355c8496c723b8cc#L1R678
FIXME with a link to the Scala ticket?
See how this is done for TypedActors (i.e. children TypedActors)
Needs docs and an explanation what "errors" means, usually this is called "message"
I remember this from somewhere else, and if so, I think it deserves som DRY treatment
could you explain a little?
The code here is the nave implementation, which is known to be broken, according to the Scala ticket referred to in the link above, a solution that doesn't have the kinds of issues the nave implementation has is outlined there as well.
do we need to do anything is channels or classes isEmpty?
This definitely wins some kind of obfuscation award. So I need to use a Batching executor not because I want to batch things but because I want trampolining. Nice.  Could this potentially be clarified in the docs for ExecutionContext and BatchingExecutor? BatchingTrampoliningExecutor? (but only if it likes the Runnables)  Im leaning towards not exposing this at all.
I agree that this shouldn't be exposed at all, but no matter what, it still needs to trampoline. And the best way to avoid reinventing the wheel is to use the BatchingExecutor.
typo: "of the it"
WDYM? Of course you can create top-level actors, why not? Children are created differently.
but it needs a name, that is for sure
it is similar to but not compatible with `SupervisorStrateg.sort()`
yes, and wants to be moved to Ops.scala
indent one level
believe me, I tried many things and this is the optimum allowed by scalariform
But it maeks kitteh cry on teh insidez :(
    val nuffin = weakTypeOf[Nothing]     if (parent =:= nuffin) abort(c, "Parent argument must not be Nothing")     if (child =:= nuffin) abort(c, "channel list must not be Nothing")
channel list? not "child"?
its either this or a _very_ long line
no, this is the childs channel list, and the error will hopefully be displayed at the right source position
Should be experimental
This looks like a foldLeft over Nil to me
    msg match {       case tr @ TypeRef(_, _, x :: _) if tr <:< typeOf[WrappedMessage[_, _]]  x       case other => other     }
please refactor CircuitBreaker.syncExecutionContext to use the same thing
interesting is it still possible to define a channel typed with Future[A] to send the actual future? (bad practice, but anyway)
that would not make it nicer (at least not in my view) due to the needed flattening
this, OTOH, is nicer ;-)
will check that one
I think it's too easy to associate A, B, C, D with declarations of type parameters, so it would be easier to understand if using some more message like class names for those
it is not currently flagged as an error, but it probably should; creating a ticket
not yet, no; it would need special TypedProps, though that name is already taken; we discussed about changing some aspects of Props a while back and I thought Id incorporate that here from the start, thus the interim solution
hmm, would it be possible to place all the Problem sections later to not scare people away. They are really interesting, but as you say, not for first time users.
yes, good point, Ill move the details of the motivation to the end
missing something here?
ok, I don't like different syntax for creating actors, and you probably need the Props functionality as well
yes, I just wrote it half an hour ago ;-)
keep the assertion with the assignment, perhaps use Rich's new "requiring"?
Use an option iso magic value=
what if this throws?
I think this is dubious, if it isn't legal, then it needs to be enforced.
It's legal, it means that the receive did not succeed and the message was   dropped.
Cool, add that as a comment
yeah it deserves it.
can isOpen thrown an exception?
closing over this
+?!?!?!?!?!      val OP_READ_AND_WRITE = OP_READ | OP_WRITE
Same as in TcpSelector. I can change both though.
if one of the channels fail to close, the other channels won't get closed.
    try selectorManagementDispatcher.execute(task) finally selector.wakeup()
explicit return type pls
well, they ARE guaranteed to be the same here
There is a long tradtition of "|" for those two. ;-)
I prefer "|", too.
The documentation does not mention any possible exceptions, so I think not.
True! I'll port it to the TCP selector, too.
so you only need try-catch around the actual "close" call
what about trying to read right again and only falling back to the selector once that fails?
same here: try sending first and only register WriteInterest if that fails
where is the KickStartDone handled?
It needs to be configured in that case, as it hogs the current thread.
it just occurred to me that you can add forwarders in the respective objects:      object Tcp {       object SO {         val SendBufferSize = Inet.SO.SendBufferSize         ...       }     }
    expectMsgType[Received].data must be === data
why is this not a child of one of the selectors in the pool?
if `selector` is a pool, then this might go to some actor and the `WriteInterest` later to a different one, which makes me think that the simple sender should be registered just as anybody else
This is exactly the plan, I came yesterday to the same conclusion.
avoid relative imports at the top level imports in a file
if the manager always is the parent you don't need the constructor parameter, but that is perhaps not the case
use the convention to enclose variable data in log messages and exceptions in square brackets      log.debug("Closing UDP connection to [{}]", remoteAddress)
not `Option` because of performance? in that case you might want to avoid the tuple allocation as well, otherwise I would prefer `Option`
this is wrong, will fail if it's the first usage of the extension, should be       override def get(system: ActorSystem): UdpFFExt = super.get(system)
same thing here
one reason for this is tests, the other is that `context.parent` is not the same for routers (as is the case here as well)
What does "low" mean?
I think the precedent for such a setting is "throughput"
Hmmm, after getting here I think we need to establish a convention for how we name limiting settings
this is a limit
Perhaps have this in a finally-block?
I still like this :-)
 and I still have the feeling that that was not a good choice way back when; it is not configuring the throughput, it is configuring the max batch size (in all cases alluded to), which _may_ then have an influence on throughput
The cost for cahnging it is higher than the value of changing it, and the cost of having multiple things describing the same thing is costlier than just using the already established one. I'm afraid.
yes, I was afraid, too ;-) But it needed saying (at least this one time)
then we agree :-)
I have never liked `throughput`, but I also agree. :-)
explicit type please
If this throws, then all is still good?
i.e. very nice
this reads weirdly. receive = workerForCommand?
options foreach { _.beforeDatagramBind(socket) }
You were catching and suppressing such exceptions in previous Actors, why not here?
I'd rather pass send and sender into doSend, and let doSend manage the pendingSend
please add `endpoint` in the message
please add bind and connect parameters here as well
same here: please add `endpoint` to the message
I have no idea other than lower than TCP. 
"upong" (seems to be there since the beginning)
I'd like to understand better what exactly is different on Windows here. I don't think the test needs the assumption you are quoting. It should test what happens when the connection is closed when the client side Actor is waiting for the connection establishment.   Under Linux what happened is that the channel was flagged `Connectable` but the connection call failed with an exception. What happens in Windows?
this is confusing: people will drop the parens and then wonder why it does not work; can we make it so that the companion object extends `SimpleSender(Nil)`?
need to check the Java API then
No exception is thrown, it looks like that the OS accepts the connection   and so the client does not fail.
payload **to** all
There is another special scenario, when the routees are provided externally.
pull in from code sample, please
All AutoReceivedMessages are treated like that; the one user-accessible case you have overlooked is `Kill`.
is this formatting looking good? My experience is that `` looks unreadable for multi-word things, but I have not tried it together with "
that was a new way of writing a reference, is it correct?
sorry, now I see, not new, it's correct
no, the trick is to use non-breakable spaces in there (option-space on mac)
ah, that's great to know, thanks
I patched the CSS to make this readable in HTML, but non-breaking spaces are probably better. Although they could mess with wrapping.
OK I'll add `Kill` too.
OK I'll add. Actually I'll probably need some anyway so I can verify what's going on.
Meaning they are not children of the router right? Can you give me some more information about this, or a link to where I can find out more?
I like links. :)
these snippets should be short and should not be intended for being wrapped ;-)
for remote deployment beware of the caveat documented in the remoting section (coupling of nodes)
whether or not the router will be restarted depends on its supervisor
yes, much better!
Before creating your own _router_  
OK I misunderstood the description of the Kill message in the actors doc, which is currently: "You can kill an actor by sending a Kill message. This will restart the actor through regular supervisor semantics." I've read the code to check the meaning and rewritten the explanation so it's clearer (to me at least).
here one piece is missing which is in the Scala version:    It should be mentioned that the router's restart behaviour has been overridden so that a restart,   while still re-creating the children, will still preserve the same number of actors in the pool.
this should probably be `Kill`
Not sure about the word `annotating` here. It made me think of Java annotations. Would `with the router's RouterConfig added` work?
I know that you didn't write this, but the config and the sample code doesn't match up. The config uses `myrouter1` and `myrouter2` while the java code uses `router` and `router2`.
Yes, that sounds good.
OK I'll fix this.
Yeah, I saw a couple of questions about this on the mailing list. :)
I saw a comment on the `really easy` somewhere; I'll change to `possible`.
Question if the defaults for 2.0.4 should be the exact same as for 2.0.3 (2.0 with a very high max) so we don't change semantics, but we allow people to configure it on an as-needed basis?)
I think that is a good idea. I'll change the factor to 2.0 so it mimics Netty defaults, and cap it at say 64?
I'd cap it to 128 since it will have changed semantics on a0 otherwise
I don't like this one, please avoid negated booleans, noDiagrams & !_contains, switch it to be true-based instead.
You are right, it's completely backwards.
good that it's possible to disable
wasn't the purpose of all this to fail the build?
Yes, that comment was made yesterday when it didn't fail the build. Will remove it.
Have you covered all the angles with this feature?  Equality of ActorRefs belonging to the same node, per-sender-ordering to the same destination, are outbound lookups creating new connections? (which defeats NAT)
At the time I was trying to be sure that my change wasn't propagating outside that class, I wasn't entirely sure another class wasn't accessing it through the val. So I wanted to see if any errors would show up. I decided to leaver it since this value isn't always going to match the local ip address used in the rest of remoting, it should only really be used to initialize the Netty server so I felt using it externally should be discouraged.
In what way does this test verify that it works over NAT?
This test checks if it can successfully deploy actors to a remote system through NAT and then receive a message back.  However it does not cover all the angles you mention above.
Currently I am using it to build a p2p voiceChat client.  Outbound lookups to initiate connections are working, and I have been able to send out a stream of Speex encoded voice packets to a remote actor and then play it on the other end.  At the moment I have only tried this inside my own home network, but it is NATed on both ends through my router. If point 3 didn't work I shouldn't be able to connect, and if 2 didn't work I dont think my voice stream would work. 
Equality of ActorRefs, I assume you mean that if I use actorOf(), actorFor("akka:node@74.74.74.74:2552/user/someActor") and actorFor("/user/someActor") are they all equal?  This I have not tested.
Since none of the new settings are used in LocalActorRefProvider.actorFor I am very doubtful that this patch actually achieves its advertised goal: it is not enough to just bind to 0.0.0.0, youll also have to accept different IPs that the current systems name as local. Have you actually tried sending TO the NATed host? Because that is the thing which does not work.
My explanation was probably insufficient. The only IP I name as "local" is the IP which is accessible from the internet. I am not supporting multiple addresses here.  I am supporting a single address, the one that corresponds to the NAT (my router).  If my internet IP is 74.74.74.74.  This is the only IP I can send to, I cannot receive messages via the IP in my internal network.  If computers in my internal network want to send messages to my NATed ActorSystem then they MUST send to 74.74.74.74.          This is how I am using it currently.      automatically find and map a port on my router, this maps port 2552 for my first PC and 2553 for my 2nd ,etc.     val routerConnection = new Upnp().routerConnection     val ip = routerConnection.ip        74.74.74.74     val port = routerConnection.port;          configure to use my modified akka.remote, with the Ip and Port for the Router     val natConfig = ConfigFactory.parseString("""     akka.actor.provider = "akka.remote.nat.RemoteActorRefProvider"     akka.remote.transport = "akka.remote.nat.netty.NettyRemoteTransport"     akka.remote.netty.force-bind-address = on,     akka.remote.netty.hostname = """ + ip + """,     akka.remote.netty.port = """ + port)      implicit val system = ActorSystem("node", natConfig)  Currently I can already use it to stream voice P2P (both directions). Such that 74.74.74.74:2552 communicates with 74.74.74.74:2553.  If I tried to have my internal network 192.168.10.246:2552 and 192.168.10.248:2553 try to communicate it would not work under this setup, which I think is what you were getting at, I would need to identify a second local IP for both addresses to work. I know this wouldn't be sufficient for all use cases, but this was simple to do and is the only way I could get Netty to bind to my external IP.  My main problem at the moment is I feel like my test is terrible.  Since it needs to be manually configured against the router Ip, it is just going to fail unless I disable it by default. But if I disable it by default then it will rarely get tested, so what good is it? Maybe I should just be testing that 0.0.0.0 gets set properly so I know "force-bind-address" is working.
I think this should be a ticket and not a comment
    override val deadLetters: InternalActorRef =       _deadLetters.getOrElse(p => new DeadLetterActorRef(this, p, eventStream)).apply(rootPath / "deadLetters")
we actually solved it during the meeting today, will update there will be a ticket for the better long term solution though
I think this wants to be a warning 
True, I'll change that and then merge. I think cherry-pick of the last two commits to release-2.1 should be no problem. Ok?
no, backporting to 2.1 should only be done once we are certain how to proceed; in 2.1 links between systems are not bullet-proof yet with or without this patch, right?
How about encoding the suffix inside the akkaPreviousArtifact so cross-publishing would technically work (aside from DRYing things up a bit?)
Sure. What I also tried was to define previousArtifact in defaultSettings, but I couldn't get hold of the project id from there.
Is there a reason why we don't just ignore it in MiMa?
Same here. Do we really want to guarantee binary compatibility on synthetic methods? I am fine with it just curious.
yes, it is very strange that it reports it as a problem.  @dotta do you know why?
we can keep it like this for now, but we should ask the MiMa folks whether this is not something which can be auto-detected and not warned about (or whether our reasoning is unsound); @dotta?
If the synthetic isn't private, then it's a binary incompatibility (otherwise, it's a bug on MiMa). Of course, I agree it may be nice to have a flag to tell MiMa not to report these sort of incompatibilities, if you expect/require clients to never link against them. You could file a ticket.
If we can't get these to run _sequentially_ and failing fast, then maybe we should do like we discussed this morning and let the job do `clean test:compile test validate-pull-request` and only add things like building docs and mima here?
ok, I'll change to that
Before merging this I must wait for a recent nightly.
Isn't it the second element?
So the first seed-node must be started first, and then the other seed-nodes in any order?
no not really, you can start seed2 and seed3 before seed1, but they will not be "active", until they have been able to join another seed node (seed1). They will retry the join procedure. So a possible startup scenario is: 1. seed2 started, doesn't get any ack from seed1 or seed3 2. seed3 started, doesn't get any ack from seed1 or seed3 (seed2 doesn't reply) 3. seed1 is started and joins itself 4. seed2 retries the join procedure and gets an ack from seed1, and then joins to seed1 5. seed3 retries the join procedure and gets acks from seed2 first, and then joins to seed2
Change all Up, Down etc to 'Up' and 'Down' etc. 
Link to the script or even better, to a doc section that explains the script. 
You mean 'Up' and 'Down' and not ``Up``and ``Down`` (double back-ticks) The latter is rather annoying to read.
Yes, will be done as part of ticket https:www.assembla.com/spaces/akka/tickets/2014
Yes: 'Up'. just to make it clear that it is a command. 
is this artifact name going to be correct? (don't we publish binary scala version in artifact id from 2.1?)
Will the version numbers above be replaced automagically? (bad memory)
Should we hardcode a nightly version?
I think the idea is to cut akka-cluster.jar when we release 2.1 and mark it as unsupported/experimental
In that case it is wrong here also: http:doc.akka.io/docs/akka/snapshot/intro/getting-started.html#using-akka-with-sbt  Do you know @bantonsson?
I'm not 100% sure though, 2.1-M2 was cut when I was away.
2.1-SNAPSHOT is replaced. The scala version replacement must be written like this:      .. parsed-literal::        |scalaVersion|  It can't be used inside plain text. @rkuhn correct me if I'm wrong.  Anyway, it doesn't matter, I can remove that from the text, and only use the |scalaVersion| in the code below.
Until we have something better.
I saw that - I like that! This doc was written before that was decided.
plain text works just fine, the parsed-literal is only needed for code snippets which would otherwise be a codeblock, since within that these replacements do not apply
ah, good to know, I missed that
a comment as to why 90 is chosen
What's with these?
certainly; got it
they snuck into the wrong branch somehow, hope you dont mind
isn't this more a LongRunning test than a TimingTest?
90 seconds added to the build time is hard to justify once this has been tested for a while, we might want to create a ticket or FIXME to ignore this test?
Yes, in 2.1 and forward it will be, but in 2.0 that tag did not exist. And I think we should keep it as LongRunning (because long running is exactly what it is ;-) ).
Might want to clarify semantics of this method.
Might want to clarify semantics of this method.
Might want to clarify semantics of this method.
I prefer: repliesFromAddresses must be === Set(node(first).address, node(second).address, node(third).address)  Then you can replace 5 LoC with 1
Is there any risk of cycles here?
Is cX a safe name? (compare $X for anon children)
I'm not sure I understand what you are looking for. I tried to explain what it does: "use the resizer defined in code if not defined in config" 
You mean like a RemoteRouterConfig wrapping another RemoteRouterConfig? That would be very wrong. I can add assert.
Yes, exactly, thanks
I don't know the background for naming the routees with real names instead of using anon name. Do you know @rkuhn?
Oh, I interpreted the docs as "Override this method if you want to use the resizer defined in code if not defined in config."  Drop "Override" and just say "Uses the resizer of the given Routerconfig if this RouterConfig doesn't have one."
ah, sure, that was confusing
looking more at it, randomName is not availble here, I'm leaving it as is, open a separate ticket if you think it should use some random name. Note that this is using the internal provider api:            val name = "c" + childNameCounter.incrementAndGet           val deploy = Deploy("", ConfigFactory.empty(), props.routerConfig, RemoteScope(nodeAddressIter.next))           impl.provider.actorOf(impl, props, context.self.asInstanceOf[InternalActorRef], context.self.path / name,             systemService = false, Some(deploy), lookupDeploy = false, async = false)
What is this intended to be used for? I'd like to avoid duplication (Why isn't it an ActorRefFactory?)
I.e. why is it a subset of ActorContext?
It was something that @rkuhn and I talked about. We don't like the way the full ActorContext is exposed to RouteeProvider and RouterConfig (outside of the actor). The purpose of this was to reduces this *full* exposure.
I think it makes sense to have a design discussion on this, Roland had some ideas on how to solve it
Previously @rkuhn had an idea of that routees should be created by sending messages to the router actor. I didn't go that path because I don't think it is possible (without major rethinking) because we rely on immediate return of routee ActorRef.  We can discuss further improvements to the router design on Monday. I'm not sure that major redesign should go into this ticket though.
Do you think it would be useful to also have a setting ``deploy-on-own-node = on`` that can be set to ``off`` for master-worker scenario where all routees are remote?
I have added this feature
I guess you meant `java.lang.Iterable`?
this file belongs into the akka/cluster/routing directory (so that IDEs can find it)
of course I meant that, thx. fixed
This should mention that it cannot be combined with `.withDeploy`.
I guess I should look at the code which uses it, but still: why is it necessary to give a Deploy if all we want is a cluster-aware router? The same is not necessary when using RemoteRouterConfig.
`postRestart` will then subscribe again, is this intended? If yes, a comment would be in order.
Im not currently fit enough to think through those fallback thingies, but all this will be marked experimental, so I think we can sit together at a later time and try to find a consistent spec (not saying that this wouldnt match it, but ).
please move local import into proper initialization block
this should still respect the `maxInstancesPerNode` setting, I think
what about `RootActorPath(target) / settings.routeesPathElements` and then in settings using the RelativeActorPath extractor to verify that the string makes sense?
If you feel uncertain about those fallback thingies you should take a look at this (closed) pull request, because similar is also in non-experimental RemoteRouterConfig. https:github.com/akka/akka/pull/650/files  There I wrote test that fails without this.  ticket #2433
yes, that is the point, to handle restart, I'll add a comment
That is a very valid remark, and simplifies a lot. I have removed this sugar stuff and let user api be:      .withRouter(ClusterRouterConfig(RoundRobinRouter(), ClusterRouterSettings(...)))  with some nice apply and constructors in ClusterRouterSettings
good point, I have moved that special case to a better place
Exactly what I was originally looking for, but didn't found then. I have changed to your suggestion.
it's a tiny bit confusing reusing the same parameter name as the outer scope
scala.Iterable I presume?
What about mutability of the Iterable?
A bit of not needed copying here, what about (assuming it does the exact same thing):      abandonedRoutees.foldLeft(_routees) { _ - _ } or (_routees /: abandonedRoutees){ _ - _ }
I'd probably unwatch them as you iterate over them in the fold above. Turns O(3N) to O(N)
I think you can replace it with:      registerRoutees(paths.view.map(context.actorFor))
However, you may or may not want to close over the context
What does the breakOut do here?      val routees = Vector.fill(nrOfInstances)(context.actorOf(routeeProps))
"keep" is not used, use "drop"?
"abandon" could technically be mutable, and in this case you're exposing it to other threads.
To be symmetric with the Scala API, do we want to return "RandomAccess"? http:docs.oracle.com/javase/1.4.2/docs/api/java/util/RandomAccess.html The docs does not say anything about the mutability of the list returned by this method.
Nice catch! :-)
how about: allow-local-routees = on  "node" is a bit fluffier than "local" wdyt? :-)
You could hit 2 birds with one stone here:      val selfAddress = system.provider match {       case c: ClusterActorRefProvider => c.transport.address       case other => throw new ConfigurationException("ActorSystem[%s] needs to have a 'ClusterActorRefProvider' enabled in the configuration, currently     uses [%s]".format(system, other.getClass.getName))     }
What are valid values for the totalInstances, maxInstancesPerNode, routeesPath etc?
As in documentation I mean
yes, it is, :-) will change
the outermost wrapping parens are not needed
This is not used
in practice that would be strange, but what type to use then? does it matter? isn't scala.Iterable thread safe? should I have to do .toIndexedSeq or something before doing ++
currentNodes is not used
I have no clue why breakOut is used here (and there), why is the view needed?, why not       registerRoutees(paths.map(context.actorFor))
Move this up to the line above
good catch, I'll make a local val of type Seq (or is there a better immutable type to use?)
preStart and postStop are called by default by preRestart and postRestart, so if you don't want to miss things that happen in between, you'll have to override preRestart and postRestart.
RandomAccess is a marker interface - wouldn't be that useful :-)  I think List is what most java users are familiar with, and this is a copy so I don't care about immutable.
I like `allow-local-routees`, another suggestion is `use-local-routees`
yes, it is, but I can move it inside the else scope
final class? (or we want users to subclass?)
and since it's only used once, it's not needed :-)
that's not a problem, subscribe sends a fresh CurrentClusterState snapshot, so what happens in between is irrelevant
ALLOW is *nice* On Sep 11, 2012 1:14 PM, "Patrik Nordwall" <notifications@github.com> wrote:  > In akka-cluster/src/main/resources/reference.conf: > > > +    # enable cluster aware router that deploys to nodes in the cluster > > +    enabled = off > > + > > +    # Maximum number of routees that will be deployed on each cluster > > +    # member node. > > +    # Note that nr-of-instances defines total number of routees, but > > +    # number of routees per node will not be exceeded, i.e. if you > > +    # define nr-of-instances = 50 and max-nr-of-instances-per-node = 2 > > +    # it will deploy 2 routees per new member in the cluster, up to > > +    # 25 members. > > +    max-nr-of-instances-per-node = 1 > > + > > +    # Defines if routees are allowed to be located on the same node as > > +    # the head router actor, or only on remote nodes. > > +    # Useful for master-worker scenario where all routees are remote. > > +    routees-on-own-node = on > > I like allow-local-routees, another suggestion is use-local-routees > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/653/files#r1575362>. > >
I try to keep lines < 120 chars -- isn't that a god convention?
You can get back down to 120 if you drop the "this." part of "this.getClass" ;-)
works for me
:-) you just lost one line of net-negative opportunity
One day the king has to step down from the throne, just making sure that there'll be a new king :-)
these things tend to want to be configurable
yes, postStop of parent is called after all postStop of children have run
After all the postStop of the children have been _called_, or after all the postStop _finished_?
This means that all other exceptions are escalated, leading to a restart of the headActor, which will due to default preRestart terminate all connections. I know you are thinking about definitions of restart semantics for the connection actors, and once that is done it needs to be hooked in here.
adding `@volatile` does not make it race-free without using CAS loops; I'd recommend a normal ConcurrentHashMap or similar
probably the easiest way of not forgetting to remove these is to switch them off in IDEA ;-)
Totally true, I realized that it was wrong, but I forgot about concurrent collections.
we usually put such imports at the top-level, unless lexical scope really matters (for implicits) or they are "unusual"
keep an eye out for Java API: we generally do not use (partly) symbolic method names on interfaces which can reasonably be implemented by user code
didn't look to closely at the usage: why does the handle need to be mutable?
maybe assert that no connection was there before the send
The child tells the parent that it has terminated after it has run its postStop. And the parent proceeds to run its own postStop after all children has told it that they have terminated.  Does that answer your question?
Always place constants in objects and not in classes, it's good style to do so since it doesn't add a field for every instance. (In this case the number of instances will be few/one but it's how we do it)
Are these intended to be public?
Is this intended to be public? For reasons of readability and quick breakage if inference changes, always use explicit return types on methods.
No need for the braces here.
Why is this public and lazy?
We most likely want to do the nulling-out in a finally block
Use j.u.c.TimeoutException (supertype of AskTimeoutException)
This should move in after the if-check
Not needed to have the addressFuture out here, just inline it into the Await.result(...) call
Looks like a good contract, needs some ScalaDoc that describes what it does and what it means. (I'd suggest the name "reporter" instead of "notifier")
Always use "override" so that you get compilation errors when you remove methods on the interface. For 1-line methods the recommended format is: def ... = : Type = ...
I'd remove this method and use remoteSettings.LogRemoveLifeCycleEvents in the notifyListeners-method directly.
Why is this placed here and not in an object?
Listen does not extend RemotingCommand (which I assume it should?)  Also, I prefer to pass the transport in the constructor instead of in a message (in Props).
Instead of the getOrElse you could simply do + senderOption and it will be Some(path) or None which is just as informative.
We might want to brainstorm about a new name :-)
Always use vars-to-immutable-datastructure _unless_ you're using a Concurrent datastructure: why? - You can freely share the data structure if it is immutable, and changing the value locally keeps mutability to the smallest possible scope.
no need to match "address", use _
I don't understand this line
what if endpoint doesn't exist in the map, should throw exception here?
Double work, on the line above the address is returned in an option, use a pattern-match on the returned value.
Since you always do remove you might as well start the method with:  endpointToAddress.remove(endpoint) match {   case Some(address) => addressToEnd....   case None => ... }
These two imports should go at the top of the file
why put it in a val here?
why val here?
I think EndpointException is not a fatal exception so you don't need the "_: EndpointException"
Why use the default dispatcher and not the actors' dispatcher?
no need for braces
no need for braces
Name seems wrong here
Order of notifications and other code in these cases seem inconsistent
Historical nomenclature has been "inbound" & "outbound"
Add a TODO to switch to a different dispatcher
A RuntimeException is probably more appropriate here
Why create all these types?
Why the local vla?
this match is unrelated to the action
This breaks actor encapsulation by writing to an internal field from outside of the receive
Pausing the review here, it's 3am..
that is a bug, and a actually a very stupid one. It is duplicated further   down as well. Will fix it.
I don't know what is the best way to represent points in time in Akka, so   I just used Deadline for now. Is there a nicer way?
True! This is an obvious remnant of old code that I already got rid of.   Thanks for pointing out!
I thought that it is better to distinguish the kind of errors by distinct   types instead of just distinct messages. On the other hand, these   fine-grained exceptions are not used now, so I can remove them easily.   What is your opinion?
Yes, we talked about this with Roland already, I mark it with a TODO as   you recommended and fix it. Thanks!
  think EndpointException is not a fatal exception so you don't need the > "_: EndpointException"  The idea was to catch the exceptions that might slipped out that were not   wrapped in EndpointException, but match the NonFatal pattern. Currently   NonFatal and EndpointException is treated the same way, so these lines are   surely redundant. I will remove it as you suggested.
This is something I want to talk about with you and Roland.
True. And now netty4 uses the same terminology for handlers instead of   Upstream/Downstream.
I think we talked about it, but just to make sure that it does not get lost: don't log messages which go to deadLetters
one more thing: I think it would make messages more easy to understand if these actors had names corresponding to the address they are serving
this will not work: the newly created endpoint is not yet known to the registry
Could it happen that the name is not available after stopping and creating   a new actor?
having different types for different failures (which are then actually distinguished by the actor) is definitely good style, because then we can fully use supervisorStrategy to decide what to do
or an AkkaException even
True! Thanks. Latch behavior has no test case yet. I will add a test and   fix it!
this should be idempotent if it only can be called once
Constants like these are always nice to group at the top of the class
Why commented out?
why Tuple2 instead of 2 params?
_isTerminated accessed outside of lock and isn't volatile. Also, name is confusing, perhaps something like _shutdownRequested
If it remains there, then during the tests I cannot observe if the actor   managed transport closed the individual handles or not (because the   transport closes them anyway).
Because of the arrow notation. The arrow indicates the direction of the   connection. At least that was the idea.
braces not needed
Yes, these classes related to DummyTransport are not threadsafe. This is   something I intend to clean up, but only after the design of the remoting   is settled.
This is only used for testing the EndpointActor's behavior during   restarts. This code is used to intentionally crash the actor and force a   restart.
You can use named parameters instead:      def connect(from: Address, to: Address) =       connect(from = ..., to = ...)
Ok! I will change it.
A Sequence might not be a proper data structure for this, a Set perhaps?
I'm skipping the rest of the Dummy* as I assume this code will be dropped later in the process.
I propose "RemoteMessageEncoder"
All of these imports are superfluous (after ._)
We never use relative imports, they are confusing and order-dependent
 > We never use relative imports, they are confusing and order-dependent  This is "IDEA magic", I have to turn this thing off... Thanks for noticing!
scala._ is always imported by default, and here scala.Some is imported explicitly, twice
Isn't the initial value always null?
True. It was an Option before, and I changed back to using null.
I guess actmote is just a temporary code name?
I think you can consider all names as temporary. Any naming suggestions   are welcome.
    if (headActor != null) try {
magic value: 5 seconds same as managedRemoteSettings.ShutdownTimeout?
what if multiple threads do start at the same time? do we need to support that?
use apply of the case class Timeout instead of new      val timeout = Timeout(
``implicit val timeout`` instead of param
yes, or `import remoteSettings.LogRemoteLifeCycleEvents`
I think we have a convention to use max 120 chars lines. Not followed everywhere though.
2.10 string interpolation?
`: Unit = {`
and (then) it should not be a relative import
is it a timestamp? System.currentTimeMillis or if it's only for later measure a duration System.nanoTime
Or if it really is a Deadline, it should be in the future, then you just need to check isOverdue and then evict.
I don't know. I will check it. I think you can currently specify one   RemoteTransport in the config and that is started by   RemoteActorRefProvider.
Why not, indeed. If everyone agrees, I am happy to use it.
Is there a class that has a different name than Deadline but still   represents a point in time? Or the only way to do that is   System.nano/milli Longs?  Storing the real deadline was my original plan, but this way I can extract   also the elapsed wait duration when the Latch opens which is needed if I   want to implement increasing backoff intervals.
so, after a restart you don't want to use the handleOption passed in to the constructor, but still use isServer derived from the constructor param?
perhaps call super.unhandled first?
could you send yourself a message instead?
do we need catch around `!` in that case it should perhaps be `case NonFatal(e)`
rely on `unhandled` instead?
no, ! should not throw exceptions in 2.1
sorry, this was not an actor
awesome documentation in here!
`val f` not needed
NonFatal? or is there a reason for `e: Exception`?
Should probably be documented that this might take a while
Yikes. Good catch.
Yeah, Viktor noticed the same. Will change to message.
This is copied from the old code. Should I fix it there as well?
I copied this from NettyRemoteTransport. I should have asked first why is   it an object... I will fix it. Should the original code   (NettyRemoteTransport) stay as it is?
It's not the fact that it is an object that is a problem, but why is it an object within a _class_?
Because that's how it is in NettyRemoteTransport (it is a class as well)   and I thought there is a reason for that which I just don't understand.   Check NettyRemoteSupport.scala line 345. I am not saying it is right :) I   will fix it anyway, but should I fix it in NettyRemoteTransport?
If it was like that already then just leave it be, I just got curious :-)
Ok, but I should change it in my code (NettyConnector), right?
Best solution is probably to pull it out and use the same version from both the NettyConnector and the NettyRemoteTransport
This could also get problematic.
Yes, this is blocking -- I just realized it yesterday. I could not find a   non-blocking name lookup in Java. Can you recommend a solution?
is this pasted here verbatim?
Why explicit type here?
no need for braces
I think the write-method should return a Future so that the logic inside the method could be extracted out, because all other transportconnectors will want to achieve the same behavior (sending to DeadLetters etc).
This code was lifted from the original transport, but I started to rewrite   it because of the blocking calls (awaitUninterruptibly) but I have not   finished rewriting it in a completely asynchronous fashion.
No need for braces (check all such occurences)
doesn't extend ConnectorEvent, on purpose?
You mean that the handling of errors should move to the EndpointActor (do   the same stuff as above when the Future fails)? I was actually thinking   about that and talked with Roland about it. He told me that as we already   have an ActorRef, a Future might be overkill. I am personally a bit   undecided about this. I conceptionally like the Future version better, but   on the other hand I feel it is a bit heavyweight. Does it have any   performance implications? What is your opinion?
No. It's a bug... *Sigh*
Do zeros need units?
No, but for the casual reader or the guy who works with this test, it is nocer to know instantly if its a count or a duration.
what about sun.misc.Unsafe?
The import for sun.misc is marked optional for now to allow for resolution of the bundle in most OSGi environment.  In my example, the package didn't get resolved and my mini-example still worked fine, but if it is really required, the only solution is probably to document that this needs to get configured properly in the OSGi framework (either using boot delegation or as a system bundle export) but that would probably also mean that Akka only works on Sun JREs, I guess?
IT's definitely a required entity. An no, it works for virtually all VMs out there, atleast according to Doug Lea.
OK, cool - thanks for the feedback! I'll remove the optional attribute from that import then and perhaps add a doc page under additional/ or something to clarify what needs to be changed for an OSGi environment instead?
I think we should only export akka*, not the other ones.
Oh, and I don't believe in such a compatibility story: Better go for [2.1, 2.2)
Akka will only be binary compatible within minor releases. (so 2.0.x, 2.1.x etc)
Yes, that's why I propose to fix the version range to [2.1, 2.2) which means 2.1 to 2.1.x.
Ah, sorry about that, glanced too quick, didn't notice the "exclusive" )
Yeah, it probably makes more sense to mark com.eaio.* and org.jboss.netty.akka.util.* as private packages.  However, for com.typesafe.config.*, I would personally keep that export because at the moment, these classes provide the most convenient way for passing along the correct classloaders to make sure that configuration files can get loaded.  Perhaps once we have these initial OSGi headers sorted, we can add an akka-osgi module which does the right thing in an OSGi environment (e.g. providing an easy to easy Activator implementation and/or a Blueprint namespace handler to set up the system.
OK, thanks for the feedback once again!
You are right: _com.typesafe.config.*_ should be exported
Okido - I'll make the necessary changes and add them to the pull request then.  Thanks for the feedback!
Jut as a FYI: com.eaio will be dropped, I've added a ticket to move the things from org.jboss.netty.akka.util to akka.util. com.typesafe.config will become an explicit dependency as well, instead of an embedded one. This will hopefully clean up the exportPackage
Are the things from org.jboss.netty.* only used internally or should they be exposed in the API? If just internal, you could consider to put them into akka.internal.util.
We'll make them package protected
Don't we want to only check this if a non-default version is used?
Don't we want to only check this if a non-default version is used?
you mean special-short-circuiting the string `DefaultDispatcherId`? would that really save that much time?
It's a ConcurrentHashMap right?
Where reads are synchronized. so all actors created on the default dispatcher would synchronize ;)
Special case can be added in hasDispatcher
no, `hasDispatcher` does not access the map, it only checks whether the `Config` has such a section
I think it needs to be race-free:         val sz = _routees.size       if (sz < 1) routeeProvider.context.system.deadLetters       else _routees((next.getAndIncrement % sz).asInstanceOf[Int])
but on the line above it is already stored in a local val    val _routees = routeeProvider.routees 
Doh, of course! (I only use _ as first letter for hidden members, not for method-private fields)
ok, I can cleanup that in my other router work, but leave this as small as possible to simplify backporting
changed my mind, it doesn't touch much more lines than I have already changed, so I'll change it right away
we don't have any public class named `DeathWatch` so I think the user instruction should be something like "Use death watch", or "Use context.watch"
True, I'll fix
why the parens?
What unit are these in?
Why do we use immutable.Traversable and not immutable.Seq which we use everywhere else? (also has efficient Java helpers in JavaAPI)
classOf[AkkaSslHandler], init ?
one char too much
its in the docs above
 is on the line above
do you fear that someone might pass it to `String.valueOf` ? ;-) (This is the Java API)
we had some discussion back when it was introduced, it boils down to that this type expresses exactly what we need and nothing more; we can use those efficient helpers nevertheless, I guess
Since it's not apparent in the type, it think it should be apparent in the name. One shouldn't need an IDE to read the code (Pull-requests etc) IMO
didnt we agree to leave the closure stuff in, awaiting the sporticles?
Didn't we already change a lot of Props even though we don't have Sporticles?
Hmm, considered that, but then Id either have to call Option#get or make the code less compact, neither of which I found appealing.
I don't think you'll need this
Could you add this where possible?
You mean adding these to the reference.conf? Sure.
Do you need this to be true?
Ah, no, sorry, misunderstood. leave them as they are until they have more efficient serializers available.
Since this looks to be public API it needs some ScalaDoc love
Great work Patrik!
yes, it should have little impact, for normal actors, without routers etc it's only the creator (Function0) that goes through java serialization.
Yeah, we could always create our own Function0 for the class-case
If we do that it would be possible to serialize that case with a class name string. Perhaps not important.  On Tue, May 15, 2012 at 4:16 PM, viktorklang < reply@reply.github.com > wrote:  > > +      case Left(e)      => throw e > > +    } > > + > > +  protected def deserialize[T: ClassManifest](data: ByteString, clazz: > Class[T]): T = { > > +    val bytes = data.toByteArray > > +    serialization.deserialize(bytes, clazz) match { > > +      case Right(x) if classManifest[T].erasure.isInstance(x) => > x.asInstanceOf[T] > > +      case Right(other) => throw new IllegalArgumentException("Can't > deserialize to [%s], got [%s]". > > +        format(clazz.getName, other)) > > +      case Left(e) => > > +         Fallback to the java serializer, because some interfaces > don't implement java.io.Serializable, > > +         but the impl instance does. This could be optimized by > adding java serializers in reference.conf: > > +         scala.Function0 (the creator) > > +         com.typesafe.config.Config > > +         akka.routing.RouterConfig > > +         akka.actor.Scope > > Yeah, we could always create our own Function0 for the class-case > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/452/files#r823633 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
See this commit https:github.com/akka/akka/commit/31ace9e83f1a9dc361bc6d1aa4e79c4825e8e07e If there is any drawback I'll revert.
changed to false
violated, mutilated, maimed, 
So I could someone explain how this is different from the previous code? I mean where is the leak and how is it fixed?
calling it a leak was a bit overly dramatic, the fix is on line 389
It was a leak tho ;-)
Why is this removed?
It has to be removed in order to allow adding () to the other apply method: def apply[T <: Actor: ClassManifest](): Props Else both have the same signature after erasure.
But they don't do the same. So won't work
In addition: Removal did not break anything, hence it was not used. And there is always the val default available, which is more expressive than apply() returning a cached default, so the removal increases intuition.
How do you know that no one is using it?
I think it was not used in Akka.  Even if someone used it, nothing should break. The old version returned a default which could not be used, because the creator field was initialized with a function that throws an exception. In order to use it one would have to call withCreator on it which is exactly what the other apply method is doing.
Alright, if you promise that it won't break anything...
Well, I can give you a future which will eventually (and IMHO very probably) evaluate to not breaking anything (sensible), but you know ...
I thought this worked now out of the box with the default dispatcher?
"with for example"?
that is new type of linking for me where does it link? to the **below** __ Props_?
Yes, you're right will change the documentation. 
Yes, it uses the Props. It is how you write an anonymous link, so the word _above_ will link to _Props_ in the same document. The old link was a broken http link that caused a warning.
You most likely want to make that: { case NonFatal(f) =>  }
Please restore to have them on one line each, saved vertical space.
One liners please
NonFatal is a really nifty trick!
Yeah, @rkuhn is a genius
def enqueue(receiver: ActorRef, envelope: Envelope): Unit = storage.push(serialize(envelope))
def dequeue(): Envelope = storage.pull().map(deserialize).orNull
Might want to comment as to why cleanUp shouldn't be implemented.
"is a replacement for the standard actor mailbox that is durable." => "is a mailbox which stores the messages on durable storage."
Add URLs to the other adopted/community mailboxes
"A durable mailbox typically doesn't implements transactions for current message." => "A durable mailbox is like any other mailbox not likely to be transactional." 
yes for anything else but documentation, I wan't to show the type returned from serialize
Where do I find them? They are not listed at http:akka.io/community/
Ask Jonas, he's coordinated that.
Ok, but comment that so no one cleans it up ;-)
there is only amqp at the moment
I don't think we should add this comment to private members
In case Scala changes visibility for other JVM languages, I think we need to.
private is not visible for Java afaik private[akka] is another thing otherwise there will be a lot of those comments
These should of course be documented
Could add that it should be a valid URL, so they know what and why. 
Could we switch to Scala Lib's MurmurHash or Phil's new hash? 
private, or define explicit return type
with the new constructor you don't need = null here, do you?
why is this case class, and other exceptions are not?
any risk with that we can't change this one? would it be possible to move to context?
yes, I see no reason why not
what purpose does this UUID serve nowadays? I hope lazy vals are not secretly `@transient` 
I don't know
I think ActorRefProvider and related things should not be user extendable yet 
protected[akka], or prevent subclassing by user code
might make sense to mention what its only use-case within the library is
didnt know that double-brace suffices
behavior == state, hence a restart must clear it, so why would we want to move it into the context? because it is a var in a trait? (and yes, it wont be private because of the constructor business)
why not private[akka] for the whole ActorSystemImpl?
I'll see what I can do
I don't think it should, yet
It'll be used by Akka Camel as well
Hmm, I know first-hand what the location transparency thing is all about, but would it not make sense for user code to ask a ref whether it is local or not in order to determine proper timeouts etc.? Im on the fence, really.
yes, I think so
I think it can be private[akka]
in what way is this more readable than the original?
use Unsafe ;-)
is the recursion here on purpose?
Because otherwise we wouldnt know how to create a Future during `ask()`. But we could also change that to require an `implicit ExecutionContext`. (That was all from akka-actor, still diggin)
Same here (i.e. we wouldnt know how to do the ask timeout thing). Essentially the underlying reason is that ActorRefProvider does not expose a reference to its ActorSystem, which is a good idea because of initialization order.
is this still correct? executorService is  now a ExecutorService, but LoadMetrics is not implemented by that?
And replace it with what, exactly?
`protected[akka]` is weaker than `protected`, so what are you saying? Subclassing applies to ActorSystem or ActorCell, so I dont see a problem which emerges from this line.
No need to repeat the name of the documented element.  A factory to create MessageQueues for an optionally provided ActorContext
No need to repeat the name of the documented element.
WDYM? I dont see how this should be improved.
I dont think it should ever.
You mean: why is it exposed? Because it does need to exist somewhere. A good way to answer these questions is by simply removing it and seeing what breaks. ISTR having good reasons to split this out when doing the actor system initialization.
(somewhat like the fact that one of the earliest required devices during Linux boot sequence is /dev/null)
great catch, fixed 
great catch, fixed.
nope, see implementation, also for traversals.
Because it makes sense, I dont see any value in hiding it. The rest of the ActorPath handling is also not private.
behaviorStack is now moved to ActorCell
yaint gonna saturate no 100GB link like this ;-)
Read the classdef! :D
So you dont want people to implement `MessageDispatcher`? I must have read that email wrong which you sent to akka-user just minutes ago ;-)
and that wasn't caught by tests? scary
Wow, that's not a good reason. Actors already have an implicit dispatcher y'know. I'll remove it.
Think again, please. And this time keep in mind that not all actors are local.
No, I mean if we make it an implicit EC then we already cover the base of actors calling Ask, and there's an implicit from AS => MD (EC) so if you have a system lying around you can obtain it from there. HOWEVER, if we go the SIP-14 route with Havoc's proposal we might not even need the EC at all.
I don't think it does. Use {{{ ... }}} (.e.g triple brace).  Or correct me :-) http:docs.scala-lang.org/style/scaladoc.html
yup, checked that after commenting.
Well, doc could be improved. Not clear at all what it should be used for nor its semantics. 
Why not fix these FIXMEs as part of this pull request? 
Because I do not have infinite time. I marked it as a FIXME instead of simply a TODO so that it's highlighted when someone goes on a hunt for FIXMEs, however, I do not believe that this setting has anything to do with binary compatibility, and I wanted to focus my efforts on that, since it's both the target of the ticket I'm working on and the name of the branch ;)
I disagree, if you want to copy&paste from the ScalaDoc it's way easier if the name is contained within the description.
I was thinking about scala style guide; http:docs.scala-lang.org/style/scaladoc.html When referring to the instance of the class, use this XXX, or this and not the XXX. For objects, say this object.  It's no big deal. I think the reason is to minimize the risk of outdated docs (renaming).
Fair point, but ScalaDoc is shoddy anyway. I'd like a documentation format where the code samples is actually compiled and you could refer to the different parts (perhaps you already can?) $thisClass etc
yeah, that would be super awesome You can refer/link to classes [[akka.actor.Actor]], but I don't think you can for members, which is poor.
I assume that this is not what we're proposing our users use, right? Casting to internal API?
shouldn't currentTransport be used?
ah, sorry, didnt properly boyscout yesterday on the plane. (this must have been a white-space only change due to slipping on the keyboard and accidentally touching cmd-shift-f, I guess)
Nope, because thats set only during send(), and the point of this exercise was to make something which also works outside of normal remoting calls.
See my answer to Patrik below. If unacceptable, we have to discuss it and come up with a better solution.
do we need a more unique name here, to not clash with user name space?
Is `akka-cluster` unique enough?
perhaps order the cases with Heartbeat, GossipEnvelope, MetricsGossipEnvelope at the top
our convention is: `[${value}]` 
Yes, I was thinking about this performance wise. Would it be faster to have it in a `Map` based on class like `fromBinary`?
Doh, of course.
explicit `immutable.SortedSet` (remove the import) that is at least the convention we decided for `immutable.Seq`
place Double and Long at the top
place Double and Long at the top
aha, clever (manifest)
Yes, that's more consistent.
In all cluster messages we have used explicit address. I think that in most cases that corresponds to the address in the sender reference. The background reason for explicit addresses is that originally there were no cluster actors, and therefore no sender. The address is the member identifier and that might not always (in the future) correspond to the sender (transport) address. I think we should keep the explicit addresses, but just wanted to mention it.
all messages are also Serializable. Do you think we should add the @SerialVersionUID(1L) to all of them, or skip it to indicate that we don't really support java serialization of cluster messages?
yes, that was my thought as well
It is only those 3 that have any kind of frequency to worry about. I think match is fine.
and when you are at it, please remove the import of SortedSet in ClusterEvent.scala :)
I think that we should skip it, since we won't support java serialization. I also removed `ClusterMessage` from all messages that aren't sent over the wire by themselves.  That reminds me of one thing I meant to ask about. If I don't make the trait `ClusterMessage` extend `Serializable` I get this warning (since the case classes are serializable by default) `Multiple serializers found for class akka.cluster..., choosing first`. Is this something that we can/should work around? Should we treat `Serializable` in a special way?
but I think we should add @SerialVersionUID(1L) to all ClusterDomainEvent, because users might send them over the wire (and that includes Member, MemberStatus, NodeMetrics and stuff that are referenced). Please add that in this PR.
Yes, hat would at least allow us to add new methods on the classes without breaking serialization, or using another Scala version that adds methods.
Since you cannot make these classes non-Serializable, ClusterMessage must also extend Serializable to make the mapping more specific. And since these classes will always be Serializable well have to make sure that that actually works (i.e. we cannot not support java serialization of our messages).
Ill take this as an example: users may include `Member` in their own messages, which means that when they use the serialization extension theyll find only the JavaSerializer for this class. Should we not offer protobuf serialization for all user-level API messages?
what are all these backticks doing here? (this line is after parsing exactly the same as `val gossip = envelope.gossip`)
So I was under the impression that this ticket was for creating a compact serialization representation for our gossip messages and everything they contain. I'm not sure what we gain by providing protobuf serialization for separate pieces of the gossip message. If we make the messages future proof with versioning, then it might make sense. The standalone Member will be encoded differently from one contained in the gossip, but that's not a big thing.
Sorry about that. Scalabuf names the vals wit backticks and IDEA expands them with backticks. I'll clean it up if it breaks reading flow.
I agree with @bantonsson. Protobuff of cluster domain events, and enclosed classes, such as member, is nice to have but not at all needed. Those are primarly intended to be consumed locally.
I must also do some low hanging tickets sometimes :)
you're most welcome :)
May i suggest java.util.Collections.emptyList?
of course you may, will fix
    private[io] val cmd, evt = {       val l = new java.util.ArrayList[AnyRef](1)       l add null       l     }
why Int max size?
Don't like the repetition of "4", put it in a val somewhere.
And by putting it in the actual initialization of the class (if you want, for instance, to send messages larger than 4Gb) 
Isn't it enough to annotate the methods in `UntypedActor`? 
explicit return type: `java.util.List[?]`
explicit return type: java.util.List
This feels scary, since they will be public for java. Any way to hide them better?
explicit return type
Unit = ()
should we also have non-symbolik names?
`case l: Left`
what is this? all this code in docs?
I think `i++` is more familiar
at some other place I saw `== TickGenerator.tick`
ok, now I understand, it is real code, that also serves as an example, good
The java message is named `tick`. Perhaps name it `TICK`?
in the java sample there is also SetTarget?
`Unit = ()` here as well
post-increment is evil (I realize that Im opinionated ;-) )
nope: wrong type (try it  )
hmm, suggestions? does `sequence` also work in infix notation?
yes, I agree: that is quite an explicit return type :-)
not that Im aware of; suggestions?
tried that first: then javac complains whenever compiling an actor (since it apparently verifies the whole hierarchy)
yes? (or: WDYM?)
good point; will make it a constructor arg and add bounds checking
because `Short` is not always long enough; or do you suggest we should enable users to accumulate more than 2GB within the pipeline before sending the whole blob onwards?
ouch, my java collections skills are obviously lacking
I thought that we had `Unit = ()` as a consistent coding style, or I'm just missing something.
that looks inefficient to me (I know that scalac will probably not actually retrieve a BoxedUnit and then discard it, but still); the method does not return anything, and `()` is a reified form of that which is superfluous here; we currently have both styles in the code-base, with roughly 80% being `()`.
{} is just a block/scope, so you're forcing the compiler to infer () essentially.
no, there is nothing inferred: you would be right if we were talking about a value of type `Unit`, but a method which returns `Unit` does NOT return a value of type `Unit`
put another way: `{}` matches quite exactly the empty body emitted in byte code, while `()` does not
I'm not talking about the value, I'm talking about the type.
the reason is @viktorklang please change to whatever, but make it consistent
okay, now we have three votes: 1 for `()`, 1 for `{}` and 1 for make it consistent. I think we need at least one more vote.
`private[io] val __cmd: java.util.List[AnyRef]`  not hidden, but at least not in normal sight
I have no idea about the efficiency of `()` vs `{}`, so I vote for "make it consistent"
I agree on the consistency part. I vote for () as it means Unit, whereas {} means empty-block-please-infer-return-for-me
it makes no difference whatsoever in the byte-code; but this means that we still need at least one more vote 
only one vote per person (even for Directors) ;-)
ask Mr. Style, a.k.a. @hseeberger
Mr Style says that it clearly **must** be ()
so be it
Team Klangberger wins
actually no: genjavadoc does not like that
also: ByteString is limited to 2GB, as are arrays
@gideondk How do you suggest handling >2GB? There seem to be many limits interfering.
WDYM with the explicit return type?
that is a non-published detail which is necessary to verify that the Java code also works as advertised
@rkuhn nvm, was just about the limits of the 4 byte header... It felt wrong to have a pre-defined size of 4, but anything larger doesn't make sense in the current architecture (and generally not that trivial in binary protocols).
@rkuhn (have seen 2 byte lengths for instance)
Yes: Im adding putLongPart / getLongPart to ByteString and using those to allow 14 bytes header size here.
trying things out: we cannot use the same names as for the static methods, because then we lose the static methods (Scala rules); and I dont like having different names for the same thing; hence Id propose to answer your question with no.
Are these intentionally invariant?
Also, () is closer to a circle, which is "the perfect shape", so for aesthetic reasons I opt for () ;)
I think this is suboptimal since this is in the documentation. Add a defaultAddress(): Address to RemoteTransport that by default returns addresses.head
hmm, this looks like duplication, will also be done by `RemoteActorRef.start()`
this should probably not be in the general RemoteTransport scala file (I assume its needed for the Netty transport)
Lets call him Fred. Or Barney.
How about making these two `lazy val`, depending on the `addressesPromise` which would go here as a simple `private val`? That obviously documents the write-once semantics.
OUCH! (I really would like to have compiler support in catching these: `context` must not ever leave the actors context, because that would be completely illogical ;-) )
this `if-else` has somewhat misleading formatting
Big Thanks!! It's just plain B.A.D.
may laymans understanding suggests that an open latch does not permit flow; this means that in this case here the `if` is inverted
No, if the latch is open, then it allows messages to flow. If it is   closed, it doesn't. A circuit breaker is the thingy that allows flow of   things when closed :) But I can change the terminology.
That triggered me to check the closest dictionary:    latch, noun, a metal bar with a catch and lever used for fastening a door or gate    (electronics) a circuit that retains whatever output state results from a momentary input signal until reset by another signal  meaning that I think latch is entirely the wrong word on these things. As we just discussed, gate would more fittingly describe the purpose of this method.
please use `extendedSystem.dynamicAccess`
Id prefer `transport.listen map (transport -> _)`
hmm, I had to think too long about this, so why not      transportMapping = results groupBy {         case (_, (transportAddress, _)) => transportAddress       } map {         case (_, t) if t.size > 1 => throw ...         case (a, t) => a -> t.head._1       }  Does that not also return a HashMap?
the method `Set.contains` is one of those which I prefer to write without the dot and parens
`val endpointId = Iterator from 0`
please somehow demarcate the end of the constructor arguments; given scalariform indentation rules this probably means adding an empty line
why print `Set()` around the address?
This is the old remoting, which should go away. This was just a short hack   to make things work.
why not inline this?
please reorder into triangle form ;-)
I guess these methods will be useful in more generic form to cover protocol stacking?
these forwarders to wrapped transports might also be useful to package in a reusable mixin, I think
`Iterator from 0` ?
They are already refactored :)
you mean because the frame is just passed through? (just checking)
Yes, it should be properly encapsulated. The way it is defined with   protobuf now will eagerly deserialize the payload part -- which violates   the layering. I will change the protobuf definition after I removed the   old remoting.
neither  nor 
please document relationship between states and data types
refactor with the case above: theres too much repetition here
it would be more intuitive to move this to WaitActivity and start out in that state in the inbound case
why not enqueue the InboundPayload objects directly?
plus: theres a superfluous dot on this line
I created a ticket to do that automatically
better include full config path in the error message
better include full config path in the error message
By using PartialFunction composition?
That makes perfect sense!
I am not really convinced about this one. What would it improve?
Ah, no It don't. There is a missing deserialization step there   (deliberately) which will be included when I split the protocol buffer   definition of payloads.
no, by not having two cases for different stateData but only one which has a match where the differences are
this should have an accompanying scaladoc comment explaining why we added the warning
Do you really mean to send a RST packet? In that case youll need to set SO_LINGER=0 before calling close().
if you just echo the string in the EchoActor, you could also do      expectMsg("pong")     lastSender must be(testActor)
out of general curiosity I wonder if it's not preferred to skip `private val` for non-public constructor parameters?  Without `private val`:  	scala> class Foo(bar: String) { def fooBar: String = bar } 	defined class Foo  	scala> :javap -c -p Foo 	Compiled from "<console>" 	public class Foo extends java.lang.Object{ 	private final java.lang.String bar;  	public java.lang.String fooBar(); 	  Code: 	   0:	aload_0 	   1:	getfield	#11; Field bar:Ljava/lang/String; 	   4:	areturn  	public Foo(java.lang.String); 	  Code: 	   0:	aload_0 	   1:	aload_1 	   2:	putfield	#11; Field bar:Ljava/lang/String; 	   5:	aload_0 	   6:	invokespecial	#18; Method java/lang/Object."<init>":()V 	   9:	return  	}   With `private val`:  	scala> class Foo2(private val bar: String) { def fooBar: String = bar } 	defined class Foo2  	scala> :javap -c -p Foo2 	Compiled from "<console>" 	public class Foo2 extends java.lang.Object{ 	private final java.lang.String bar;  	private java.lang.String bar(); 	  Code: 	   0:	aload_0 	   1:	getfield	#10; Field bar:Ljava/lang/String; 	   4:	areturn  	public java.lang.String fooBar(); 	  Code: 	   0:	aload_0 	   1:	invokespecial	#15; Method bar:()Ljava/lang/String; 	   4:	areturn  	public Foo2(java.lang.String); 	  Code: 	   0:	aload_0 	   1:	aload_1 	   2:	putfield	#10; Field bar:Ljava/lang/String; 	   5:	aload_0 	   6:	invokespecial	#20; Method java/lang/Object."<init>":()V 	   9:	return  	}
Is there any risk of confusion by using the term `MessageDispatcher`, since that is something already used in `akka.dispatch.MessageDispatcher`?
might add a comment that this is not actually returned
we have a convention to use square brackets around variables in log and exception messages      log.error(e, "Tried .... [{}] ...", remoteAddress)
same here, and possibly on other places
Where did the `isSelfAddress` method go? Shouldn't those repetitive checks be extracted?
I'm confused. Shouldn't this show the diff with master. Looks like something is missing. https:github.com/akka/akka/blob/master/akka-remote/src/main/scala/akka/remote/RemoteActorRefProvider.scala#L196  Make sure that the changes I did here are not lost: https:github.com/akka/akka/commit/49500ab
addresses.head is scary, since that is a `Set`. We should have something like `defaultAddress` or `primaryAddress` which corresponds to the transport configured first in the config list.
type should be `immutable.Seq`, or `Map`
I prefer    filter { case (t, _) => t.isResponsibleFor(remote) }
This was posted before that was pushed. I need to merge it.
This is not used by the new remoting, it is there to make the old   NettyRemoteTransport happy (it inherits this stuff). I have a completely   separated serialization service.
please add a comment what exactly is wrong with this address
could be done with `Deadline`
Perhaps `try transport.shutdown() catch {`
why the @BeanProperty ?
just FYI (i.e. no need to change): `expectMsgType[InboundAssociation].handle`
consider adding descriptive messages to all these
ah, sorry, wrong `awaitCond`. Hmm, consider adding the possibility to adding descriptive messages to the TestKit.awaitCond API then ;-)
Perhaps mention what a PDU is? protocol data unit?
please always use `testActor`, which is more obvious
Don't know yet, as in FIXME?
if AkkaProtocolSettings would assume to get an appropriately selected sub-config (i.e. `conf.getConfig("akka.remoting")` in the normal case), that would make configuration even more flexible, right?
when using one parameter per line it's preferred to start the first one on a new line
should the return type be `FailureDetector`?
Someone commented that I should go with plain Long timestamps :)
 > why the @BeanProperty ?  This is how it is done in the old RemoteLifecycleEvent classes. I just   followed that. I suspect this is for Java compat.
why are you sleeping? if waiting for some event, that should be explicit, if not it needs a comment
Yes, OSI terminology also used by IEEE. I will add comment to explain.
some `{ }`  could go away
Strange, that should not be there. Thanks!
Closed is for we dont have an underlying transport yet while WaitActivity is for we are waiting for activity on the underlying transport. Incoming connections logically start out in the second stage, the first stage happens unbeknownst to us.
ah, okay. That would have made a nice FIXME comment ;-)
Inconsistent naming `Ssl` vs. `SSL`
there is no need for the `|` and `stripMargin`
I also thought about this, but we are dispatching messages in several places and several distinct ways, all entitled to receive the one appropriate name 
no, should simply be removed, making the whole strategy a one-liner
Ok, I started out with a state-chart which is in the draft doc, and   followed that. It made sense for me to have one initial state for both   cases. But I can change if you like.
ok, someone commented that I should use `Deadline` :) see `reconnectionDeadline` in `netty.Client`
I see, but I don't think there is a good reason for it. The other properties are not BeanProperty. Can't be very important to have `getCause` for this API.
remove the use of addresses.head and instead create a method called defaultAddress and use that
we agreed with Roland to keep the volatile var
perhaps we should mention that my dream blew up in the face of error handling, which would be less nice when using lazy vals.
The path is not always available at this point?
My plan was to only return the ref if the test passes, and a descriptive error message if it doesn't. I think this fulfills both.
Ok, that was not clear to me. It fulfills both :)
after the first await, should not the others be there already?
not necessarily when using fine grained events, the clusterView will be updated by published fine grained events  some of these conditions could probably be written as you suggests, but I think it's more robust to assert all in the same way and don't make to much assumptions about the how the clusterView gets updated
this answers my question: the test is not overly cautious, maybe there should be a comment explaining this (because it might somehow pass by chance anyway if making the follow-up tests strict)
comment explaining why
Why are specific scalac and javac options required here? How does the user know which ones he/she needs to have?
Oh, they're not. I'll just remove the whole section.
Why change to JVM? the tasks are `multi-jvm`
All other places in the text use that capitalization when talking about multi-JVM stuff. This was the odd one out. The examples clearly show `multi-jvm:whatever`.
I prefer `to[immutable.Seq]`, even though you get the same thing. The intention is more readable. This is the only place with `to[Vector]` that I could find.
Great, this is exactly the type of feedback I want (i.e. why I used different techniques in the PR)
Would you prefer immutable.Seq(...) over Vector(...) here too?
No, this reads fine. Using `List` or `Vector` as arguments is more readable. It's only in the case where there is an explicit conversion, that I think it needs to be overstated.
that's bad, so one must use `immutable.IndexedSeq.empty` to get the "right" `IndexedSeq`
Yeah :/ Want me to switch to immutable.IndexedSeq.empty?
pro/cons for `to[InexedSeq]` vs toIndexedSeq` ?
Note that immutable.Seq() returns List, not Vector.  Welcome to Scala version 2.10.0-RC1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_10-ea). Type in expressions to have them evaluated. Type :help for more information.  scala> collection.immutable.Seq() res0: scala.collection.immutable.Seq[Nothing] = List()
I think toIndexedSeq can be optimized to return "this" to[IndexedSeq] doesn't. OTOH I think it's a code smell with all these toX methods... So for performance we might want to use toX but with a TODO to switch to to[] when it is smarter
But please note that toIndexedSeq is NOT the immutable.IndexedSeq
Well, it can be, but not always
traversable toIndexedSeq is implemented as:        def toIndexedSeq: immutable.IndexedSeq[A] = to[immutable.IndexedSeq] 
Same with scala.collection.Seq() then
do we want to do this api change? array is pretty convenient way to define the classes
    import static java.util.Arrays.asList;     asList(Foo.class, Bar.class);
I understand that this way of writing out explicit `immutable.Seq` makes it clear, but it's not very concise, an alternative would be that we always have `import scala.collection.immutable.Seq` in all files where `Seq` is used. Starting to feel the need for scalastyle
How would you know if you were using scala.collection.Seq (which is autoimported by Predef) and immutable.Seq?
that's a costly thing to do, the byte array will become a Vector
but it wasn't really optimized before either 
Yup. I think it should be a ticket for another version to switch to ByteString. I don't see how we'll have time to change this atm
Sure?       $ scala     scala> IndexedSeq.empty     res0: IndexedSeq[Nothing] = Vector()
 type IndexedSeq[+A] = scala.collection.IndexedSeq[A]  So you get scala.collection.IndexedSeq, which isn't immutable :/
we would make import scala.collection.immutable.Seq part of our copyright header :-) we must watch out for `Seq` anyway I'm seriously thinking how this could be auto-checked. If not by scalastyle, with regexp, perl or whatever, part of the build. File containing `Seq` must always contain import scala.collection.immutable.Seq
ok, great that you created the ticket
exactly, see and believe:      scala> val i: immutable.IndexedSeq[String] = IndexedSeq.empty[String]     <console>:30: error: type mismatch;     found   : IndexedSeq[String]     required: scala.collection.immutable.IndexedSeq[String]        val i: immutable.IndexedSeq[String] = IndexedSeq.empty[String]
I think the appropriate thing would be to disallow Predef.Seq :-)
yeah, can we have our own scala fork -- I think there is a company that provide such things :-)
No, I mean, in the style enforcer :-)
Such a tool would be awesome.
Isn't it `Util.immutableSeq`?
Can you please motivate this api change? The only reason I can think of is to make it possible for high performance java implementation of a router to return a real `immutable.Seq` instance, which is rather inconvenient an unlikely to happen, so there will be a conversion anyway, and then I don't understand why we don't do `Util.immutableSeq` for them.
The whole point of Routers is that normal actor performance does not suffice. In that case it will be beneficial to make the API user do the conversion instead of hiding it, so that the implications are completely clear.
Damn these non-compiled documentations
and then that should perhaps be mentioned in the above doc, and not only refer to the conversion
roleS && configS
Entire forcomp can be replaced with      val c = configs.reduceLeft(_ withFallback _)     _nodeConf ++= roles map { _ -> c }
would be nicer to make this `extends AnyVal`
why this dance? just `extends TestKit(ActorSystem("TimerBasedThrottlerSpec"))`
no need to say `remaining` here, thats implied
same applies below
how about also testing `SetTarget(None)` with/without queued messages?
I've copied that from somewhere, but makes no sense here indeed.
Hm, I think not: After sending 1-3, we want to wait a full second. Without the `expectNoMsg(remaining)`, we'd expect 4-6 too early.
Always good idea and indeed: found a bug. Two more tests are in now.
What I meant is that removing `remaining` does not change the behavior:      def expectNoMsg() { expectNoMsg_internal(remaining) }  (from TestKit.scala)
Ah, this way, thanks.  I removed the argument in the tests, see https:github.com/hbf/akka/commit/fb237b859e886aa651294303d9665065222a8bc2
I'm always avoiding `self` for the self type of actors, because Actor has `val self: ActorRef`. It might not be a problem.
I'm afraid this will not work on the slow jenkins machine with `timefactor = 5`. The within durations are dilated but the durations passed to the throttler is not. Shouldn't all durations be dilated. Perhaps it also needs to be taggedAs TimingTest.
Its not problematic here, since this `self` name is only visible within the `Throttler` trait, but I agree that it should be changed to `this: Actor` as a matter of style.
Ah, I didn't know this.  I added dilation to the throttler rates and tested it by (locally and temporarily) setting `timefactor =  5.0` in `akka/akka-testkit/src/main/resources/reference.conf`. With this, the tests still pass.  See https:github.com/hbf/akka/commit/aeb4e471415b2f757d4065cc5baed8a940e78cc4
Fixed in https:github.com/hbf/akka/commit/aeb4e471415b2f757d4065cc5baed8a940e78cc4
No code in rst doc please, put it in a scala file and import it.
No code in rst doc please, put it in a scala file and import it.
What implication will this have for the user, i.e. what are you trying to convey here?
Cool, great work Piotr! Docs is always the hardest thing to write!
unnecessary imports from java.lang
They are for the test
I mean java.lang._ is always in scope, isn't it?
The IDE appears to differ on that :-)
default argument with Seq.empty and remove the accept overload?
What are valid values for the parameters?
Might want to link to the JDK docs for the settings
What are valid values for the parameters? Might want to link to the JDK docs for the settings
What are valid values for the parameters? Might want to link to the JDK docs for the settings
what happens if linger is negative or 0?
what happens if negative or 0?
What are valid values, and where can I read more about it?
same here as with accept()
same here as with accept
import channel.socket  Then use "socket. "
Great stuff, after these last tweaks I think it's ready to go in.  Have you've signed the cla? (Will be needed before I can merge it in: http:www.typesafe.com/contribute/cla)  Cheers, 
It looked like the java docs didn't specify this stuff, but I just realized if I click on the api they jump to a fuller definition. I'll link to that.  Turns out the apis can also throw errors. Any ideas for the cleanest way to deal with that? Should we fail the operation and make a new error case class to send back to the owner? (eg for Accept, Closed isn't quite the right response because it might be possible to Accept again with different set of options)
Since you linked the ticket to this pull request, do you want me to stop recreating the pull request so it's a clean commit and instead add additional commits on top of it?
Yeah, just add additional commits.
Nah if we want to signal to the sender:  sender ! Status.Failure(exception) throw exception
Depends on how we want the failure to be handled
Any difference from a performance (obj allocation) perspective to define those msg classes here or in a ConcurrentSocketActor companion object?
params collect {case p: PollDispatcher => context.system.dispatchers.lookup(p.name)}
to be very sure of intialization order but I suppose it doesn't really *need* to be lazy
thank you I'll do that in the other methods too,  totally forgot about collect
perhaps place 100.millis in reference.conf
I think you can use pollTimeout > Duration.Zero
should this transformation be done on each notifyListener call, or can it be placed in val guess you can use collect here also
Might be advantage to use watch instead? isTerminated always returns false for remote refs, but we have more reliable remote death watch. What do you say Viktor?
should not be necessary, since it is already defined in fallback reference.conf
I couldn't see anything that was order dependent in there, but I might be wrong. Remove lazy if not needed.
a duration of zero is supported in zeromq though, it will then block forever
sorry, forget that suggestion, I read the code wrong poller.poll(pollTimeout.toMicros) > 0  On Thu, Jan 19, 2012 at 12:57 PM, Ivan Porto Carrero < reply@reply.github.com > wrote:  > > +  private def pollAndReceiveFrames() { > > +    if (currentPoll.isEmpty) currentPoll = newEventLoop > > +  } > > + > > +  private lazy val eventLoopDispatcher = { > > +    val fromConfig = params.find(_.isInstanceOf[PollDispatcher]) map { > > +      option => > context.system.dispatchers.lookup(option.asInstanceOf[PollDispatcher].name) > > +    } > > +    fromConfig getOrElse context.system.dispatcher > > +  } > > + > > +  private lazy val pollTimeout = { > > +    val fromConfig = params find (_.isInstanceOf[PollTimeoutDuration]) > map (_.asInstanceOf[PollTimeoutDuration].duration) > > +    fromConfig getOrElse 100.millis > > +  } > > + > > a duration of zero is supported in zeromq though, it will then block > forever > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/225/files#r366719 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
Remove this line
Put the code in a compiled file as the other docs, this makes it way cheaper to maintain.
Very nice work
nice and clean
Is this user API? If so, ScalaDoc, and what does the Java API look like?
Specify return type
specify returns type
specify returns type, extra cred if braces are dropped :-)
same as above
You know it
Might be polite to 'slain what it stands for, I'm guessing "sendHighWaterMark"
Drop the companion object, doing getBytes w/o specifying character encoding makes the program platform dependent.
scaladoc'ed now  You made me write my very first lines of java code ever.
Still code in the rst files.
And no Java-version of the rst file
Needs to be: def lookup(): ZeroMQExtension this, otherwise Java won't work
I think you really need to write Java example code for the Java documentation of it, then you really see what code Java compiles with and what it doesn't.
I have all intentions of doing that but that's not something I can do in 20 or 30 minutes so I won't be able to do that today. I have never written java or a java unit test so I imagine some cursing and lots of cigarettes will be involved 
Hehe, alright mate, what do you think of the following:  I merge this into master, we create another ticket for the next milestone, and I could also possibly help you out with the Java tests & examples. Deal?
Is this something users have to do all the time when using TMap? Could it be incorporated in JavaAPI.newMap?
Highly un-java-package name. should be .stm.java ?
How is current txn determined?
A is much better than T
It's not a package, but an import static from class JavaAPI  /Patrik  19 jan 2012 kl. 16:01 skrev viktorklang<reply@reply.github.com>:  >> @@ -7,15 +7,11 @@ >> #class >> import akka.actor.*; >> import akka.transactor.*; >> -import scala.concurrent.stm.*; >> +import scala.concurrent.stm.Ref; >> +import static scala.concurrent.stm.JavaAPI.*; >  > Highly un-java-package name. should be .stm.java ? >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/223/files#r367159
If you're supposed to import it, it essentially a package :-)
I can agree with that JavaAPI is maybe not a good class name. Some may prefer to use it without import static.  Suggestion scala.concurrent.stm.japi.Stm  Stm.newRef  /Patrik  19 jan 2012 kl. 16:13 skrev viktorklang<reply@reply.github.com>:  >> @@ -7,15 +7,11 @@ >> #class >> import akka.actor.*; >> import akka.transactor.*; >> -import scala.concurrent.stm.*; >> +import scala.concurrent.stm.Ref; >> +import static scala.concurrent.stm.JavaAPI.*; >  > If you're supposed to import it, it essentially a package :-) >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/223/files#r367197
Yes, I've also thought of combining in a newMap method. I guess it will be the common thing to do... works well in the tests anyway.
Txn is looked up dynamically with Ref.View (thread local).
Yes, nothing can be better than the first letter of the alphabet :)
Okay, I'll change it to japi.Stm, which is similar to what I had in the transactor package before, and was used without static import.
Is 4 enough for the transactor tests?
I guess so, since core-pool-size-factor = 2 and we have 2 processor on jenkins, right?
Merge away :-)
Hmm, I seem to remember hitting a deadlock with 4 threads, now that you remind me. I just dont know which exact test it was, I think it might have to do with CallingThreadDispatcher, and the minimum required parallelism was 5.
I kinda like this approach, though I suggest the name "whenBecoming"
Very clean and "non-intrusive" addition, very nice work Havoc!
`become()` does not really mix all that well with inherited behavior hierarchies: who is responsible for `become`/`unbecome`, how does a thunk higher up in the hierarchy change itself (i.e. `handler` in the example above), how can you do it so that it does not result in a spaghetti mess?  That being said, I also like the look of this, and `mapBehavior` is more precise than `whenBecoming`, because it also applies to the initial behavior. Though we might want to use a word which does not have different spelling depending on which side of the ocean you live. `mapReceive` anyone?
make ref a val as well since you use it in a method.
shouldn't you test for >= 1
Should this be invokable by users?
I don't wan't to make it visible (accessible) for custom routers, that is why it's not a val RoutedActorRef is akka private.
If they write a custom router or resizer it can be used by users, yes.
0 is covered by the line above I'll change the first to  case (x, Nil) if x <= 0 => throw 
FIXME on this line to avoid the cast?
Some(deploy.copy(routing = new RemoteRouterConfig(deploy.routing, nodes)))
Shouldn't this be a string?
Why not just: case _
it is. quotes are optional in the new config format
that would have the same meaning, but it is a specific check for if routees (xs) was defined, and I wanted the case to highlight that
If you want to enable users to write their own RouteeProvider (to do fancy stuff), then we need to drop these restrictions.
I removed the ref: RoutedActorRef, it wasn't needed
`getOrElse(null)` can be written as `orNull`
now there's an easy performance win here: just turn it around to make `def child(name)=Option(getChild(name))`
Shouldn't getChild be only in UntypedActorContext? It's purpose is for java api.
ActorCell must implement both, and ActorContext does not expose the Java variant, AFAICS
ok, now I see it
I don't get the performance optimization comment. How I would rewrite the `getChild` method so it returns null for both `None` and `Some(ChildNameReserved)` in a nice way without basically doing the same thing that I do right now in `child`?
I think his idea was to move current child code to getChild and return s.child or null
I guess so, but what should I use then? If I flatMap I get back an `Option`.  I'm running out of ideas here, how do I do that filtering/transformation to the Option without ending up with an Option as the product? 
I'd just use a pattern match instead
Aha, you are not true to Scala ;) "A less-idiomatic way to use Option values is via pattern matching"
why not use Future.failed?
Id use `future either delayed`. I also dont particularly like that method name, but somehow the more natural `||` didnt make it into the stdlib.
unless Im wrong, `either` would also work from Java and make it a bit shorter.
well, you tell me why all other failed in this file are written like Promise.failed(...).future Shall I change all to Future.failed?
yes, it works! changed. thx
I changed to Future.failed here but leave the others for ticket #2314. There might be a good reason.
the reason was that Future.failed was only introduced after Viktor and I had discovered that it was missing, and given the relative development speeds this resulted in what you see now :-)
Yes, this is better. I originally thought that if you configured a dispatcher you had already set your boundaries. But since Netty gladly maxes out that pool by using those weird defaults the settings should be used here as well.
The Kitteh is right. Superflous brace here or missing brace down below. This doesn't compile. ;)
why not apply the same niceness as above?
A very good idea!
was it something else we also redirected to deadletters?
Are you sure you want to completely remove this? I know users still on 1.x that want to upgrade to 2.x. 
I think we should point them to http:doc.akka.io/docs/akka/2.0.3/project/migration-guide-1.3.x-2.0.x.html 
Sure we can do that. But then I think we should add it to the top of the file. So they can find it there themselves. 
Just call the page migration-guides. And have links to the different ones. One in this doc (current) and the other one to the old one.
good point, will do
By doing this we have to make sure that we never delete these old docs. Can we ensure that? 
I think that is what we need to do for supported versions anyway.
Can't grab these from code?
the docs are also in the distributions
Too long for PDF. Check all other snippets as well. 
You just refer to 'deadletters' as a noun. Link to what it means or explain inline.
no, that doesn't work if you in 2.2 want to include 2.1 migration guide it will break also, it's not possible to include the 2.0 snippets in this one
Re-add this page. See comment above. 
Right, you would have to duplicate it. No good. Leave it. 
this does not actually use `ec`, should probably add some combinator to the example
or change the comment that `ec` no longer needed for Future construction
that was also changed for failed remote sends
If I didn't miss anything, then the `getInstance` pattern should hold for all singletons, including Kill, GetRoutees, and some others I'm currently forgetting. Differences between docs and code will then have to be settled by changing the code ;-)
fixed migration-guides.rst is back, and 1.3 -> 2.0 links to 2.0 docs
This implementation detail should most definitely not leak into user API.
To have PIDs (the uids) exposed to users is not something we should add without much careful thought.
This has implications performance-wise. One could argue that removeChild should have an optional uid-parameter to do conditional removes to mitigate the cost of check-then-act style solutions.
I agree with Viktor
okay, I'll make it `private[akka]` then
no, this should be a Timeout or Duration, as everything else
also, why haven't we specified type on all these? boy scout!
ah, wasn't that because of the special optimization that is done when type is not specified for final val. would be awesome with a comment about. I forget it all the time.
yes, it was done like this because someone thought these might then be constants, but of course they cannot be anyway (after all they are not known at compile time), so I'm not sure what to do with them. To me their types are obvious enough, it would disturb the visual impression to repeat them.
but we have agreed on using types on public api, makes it harder to break the api for example, the type of Duration.apply is FiniteDuration, but we might want Duration if it is for an optimization we can leave it out, otherwise we should stick to our decision to use types on public api
good, motivates the usage of ReentrantLock over plain synchronized also
The setting in the cluster with the same name is a Duration. Is there a reason to specify it as a FiniteDuration here?
yes, that contains strictly more information. I'd like to use FiniteDuration more in method signatures which expect finite durations 
Then maybe it should be a FiniteDuration in the cluster for symmetrical reasons.
um, but isn't it better to use the most abstract type needed I have not seen any need to use FiniteDuration. We don't return Vector just because it's an Vector, we prefer to return IndexedSeq.
The reasoning behind FiniteDuration is that you can then statically be sure that .toMillis will not barf, which is a piece of information which I would like to keep. Unfortunately we have used plain Duration in places where it is not so good, leading to runtime errors for infinite timeouts, for example, but that does not mean that we cannot fix it. In order to do this in a nice way, producers as well as consumers of Duration values need to be changed.
exactly, we haven't used FiniteDuration api consistently anyway, the important review comment was that it shouldn't be a Long
yes, and that has been fixed. I'm just compile-testing a change of all Duration settings I found which are in reality FiniteDurations. Let's start at the producer side, then we can with less pain tighten the consumers in 2.2 (or something).
Should FiniteDuration not be documented if it surfaces user APIs? 
yes, I guess it should. but it is in the standard library now meaning that it would take a bit longer to do, and it would not show up in M7 since that is fixed already.
Perhaps just a link to the std lib ScalaDoc? 
that's what I meant: it is not documented in the stdlib right now, I'll have to submit a pull req to change that.  But I just now rediscovered common/duration.rst which I'll fix.
That was the doc page I was talking about. 
in that case it is done
This explanation tells the reader nothing about what this is useful for.
This explanation tells the reader nothing about what this is useful for.
I'd suggest:      val xcgPattern = if (oneWay) ExchangePattern.InOnly else ExchangePattern.InOut     produce(transformOutgoingMessage(msg), xchPattern)
Might want to clarify which headers override which.
It's not immediately obvious what this means
you want more DSL syntax in-between?      transform {       case x => ...     } using {       case s: State => ...     }
yeah, otherwise it'll trip people up
Shouldn't this be in some other reference.conf? It's not part of the official akka-remote user API right?
well, I just needed a token to send around, and protobuf does not really have this feature, so I thought a string is extensible at the same time (i.e. containing what it was that is done now, should that become necessary)
okay, will put all of this into a new sub-project
Why don't we just reuse the same config option to use a dispatcher for all of the remoting?
Why a nested object? (binary compat issues?)  Needs docs
Should include what happens if there's an exception thrown
dropped the executor?
I left that change for another day.
Yes, but I dont see any compat issues: since it is the implementation, it should not be overridable, instead users should override `createPipeline`. Then they may call `defaultStack`, so we are bound to its signature, but thats the end of it (correct me if Im wrong).
of course that reasoning required some sprinkling around of `private` and friends.
not needed anymore since the `executionHandler` comes from the outside.
Should this be public?
Is this public API?
why not "var initialPatricipants: Int"?
 because I dont know which kind of pants Patricia wore initially?!?  Ah, now I get it. Yes, will change.
TODO or ticket?
default case here?
Needs a metric tonne of docs
public API? seems like an utility method that should go somewhere?
Alright, finished review. Docs (ScalaDoc etc) is needed :-)
made the ConductorHandler `private[akka]`
Hmm, lets see whether we need that; Ill keep the TODO
does it matter which exception is thrown?
deferred. Ive added some brief explanation in ScalaDoc, real docs will have to come later once we fixed the whole API.
moved into encoders/decoders
Is it by intention that you don't call super.preRestart? The default behavior is lost.
Raymond is working on it as we speak. Also he will replace postStop with DeathWatch and try not to override any lifecycle hooks.
Make this a def
Might want to include some more debug info about which actorref didn't manage to activate within how long.
two spaces here
Just name it "val system" and drop the def system below
What happens if template.start() fails?
if context.stop fails, then template isn't stopped
Why is this here?
Yeah, we played with it with Raymond and didn't come back to clean it up
This reminds me our conversation about tryAll - did you think about how can we solve shutdown of multiple services in elegant way without tryAll?
try context.stop() finally template.stop()
the initial reasons were: - for testing, but maybe there is a neater way - if someone wanted to initialise camel outside of akka CamelExtension and share it between systems, but then we changed the relationship between system and camel to 1-1 so overrides would not make sense any more...
Alright, remove it
Good point. Well it would be in a good tone to cleanup after ourselves. Just need to figure out a clever way of doing it without nested try-catches (just kidding :) )
Where is this stopped?
Honestly? This swallows first exception if the second fails.
Atleast it attempts to stop both
Replace with:  val ref = actorSystem.actorFor(path.actorPath) if (ref.isTerminated) None else Some(ref)
Put a space before the brace on every such occurrence please
Is this idempotent? (It will be done on every restart=
This should most likely be a config option
This is most likely a bad default
I don't like the name "BlockingOrNot", and it's confusing to have a method named "blocking" that can return a NonBlocking
I'd suggest just using a Duration here, if it's <= 0 it's non-blocking
Then you can drop this
Should include more debug info, which consumer etc.  Perhaps good to have a base-class for CamelExtensionExceptions
This ain't right
Mutable state in messagE? Uncool
Confusing method name
for((k,v) <- m.headers) cm.getHeaders.put(k,v.asInstanceOf[AnyRef])
why the space here?
why not just protected? And why a val and nota def?
Is the context threadsafe?
make this:  receive = failedToActivate(cause)
def failedToActivate(cause: Throwable) ...  sender ! EndpointFailedToActivate(ref, cause)   etc
Why not use DeathWatch to remove refs? WeakHashMaps have performance implications.
Just use if
Together with the actor system. The camel lifecycle is bound to the actor system lifecycle so there is no need to stop it.  You can not stop camel extension (from outside of the package) other then by stopping the actor system.
Return type please.
What happens if there's no ":" in the uri?
Should be a ticket, and should most likely go into the config section for the Camel Extension, right?
Will do. Is the sbt plugin not going to do it for us anyway? I assumed so, and I didn't pay that much attention to formatting.
Yes it is.
Shouldn't this be private[camel] as well?
make this:  private[camel] sealed trait ActivationMessage { def actor: ActorRef }
is there a reason why ActorPath isn't good enough? It starts with akka: no?
new ActorEndpoint(uri, this, Path.fromCamelPath(remaining), camel)
I don't like the name too :) Give me a constructive suggestion please:) I didn't want to use the Option here and be explicit. 
Why do you need camel: Camel here when ActorComponent already has a Camel?
Well, this stuff is inherited and there is another ticket for fixing the docs.
What kind of config do you mean? The idea was you can override the method, so it is "configurable" as the rest of the stuff (blocking, autoack, etc.)
Agreed, shall I change it to 10/20 seconds? Also: 1. I don't like the name - suggestion please :) 2. I was considering using the actorTimeout here, but probably it would be bad  to overload the meaning of this setting
Open a ticket
d parens to the method
C++ like, yak... I'd rather use explicit stuff rather then magic constants... Also can you imagine all these ifs in TestableProducer ? Now it is:  blocking match {    case NonBlocking => do x     case Blocking(timeout) => do y with timeout }  It would surely loose readability.
rename "outCapable" to "isProducer"?
Still don't like this one
If this is intended for tests it should be in another jar.
This code above smells weird
We could look into it. I haven't had time to look at it yet. 
The method signature is completely unrelated to the body of the method
All side-effecting methods should have parens
I can rename it to CommunicationStyle so it doesn't get people confused
new is not needed here
method name completely unrelated to the body of the method
use pattern match instead
Is this public API
Is this public API?
Thanks for pointing it out.  Again, lets address all the comments in separate ticket.  Btw. what's the Typesafe's policy on Copyright section and @author tags? Personally I'd save myself adding this stuff to every file.
Why is the test writing to stdout?
Why is the test writing to stdout?
I only modified Message so it compiles in 2.x - ticket please:) And btw. it was using singleton before - so now it is much better :)
Braces not needed
Is Raymond getting this comments as well? Producer is his domain.  I spoken to him and what you see right now is a port of 1.x "as is" so it compiles under 2.x and he is working on this issues so they work the same way as with consumers.
Why change this?
Where does this come from?
it was moved "as is" 
Does any of the code after this even compile? I'd say, no?
snt't simply "getSender()" work?
depend on testkit and use that
skype me please - it's better to talk this trough Or look into IdempotentConsumerPublisher to see how they work together.
Copyright section goes at top of every file
We use DeathWatch to automatically unregister endpoints. Then the EndpointDeactivated is sent to the stream, so ActivationTracker can notify those awaiting activation only after the unregistration.
No need, just do as the comments that follow say
Human brain doesn't deal with negation well. And ! operator is easy to miss. Unless is natural (I admit might be more natural to native speakers)
copied "as is" but it will return "" see:  scala> "fwef" take "fwef".indexOf(":") res2: String = "" 
as a good practice, or there is another reason?
could be. I think most of your comments will become tickets, am I right?  Yes, I also think that CamelExtension config would be a better place for this.
yep  Also, I think I don't want AwaitActivation/Deactivation to subclass it.
I forgot... :)
There is - you can not easily  create ActorPath from a string unless you want to go system.actorOf(string).path route but I'd rather avoid that.  Also I've moved some behaviour scattered somewhere else to be nicely encapsulated here. The new version of this class looks like that:  private[camel] case class ActorEndpointPath private(actorPath: String) {   require(actorPath != null)   require(actorPath.length() > 0)   def toCamelPath =  "actor:path:%s" format actorPath }  private[camel] object ActorEndpointPath{   def apply(actorRef: ActorRef) = new ActorEndpointPath(actorRef.path.toString)    /**    * Expects path in a format: path:%s    */   def fromCamelPath(camelPath : String) =  camelPath match {     case id if id startsWith "path:"   => new ActorEndpointPath(id substring 5)     case _ => throw new IllegalArgumentException("Invalid path: [%s] - should be path:<actorPath>" format camelPath)   } } 
sure, could do If you looked up the history this method wasn't that short originally :)
Because ActorProducer needs it?
Be careful here - producer/consumer thing depends on the point of sitting. Also outCapable is the name camel give's it so it would make things difficult if we gave it another name.
didn't get this comment? Can you please rephrase?
What if I rename it to CommunicationStyle?  See response to your other comment.
Really? It reads well here.
nop. It's only used for automatic deserialization of endpoint inside of camel. Also I have rewritten it in the meantime:  object DurationTypeConverter extends CamelTypeConverter {   def convertTo[T](`type`: Class[T], value: AnyRef) = {     require(value.toString.endsWith(" nanos"))     Duration.fromNanos(value.toString.dropRight(6).toLong).asInstanceOf[T]   }    def toString(duration: Duration) = duration.toNanos +" nanos" } 
consider migration package as a temporary junk - no need to review it
to be cool:)
Consider on-hold as on hold. This classes haven't been migrated yet and might never get. We just borrow ideas from them.  The changes there come from 1.3 RC5 and we should ask whoever made them why :)
already moved to on-hold in my fork :)
Why why? preRestart needs it
test kit doesn't buy me much here.  I happily use it in ActorProducerTest though:)  I could use something like SharedCamelSystem trait here, but not shared.
should be 1 second instead of 1 millis 
Don't know if it does
Like something you put in your configuration file
Well, you _did_ change the wording...
Create a ticket
The message shouldn't need a CamelContext, remove it.
That's fine, but we employ the BoyScout Rule
I hope he is :-)
So why is it a WeakHashMap if you already use the DeathWatch?
I get the point, I just don't think it's worth it since it creates more line noise and introduces overhead in terms of allocations and calls.
Return type of methods :-)
Yeah, open tickets.  I also think that the config should be in the Extension.
Why not system.actorFor(string).path?
As I said, comp.camel should do just fine, no need for an extra camel here.
Oh, sorry, meant: Add parens to the method, i.e. "def createProducer() ..."
Very good point, makes sense with the distinction here. Perhaps something like "replies" or "responds"?
Examples should be general guidance, if this is not the recommended way of creating children, then we shouldn t have it in the examples :-)
If noone is referencing actorRef (assuming there is only one actorRef instance pointing to an actor) then there is no need to keep the state machine for the actor. Remember that someone might want to wait for (de)activation long after the actor is dead so until there is a reference to an actor we need to keep the ActivationStateMachine for it.  Obviously it would only work with local actors, but we should only support local actors in camel module and if someone wants remote actor they should add a local proxy actor.
If there are no references to it, it means that it's been terminated, if it has been terminated then DeathWAtch has been triggered. No need for WeakHashMap
Should I add that work here or wait for this to pulled in first in akka, and create a pull request from there?
@piotrga this reminds me of something ;-)
We need to hookup the akka-camel build to the AkkaBuild stuff, I saw there where formatSettings there
Well, I didn't! :-)  But now I'm looking at them. Just again a question. Can the code of Producer be accepted as is so I do a pull request on akka repo, or do I fix it in piotr's repo?   The context is a val, so I think so.    val context = {     val ctx = new DefaultCamelContext     ctx.setName(actorSystem.name);     ctx.setStreamCaching(true)     ctx.addComponent("actor", new ActorComponent(this))     ctx.getTypeConverterRegistry.addTypeConverter(classOf[BlockingOrNot], classOf[String], BlockingOrNotTypeConverter)     ctx.getTypeConverterRegistry.addTypeConverter(classOf[Duration], classOf[String], DurationTypeConverter)     ctx   } 
Ray proposed try-finally but I didn't like it :)
Viktor, I was thinking along the same line, just using the 'akka:' as the indicator for the 'akka actor camel component'. In the docs it refers to clustered actors (tbd) as cluster:, which would then not work well. the actor: part would take that problem away, but I for one would rather see 'akka:' as the 'camel component protocol', but then you would have to change the 'cluster:' part to be consistent here?
code 'on-hold' should not be in the pull request
its not in the src/main/scala or src/main/test path, so it will not even be compiled. But it should not be part of the pull request.
You are right! :) I remember now. Application is user application, so in this case auto-acknowledged is system-acknowledged so the comment would not make sense (unless system-acknowledged meant acknowledged by user application which would be weird).  Yes this needs clarification. 
Yes, but how? It needs it for conversions...  At some point I added factory methods for message to CamelExtension so messages could only be created from camel instance which should be available from every actor. I dropped it - can't remember why?  I can reinstate it. Good idea?
code 'on-hold' should not be in the pull request
Maybe a good idea to go back to the way Martin did it in the beginning (the context was not part of the messages before), I think he used some implicits etc?
And those factories, indeed.
automatic formatting perhaps? or my fingers in auto-pilot mode :)
Let's say we use normal map and remove STM after the actor terminates. What do we do if someone is awaiting activation? case 1 - actor hasn't been started and registered yet case 2 - actor has been terminated and we removed the STM from our map  In both cases we have no information about activation. In case 1 we have to wait; on the other hand, in case 2 we have to respond with EndpointActivated message.   What if someone is awaiting deactivation. In case 2, if we remove the STM from the map after termination, we can not determine wether the actor hasn't been deactivated, or simply hasn't been activated yet.  What say You?
What if, instead of the line noise we look at it as a form of abstraction.  We like clear abstractions, don't we?  Scala gives us powerful abstractions like higher order functions for collections - no-one says it's a line noise :)
Re allocations and calls - @inline baby! :)
Ok - I take it. Forgive my ignorance - what's wrong with starting children that way? Is this violating the rule of not sending behaviour to an actor?
wrong - the changes are probably some auto-formatting thing in IntelliJ
and here it should be 100 millis - we don't want to make test slow
He was using camel context singleton in the beginning so this is a no-go.
I think Viktor was asking if getEndpoint(...) is thread-safe. Also is it idempotent, and fast enough so restarts can be quick?
object Tst extends App {   val ITERATIONS = 1000 * 1000 * 1000   var i = ITERATIONS   var v = 0     i = ITERATIONS; v = 0   println("If took "+time{     while(i>0){       if (! (i % 2 == 0)) v = v +1       i-=1     }    })    i = ITERATIONS; v = 0   println("Unless took "+time{     while(i>0){       unless(i % 2 == 0){v = v +1}       i-=1     }   })    i = ITERATIONS; v = 0   println("Unless2 took "+time{     while(i>0){       unless2(i % 2 == 0){v = v +1}       i-=1     }   })     i = ITERATIONS; v = 0   println("If took "+time{     while(i>0){       if (! (i % 2 == 0)) v = v +1       i-=1     }    })      def unless[A](condition: Boolean)(block  : => A) = if (!condition) block   @inline def unless2[A](condition: Boolean)(block  : => A) = if (!condition) block    def time[A](block : => A) : (Long, Long) ={     System.gc()     Thread.sleep(1000)     val memory = Runtime.getRuntime.freeMemory()     val start = System.currentTimeMillis()     block     val duration = System.currentTimeMillis() - start     (duration, (memory - Runtime.getRuntime.freeMemory())/1024)   }   }  [Full GC 13919K->539K(63424K), 0.0175747 secs] If took (1272,324) [Full GC 1528K->518K(63424K), 0.0134099 secs] Unless took (1290,384) [Full GC 1227K->517K(63424K), 0.0134961 secs] Unless2 took (1272,384) [Full GC 1566K->523K(63424K), 0.0139020 secs] If took (1250,0
Yes, it can be optomized, I veto it though, since you're replacing a single-character pillarstone of logic programming and replacing it with 6 characters.
looks nice would have been even nicer without the double [Bar], but I guess that is the problem you mentioned
If I drop it I get: [error] {file:/Users/viktorklang/Documents/workspace/akka/akka/}akka-actor-tests/test:compile: java.lang.Error: trying to do lub/glb of typevar ?T  Not sure I can fix it, we probably need a type mage for that.
you will have merge conflicts here ;-)
perhaps an import of akka.actor.TypedActor.TypedActor
I think the style guide say that parameters like this should start on new line case class TypedProps[T <: AnyRef] protected[akka] (   interfaces: Seq[Class[_]],
why would one want to use .withTimeout(None) when default is None?
why would one want to use .withLoader(None) when default is None?
def f(t: TypedProps) = t.withTimeout(None).withInterface(classOf[KickAss])
looks a bit strange to match here, since it is plain if-else
I think that is a not very common usage and for that you can do t.copy(timeout = None) We don't have that for other things, like dispatcher. I think it only pollutes the api.
I like to have resetters, also copy doesn't work for Java.
not a big thing. What about  props.withoutTimeout or props.withDefaultTimeout  Now you need to methods for java/scala anyway.
It looks like it 'that' fails before 'this' succeeds the promise will be completed with the failure. Is this intentional, or should register take both futures as args (instead of hardcoding 'that')?
Nice catch! :-)
is it intentional that if `that` also fails, `p` is never completed? (I as a user would not expect that kind of behavior)
What would you expect? The failure of the first, or of the second?
Doesn't system.actorOf(Props.empty) work better?
Use same approach for line above
Is this user API? If yes, then it needs to be document, if it isn't, that needs to be documented.
works the same, but will change to the cleaner variant
you mean `(Option(dispatcher) getOrElse this).getClass`? Sure. 
use case is already documented below, will add more here. 
No, I meant doing it like the if-check
dead comment. FIXME, TICKET or REMOVE
feels a bit inconsistent in the naming, one with akka, one without. Rename sourceThread to akkaSourceThread or use use sourceThread and source.
I might be wrong, but doesn't this mean that if I do Logging(system, "myown.logging.category") the logClass will be java.lang.String which will be used for creating the slf4j logger  I think that we shouldn't use logClass: Class[_] Instead it should be a String, so that "myown.logging.category" can be passed all the way to the event listener
doh: forgot to re-enable, only commented out to ensure that we are not using it ourselves (because I wanted all logging to include system name if possible).
hmm. leaning towards the former. But `sourceThread` sounds more akka-independent than our completely self-baked `logSource` mechanism.
Yes, it means that if you supply an `ActorSystem`, you get it appended to the string. If you dont want that, supply a `LoggingBus` (i.e. `system.eventStream`). Will make docs more explicit in this regard.
yes, it can be good to prefix them to avoid name clashes, because in the end it's just an Map and applications may define same in their own logging infra  but on the other hand since we "force" people to use sourceThread instead of ordinary %thread in their appender config they might want to adjust their own logging infra to use MDC.sourceThread as well, if they don't want to include both %thread and sourceThread in the appender config  doesn't really matter as long as we have consistency
so you mean Logging(system.eventStream, "myown.logging.category") will end up with SLFLoggerFactory.getLogger("myown.logging.category") then I'm pretty happy, but it feels a bit confusing to sometimes use Logging(system, this) and sometimes Logging(system.eventStream, "myown.logging.category")  but I have no other suggestion than documenting it then  
weeell, I actually liked the more concise formulation better. If this ever becomes a hot path, well have other problems ;-)
I agree with Roland, this is in a Spec
Okay, the argument that sourceThread is forced on them tips the scales towards keeping everything as is and writing more docs about it.
I think I prefer: import org.jboss.netty.akka.util.{ Timer, TimerTask, HashedWheelTimer, Timeout  HWTimeout }
I think the message should be delivered, but the task shouldn't be rescheduled, otherwise there's a semantic difference between schedule once.
Would it make sense with:  new TimerTask with ContinuousScheduling with Runnable {   def run = f   def run(t: HWTimeout) {     dispatcher execute this     scheduleNext(timeout, delay, continuousCancellable)   } }
replace "isCancelled" with "cancelled || delegate.isCancelled()"
I totally agree, just copied the code as it was' but I thought of exactly this.   On Jan 13, 2012, at 20:28, viktorklang<reply@reply.github.com> wrote:  >> -  def scheduleOnce(delay: Duration, receiver: ActorRef, message: Any): Cancellable = >> -    new DefaultCancellable(hashedWheelTimer.newTimeout(createSingleTask(receiver, message), delay)) >> - >> -  def scheduleOnce(delay: Duration)(f:  Unit): Cancellable = >> -    new DefaultCancellable(hashedWheelTimer.newTimeout(createSingleTask(f), delay)) >> - >> -  private def createSingleTask(runnable: Runnable): TimerTask = >> -    new TimerTask() { >> -      def run(timeout: org.jboss.netty.akka.util.Timeout) { dispatcher.execute(runnable) } >> +  def schedule(initialDelay: Duration, delay: Duration, receiver: ActorRef, message: Any): Cancellable = { >> +    val continuousCancellable = new ContinuousCancellable >> +    val task = new TimerTask with ContinuousScheduling { >> +      def run(timeout: HWTimeout) { >> +         Check if the receiver is still alive and kicking before sending it a message and reschedule the task >> +        if (!receiver.isTerminated) { >> +          receiver ! message >  > I think the message should be delivered, but the task shouldn't be rescheduled, otherwise there's a semantic difference between schedule once. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/214/files#r351951
I guess so, looks more clean, thx   On Jan 13, 2012, at 20:31, viktorklang<reply@reply.github.com> wrote:  >>  >> -  private def createSingleTask(receiver: ActorRef, message: Any): TimerTask = >> -    new TimerTask { >> -      def run(timeout: org.jboss.netty.akka.util.Timeout) { >> -        receiver ! message >> +  def schedule(initialDelay: Duration, delay: Duration)(f:  Unit): Cancellable = { >> +    val continuousCancellable = new ContinuousCancellable >> +    val task = new TimerTask with ContinuousScheduling { >> +      def run(timeout: HWTimeout) { >> +        dispatcher.execute(new Runnable { def run = f }) >  > Would it make sense with: >  > new TimerTask with ContinuousScheduling with Runnable { >  def run = f >  def run(t: HWTimeout) { >    dispatcher execute this >    scheduleNext(timeout, delay, continuousCancellable) >  } > } >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/214/files#r351964
Why?  On Jan 13, 2012, at 20:35, viktorklang<reply@reply.github.com> wrote:  >> -class DefaultCancellable(val timeout: org.jboss.netty.akka.util.Timeout) extends Cancellable { >> +/** >> + * Wrapper of a [[org.jboss.netty.akka.util.Timeout]] that delegates all >> + * methods. Needed to be able to cancel continuous tasks, >> + * since they create new Timeout for each tick. >> + */ >> +private[akka] class ContinuousCancellable extends Cancellable { >> +  private var delegate: HWTimeout = _ >> +  private var cancelled = false >> + >> +  private[akka] def init(initialTimeout: HWTimeout): Unit = synchronized { >> +    delegate = initialTimeout >> +  } >> + >> +  private[akka] def swap(newTimeout: HWTimeout): Unit = synchronized { >> +    val wasCancelled = isCancelled >  > replace "isCancelled" with "cancelled || delegate.isCancelled()" >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/214/files#r351980
It's either that or rewrite it as lockless :-)
imports at top of file is normally something I just wan't the IDE to take care of and Eclipse doesn't do this kind of multi-import, but I have changed it for you
second thought, there is no need for atomic update of the 2 fields, changed to plain volatile
I'd prefer either:  case X => Y  or having an empty line between each case statement
I'd advice not to use a Symbol, go with a String or a case object.
I'd advice not to use a Symbol, go with a String or a case object.
I'd advice not to use a Symbol, go with a String or a case object.
I'd do this in a finally block since socket.close might throw up
Strongly prefer:  def sendBytes(bytes: Seq[Byte], flags: Int): Unit = socket.send(bytes.toArray, flags)
This looks fishy, I'd rather have:  private def pollAndReceiveFrames(): Unit = if (currentPoll.isEmpty) currentPoll = newEventLoop
You should definitely not capture context in the closure.
Users might want to have this configurable.
I don't think you need to check that, you're entering a race.
Big no-no writing to mutable shared state.
Drop the extra braces please (true for all of the same cases below)
Why is this commented out?
Shouldnt this be configurable?
Error message should include which versions are supported.
Otherwise very much like "Computer says no"
this is just the timeout for the reply of creating the socket.
but I can make it configurable
it seemed neat to make it a system actor but it doesn't need to be
People using unsupported internal API is a huge failure for me personally, if you don't need it, please just use normal actorOf. Thanks :-)
This should not be added to reference.conf of akka-actor. akka-zeromq should have its own akka-zeromq/src/main/resources/reference.conf It will be merged with the system.settings.config automatically. 
Use your own log instance for this, so that the logSource is correct, not the system. val log = Logging(context.system, this)
what happens during a restart? any need to unregister the previous socket? 
use context.system.dispatcher instead (same thing)
I think it needs to be unregistered and the way it's written now, you wouldn't be able to survive a restart.  The socket options need to move from being messages received to be part of the parameters instead I think. Because I wouldn be able to restart the socket but I wouldn't know whether I should bind it or whether it should publish.  Also all the configuration would be lost as they are all sent through messages to the actor. right? 
I think an explicit return type would make it easier for readers
Would it be a better idea if this method returned Props instead of an actor ref? That way the user is in control of the path to the socket actor
Maybe, or pass the name as parameter.  so this is a public api. then I'm in doubt that pollDispatcher should be a Option[Dispatcher] I think that should be a Option[String] and you do the system.dispatchers.lookup here to align with other api, such as Props.withDispatcher
You don't need to do AcceptWithOptions, just instead of having the empty Seq as a default-parameter, create a new constructor that has 2 parameters and that delegates the empty seq to the 3-param constructor
I started down that path but migration manager flagged that things like copy() and unapply would no longer be compatible. I don't think any client code should be doing that, but who knows. Should I go ahead and ignore those?  Thanks sRp  On May 8, 2012 3:28 PM, "viktorklang" < reply@reply.github.com> wrote: > > >        val socket = SocketHandle(socketOwner, ioManager) > > -      ioManager ! Accept(socket, this) > > +      ioManager ! AcceptWithOptions(socket, this, options) > > You don't need to do AcceptWithOptions, just instead of having the empty Seq as a default-parameter, create a new constructor that has 2 parameters and that delegates the empty seq to the 3-param constructor > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/438/files#r790997
No, I meant.  case class Accept(orig, orig)(val options) {   def this (orig, orig) = this(orig, orig)(Seq.empty) }  Doesn't that work?
I tried the following:    case class Accept(socket: SocketHandle, server: ServerHandle)(val options: Seq[SocketOption]) extends IOMessage {     def this(socket: SocketHandle, server: ServerHandle) = this(socket, server)(Seq.empty)   }  But the compiler fails with:   [info] Compiling 1 Scala source to /home/srp/git/akka/akka-actor/target/classes... [error] /home/srp/git/akka/akka-actor/src/main/scala/akka/actor/IO.scala:254: ambiguous reference to overloaded definition, [error] both constructor Accept in class Accept of type (socket: akka.actor.IO.SocketHandle, server: akka.actor.IO.ServerHandle)akka.actor.IO.Accept [error] and  constructor Accept in class Accept of type (socket: akka.actor.IO.SocketHandle, server: akka.actor.IO.ServerHandle)(options: Seq[akka.actor.IO.SocketOption])akka.actor.IO.Accept [error] match argument types (akka.actor.IO.SocketHandle,akka.actor.IO.ServerHandle) [error]     def this(socket: SocketHandle, server: ServerHandle) = this(socket, server)(Seq.empty) [error]                                                            ^ [error] /home/srp/git/akka/akka-actor/src/main/scala/akka/actor/IO.scala:253: ambiguous reference to overloaded definition, [error] both constructor Accept in class Accept of type (socket: akka.actor.IO.SocketHandle, server: akka.actor.IO.ServerHandle)akka.actor.IO.Accept [error] and  constructor Accept in class Accept of type (socket: akka.actor.IO.SocketHandle, server: akka.actor.IO.ServerHandle)(options: Seq[akka.actor.IO.SocketOption])akka.actor.IO.Accept [error] match argument types (akka.actor.IO.SocketHandle,akka.actor.IO.ServerHandle) [error]   case class Accept(socket: SocketHandle, server: ServerHandle)(val options: Seq[SocketOption]) extends IOMessage { 
Wow, what a shame. Then I suggest we push this feature in to 2.1 instead of 2.0.2 since we can't make it non-kludgy while retaining binary compat.
This worked (no complaints from migration manager either), not sure how clean it'd be considered:     case class Accept(orig, orig) {       var options = Seq.empty       def withOptions(opts: Seq[..]) = {          options = opts          this       }    }
A good effort, but a mutable case class would be very confusing.
Oh you're right, passing a mutable message is a really bad idea. I'll go ahead and amend the patch for 2.1 instead of 2.0.2
It should not be 2.2? And SNAPSHOT is volatile :)
This is for 2.1.x to notify people that use akka.tcp that they shouldn't. And 2.1-SNAPSHOT will be replaced by the correct version by the release script.
why is `matches` better than `==` ?
It isn't. Some bad habits pop up now and then. Will change.
do we need this?
remove this comment
I don't think we should define the node, because we want different node names when running multi-jvm tests. I don't know what is best, to use host:port which will generate many, many nodes, or let multi-jvm set this for each jvm, node1, node2, node3 ...
I dont know, I just followed the atmos instructions.
yes, indeed, that was an oversight (I used it to find out whether this file was being read at all since in the beginning things were not working)
the docs say that at least I should define the node; how is it supposed to be done?
ah, I hadnt noticed that this slipped in; is it okay or shall I move this part to a different PR? In theory it should not matter much.
in tests we use our own TestEventListener so this will not be used, but it's confusing to define it here
it's not needed for atmos tracing initself
hmm, I thought this would be merged with the TestEventListener, need to improve my HOCON-fu; but if it is not needed then it should of course go
It's supposed to define a "logical" name of the node for a production system, so that you can move the logical node to another host:port and still compare with historical metrics. That is not something we need for our tests. I think that if not defined it will use host:port as node name, which might be good enough for us, but that will mean that many nodes will be shown in the console, since we use dynamic ports.  cc @h3nk3
no, string lists are not merged (how would you then override)
Shouldn't it be the other way around, you _enable_ it when you want to use it?
No, because we always want to run all tests with it enabled etc. Just because I verified that the AST does not change does not mean that I trust it without running the tests at all times. Id also prefer not having to remember to switch it on when building a release.
I will agree with you if we run scaladoc everytime we compile as well.
running ScalaDoc has nothing to do with the class files we generate; running genjavadoc does
I would prefer to rename the property `genjavadoc.enabled` and check for `false` 
removing double-negation is a good suggestion, will do
No shutdown anymore?
No, there can never be any replacement. We check for existence a couple of lines above.
Good call :-)
Ummm, this won't work as advertised wrt remoting. Also, an ActorRef can exist as an entity long after the actor is dead.
I have created a ticket for this: https:www.assembla.com/spaces/akka/tickets/2744 We can release 2.1 without fixing it.
Great job, thanks Patrik
Why lazy val and why are we caching the configurator?
    trait SupervisorStrategyConfigurator extends Creator[SuvervisorStrategy] ?
besides a coincidence wrt. method name, what does this buy us?
yeah, I was keeping the spirit of over-engineering, will remove
good catch, thx
harm/gain? doesnt do much 
It prevents us from having to worry about people extending them.
Having a common parent for all of our XConfigurators would be desirable, right?
Why would that be worrisome? They cant change the meaning of FQCN akka.actor.DefaultSupervisorStrategy, can they?
Should probably mention that it's a OneForOneStrategy. What number of restarts should it be configured with?
No, but if we change the implementation we need to worry about name clashes etc. There is no reason to allow subclassing, so we shouldn't offer it. make it final
okay, got it.
That is not obvious to me ATM. What would we use it for? Also, we have lots of Configurators and FactoryProviders, and none of them extend Creator[], which is only used for actors anyway. When would you want to abstract over the thing which is created?
WDYM: there are now restarts. (one-for-one was added)
 * @param maxNrOfRetries the number of times an actor is allowed to be restarted, negative value means no limit  * @param withinTimeRange duration of the time window for maxNrOfRetries, Duration.Inf means no window  * @param decider mapping from Throwable to [[akka.actor.SupervisorStrategy.Directive]], you can also use a  *   `Seq` of Throwables which maps the given Throwables to restarts, otherwise escalates.  */ case class OneForOneStrategy(maxNrOfRetries: Int = -1, withinTimeRange: Duration = Duration.Inf)  Since you use defaults you're saying unlimited restarts, but that's not true, since actors are either stopped immediately or escalated (which in it's turn stops the children, right?)
I just got the feeling that we were sort of reinventing Spring.
no, because in that case we would abstract over the things we create ;-)
You mean like a CreatorProvider? ;-)
This is the opposite of Spring. In Spring you have abstraction over abstraction over abstraction. A class has a base class that has a base class that has a base class that has a base class that has a base class that has a base class that has a base class that has a base class that has a base class..und so weiter...
And the reason it dies from the Kill is due to the escalate? Is it documented that Escalate on the /user will kill everything?
kitten died here
yes, see https:github.com/akka/akka/pull/623/files#L2R86
ouch, debug residue
It would definitely be logical to pass uid into initChild
Store away a boolean for this check inside the actor?      val isSystem = context.self.path.name == "system"
What about Fatals?
This closes over outer.rootGuardianStrategy is this intentional?
or Long, if that was intended
no, initChild is used from two places: one which cares about uid and one which does not
or better, pass it into the constructor. which I'll do
Int it will be
Fatals are not caught and thus do not fail actors.
This message seems a bit odd, also, if we get multiple Failure(e: AskTimeoutException) we schedule multiple joinSeedNodeTimeouts
the AskTimeout comes from ScatterGatherFirstCompletedRouter will that send multiple Failures?
case Failure(e: AskTimeoutException) is a very generic type of message and I'd recommend wrapping it in something more domain specific if possible.
Or simply check the sender if it's the router?
alright, I'll try to break out this seed node joining to a child actor to make it more separated
If that will make the code more awesome then I'm all for it
should not the seedRouter shut itself down after InitJoin instead?
possible to use receiveTimeout instead?
I can easilly implement this without ScatterGatherFirstCompletedRouter. I tried to dogfood. Why is it bad? This is not performance critical in any way. If ScatterGatherFirstCompletedRouter can't be used for this I think we should remove or rewrite it.  /Patrik  14 aug 2012 kl. 18:27 skrev Viktor Klang () <notifications@github.com>:  > ScatterGatherFirstCompletedRouter
Never mind me, brainfarted. Nothing to see here, move along...
I appreciate your comments, and interpret them as the code is not clear enough, so I removed ScatterGatherFirstCompletedRouter from JoinSeedNodeProcess. Hope that is less confusing, more straightforward.
Why this temporary val? 
So we give up permanently after the seed node time out? 
no retry is performed, see L869 this is in done state, when we have received one ack, it lives for one idle period of ReceiveTimeout to collect other InitJoinAck (to not pollute deadletters)
I like the use of SGFCR. Good thinking. 
Interesting, wonder what that link does? I have not updated it on my machine.
I guessed the link was to let Sphinx know about the Akka documentation style, but maybe it's not even necessary. Do you have an Akka style defined in your system pygments style directory? Maybe now it picks it up from the local akka-docs pygments directory.  Certainly the old link is wrong since the _themes_ directory doesn't seem to exist anymore.  I'll remove my link and see if the docs still build. If they do then we can probably get rid of that line altogether.
Need for symlink to pygments style disappeared with 4cd2693340104d6d0bf5675e620924ab59a8afa9. I'll update the docs accordingly.
Are you cross-referencing this from the supervision and fault-tolerance sections? 
The Chicago Manual of Style says that title-casing shall be done according to importance of each word, and "level" is one of the more important ones in this title.
That is ugly anyway. Screw Chicago, the only good things that have ever come from Chicago is Michael Jordan and Chicago style pizza. 
This is sooo good. 
why the comma between 'shutdown' and 'too'? 
Awesome page. Exactly what I wanted. Good job. 
You also have to add a cross link from both:  * http:doc.akka.io/docs/akka/2.0.2/general/supervision.html and * http:doc.akka.io/docs/akka/2.0.2/scala/fault-tolerance.html and * http:doc.akka.io/docs/akka/2.0.2/java/fault-tolerance.html   And in this cross link also add that the prefixes/scopes (/user etc.) are explained there.
that's plain English, just like the comma before a final yet
"This is realized by having the system guardian watch the guardian"  Which one? The User Guardian? It is not completely clear for me.
would it make sense to remote this method altogether and call `shutdownClientConnection` instead (it's only used at one place)
if you remove this, then isBoundTo is not called anywhere else, are you sure you didn't break anything when removing this?
we should move the .foreach(_.shutdown()) to after the writeLock has been released.
I missed the `isBoundTo` cleanup. Should have looked better.   I can only find two places where we put things into the map. We use the same address that we just used to create the clients as key in the map, and `isBoundTo` just compare the addresses. This just seems like leftovers from some other implementation.
Maybe. I thought the symmetry of bindClient/unbindClient was nice.
Are you sure? Because it checks if the RemoteConnection's address is the same as the given address, it doesn't check the key in the map, hence the remoteAddress of the connection can vary from the key in the map, no?
My previous statement was meant to show that I couldn't find any way for the key and the RemoteConnections address to differ.  They are always the same as far as I can see.
Ok, if you are sure then you can of coruse remove it.
I think we normally do `= ()`
 an exception
didn't we just do that a nanosecond ago?
Not necessarily. We did that when we caught the InterruptedException, but we might have kept on sending any number of messages since then.
should we clear AND rethrow?
does the nullness matter?
we should use "remaining" instead of "timeout.duration"
Yes, that's the java idiom, "By convention, any method that exits by throwing an InterruptedException clears interrupt status when it does so". Will add better comment.
No, it's residue from the old code. will remove.
absolutely will change all cases in this test...
That's not how we roll, create an ADT
crap added that in for testing /debugging purposes for some messages I saw during the unit tests. Will change.
Why was this moved out here?
use testActor as sender and expectMsg instead of Await.result + ask
to make it idempotent. the registrar still has to get the register message   On Sat, Nov 24, 2012 at 7:27 PM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/main/scala/akka/camel/internal/CamelSupervisor.scala: > > >          parent ! AddWatch(producer) > >        } > > +      producerRegistrar forward msg > > Why was this moved out here? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/893/files#r2215496>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
OK. this test doesn't use TestKit yet, will change.
Ok, so it was just bitrot?
    List[ActorRef]() == Nil
Well it did come in handy, so I'll add some case objects.
Added ADT, gives better warnings, it did help me during the tests, so would be better to keep
This should be sealed
Name is too anonymous/generic
Doesn't this really imply that this Actor ought to be a FSM actor?
No, I mean, it's in the ...camel.internal package, and is private[camel], since it's for the ActivationTracker, put in the ActivationTracer companion and make it private[ActivationTracker]? The name "State"/"StateName" doesn't make sense to me if I just see it in ...camel.internal
Yeah you could to it as an FSM, it grew this way I guess. Rather leave it like this first, create a separate ticket if you want it done, I'll look into it later then.  On Sat, Nov 24, 2012 at 8:36 PM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/main/scala/akka/camel/internal/ActivationTracker.scala: > > > @@ -22,12 +23,13 @@ private[akka] final class ActivationTracker extends Actor with ActorLogging { > >      type State = PartialFunction[ActivationMessage, Unit] > > > >      var receive: State = notActivated() > > - > > +    var state: StateName = NotActivated > > Doesn't this really imply that this Actor ought to be a FSM actor? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/893/files#r2215597>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
If we aren't making this an FSM Actor now, I think we should also omit the entire "state" thing, I don't like partial/ad-hoc solutions. Do you agree?
Ok, let me take the names out, it was purely for logging.   On Sat, Nov 24, 2012 at 9:04 PM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/main/scala/akka/camel/internal/ActivationTracker.scala: > > > @@ -22,12 +23,13 @@ private[akka] final class ActivationTracker extends Actor with ActorLogging { > >      type State = PartialFunction[ActivationMessage, Unit] > > > >      var receive: State = notActivated() > > - > > +    var state: StateName = NotActivated > > If we aren't making this an FSM Actor now, I think we should also omit the > entire "state" thing, I don't like partial/ad-hoc solutions. Do you agree? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/893/files#r2215629>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
why not an implicit def?
No need for the extra braces
No need for extra braces
No need for extra braces
What happens if resetStreamCache is called by one Thread as another thread is reading the body? (i.e. the same CamelMessage is sent to 2 actors)
yes, that will not end well, body is `Any` and can be whatever mutable non-thread safe instance, so it's not much we can do about it. I would say that the recommendation should be that the user should convert the body to a real immutable instance in the first endpoint (actor) before passing it on to other actors.  This fix is still relevant, because you expect to be able to call getBodyAs several times (in same actor) with the same result (e.g. adding a debug log stmt should not break things). 
On Mon, Nov 26, 2012 at 8:09 AM, Patrik Nordwall <notifications@github.com>wrote:  > In akka-camel/src/main/scala/akka/camel/CamelMessage.scala: > > > @@ -108,7 +108,21 @@ case class CamelMessage(body: Any, headers: Map[String, Any]) { > >     * Java API > >     * > >     */ > > -  def getBodyAs[T](clazz: Class[T], camelContext: CamelContext): T = camelContext.getTypeConverter.mandatoryConvertTo[T](clazz, body) > > +  def getBodyAs[T](clazz: Class[T], camelContext: CamelContext): T = { > > +    val result = camelContext.getTypeConverter.mandatoryConvertTo[T](clazz, body) > > +     to be able to re-read a StreamCache we must "undo" the side effect by resetting the StreamCache > > +    resetStreamCache() > > yes, that will not end well, body is Any and can be whatever mutable > non-thread safe instance, so it's not much we can do about it. > I would say that the recommendation should be that the user should convert > the body to a real immutable instance in the first endpoint (actor) before > passing it on to other actors. > > That is definitely a best practice (and people start doing that quite naturally since the CamelMessage is hardly ever seen as part of the domain and the normal immutable messages are easier to work with).  > This fix is still relevant, because you expect to be able to call > getBodyAs several times (in same actor) with the same result (e.g. adding a > debug log stmt should not break things). > +1  >   > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/894/files#r2218705>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
you can drop such braces in matches
Id say `t: Some` instead
This class should go: anything not otherwise known is queued. This way you can use the throttler transparently in front of any other actor; otherwise the source of the messages would need to be adapted.
Sending messages will not throw (we changed that recently), so there is no need to be that careful here.
The queue size should also be bounded, otherwise were looking at OOM quite easily.
Hm, if you ment `case Event(SetTarget(t : Some), d) ` then this does not work...
Okay, simplified it accordingly.
So what happens when the queue is full? How does Akka handle queue length internally (I am asking so we get similar semantics...). For the time being, I have not changed this.
Fixed, good point.
We probably want to discuss this change, which is compatible with how it was, but perhaps not what we want later on? The problem with maintaining the collections with ActorRef is that the registered ref might not contain the uid (read actorFor).
more things that needs to be corrected in `addrssing.rst`, please read it
This is an interesting one: on a certain level your fix is wrong, because at that level the test was always wrong, since it was not that `child` which terminated but the previous incarnation. However, this points out the need for adding a warning to the docs, since the previous precise match will lead to `DeathPactException` after a restart (precisely due to the late arrival of the previous incarnations `Terminated` message).  Hmm. Not nice.
I guess this will be fixable by making `Terminated` a system message and processing it in the pre-restart incarnation of the parent; this also means that we will have to document that DeathWatch then is not suitable as a conversion end marker anymore.
with three fields we now have four bytes unused within ChildActorPath, so I think implementing the string length caching is 100% beneficial: put in a val which holds the offset of where this path element begins relative to the beginning of the paths string representation, then pre-allocate the StringBuilder with the right length and just paste the entries in (using `builder.replace`).
this needs a comment in `ActorCell.newUid` so that nobody gets crazy optimization ideas which then break hashing
strictly speaking the UID is not even necessary in the Supervise message anymore AFAICS
* once `actorFor` is removed you can only `watch` things you know for real, at which point this change must have been reverted * if we do not do this change, then we must remove a wildcard-`watched` upon reception of a `Terminated` even though it does not match by ActorRef * if the aforementioned strategy leaves a program broken, then my intuition tells me that it was broken before  This leads me conclusion that this change should not be done.
shooting from the hip Id think that in light of my suggestion above this should turn into also replace a wildcard-watched with a precise one if the opportunity presents itself
this should also only wild-card remove if the subject has undefinedUid
too tired to figure out the rest of this file right now, hence no comments
the rule of the boyscout :-)
this should have a FIXME, since the change will not be necessary anymore once having an ActorRef means having the UID
will do so tomorrow
Yes, I have been thinking about this as well, and I agree that it would be better to do it the other way. I will try it out tomorrow.
in case `ref ne null && <uid mismatch>` you can directly return Nobody
If it contains a uid, is it really a path any more? I think we should try to keep ActorPaths unbound to any particular ActorRef. A lookup method can be used to get the current ActorRef.
How do we handle collisions? My calculations (possibly wrong!) suggest a 1% collision rate for every 10K calls to nextInt, which I think is large enough to need consideration.
the collision needs to happen between the current incarnation and the previously obtained reference, i.e. two calls and not 10,000; but was this not discussed before?
And for the exact same path. And I think it was discussed at the office and not online.
OK, I didn't realise it was on the exact same path. I assumed the uid would now be used to key actors within the parent, but the actor name is still used.
Is this something we're going to fix?
yes: actorFor is going away for that precise reason
No, ActorRefs cannot be looked up, which is why actorFor is going away; ActorRef is bound to an instance/incarnation while ActorPath logically is just about the names. ActorPath has also a physical function in locating an ActorRefbut this function is completely internaland hence it carries the UIDs. In the mailing list discussion you linked to in another comment you make some good arguments why the UID should be stored together with the path: foremost it boils down to that ActorRefs dont have parents.
for the record: the discussion was at 4f258c7523c577e24568333a119d04c7c3e224cf
I'm not sure I understand what you suggest that I should do, but I have noted that I should describe something about "exact match" in the migration guide
I created https:www.assembla.com/spaces/ddEDvgVAKr3QrUeJe5aVNr/tickets/3156 to deal with this case; you dont need to do anything at this place specifically apart from maybe a FIXME pointing to that ticket, since when that is done this change should not be necessary anymore.
I'd rather have something like "asString" or "convertToString". "raw" doesn't say anything imo
What is the semantics of this method (docs)?
`raw` was inspired from `URI`, but I don't mind changing  `toRawString` and `toRawStringWithAddress` could be "moved" to `ActorRef`. We currently only use them for serialization, and therefore a name suggestion: `toSerializableRepresentation` or `toSerializationFormat`
+1 to ``toSerializationFormat``
2 times doesnt => do not
 i.e. the child after the restart will be a different incarnation than the child before the restart.
use incarnation here; that should also be added to the terminology section
2 times doesnt => do not
* there should be a note between the ActorRef and ActorPath sections clearly stating the difference in meaning (i.e. name vs. incarnation) * Reusing Actor Paths needs some love as well (i.e. its <path=>ref> will point to the dead letters)
Ah, the elusive forward slash on Windows.
what if the the path contains spaces?
thats why `scalacOptions` is a `Seq[String]`; it should just work, especially with [my fix to genjavadoc](https:github.com/typesafehub/genjavadoc/commit/007bee9624bb813b8e6f58fff86026247a13ddfc)
that sounds awesome, I was afraid it was string concatenation going on there as well: `"-P:genjavadoc:out=" + (t / "java")`
`allRoles.map(r => r -> latestGossip.roleLeader(r))(collection.breakOut)` should also do it
Wow. How does that work?
Yeah, I want to know too. :-) 
Explained here: http:stackoverflow.com/questions/1715681/scala-2-8-breakout
Nice! I might prefer to type that vs have roles be a String, pertaining to a firmer definition of a 'Role'
Assuming on leave - *roles
What do you mean?  15 mar 2013 kl. 18:34 skrev Helena Edelson <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/ClusterDaemon.scala:  > -    case ClusterUserAction.Down(address)  => downing(address) > -    case ClusterUserAction.Leave(address) => leaving(address) > -    case Exit(address)                    => exiting(address) > -    case Remove(address)                  => removing(address) > -    case SendGossipTo(address)            => gossipTo(address) > -    case msg: SubscriptionMessage         => publisher forward msg > +    case msg: GossipEnvelope                    => receiveGossip(msg) > +    case GossipTick                             => gossip() > +    case ReapUnreachableTick                    => reapUnreachableMembers() > +    case LeaderActionsTick                      => leaderActions() > +    case PublishStatsTick                       => publishInternalStats() > +    case InitJoin                               => initJoin() > +    case JoinTo(address)                        => join(address) > +    case ClusterUserAction.Join(address, roles) => joining(address, roles) > +    case ClusterUserAction.Down(address)        => downing(address) > +    case ClusterUserAction.Leave(address)       => leaving(address)  Assuming on leave - *roles  -- Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1254/files#r3395567> .
Curious how this would play out with clusters of clusters as a topology. Say one would like to partition cluster A, of clusters A,B,C,D as a singleton role  cluster with each node instance falling within a range of akka port usage to lock down all nodes of a particular role for security. Cluster B is deployed as a multi-role cluster, with 2 other roles. Cluster C with yet another role, another singleton role cluster. How would one manage this if roles are not dynamic but hard coded is what I am getting at. I'd love to see a start up strategy that is dynamic, typing roles with a few fields, but have the role class a configurable FQCN that one can easily, and probably would, extend with akka providing the base trait/impl.  I see a role as implicitly defining specific behavior vs just used for partitioning message routing. Will talk more to this on dev list.
I guess since the node implicitly shuts down on leaving the cluster vs an auto-join that is on/off for manually leaving the cluster but staying operational, means my comment is moot. Disregard.
Yes, I don't follow. Anyway, if the role of a member should be static (not changed after startup/join) or dynamic (possible to change without restart) is a question we are very interested in. We have discussed it. It's a lot easier to implement the static role assignment, but we are curious to know if there is a good real world use case for dynamic roles.  /Patrik  15 mar 2013 kl. 18:54 skrev Helena Edelson <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/ClusterDaemon.scala:  > -    case ClusterUserAction.Down(address)  => downing(address) > -    case ClusterUserAction.Leave(address) => leaving(address) > -    case Exit(address)                    => exiting(address) > -    case Remove(address)                  => removing(address) > -    case SendGossipTo(address)            => gossipTo(address) > -    case msg: SubscriptionMessage         => publisher forward msg > +    case msg: GossipEnvelope                    => receiveGossip(msg) > +    case GossipTick                             => gossip() > +    case ReapUnreachableTick                    => reapUnreachableMembers() > +    case LeaderActionsTick                      => leaderActions() > +    case PublishStatsTick                       => publishInternalStats() > +    case InitJoin                               => initJoin() > +    case JoinTo(address)                        => join(address) > +    case ClusterUserAction.Join(address, roles) => joining(address, roles) > +    case ClusterUserAction.Down(address)        => downing(address) > +    case ClusterUserAction.Leave(address)       => leaving(address)  I guess since the node implicitly shuts down on leaving the cluster vs an auto-join that is on/off for manually leaving the cluster but staying operational, means my comment is moot. Disregard.  -- Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1254/files#r3395995> .
Sorry, when I say Dynamic I refer to load time detection vs compile/build/deploy time, I should have been clearer.  Here is one real world use case:   As a _ I need to bootstrap a singleton or multi-role cluster     - and deploy with Akka MicroKernel for quick IT testing cycles locally   - with a servlet listener to deploy to tomcat in S3 to various dev/beta/prod etc environments  with env variables that the node doesn't detect until load-time begins.  * optional hard-coding in config files * optional dynamically detected from environment variables (pushed by chef in the cloud, vm's would have differing env vars)   I have code running in the cloud that does this strategy already. But having it standardized for bootstrapping roles via Akka would be cleaner and more integrated. In my cluster I leave the environment variable to look for configurable in the akka conf file.   The biggest use case here is deploys to servlet containers in the cloud, where for example the web module itself is a common sdk to bootstrap any app, depending on what is deployed and what roles are detected to deploy. 
@helena   "users can override the config with Java system properties, java -Dmyapp.foo.bar=10" - https:github.com/typesafehub/config
Thanks for clarifying. That falls into the static category (with mine definition). Note that config library supports env variables, se section 'Optional system or env variable overrides' in https:github.com/typesafehub/config  /Patrik  16 mar 2013 kl. 18:17 skrev Helena Edelson <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/ClusterDaemon.scala:  > -    case ClusterUserAction.Down(address)  => downing(address) > -    case ClusterUserAction.Leave(address) => leaving(address) > -    case Exit(address)                    => exiting(address) > -    case Remove(address)                  => removing(address) > -    case SendGossipTo(address)            => gossipTo(address) > -    case msg: SubscriptionMessage         => publisher forward msg > +    case msg: GossipEnvelope                    => receiveGossip(msg) > +    case GossipTick                             => gossip() > +    case ReapUnreachableTick                    => reapUnreachableMembers() > +    case LeaderActionsTick                      => leaderActions() > +    case PublishStatsTick                       => publishInternalStats() > +    case InitJoin                               => initJoin() > +    case JoinTo(address)                        => join(address) > +    case ClusterUserAction.Join(address, roles) => joining(address, roles) > +    case ClusterUserAction.Down(address)        => downing(address) > +    case ClusterUserAction.Leave(address)       => leaving(address)  Here is one that may fit:  As a _ I need to bootstrap a singleton or multi-role cluster     - and deploy with Akka MicroKernel for quick IT testing cycles locally     - with a servlet listener to deploy to tomcat in S3 to various    dev/beta/prod etc environments  without hard-coding in config files but rather via env vars pushed by chef to vms in the cloud. Dynamic env variables that the app doesn't have until load-time begins is the use case I refer to vs static pre-defined in config files. Sorry I should have been clearer.  I've written code for this already and could put up a pr to look at this week if there is interest.  -- Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1254/files#r3403512> .
Yes, I code functionality using Config and environment variables often, quite familiar, but thanks. I think the distinction I'm trying to make here is not hard-coding all settings into a war file ahead of time. So if, for instance, roles could first be detected by env variable, and if not found, then read from config, great. I do this too so I know it's a simple request :)
My point was that even if you hardcode "default" config into the war, it can always be overridden by sys props or env vars.
Id prefer calling this `leaderOf`
ff => if
this `getOrElse` looks wrong; Address or None?
`roleLeader: Map[String, Option[Address]]`  it might exist a role, but currently there is no member with that role
ah, I confused it with a different method of the same name then (on a different Class); maybe call this one `roleLeaderMap`?
ok, good suggestion, then I can add the method `roleLeader(role: String): Option[Address] which corresponds to the java equivalent `getRoleLeader`
ok, changed to breakOut
undefined or empty?
yes, "undefined or empty" is more correct, I guess I was thinking that the user would not specify it, and then it is empty by default
This file needs to get deleted somewhere, right?
Would it be possible to make this less specific to ScalaVersion and make it something like "dealias" or something that takes a key-value map for things to be replaces with something else
not as far as I can see: why?
It would be trivial to do so when the need arises: the mechanism is established. I cannot think of another usage right now, so Ill leave it to the next addition to figure out the best name then.
close in finally just joking ;-)
Given that the person realizes that this solution is just specific out of JIT-feature-add...
what we heard from James there are no actors in play 2.1, but Futures (and actor system)
and if you change above you might have to change `also`
I'll change both to built with Akka :wink: 
I'd probably collapse these two lines
Is Exception the most specific thing here?
I'd probably not do it with .failed and then Await.ready + value but:  val e = intercept[AkkaCamelException] { Await.result(producer.ask(message)(timeoutDuration)) } e.getMessage must be("failure") e.headers must be(Map(CamelMessage.MessageExchangeId -> "123"))
Yes, that's how the tests where written. I can change to a specific TestException. That's probably much more robust.
Boy Scouting :-)
You should switch to that approach for the rest of the tests that have the same structure as it is less code and less repetition.
Very nice catches!
Maybe I'm missing something, but I can't get that to work. You'll have to show me later. 
Do we need this separate construct? If we publish directly to system.eventStream then the integration opportunities are endless.
yes, that was my initial idea, but when looking at it I got scared of its specialization for logging (LoggingBus), but if you say that is ok then I'll change
I think we should try to use it, if we get any performance issues etc we should address them in the eventStream so we give extra power to everyone :-)
makes sense, I'll change
fixed, using system.eventStream instead, much better, thanks
shouldn't it unsubscribe itself when in postStop?
What is the intended semantics for MembershipGossipChanged?  if you call:  publishState() publishState() 
Possibly start the comparison by checking eq, then check size, then do the map's (since they allocate quite some objects)
well, I took the easy path first, to just publish current state, i.e. the whole Gossip. publishState() publishState() will result in two MembershipGossipChanged even though there is no changes.  I'll do something better. I think Gossip in itself is something internal, which we should not publish, instead I think we should publish things like (real changes): MembersChanged ConvergenceChanged
Yep. I wrote that up in the ticket. You didn't see that? The Gossip does not help the user much. We should do the diff and publish actual changes. 
Yes, I'll de-expose the Gossip, but I'm not sure it's worth it to go all the way to detailed change events, such as MemberJoined, MemberLeft I think the granularity of MembersChanged is good enough.  On Wed, Aug 15, 2012 at 9:17 AM, Jonas Bonr <notifications@github.com>wrote:  > In akka-cluster/src/main/scala/akka/cluster/ClusterDaemon.scala: > > >      if (PublishStateInterval == Duration.Zero) publishState() > > +    publishMembers(oldGossip.members) > > +  } > > + > > +  def publishState(): Unit = { > > +    eventStream publish MembershipGossipChanged(latestGossip) > > Yep. I wrote that up in the ticket. You didn't see that? The Gossip does > not help the user much. We should do the diff and publish actual changes. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/617/files#r1380314>. > >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
I'm not sure that the changes is enough. Problem is if someone starts listening just a ns after the initial MembersChanged, then it will never get the full picture?
fixed that optimization
We should make this more finegrained. Detect the diff and only publish that change, if any. 
You mean like:      MemberJoined(address: Address)     MemberUp(address: Address)     MemberDown(address: Address)     MemberBecameUnreachable(address: Address)  I'm not sure that adds much value, but if we think it is useful I can do that also.
I think it adds value.   1. Else the user needs to store away the old status for all nodes, do a full diff to understand the difference, then grab the changed nodes.  2. The user will only be notified if there is an actual change he cares about, i.e. only subscribe on the changes he care about  Why would this not be valuable?  And it should not be *also* but *instead*. 
Nice catch :)
Should this be ever serialized?
Isn't this racy?
Probably not. None of them should.
actually dissociate is fine
I can't see how you would ever escape the death-race with Disassociate. It's for testing.
Yes I know that you like long words :wink:. Need to reset my brain not to use dissociate.
Long words make you smart! ;)
 > actually dissociate is fine  But I use disassociate everywhere else. Shouldn't we stay consistent?
"is" is usually prefixed to boolean values?
Yes, we should stay consistent.
You're right. Will change to `joiningAddress`.
clusterView will not be updated with Joining, unless you refresh it. There is no MemberJoined event any more.
Thanks, it slipped my mind.
Some of them are sent. Will leave `SerialVersionUID` on all of them.
`currentlyJoining`, `joiningInProgress` ?
Yes, `joiningAddress` is ambiguous. Settled for `tryingToJoinWith`.
the ask pattern implementation does exactly this check as well, resulting in a failed promise; to make it water-tight would mean to have the PromiseActorRef watch the `throttlerActor`, which means not using ask but a custom variant of it
thinking about it some more, I dont get this change: it seems more proper to me to add a `recover` clause to deal with the `isTerminated` situation (if it somehow does not really matter) or to fix it for real as I suggest; this code looks more like a leaky band-aidbut I might be missing something (in which case it is lacking a comment)
So there are two things here.  One is that if we know that the actor is terminated, then we can just skip trying to set it , since the throttle settings will be picked up when a new connection attempt is made.  The second is the race between ask and and termination, which I think would be ok with a smaller timeout and a recover. We basically assume that if we haven't heard from him in X, he must be dead, and the settings will be picked up by the new connection attempt. 
Ah sorry, so the ask implementation does it for me, then I'll just use recover.
I just followed the existing codebut I don't think NonFatal is really needed here. Shall I leave it in or remove it?
I think you can remove it.
You might also use the TestTransport and its SwitchableLoggedBehavior to override the low level associate call and fail the test (associateBehavior allows a become() like operation).
Great idea, I'll try that out.
and the `reason` should not be included in the error log?
why is it not needed?
Isn't NonFatal(e: Blah) redundant if Blah is NonFatal already?
eclipse scalariform will reformat this line, because of the `;` (and I will be blamed), you can change it to ` { publish(e); stay() }`
The error logging here applies when publishing went wrong. Is it safe to   refer to the reason then?
ok, similar to that stack overflow when serializing Warning,  I thought of adding toString or message of it to the log message, and that should be reasonably safe, I think.
that is true, but don't remove the last `case NonFatal(e)`
Yes, I was unclear that I refer to the redundant pattern match. The last   line must stay.
Hmm, the only safe thing I can think of is logging its Class[_].
nice catch :-)
yeah, was a huge wtf moment for me
Why is this unaligned now?
please make a case class instead of using existentials
why? (meaning: we usually rely on automatic cleanup at the end of the test suite unless we have a reason to do it earlier)
`deadline.timeLeft` can theoretically become negative here, and Thread.sleep boom Wouldn't it be good enough to always sleep(step)?
was it intended to change the `* 2` waiting?
Why isn't MAX_WAIT_TIME defined as a Duration in the first place. MAX_WAIT_TIME naming is not scala style.
Yes, I don't want the backoff, because it wastes a crapload of time.
it wouldnt if it were properly bounded against the deadline (as you have now done); since this is for tests, exponential back-off versus straight polling does not seem to be a hot topic
You should probably add the unit of size (bytes) in the error message. Just to be sure :)
any exception from serialization should be treated the same, hence please wrap things which are not NotSerializationException in that exception before catching it again down-stream
you should also stop the actor
please also verify that the `reason` was `NotSerializableException`; similar below
this waits three seconds, one would probably be more than enough; better yet: send a marker message after the test and verify that you didnt get an error in between, which will save some time when running the test suite
the duration is already dilated in TestKit `expectNoMsg`
any chance we can include some info of what message type that was rejected?
I decided to reintroduce a separate exception because the error now occurs in both reading and writing, and I feel it happens at a layer below serialization (alternatively could use DeserializationException for reader, but it doesn't feel quite right).
Made nested class to access all the good stuff in EndpointWriter, especially publishError().
no, there must be another way, nesting actor classes is asking for trouble, e.g. accidentally accessing unsafe state in enclosing actor
oh, now I remember what that was which has been nagging me
save one allocation
Why not list the leader statuses and use filter instead? (avoiding inversions)
is there a reason for filterNot + forall and not forall xxx && yyy?
Yes, I'll redo and skip the double negation.
nope, will fix
You might have to re-word the boolean expr tho.
I would rename this to nonLeaderMemberStatus and then you can write nonLeaderMemberStatus(x) instead of nonLeaderMemberStatuses.contains(x)
you should be able to use that Set in `Gossip.leaderOf` as well
Yes, I am using that in `Gossip.leaderOf`.
remove the cast, please
Well, technically this only tests that the actor receives the messages in the same order in which it sends them. Inte s spnnande.
Why send the messages from inside the actor? Perhaps two different tests, one sending from the outside and one from the inside (from constructor like this).
ah, looked at the whole file, now I understand the purpose of it, sending from the inside is smart. I agree with Roland, the msgs should be shuffled first.
Is this intended to be purely non-deterministic (i.e. no seed)? I would recommend logging the seed then.
what is recv? do you mean `i`
hmm, any chance that this will fail with an infinite loop?
I see some inconsistency in the naming for setup and teardown. become and unbecome are calls to perform something while whenFailing and whenRestarted are callbacks setup and teardown should perhaps be named whenSetup and whenTeardown to indicate that they are also callbacks, or even better whenStart whenStop
Good point, Ill cap the sleep time by akka.actor.creation-timeout.
Also a good point, I think I prefer `whenStarting` and `whenStopping` to make it clear that the callback is executed before that overall action is finished.
Here the double param list becomes both very ugly and unintutive. Is this the best we can do? 
no, I was indeed able to remove it. 
    val echo = system.actorOf(Props(ctx => { case x => ctx.sender ! x }))  ;-)
Can we get rid of the sleeps and use latches instead?
yes, we can. of course. where was my mind?
That is wicked awesome. Didn't know about it. Is it in the docs? 
Yes, it's been there since 2.0, it's the shorthand syntax to create an Actor, can be dropped now when Roland has created the more rich DSL for it.
The wicked awesome about viktors proposal is that the tell operation would use the wrong sender reference, hence I think it should be removed on those grounds alone.
Oh, that's even more wicked awesome. 
IMHO it's just as wrong as tell(msg) uses the wrong sender reference. But I think it should be removed when the actor dsl is done. Also, there has been 0 complains regarding this and much more regarding the missing sender of tell...
well, missing sender is one thing, wrong sender another.
They'll be deadLetters in both cases, wdym?
I mean if you use that idiom within an actor, then that actor will be the sender, which would be quite unexpected, I guess.
Now, THAT is a different story. Looking forward to the PR :-)
Ignore my previous self, I must have written that in some euphoric dilirium: since `receive()` and `select()` block until its too late, this cannot be done without sleeping.
I liked your previous self better :(
I know, but what can I do?
Big fat deadlock warning?
don't run this test on systems with webmin started; OTOH no port is guaranteed to be free.
doesn't `foreach` look better than `map` for this?
and here also different _ usage than above
use scala style for constant, i.e. Address
yes, was just going to say that this should be a dynamic port
should be closed in finally (or afterAll)
Need to beat the Java/C/C++ remains out of you...
needs to return an Iteratee
foreach doesn't return an iteratee
while this is already something, wouldn't it be nice to `expectMsg("eof")` here?
Don't these messages have something in common?
Completely impossible to guess what this is for
Don't use default params AND aux ctor
So there's never any cause of invalidity associated?
Shouldn't this also be private[akka] and documented to be internal api?
Shouldn't this also be private[akka] and documented to be internal api?
Document as internal?
I think we ought to give better names to origCause and msg
Avoid the closure for msg.map(_.getClass)?
give better name to origCause?
Don't mix default param value with aux ctor
Does this imply that all Throwables are serializable?
lots of repetition here, can we improve on that?
why not used?
I think this is a slightly unorthodox method name which is way costlier than the name implies.
This has a tiny bit of smell to it
Shouldn't this have a big warning sign plastered over it?
Quite illogical to suspend childrenNotToSuspend
rename inResponseToFailure to causeOfFailure
what are you getting at?
Realized it was in a test. Would be nice if things in /test/ were sort of colored differently from /main/
not currently, no. same goes for ActorKilled and IllegalActorState. Im leaning towards removing those causes as well.
That is also not present on the (public for Java) constructor, and I think it would look weird to add it.
I just wish it was easy in Java to see that they shouldnt call it.
But on the other hand its not as if calling this constructor would immediately end the universe, either. Its more meant to give a hint that the origin of these exceptions is within Akka.
For `msg` Id propose `optLastMsg`, but what is wrong with `origCause`?
also here Id just remove the cause, since it was never used
how about "message" and "originalCause"?
I made it messageOption (as in headOption et al).
only those which were sent within `Failed` in the other direction already.
not without creating closures
because before were even started we know for a fact that we cant have failed
costly in which way? it is done precisely to avoid allocations
its that mossy smell in the forest when the sun shines through the rainbow onto the freshly fallen rain drops?
yes, it probably should
costlier in the sense of "compared against a load"
changed to `suspedChildren(exceptFor = skip ++ childrenNotToSuspend)`
the resume is not the cause of the failure, so Ill go with `causedByFailure`
you are pretty generous with that `flow` thing  (in this case its superfluous)
technically this also does not need a `flow` around it, because it does not contain a `Future#apply()`
yes, good one!
Have you tried this from Java code? I think there is a surprise when extending a `trait`. In all other places where we have `getInstance` we extend and abstract class (for this reason).
I'm curious to know what good the `final` does here. `object` can't be subclassed.
So what you're saying "Write a test for Java on the usage of this object"
great, then it was some other combination that breaks it
better to use watch and expectMsg (especially since isTerminated will go away)
`info` is not really used here, more than checking for null. Should we have provide `Serialization.hasTransportInformation: Boolean` or can we put everything into `serializedActorPath` ?
The description in the .rst docs must also be updated.
similar thing here, and I don't think we should leak `DynamicVariable` into public api
So we're already _leaking_ `DynamicVariable` in the `JavaSerializer`, even if we're subclassing it. Do you have a suggestion for how you want to hide it? 
ok, if it can't be removed we can add `def hasCurrentTransportInformation: Boolean = currentTransportInformation.value eq null`, but I was also thinking if serialization is always supposed to be done like this the check can also be implemented in `Serialization.serializedActorPath`      val identifier: String = Serialization.serializedActorPath(theActorRef)
Yes, we can definitely just have a boolean that people can check if they want to do something special for the _non normal_ case, since we do the same check in `Serialization.serializedActorPath`.
you could construct this path only once up-front (I only comment because you use two different methods for constructing it ;-) )
Does the last getOrElse statement make sense? If getExternalAddressFor() fails, that means that we cannot communicate with that system anyway.
I would say that it means that if we can't communicate with that system using _this_ protocol, then fall back to the default protocol for our system.
Hmm, ok, that might make sense :)
s/self/testActor/ to be less confusing
if this line would fail, the line before has already thrown a TimeoutException, or?
Yes, of course. Completely redundant.
Isn't ExtendedActorSystem enough?
no need for val
Why isn't "Serialization.serializedActorPath(actorRef)" hidden inside the SerializedActorRef ctor?
    case SerializationInformation(address, null | `originalSystem`) => path.toSerializationFormatWithAddress(address)     case SerializationInformation(address, _) =>       val provider = originalSystem.provider       path.toSerializationFormatWithAddress(provider.getExternalAddressFor(address).getOrElse(provider.getDefaultAddress))
Java Serialization is an ongoing nightmare
It should be.
But I just realized that it doesn't convey the same meaning. If the `originalSystem` is `null` I don't care about the matched value. 
I think we normally place the `INTERNAL API` on the first line
Don't we need to change this note also? `toSerializationFormatWithAddress` and `toSerializationFormat` is no longer in the doc sample. 
Great catch. `toSerializationFormatWithAddress` is in the scala example but not in the java one. Will fix. 
this PR was only about the rename, not finishing the UDP stuff; Ill think about whether that warrants its own ticket or not.
updFF? And it closes over "this"
No, these are messages between actors.
My understanding is that the doc is where it belongs right now; these classes are not `private[akka]`  (scratch head)  for some reason I cannot recall, but they are for sure not supposed to be well-documented public API. Ill add a clarifying warning at the top.
Doc above says rootPath "... not including any remote address information." then how is this supposed to work?
Ah, Ok, this is an implementation only for the local provider :)
update comment also
Can we solve all those `asInstanceOf[InternalActorRef]` in some more elegant way? Some implicit conversion that is only for internal use?
Since it seems to be used mainly for sending system message, I think it's good enough if we make an internal utility method
Maybe watcher should be of type InternalActoRef? Then no casting is needed.
    self.tell(Terminated(actor)(existenceConfirmed, addressTerminated), actor)
Good observation Endre!
match on type instead of extractor?
child: ActorRefWithCell with ActorRefScope ?
Dummy question, but what should it be? I have never used Java   serialization before :)
It can be just anything. Normally you start with `1L` for a new class, and then bump it when doing non-compatible changes
Yep, so then you can see how "evolved" the class is :)
Thanks! Me dummy :)
Im not sure about the second condition: by default `preRestart` unwatches the children, which should get rid of `Terminated` messages during restart; what this check means is that any watch notification will be dropped silently during restart, which is probably not expected.
this should be `addressTerminated = true`
this should not be done if the previous one was already done, its a duplicate
so Id prefer a match over `foreach`
this line should not be so empty
Thanks for noticing!!
I think we should create a ScalaCheck checker here so we have better coverage
What if one of the path elements is the empty string?
isn't this a correlationId?
Why is next "Any"?
This seems kludgy, can we fix this properly in the rootGuardian?
no, the request message has an message id and the correlation id is in the reply message, at least in the terminology I use: http:www.eaipatterns.com/CorrelationIdentifier.html
what else should it be? in the end it is the application message that is sent via the ActorSelection
Must be documented to be racy if called from the outside
Just match on type and not extractor?
yes, it is, but I don't see how it can be fixed in rootGuardian. Suggestions? It might be fixable as part of the optimization to not pass those messages at all: https:www.assembla.com/spaces/akka/tickets/3073
The user will ask "how"?
ops, thanks, fixed
What about having Option[ActorRef] in ActorIdentity instead indicating unknown-ness?
I'm sorry, but neither the name of the field nor the type of the field says anything about what it will contain.
Is it some kind of Either?
I like that!
it is a linked list
why first filterNot and then map, sounds like a collect with a breakOut into an Array to me
if this were public API then it might be `Either[SelectionPath, Any]`, but that is only so slightly more useful that I collapsed it to `Any` back then (also because of object allocation overhead, since these beasts are created per message send)
this could be renamed to `wrappedMessage` or similar
hmm, would it not be more idiomatic to keep around the actual `ActorRef` and compare to that one?
Shouldn't this be a failure? Is `initialize` called from anywhere else than just after creation of the actorRef?
Why are we merging the queues? This behavior is different from a normal cell. And why is the suspend count removed?
Nice! This reads so much better than `lock.lock try { ...` 
Shouldn't the null case be an exception? That should never ever happen.
Roland asked me to rethink that question, and yes the messages are of course handled correctly by the *real* actor later. Would have been nice with a comment explaining that when we transfer the messages.
this is not atomic with swapCell underlying might be null here, someone else calls initialize and both are creating new UnstartedCell, potentially overwriting cell that has already been started to be used
remove `val` for all those parameters except `uid`
I like this. Feels better to not reorder the messages.
actually Id like to leave those `val` in where the thing to the right will be a field anyway, which AFAICT leaves only `props` to be un-`val`-ed
(or what was the conclusion from the convention of `private[this] val` vs. leading underscore again?)
yes, it would indeed feel better, just one small problem:  * CallingThreadDispatcher * actor sends to itself from the constructor, say three messages * fails at the first message, which will be processed in here * but its Suspend message will only be dequeued after the other two have run  Solution: enqueue system messages which arise from this processing loop at the front of the queue (i.e. after all system messages at the real front).  Ugly.  I love-hate that dispatcher with a passion ;-)
`val` generates extra bytecode (not as efficient) as without the `val` `private[this] val` generates same thing as without `val``  I didn't hear any more comments regarding my objections to `private[this] val vs. leading underscore`.
Having `val` somewhere in there says this will always be a field, without it you will have to read the whole class body to find out. Adding the underscore (to `props` in this case) is lighter weight. Hmm.
but that's something the IDE should help you with (mark occurrences), all this just reminds me of a naming scheme using prefixes of different types of variables in java (m = member, p = parameter, s = static), so bad, wrong, those memories hurt -- anyway, as said earlier, I will adopt to whatever we decide perhaps easiest to discuss it in the same physical room, whole team, some day
Exactly that logic/reason should have been present in a comment for that code...
There is a huge comment right above this loop, and it explains it. Hence I refute your claim.
Then I argue that it's written in a suboptimal way since neither of you nor I touched this subject when we discussed rewriting it.
I submit that the reason was rather that neither of us bothered to actually read the comment. It might make sense to describe the solution I proposed (if you agree with it) in great detail, though. 
Yeah, I'll just add a boolean which will be protected by the lock, and when it's tru it'll just insert system messages after the last system message in the queue.
but it would be nice if we could create the Mailbox in a not-activated mode and then drain the queue and then flip the mailbox on.
It is called by the guy who creates the ActorRef, immediately after, this just makes sure that if he calls it twice things don't get messed up
see sendSystemMessage now
I know, but my point is that it doesn't always protect against that. I think you should throw exc as suggested by Bjrn.
Yeah, I have reconsidered, I think you guys are right. Lets do that.
Sounds like a perfect topic for the eng meetup ;-)
`isEmpty` and `hasNext` doesn't have side-effects, I hope :-)
The ticket states: In akka-mailboxes-common and akka-remote, the import for com.google.protobuf should have a version # e.g. ;version=[2.4.0,2.5.0)". You changed zeromq?
mailboxes-common depends on remote, I am no expert but that sounds like it should work for me? Zermomq also depends on protobuf.
But I might be wrong tho, so I'll add it explicitly to mailbox-commons as well
Isn't there a stray `"` in the line `10, TimeUnit.SECONDS)"`?
So if I'm not mistaken this should only be an issue with long version like `x.y.z-RCX`, and `cross CrossVersion.full` which is only needed for Scala Milestones and RCs. Doc for normal releases will look funky I think.
Why was this completely removed?
ok, I'll revert linebreaks in this and similar dependency lines
because it was a duplicate (its already in fault-tolerance-sample.rst)
well, that was done by Roland in the original fix for 2.0.4, and I can agree that the sample is too long to be included here. It's included in the linked `fault-tolerance-sample`
Ok, thanks for the clarification!
this might need some !!! or caps or  ;-)
This will improve the performance for any mailbox type used
Hey Chico,  I've added something similar to Akka 2.0  Cheers,   --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
Hey cool. Tell me when you make the commit so that I can point my project to the online version.  Cheers, Francisco
Are you on 1.1.2 or current master?  Will commit tomorrow, at the airport going home right now. On Jun 9, 2011 3:08 PM, "chicofranchico" < reply@reply.github.com> wrote: >> @@ -952,7 +957,7 @@ class LocalActorRef private[akka] (private[this] val actorFactory: () => Actor, >> >> protected[akka] def checkReceiveTimeout() { >> cancelReceiveTimeout() >> - if (receiveTimeout.isDefined && dispatcher.mailboxSize(this) <= 0) { Only reschedule if desired and there are currently no more messages to be processed >> + if (receiveTimeout.isDefined && dispatcher.mailboxIsEmpty(this)) { Only reschedule if desired and there are currently no more messages to be processed > > Hey cool. Tell me when you make the commit so that I can point my project to the online version. > > Cheers, > Francisco > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/81/files#r42071
Im on 2.0.. Planning to do some remoting stuff until the cluster comes out.. Have a nice trip back home!
I've pushed my changes to 2.0 and 1.2,  ActorRef.mailobxSize/getMailboxSize is removed in 2.0 and deprecated in 1.2  Cheers, 
Hey! Many thanks for that. Really appreciate it. Keep up the good work!  Cheers, Francisco
On Mon, Jun 13, 2011 at 5:46 PM, chicofranchico < reply@reply.github.com>wrote:  > > @@ -952,7 +957,7 @@ class LocalActorRef private[akka] (private[this] val > actorFactory: ()  Actor, > > > >    protected[akka] def checkReceiveTimeout() { > >      cancelReceiveTimeout() > > -    if (receiveTimeout.isDefined && dispatcher.mailboxSize(this) <= 0) { > Only reschedule if desired and there are currently no more messages to be > processed > > +    if (receiveTimeout.isDefined && dispatcher.mailboxIsEmpty(this)) { > Only reschedule if desired and there are currently no more messages to be > processed > > Hey! Many thanks for that. Really appreciate it. Keep up the good work! >  You're welcome!  Cheers,    > > Cheers, > Francisco > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/81/files#r44059 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
So this was the missing path. Nice work!
why does ``managementCommand`` take a Promise, wouldn't it be better if it just returned a Future?
This promise juggling seems quite brittle, whose responsibility is it to complete it? Is it safe to call the exception-throwing ``success`` |``failure`` | ``complete`` methods etc.  How about having ``setMode`` return a Future[Boolean] instead, and doing ``handle.throttlerActor ? ManagementCommand(mode)``
you mean the `def managementCommand`? there is already one `def managementCommand` that creates the promise, delegates to this one, and return the future see Remoting.scala
I mean: Future.fold(allStatuses)(true)(_ && _) pipeTo sender
I also think it is brittle, but I'm not sure `ask` will make it that much better, but I don't see the full picture of all this. When I, @drewhk and @rkuhn talked about the issue we decided that the promise should be propagated down, but that can of course be changed.
I think ask can work here, too.
Well, of course sending something else than a plain boolean, but just for the sake of things.
I have created and alternative implementation using ask. https:github.com/akka/akka/commit/760485163e9ccd8ea4b8de980962da37c29ab089 Which one do you prefer? I'm on the fence.
that will be a global find-replace https:www.assembla.com/spaces/akka/simple_planner#/ticket:2879
``s: SetThrottle`` ?
    case (`naked`, handle)  setMode(handle, mode, direction, statusPromise)     case _ 
yes, that's nicer!
what do you mean?
I'd use "mode" here, instead of reading a var
ah, the params, of course
if(members.isEmpty) empty else empty.copy(members = members) ?
hmm, I find this style slightly confusing: using named arguments which are not Booleans suggests that the others use default arguments; this is kind of true, but in fact this is a real single-arg method.
ok, that is not important, I can change back to the default arguments it was that I wanted to replace all usages of Gossip() with Gossip.empty
If you want to enforce that, then you could make the members argument the first and be required, and add the `empty` value for the truly empty cases.
yes, that is good, I will change, thanks
this parses weirdly for me, how about ``idGenerator.next()``
is the original `sender` of the delayedEvents not of interest?
is usage of this wrap-around-safe?
`Iterator from 0` reads nice, but it's dead slow compared to something like this:      var count = 0     def nextId(): Int = {       count += 1       count     }  if it's used in a hot path in the remoting I think we should consider using the primitive id generator (and perhaps go for long)
No, they are coming from an interface anyway (Transport).
then I think you should use `tell` with no sender, propagating current sender with forward is misleading
Yes, that's true.
@patriknw I think youre wrong here: [check the impl](https:github.com/scala/scala/blob/v2.10.0/src/library/scala/collection/Iterator.scala#L140)
ok, then tell me what is wrong with this microbench      val idGenerator = Iterator from 0      def loop(): Unit = {       val t0 = System.nanoTime       var n = 0       while (n < 100000000) {         idGenerator.next()         n += 1       }       println("took " + ((System.nanoTime - t0) / 1000 / 1000) + " ms" + " (" + n + " -- " + idGenerator.next() + ")")     }      1 to 100 foreach {_ => loop()}   compared to      var count = 0     def nextId(): Int = {       count += 1       count     }      def loop2(): Unit = {       val t0 = System.nanoTime       var n = 0       while (n < 100000000) {         nextId()         n += 1       }       println("took " + ((System.nanoTime - t0) / 1000 / 1000) + " ms" + " (" + n + " -- " + nextId() + ")")     }      1 to 100 foreach {_ => loop2()}  on my machine loop() takes ~ 240 ms and loop2() 8 ms profiler tells me that loop() creates a lot of java.lang.Integer instances (boxing)
hmm, strange that it does not elide the allocation here; I *do* like the nice syntax, so we should maybe roll our own little thing, possibly with `Long` and `Int` variants
Yes, that's actually what gave me any information at all about what had probably happened. Maybe a more descriptive message though.  Do we have any preferred way to output test information that might be useful when diagnosing failures?
we should have a ticket for making the TestEventListener not print anything unless the test failed: https:www.assembla.com/spaces/ddEDvgVAKr3QrUeJe5aVNr/tickets/3101
Could you use `for (count <- 0 to rounds) {  }`?
Is it impossible that loopDuration.toMillis is zero?
I would say so, unless `convergenceWithin` returns a negative number.
now it looks like this comment is misplaced
So this doesn't work? http:stackoverflow.com/questions/8699521/any-way-to-ignore-only-connection-reset-by-peer-ioexceptions
getMessage() is definitely localized. I might try setting the locale for   the test to en_US and try getLocalizedMessage(). I don't have high hopes   though. It also fails on scalable1 jdk7 build with a Swedish message, so   it is not Windows specific at all.
Currently a magic number. Sometimes you get "Connection refused: no   further information" sometimes the message is slightly different. I don't   know what the Swedish build will say to this, but I could not find better   solution here. It would be nice if I could expect the colon to be always   there, but I don't know if this is guaranteed or not.
could you do a search-and-replace in the TcpConnectionSpec for "upong" and replace it with "upon"?
Yes, but first I have to merge other stuff (it was fixed in another PR)
I think we should put the ConnectionResetByPeerMessage in the test name, so it's easy to inspect what was tested even in success.
Couldn't we expand on this to send N packets, to get more coverage.
Ita's a bit WET, needs a bit DRY
why 100 ms?
Ah, this should come from the RemotingSettings instead (BackOff   parameter). Thanks.
Why is this a val?
Isn't there a race between setting it writable and doing this write?
    case DisassociateAttempt(`addressATest`, `addressBTest`)  true
There is one: #2825. I added already a comment to that ticket
I don't see any.
Backticks are nearly impossible to get from my keyboard :) But right, that   looks better.
Ok, inlining it.
A lot of repetition below
listener1 and listener2 seems a bit too generic, they obviously have a deeper meaning than that
this snippet is duplicated below, perhaps make it a method?
Just a tiny nitpick, but if you're already rewriting these imports then I think it's nicer to have all the akka.remote imports grouped together.
Is it possible for `defaultWrite` to take two named parameters rather than a pair? Named params would avoid the `params._1` or `params._2`, making it easier to understand the body of the method.  The partial application of `defaultWrite` in `writeBehaviour` could still be made to work.
Maybe use se `map { case (xxxx, yyyy) => if (locaHandle.inbound) xxxx else yyyy }` to destructure the tuple, so you can give each entry a name?
Maybe mention ticket number in comment.
I took the opposite route, and the ticket references the comment :) But   yes, I can add a link here as well.
No, this is needed because it is a SwitchableLoggedBehavior that accepts   one parameter.
this would probably have been benefitted from a you must not ignore return value annotation (in the sense of being found earlier), but thats life
This was ignored on purpose, and there was even a comment about it. Of   course the assumption was wrong :)
Why does it require a HashSet?
Ehm, why use Scala Actors?
This impl is weird. Uses system identityHashCode for hashCode AND equals?
nope, non-negative is the correct one here
also for ActorInitializationException? (which is the only change wrt. the default which matters)
will these rejects not be sent directly to the router again?
No, we changed that idea along the road.
is NonFatal() to be used here?
the Bound should go to the original sender, not to the handler
Hmm. Then we must be very careful to maintain (or store) the sender of the original Bind
that = changes the story altogether ;-)
> 0 -- positive < 0 -- negative >= -- nonnegative =< -- nonpositive zero is not considered as positive as far as I know (http:en.wikipedia.org/wiki/Positive_number).
no, what I meant is: do you really want to override the default behavior of killing the actor if it cannot be initialized?
which one is correct maximal or maximum?
Good question, I think you can use either. My point was to establishing consistency between the bullet points in that all of them are nouns rather than a mix of nouns and verbs.
Actually, according to [this discussion](http:english.stackexchange.com/questions/38260/maximum-vs-maximal) maximal is probably the better choice.
Id remove the injected `as initial API clients` and replace it with a prepended sentence, like This IO implementation is meant to be generally useful and extensible. (feel free to improve on that, its after-lunch-sleepiness which keeps my brain from functioning properly)
please retain proper em-dashes instead of latex pseudo-syntax (the right one for interjections is  (<shift>-<option>-dash on Mac)
this is a German-only comma, I think ;-)
ok, no problem. (sphinx does convert `--` to en-dashs and `---` to em-dashs though ...)
but it probably does not remove the surrounding white-space ;-)
Ah, I see. You want _unspaced_ em-dashes. [According to wikipedia](http:en.wikipedia.org/wiki/EMdash#En_dash_versus_em_dash) you are then following the US school of dash usage, rather than spaced en-dashes, which are more the UK way to go. Probably makes sense, since typesafe is incorporated in the states... :)
off | 0..N?
I think you have to do this for Java: ``override def get(system: ActorSystem): TypedActorExtension = super.get(system)``
Props.empty & no TODO?
why do you need this var? I can't see that it is used outside of start
what is the threading characteristics of this thing? should these be volatile?
you only expect this once, right? I think you should use `become` to separate the two different states
you have reversed the order, List might not be the appropriate collection for the buffer
so for error you log two entries? doesn't feel right
your right (I corrected not useful log in stop without correcting this, thanks
Thanks, sorry for forgetting
you're right, forgot to move the first log in the second case 
thanks for that note, I use Buffer then.
The name of this method is misleading.
for { log <- logService; sys <- system } sys.eventStream.publish(log)
You're speaking about the actorSystem or the LogService? (cause I'm not the author of the ActorSystem) I would say no, it does not need to be volatile. from www.osgi.org/javadoc/r4v43/core/org/osgi/framework/BundleActivator.html :  " The Framework must not concurrently call a BundleActivator object." But it's really good pointing that (as, most of the time, I let akka manage the concurrency issues). So thanks to tell me if I'm wrong. 
Is that _really_ using the right ClassLoader?
The LogService is provided as a service from OSGi. For the CL question, I'm still thinking that wrapping  akka-actor and akka-osgi would be a nice thing and also avoid the CL problem here. 
changed to _ => logService.get.log(event.level.asInt, messageFormat.format(event.logSource, event.message)) instead of logging twice, now log is done in the match. You're too fast for me ^^ 
For me, this is more direct/simple than using SLF4J. As long as this is only part of akka-osgi (as won't work without an OSGi-environnement),  this is just alternative.
ok, better idea
I guess Viktor reacted to the `.intern` part of it?
you would not really need any of the two fields in the actor: capture locals in the closures for each behavior and youre set; that also lets you elide the Option[] wrapper
thanks a lot, really nicer
thanks, so it was mostly because I based this Actor on StdOutLogger. It should save some memory in the case this string is used from different places but this is not expected here, so I removed it in last commit.  (based on http:www.codeinstructions.com/2009/01/busting-javalangstringintern-myths.html)
`messagesToLog :+= logEvent`
here you should also unsubscribe to the eventBus
what about the `event.message` for errors?
why not log same kind of things as in `StdOutLogger`? - timestamp? - event.logSource - event.thread.getName  Note the two different cases for error, with or without stackTrace
now I remember why I had in my thought implementation also put the messagesToLog in a closure (to be discarded)
is it conceivable to keep that LogServiceTracker running and dynamically adapt the actor systems logging output?
Sorry, mistake in precedent correction.
Well, I was using karaf for my tests (which already gives a timestamp and other information) I think I will add those informations, tests it with OSGi Felix and then commit the changes
I agree with @patriknw but I don't get you, @rkuhn : what kind of event do you want to adapt to? If it is bundle.stop, the ActorSystem is going to shutdown and shutdown the EventHandler. Do you think about something else?
if the LogServiceTracker see that service publishing a change, then it could send another LogService message to this actor in order to change the logging destination
Good point: I'll change the log service tracker to a log service listener, which will advice the EventHandler on changes. 
remove, not needed
???  use explicit return type, such as `): Unit =`
note that error.cause will be `object NoCause extends NoStackTrace` when error logging without Throwable above might not cause any trouble, but you might consider to guard it with `if event.cause != Error.NoCause`
you can place this inside the `initialisedReceive` behavior
    def uninitialisedReceive: Receive = {       var messagesToLog = Vector.empty[LogEvent]         def setLogService....         {          case logService: LogService  setLogService(logService)          case logEvent: LogEvent      messagesToLog :+= logEvent        }     }
which implies that you don't need to clear the vector, it will be gc:ed together with the behavior block
I suggest running the test with custom ticks setting so we don't need to stall for 60 seconds to verify that it works.
Makes sense; do you prefer updating pull requests with additional commits or should I squash the commits and submit a pull request on a new branch?
we actually tend to fix up such things by amending the commit and pushing with `-f`: github is smart enough to figure it all out, keeping the PR history and comments intact (yes, gh is truly awesome :-) )
argh, just pushed an additional commit.  sorry about that.
I think this should be located in SchedulerSpec instead
I would feel more comfortable with 3200 millis
the exact value is important here and I just don't trust doubles for exactness, even though it is tested to work here
Makes sense.  I put it here because the previous test also exercises some HashedWheelTimeout behavior.  Should that one move as well?
This fix just papers over the problem, it does not properly root it out: the underlying issue is that slipped timeouts are re-scheduled unter the write-lock after the wheelCursor has already been advanced, which means that the readLock is taken by the same thread that holds the writeLock already, breaking the assumptions about remainingRounds vs. offset.
I actually think we've already approched the point where we should have the entire test suite for the scheduler, as we've modified the original  too much to rely on its tests. By extension, if we need to add all the tests ourselves, we should really rewrite this in Scala.
yes, that sums it up nicely
Slipped timeouts always get rescheduled with a nonzero delay, so they never get put back in the same tick bucket.  That is, the slippage may cause them to be delayed by one tick, but never by an entire revolution of the wheel.  So I think this fix is valid, and our testing bears out that it solves the problem.  Though whether you want to bite the bullet and rewrite the HashedWheelTImeout in scala is a larger project decision.
re more extensive testing: I don't see any kind HashedWheelTimer test suite in the netty code base (either the 3.5 or 4.0 branches), so this would have to be developed from scratch, presumably starting from the existing test of long timeouts and the one in this patch.
Isn't this always needed in concert with ``serverSideChannel.close()`` and if so, lets just make a method to avoid the code duplication.
I think I covered all occurrences where it matters, but making it a method   is a good idea.
Sounds good, great work!
Should the 3 seconds be dilated?
s/protocol/transport, protocols will be sandwiched on top of transports
I think you need to clarify what a "local sender" and "local reader" is.
It's nice to show that we are dogfooding, but we might also emphasize that it is intended for a more general audience.
Why extract child, cause and uid here?
No need. In fact it is dangerous, as some of the names shadow field names.
Would it be possible to use a null-object for FailedContainer instead of null?
So what happens if an exception has been thrown?
Needs documentation. First needs to be "virgin".
then we will not have dequeued the messages and will overwrite the perpetrator field, thus losing the messages; should be fixable in line 195 below
what do you mean with virgin?
Sorry, I retract that, it will only write next conditionally.
The test isn't necessarily finished since the thread could have been interrupted by anybody. Some better signaling and a loop might be in order.
It would be great if you could add information from `findDeadlockedThreads()` to the dump.  Maybe you should check if the JVM the test runs on supports the flags and operations used, by calling things like `isObjectMonitorUsageSupported()`, so the dump code doesn't throw exceptions if it runs on a non-standard JVM.
I think this should be dilated like other timings in our tests, based on the `akka.test.timefactor`.
shouldnt it be called a Vigilante?
Only if it beats up the thread for misbehaving.
hmm lets see 
`deadline.timeLeft <= Duration.Zero` would also work
but in fact Id recommend `deadline.isOverdue`
this line is not correct: `testRunningLatch` is not volatile, and if it were, then it would lead to NoSuchElementException on line 112
Use a Timer instead: http:docs.oracle.com/javase/6/docs/api/java/util/Timer.html
Use multiline string iso dualprint
no, not Timer, it's horrible, incredibly easy to generate memory leaks with it. Use this instead http:docs.oracle.com/javase/6/docs/api/java/util/concurrent/Executors.html#newScheduledThreadPool(int) but I think current Thread solution is fine
Memory leak? You mean if the thread isn't shut down? If so, then the current solution is just as flawed. But yes, a ScheduledThreadPool is fine by me.
are these threads and what they hold on to released after the test?
not as far as I can see: youll need to interrupt them to release them (and our class loader)
no need to change, but you could have used stripMargin to make the multi-line string indention look nice
Good catch, but it's a bit more complicated. One of the future purposes of this test is to use it as long running test (and then I mean long), and therefore a lot of things are configurable, such as approximate run durations. I think this must also be configurable in this test and the default value perhaps calculated from the sum of the other Duration settings.
I try to read this, but can't really understand the *change* in ordering. Is there something else that I'm not seeing? Could you please explain?
This line was concurrent with the future returned:  listenerPromise.tryCompleteWith(interceptListen(listenAddress,   upstreamListenerPromise.future))  This was not obvious first because it was called in an andThen block, but   that itself does not enforce that interceptListen is completed before   finishing the future provided to upstream. It just enforced that   listenerPromise was wired together with the result of interceptListen   before finishing the upstream future.
The `andThen` version has the exact same semantics as your proposed new version: `interceptListen` will have been executed in full in both cases before the `map` is executed. Hence there must be a different problem somewhere.
interceptListen returns a future, and that is not guaranteed to finish   before the whole expression is completed.
oooh  yeaaah   sorry, youre right, of course. Damn but these Futures can be confusing.
Subtle one, yes :) This implicitly implies the need for a better comment there.
maybe make it `protected` so that this trait is reusable as a template for similar implementations (like the one you may have in mind)
These methods only exist in the private `SecretAgent` implementation, so there would never be an opportunity to subclass.  Or are you suggesting to pull them up into the trait as protected abstract methods?
then again much more would need to be protected  hmm.
Seems sensible to me
There is no implementation in this, right? Isn't it better to use `trait` instead, so that it becomes a java interface?
what is Secret? `DefaultAgent`?
I think there is a typo here, and we normally mark these kind of methods like      /**      * Java API      * Factory method for creating an Agent.      */
BTW, one more thing, add a section to the migration guide also, since it looks like we promoted the constructor for the Java API documentation.
It started as a trait, but the compiler was unable to generate the static forwarder for the Java API `create` method in that case.  This was the fix :)
Must admit, I prefer this form.  I went with create because I saw we were already doing it for `Duration`.  Plus it allows Java users access to the tiny bit of inference that their language affords...
oh yes, right you are; the alternative would be `Agents.create`, but I dont like that either
yes, it was not a complaint of the change, just that it needs to be documented in migration guide, since it's a breaking api change
ah, thanks for clarifying
Why a ReentrantGuard for this and a ReentrantLock for the one above? How do they relate to eachother? Is the any risk of deadlocking interleavings?
What is the purpose of this switch if we're using the systemGuard anyway?
No real reason. The `ReentrantGuard` seemed convenient, so I'll change the `cdtLock` as well.
The `suspendSwitch` blocks normal message enqueues, during `cleanUp`. The `systemGuard` blocks system message enqueues.
Yes, there is a possibility of an interleaving since I hold the guard over a too long region. Will fix.
lets suppose that this succeeds just before this actor walks into the cleanUp section: would we not have the exact same problem as before? Now cleanUp is blocked on systemGuard and we are blocked on the ctdLock
This is the section that I was talking about when answering @viktorklang about lock interleaving. Still thinking about it.  OTOH, your example shouldn't be a problem since `cleanUp` doesn't touch the `cdtLock` of this actor.
My bad @rkuhn. You're right.
``return`` is verboten       systemQueueGet match {       case null | NoMessage => null       case head => if (systemQueuePut(head, newContents)) SystemMessage.reverse(head) else systemDrain(newContents)     }
no return please. create an inner tailrec method instead that aborts on this condition
this can now throw `InterruptedException` which previously (for `lock`) was not true; please wrap appropriately
Yes, that is what "Thread.interrupted()" does, the important part of that comment is to describe _why_
Is the recurse boolean really needed? I only see a couple of places that return true, which could call the line below.
I think you meant `intEx` here
Absolutely. Nice catch.
style suggestion:      mbox.suspendSwitch.fold[Envelope](null) {
no need for the `ret`
The nesting and flags and var is a bit hard to follow here. Do you need them for the tailrec, or is it possible to call `process` from within the cases and get rid of `recurse` and `intex`?
They are unfortunately needed for the `@tailrec`.
I think the distinction between null and NoMessage is confusing at best. Can we rename it to Tombstone or similar?
that will be done in the context of reworking the SysMsgQ (i.e. by @drewhk )
This has the distinct DNA of the Klang :D
nooo, don't use nanoTime directly, abstract the actual clock implementation out into a method so it can be stubbed out for tests
Spidey senses go tingly here
Batman has no parens, this one should (cancel())
``override val resolution``
``override val resolution``
*warning* multiple calls will be oppa gangnam style
why is it a var?
What happens on InterruptedException here?
This Runnable needs hardening as not to kill the scheduler if there's a random exception happening
This can throw RejectedExecutionException (or other cool things)
Would probably prefer Runnable with Cancellable
Cramming in features, are we? ;-)
try removing it ;-)
old impl does the same thing: stop recurring when scheduler is shut down
yes, we need to talk about that 
yes, good one
okay, Ill remove three lines, then ;-)
well, not really: this is the constant we take from the config, the other is a derived quantity (whose relation to this constant is about to change)
it blows up; what are the requirements?
because it is used to communicate the intent of stopping the timer thread; there needs to be some mutation going on, and it needs three states
wow, cool  :-) (yes, this needs hardening)
So what happen if you call stop multiple times?
what does this comment mean? I prefer a distinct marker for tasks, and I chose not to expose the meat of the task itself, therefore the `runDirectly` method.
ah, now I get you; hmm, too many Runnables nested here
 or cleaning up after others, depends on the point of view ;-)
The Rule of the Boyscout!
when Im done fixing that bug: youll get back an already completed future
As we talked about, it should use an ExecutorServiceConfigurator so it can be managed externally.
can new tasks be added while this is being called, and if so, what is the intended semantics?
Apart from logging, can some interesting property tested used the histogram values?
1000000l -> 1000000L I completely misparsed it at the first glance.
Document return value?
(ticks & (ticks - 1)) == 0?
I would rather read the comment :)
+1 If we could implement pluggable clocks that are effective enough that would enable very interesting things.
is this enforced when reading/using the property?
also document what interface it has to implement and what constructor parameters that are required
how is the SchedulerException handled?  I hope it is loglevel debug for this case.
why not let it go to deadletters, as it would without this check?
ok, here was the check, is it the same for DefaultScheduler?
I'm not a big fan of prefixing classes with the product name (Akka)
I meant != of course
why this dance with Map and JavaConverters?      ConfigFactory.parseString(s"akka.scheduler.tick-duration=${SchedulerTickDuration.toMillis} ms")
Suggestions for a cool name a very welcome! (I changed it from Timer to AkkaTimer  )
because it was not immediately dead-obvious to me that that would work, but you are probably right
yes, my idea with this was that we should be able to get some measurement of how reliable the timing is of each jenkins instance over time
we need to stop rescheduling that timer
but isn't this scheduleOnce?
The only reason we add this new scheduler is that we want to replace the old DefaultScheduler (and akka.util.internal.HashedWheelTimer.java). The old has in practice not been part of the public api (even though visibility modifier and doc annotation of that is missing). Therefore I suggest to rename the old to `LegacyScheduler` (or whatever) and this new one to `DefaultScheduler`.  AkkaTimer could be named akka.util.HashedWheelTimer (private[akka] internal api).
The thing is that no hashing is involved in the new timer, hence its more like a LamettaWheelTimer (with all those lists dangling from the spokes; Lametta is German, I think the English word for it is tinsel). My idea would be to back-port it to 2.1, but then the Default needs to stay in and we just add a new one which is not selected by reference.conf.
`WheelTimer`?  shouldn't it be private[akka]?  My suggestion of `DefaultScheduler` would still be possible in 2.2 (hopefully we can ship 2.2 without old scheduler. For 2.1 you can keep the old as `DefaultScheduler` and name the new one as `ExperimentalScheduler`.
yes, good point, was looking at the wrong thing
no, this is called after filling in the `stopped` Promise
why is this not a for-expr? To easier see which line fails? If so, please add comment about that
? not LARS?
I'd numerate the arguments
What happens if it times out?
using "class" as a FCQN config element isn't what we usually do
Is there a test that validates the behavior when this throws an exception?
It's your name on the commit :-)
Could we group these a bit?
no isTerminate-check here?
Do we really need to do this more than once?
How will this blocking-call help? You're creating a thread of your own?
I'm assuming that the race between this line and cancellation is benign
case null | CancelledTask
that syntax has been deprecated, right?
would be nice if there was a failure context which could hold the loop counter
can you think of a better name? I certainly didnt like `akka.scheduler.scheduler`  (which would match other uses like `akka.actor.provider`)
No; currently failure to start the actor system will just abandon it in whatever state it might have failed in. Making it clean up reliably is outside of the scope of this ticket.
but Im not the author; Patrik found the reason in a different PR
I thought I had done that, but some reformatting or automatic importing must have messed it up again. Will fix.
no, why would there?
So you are arguing that a `Float` should be enough, right?
yeah, I guess that Schedulers will not be measured in bytes, so will make a `val`
OTOH that leads to all kinds of cast ugliness all over the place, also for user code: `Double` is the default floating point in Scala.
that syntax does not work for the comments (which are the reason for writing it like this)
has it now  I didnt see any warnings
then Ill take the full implementation
True, invalidly configured parameters in the old impl would also have had the same issue. Open a ticket for it?
We don't have authorship, it's everyone's code.
There's a semantical difference between this and the other then. I.e. check should be after send, not before.
yes, but if the machine does it then Im powerless; I think Ill just make both cases multi-liners
No, my point was to keep it as an Int or Long.
okay, thats a good point
nope, can plausibly be between 0 and 1
 null == expired, Cancelled ... do the math
Ok, so lets not use trailing dot syntax, use the D
If it truly is in Hz, just have 1000Hz or 10000000Hz or whatnot, I still don't see the need for involving things that can be NaN.
I don't see a use case for less frequent than 1Hz, can you come up with one?
@viktorklang and @bantonsson, could you in particular have a look at this part: is this how to make a timer thread reliable? Or is it an atrocity for which well burn in JVM hell?  Apart from this I incorporated all feedback so far and wrote some docs.
You might want to rethrow that t
Yes, that is true. If that is the only problem with this code, then Im happy!
You also have a race between thread.start() and timerThread = thread
Could you remind me why we have to allocate a new thread here?
We should only get fatal exceptions here, which means that something is wrong with this thread; if we want the Scheduler to try its best to stay alive I came to the conclusion that trying to start a new thread might be the last resort (which will fail if the fatality of the exception was not local).  The race you mention is between stopping and recovering, and only for the Thread.interrupt() purpose which is an optimization, so it is not really relevant AFAICT.
Do we really want to create a new Thread on Fatal?
the alternative would be to die, which is precisely why I wanted to have other opinions on this part (the old impl just kicked the bucket, BTW); another possibility would be to try to shut-down the ActorSystem, because that does not work in this case anymore anyway
If that should be done, then I think it should be done by the UncaughtExceptionHandler we register to all threads created by that factory, wdyt?
no, in general you cannot re-run a Runnable this way, which means that that would be quite a special ThreadFactory; in particular it would not be the default thread factory of the ActorSystem, and it might not be applicable to all Scheduler implementations either, so how would the Scheduler select this feature?
No, I mean, the UncaughtExceptionHandler should be the one which deals with Fatals and shuts the actor system down.
wouldn't it be better to include the println message in the thrown exception instead of `println`, i.e. wrap the original exception?
what is the reason for the various strategies for logging? here `log` previous `println`?
yes, this feels wrong, uncaughtExceptionHandler should handle fatals
should this be public?
hmm, SchedulerException is used for two very different things; receiver terminated and when the scheduler is stopped. When scheduler is stopped it is converted to IllegalStateException (which is part of the contract). Feels fragile, I think you should use two different exceptions.  Also, is SchedulerException ever exposed to users? Make it private?
double format might look weird in the log output
I think this inclusion to document the api is questionable, especially in the java documentation It's two methods, and I think it would be better to mention them here briefly and then refer to API docs for details (yes, I know, it's scaladoc, but that is at least better than scala code).
actually, the message send methods of the api is not included by `include: java-scheduler`. As suggested, document the api in another way.
no way to avoid this kind of  `awaitCond$default$4` ? 
So I discussed this with @rkuhn, and the shutdown of the actor system in the `uncaughtExceptionHandler` needs the timer thread to be alive to work properly. I really think that this thread needs to be handled specially.
Comment here that this is used during shutdown would be nice.
hmm, system.log is not always reliable, but in this case Ill just wrap the exception
no, it should be removed  but will fix for now
unfortunately not possible, but `protected[actor]` will suffice (plus INTERNAL API comment)
it might also print `100.0ms` which did not look too wrong to me
why would I want to avoid it here?
hmm, thats a tough one: I cannot really write that class in Java for real, and writing a Java copy runs the risk of bitrotting, as would documenting the method signatures translated to Java.  Wait a second, I have an idea!
is that parsable by the config lib? the outcome also depends on what values we actually use, I bet we will not remember to change this if we change maxFrequency e.g. `66.66666666666667ms` doesn't look that nice I suggest `(1000 / system.scheduler.maxFrequency).toInt`
looks magic and fragile, and eclipse doesn't understand it, but if it's not possible, then just ignore my comment
okay, will fix
its DRY: otherwise I would have to repeat the default argument here and then wed forget to change that
immediately following what?
Why function + match instead of partial function?
Looks a bit fragile
Is the ordering important here? i.e. is it fine to move this down?
Nice one :-)
that is not important, but feels better to publish the instant events before the converged events
um, changed to       val isInstantMemberEvent = classOf[InstantMemberEvent].isAssignableFrom(to)     if (classOf[ClusterDomainEvent] == to || isInstantMemberEvent)       publishInstantClusterState(subscriber)     if (!isInstantMemberEvent)       publishCurrentClusterState(Some(subscriber))  Tests cover this.
no reason at all :-) thanks, fixed
fixed, spelled it out
Yeah, just in case of bounded mailboxes it could take time before we are completed here.
documentation doesn't say what happens if 0
what happens if 0?
request-timeout == time-to-live?
What if 0?
     <why>
no need to copy on contains
no need to copy on !contains
     <why>?
This shuffle by using hashing doesn't work when there are collisions, some will be lost when producing senders. We should use a simple deterministic sort (shuffle) algorithm instead.
oops, that was a complicated way of saying `nodes filter { sender  receivers(sender) contains receiver }`
ok ` not interested in other types of InstantMemberEvent`
yes, heartbeat-request-time-to-live is good
will never be the case, but good principle anyway, I will change
collisions can be resolved by normal Address ordering   Ciao,  Roland  On 16 jan 2013, at 07:40, Patrik Nordwall <notifications@github.com> wrote:  > In akka-cluster/src/main/scala/akka/cluster/ClusterHeartbeat.scala: >  > > + * INTERNAL API > > + * > > + * Data structure for picking heartbeat receivers and keep track of what nodes > > + * that are expected to send heartbeat messages to a node. The node ring is > > + * shuffled by deterministic hashing to avoid picking physically co-located > > + * neighbors. > > + * > > + * It is immutable, i.e. the methods return new instances. > > + */ > > +private[cluster] case class HeartbeatNodeRing(selfAddress: Address, nodes: Set[Address], monitoredByNrOfMembers: Int) { > > + > > +  require(nodes contains selfAddress, s"nodes [${nodes.mkString(", ")}] must contain selfAddress [${selfAddress}]") > > + > > +  private val (hashToIndex: Map[Int, Int], nodeRing: Vector[Address]) = { > > +    val nodesByHash: immutable.SortedMap[Int, Address] = immutable.SortedMap.empty[Int, Address] ++ > > +      nodes.map(node  hashFor(node) -> node) > This shuffle by using hashing doesn't work when there are collisions, some will be lost when producing senders. > We should use a simple deterministic sort (shuffle) algorithm instead. >  >  > Reply to this email directly or view it on GitHub. > 
Yes, and that would dramatically reduce the number of monitoring links across data centers, which I thought was a bad thing, but I can be convinced that it doesn't matter, or even is a good thing.  In that case I suggest that we pick half of the neighbors to the left and half to the right. That would result in more bidirectional links, which would be good in the case of 3 data centers.  WDYT?
No, you got me wrong: only if the hashCode of two Addresses is the same do we sort them by their natural order. This does not affect the probability of neighbors being in the same datacenter. Choosing neighbors left or right also does not have any influence on this, but it would be bad from a cycles point of view: if everybody looks forward then cycles will be formed around the ring (depending on whether or not the ring size is a prime etc.), if everybody looks forward and backward then most cycles will be of size two (i.e. monitoring pairs watching each other mutually).  On that other note: I think it would make sense to make this hashing be skippable by configuration for those who want to prefer local monitors (in Address natural order), but that could also be a later addition.
This number 3 feels rather magic. Should it be configurable?
ok, got it!
Is there a reason why we need a hashToIndex instead of a binary search?
As clustering uses consistently one transport, it is safe to omit the scheme part from the hash, but it might deserve a comment here.
Be aware of #2690 
I prefer seeing the conditional on its own line, otherwise it is missed too easily
could it potentially happen that we still have `isMonitoring==true` for someone who after a subsequent ring change stopped sending heartbeats, and that that leaves us in a state where we dont send the request ever? (I have not yet fully grokked the rest of the code)
this section is basically unreadable, it would greatly benefit from some white space
AFAICS this would be more efficiently encoded using a TreeSet: you wont need the addressToIndex mapping and could just use `nodeRing.from(addr).drop(1).take(nrOfNeighbors)`. In Scala 2.10 that should be really optimized (so Philipp told me).
of course you would need to fill up with `nodeRing.take(rest)` if the neighbors wrap around the seam
I kind of agree, but we use that style at many places, search for this and you will see some of them: def.*= if \(.*\{
Im seeing this assumption cropping up in several places: do we make sure not to let that invariant be violated? Do we throw obvious errors in the users face if trying to join non-matching clusters?
why not make more sure of that by black-holing on `first` immediately following the joinLatch?
It is designed to be covered by the resends. When a node stops sending heartbeats it continues to send some EndHeartbeat messages, which will remove it from the failure detector, i.e. isMonitoring=false. We also send a few HeartbeatRequest in case of message loss.
yes, but I need to start somewhere ;-)
is my intuition correct, then, that the worst case is that we declare a node unreachable (in extremely convoluted cases) when it actually is not?
sounds good any reason for `from(addr).drop(1).take` instead of `slice`?
good point, I'll add a check when a node is joining
that is not completely race free either, why would it be better than a hard shutdown?  (btw this test is "always" failing with the old heartbeat strategy)
sure, just tell me and I can change all of them (but I can't promise that @viktorklang will not change them back to earn some net negative points ;-))
yes, and it should be possible to adjust with the config of heartbeat-request.expected-response-after heartbeat-request.time-to-live NumberOfEndHeartbeats  I see now that NumberOfEndHeartbeats is derived from other settings. I'll change that to an explicit property to make it less magic.  The StressSpec should generate nasty scenarios and detect unexpected failures and I have enabled some logging that should be enough to be able to diagnose this kind of problems.
youd have to find the index first; might be similar in performance but a little less obvious; try out both variants and make an informed choice (ah, and look at the impls, there may be gotchas)
no, what I meant is that you rely on the shutdown being soon enough after the join to test what you what to test; you could make that a bit more reliable by dropping those messages, because then it does not matter when the shutdown actually happens
Ill ask this Klang guy to leave those lines alone, then ;-)
I actually agree here, if it doesn't fit on that single line, I think we might want to move them down to the next. Good point!
great, I'll change them all over the place in a separate PR, to make it a visible style decision
ah, yes, it was a leftover from a debug session, thx
reader foreach context.stop
How expensive is this?
What about wrap-around?
Do we need "time of failure"? or would it be enough to store a Deadline for when it is to be pruned?
A "why" would be nice to comment here
GAAAAA! Nooo.... Thanks!
Is there a reason why the notification goes out prior to the registration?
We use it during shutdown. This whole structure is specific to this actor,   and should not be used anywhere else.
We can go with Deadline.
It does not really matter here, but it probably makes sense to move it   before the creation of the endpoint.
I don't see what "inbound" as a var buys us, as a def it will be false when handle is set to None in the line above.
It affects lifecycle event logging (which need to know the direction), and   the inboundness of an EndpointWriter could change by a restart (after   restart it could be only outbound)
why not ordinary `Map`? AFAIK a Map is a HashMap when more than 4 elements
so the race was around here, looks good
you dont need `Long` here, it should also work with `Int`
what is supposed to happen if registering two writers for the same address? (i.e. why is that not tested?)
It is not the job of this structure to enforce that. On the other hand it   makes sense to throw an exception in that case.
Do you think it is needed? This is local communication.
I think it should be dilated (doesn't hurt). 3 seconds is not always enough on slow jenkins
Great name change.
was this a bug previously or is the fact that the unreachable set can contain Down members a new?
The new thing is that I implemented the transition from Down to Removed. Previously down members stayed in unreachable forever.  A member is normally in unreachable when Down (until leader removes it), but it's also possible to down a reachable member to skip the leaving/exiting/removed steps.
okay, thanks for the explanation
Since we're already returning a tuple, why not return a Tuple3 with Stats included, to avoid side-effecting within the match
No need for parens in the guard
Do we prefer one style or the other?
I have never written a guard with parentheses, so we might possibly not have that in the code base; my preference is without.
`+` is not a good operator, since it's already used for string concatenation we have used `:+` in other places in cluster code
and then to be consistent this should be `:-`
an alternative is to use `withDefaultValue`
That's what came to my mind. I guess I could do `overview.seen.get(address).map(_ == version).getOrElse(false)`.  Feel free to suggest something better.
    overview.seen.get(address).exists(_ == version)
Absolutely. Will change.
It's cheaper to do:       def totalClusterStats = results.foldLeft(ClusterStats()){_ + _.clusterStats}
Nevermind the old comment. No need for "toSeq" here, iteration over a Map is by tuples
So perhaps something like:      def formatStats: String = if (clusterStatsObservedByNode.isEmpty) ""       else ("ClusterStats" +: clusterStatsObservedByNode.map({ case (monitor, stats) => s"${monitor}\t${stats}"})) mkString "\n"
    comparison.isEmpty
    val lines = clusterStatsObservedByNode map { case (monitor, stats) => s"${monitor}\t${stats}" }
Isn't this formatting quite... inconsistent?
Yes, this is why the scalariform in Eclipse does it this way.
I don't understand your comment. Why did it create a newline for the extractor case but not a newline for the default case?
that was the point of my comment: I dont know, it is inconsistent, and that is how it does it.
you mean: s/why/because in your comment?
the why was the sarcastic piece (sorry, should have used XML markup)  Are we annoyed enough to file a ticket? We have until now worked around by adding braces.
um, I'll add braces around them (-- you should get rid of those old java `;` habits ;-))      { setFailed(child); Set(child) }
at first I thought: why this?  then I thought: should be `sclass` now.  then I realized that there is a deeper issue: we used to allow people to write logger configuration filtering on which kind of actor threw an exception; this is not possible anymore, since the parent does not know which actor instance was behind the failing ref, unless we transport that in the Failed message as well. Thoughts?
this is a change which needs to be documented: it used to be that returning `Escalate` and re-throwing the exception did the same thing
yes, to not complicate it I have used the supervisorstrategy class, i.e. the instance that performs the logging. I was also thinking about this and it's good that you bring it up. I'm not sure how important that class filter is now, when we provide a way to customize the logging of actor failures, which we didn't had before. The fine grained logging configuration ticket is related https:www.assembla.com/spaces/akka/tickets/1715 
was/is it documented that re-throwing the exception from the decider function means the same thing as returning `Escalate`?  The reason why I added this was that some test failed without the logging. If it's important I can investigate which test it was.
Hmm, it is not documented I think, but we use it in tests somewhere. I said this needs documentation because I think it is a good change.
Viktor just pointed out that users can do the logging in preRestart, which even has access to the Throwable, so we should just document it.
ok, but I still don't really understand what the documentation should be? The only change is an extra logging, which feels a bit wrong if re-throwing is normal, because then it should be logged at the level handling the failure.  Perhaps this logging should only be done if `e != cause` and then this logging should log `cause`, since `e` will be logged later.  I will look again at the test that fails without this logging.  
but that is in the child actor, either we promote failure handling and logging in the supervisor, or we scrap all this and do the default logging in the child actor as before, but make it possible to turn off per actor (e.g. failureLoggingEnabled property in ActorContext) and document that custom logging can be done in preRestart.
lets keep the baby in the bathtub: I meant that only as a future canned answer to someone asking for the failing actors class in the exception logging, which I take to be a special case; I just wanted to have a draft for that answer.
alright, the failing test was here: https:github.com/akka/akka/blob/master/akka-actor-tests/src/test/scala/akka/actor/SupervisorSpec.scala#L406  I have changed `occurrences = 2` to `occurrences = 1` and removed this catch. Sorry, I was a bit blind in making the tests pass.
good catch, thanks!
Thinking about it, couldn't we drop the "akka" part of the Address, since you don't really have any choice...
Technically speaking it is part of the protocol stack. In fact, it is a   transport adapter (although one that is automatically attached).
Which makes it boilerplate. I vote for removing it.
In that sense the original akka: field was also boilerplate. I am happy   to change it though, if everyone agrees.
Lets see what people say!
Power to the people! :)
Why use scala.util.Random and not ThreadLocalRandom?
    case `cancelled` | `expired`
why println and not log?
    case (as, bs) if as == bs  ordering.compare(a, b) <= 0     case (Down | Exiting | Joining, _) => false     case (_, Down | Exiting | Joining) => true     case _ => ordering.compare(a, b) <= 0
Can we avoid the repetition of the address parameters?
    def leader: Option[Address] =       if (members.isEmpty) None       else members.find(m => m.status != Joining && m.status != Exiting && m.status != Down).                    orElse(Some(members.min(Member.leaderStatusOrdering))).map(_.address)
The code snippet above saves both copying to a new Vector, then sorting that vector (producing a new vector) and then pull out the first item.
you describe a different ordering than Patrik here, and I find Patriks more useful
That was an undocumented feature, but I'm cool with that
ah, that usage of `min` is excellent - thanks
exactly, and I'll add the documentation:      Orders the members by their address except that members with status Joining, Exiting and Down are ordered last (in that order).
You're welcome :)
should all these not be handled by stopping the actor system after the test?
Yes, they are, but at that point we have accumulated 51 threads. Maybe I'm being to eager to clean up.
ah, okay; maybe add so they dont accumulate to not send pendants like myself onto that particular hunt ;-)
can't this be done in some afterEach or in a function wrapping b/getController usage?
ah, right, why didnt I think of that: `withController( b => ... )`
Sorry about being so quick to merge. I will fix this on the 2.1 PR, and cherry pick it forward.
Nice net-negative PR :-)
no need for `immutable.Set`, `Predef.Set` is immutable
Really clean, I like!
No need for creating an new collection, members is a Set right?      latestGossip.members foreach {       node  if (!localGossip.members(node) && node.status == Joining) failureDetector.remove(node.address)     }
Why do the filter before the fold?
Would it be better to have `allowed: immutable.Set[Address]`? Then you operate on `Address` everywhere here, and can also use `contains` instead of `exists`.
@viktorklang I filter out the members that shouldn't be there from the starting map and then don't add them from the map I fold over. Would it be better to just filter them out at the end?  @patriknw I thought it unnecessary to create a set of all member addresses just to use it for filtering. Don't know which will be faster/costlier, sequential scan v.s. hash-code calculation and object allocation.
I'd skip the filter and instead do the check inside the fold to return the map unchanged, effectively filtering them out.
you do rather many scans here so I would guess it's better with the Set, and it makes the function more uniform
@viktorklang I need to filter them both. How would I do that in the fold?
@patriknw Maybe you're right. I'll change it.
@patriknw Yes, that's a great idea. And also, switch to: merged.updated(key, value) instead of + & Tuple2 to avoid allocations if possible
I changed this around so that I fold over the allowed members and build up a the merged seen table instead. No extra `Set` no unnecessary `filter` and lookups in `Maps`.
The status-check is probably cheaper to do before the members check, right?
Waay cleaner! :-)
No need to make it an inner def IMO
Yes, it's not that complicated, will clean it up.
I think this val can be dropped and instead of !hasUnreachable and allMembersInSeenHasLatest, instead have:      unreachable.forall(_.status == Down) && members.forall(m => seenByAddress(m.address))
this case should not really happen, right? I am not 100% certain, but Id try it out and see what happens if you replace the RHS with `???`
So this is actually something that I'm wondering about. I absolutely think that it can happen since the version seen by a node can have traveled different paths and be conflicting. But should I remove the clock like I do now, or is there something better to do?
Yup, so much cleaner. Thanks.
Well now that you mention it, wouldn't the clocks on a single node node always be ordered correctly since it does the merging locally, and only add clocks that are fully merged. I think you're right.
The seen table is updated by which version that host has told someone else about. I dont see how one node can gossip out conflicting versions, because before gossiping it will have merged them.
    case Event(Terminated(r), _) if r == reader.orNull  publishAndThrow(new EndpointDisassociatedException("Disassociated"))
Isn't there a risk of silent issues here?
This should not be here, this is specific to the TestTransport
without it this test fails: https:github.com/akka/akka/blob/master/akka-remote/src/test/scala/akka/remote/RemotingSpec.scala#L139 Let us verify that again, but I think it had something with "akka.test" protocol not registered, which is a good error to log.
No, it does not mean that "akka.test" is not regisered -- that would is   logged elsewhere. This specific error message is the way of TestTransport   to tell you that there is no remote transport that listens to that   address. This is an analogue of trying to connect to a port that no one   listens to.
ah, then I misunderstood the error message, I'll revisit that and probably have to delete that test (or do you have a suggestion of how to test)?
You might as well change the error message :) You can listen to   AssociationErrorEvent as it will be published by the publishAndThrow call.
verified with Endre
s/size/size in bytes/ ?
or rather "the number of bytes"
explicit return type
explicit return type
Any reason for Seq and not Set?
We just moved lines around here. Not sure what the original reason was, @rkuhn? Why would using `Set` be an advantage?
I don't think we should bake in stats into this
the `ack` object is transparent to our implementation. If provide something non-null you'll get it back as an Ack message.
What's the use-case for the "cause"? (note that the Throwable need to be serializable otherwise we're going to have issues in remoting)
Is it OK for retriesLeft to be negative?
Is it OK for retriesLeft to be negative?
`ack` is a user defined message to be sent back when the write is finished or `null` if no ack is wanted.
This is a simple facility providing introspection onto how many connections the selector actors are currently handling as well as how many connections they have handled so far. Isn't this something valuable?
No, retriesLeft will never be negative. Just realized that this message (as well as all others underneath `/ INTERNAL`) should not be publicly visible. Will fix. Do you usually add verification (`require`s) also for internal messages?
this will be called on every Restart directive, is that the intention?
I think it should be a Set. Is there a valid reason to have a duplicated option passed in?
Any reason why we don't use an Option here instead of manually checking for null references?
Note that you might still have two different options with the same type but different arguments in a set. So a Set doesn't really buy you much. However, it makes construction slightly slower and leads to undeterministic ordering in option application. For simplicity I'd prefer an immutable.Seq, but if there is a real advantage to a Set, why not...
TcpConnection actors musn't be restarted, so we thought that shouldn't be an issue. Is it? Is there any way to specify that an actor musn't be restarted?
You can use a SortedSet with a given Ordering (which might be the name of the Class of the SocketOption? (if you want to avoid duplicates by type)
Should be an Option[Any] in that case, ``null`` is the Devils Value.
No, we don't explicitly put monitoring code in, it will be woven in (everyone has their own preference of stats collection).
Ok, that would work. Looks a bit over-engineered to me, just for a few socket options.
This can happen quite often with a Nack protocol, do we want to log it by default (even if it's debug?)
Ok, no problem. Will be ripped out.  --- mathias@spray.io http:spray.io  On 16.01.2013, at 17:09, Viktor Klang () <notifications@github.com> wrote:  >>   case object StopReading extends Command >>   case object ResumeReading extends Command >>  >> +  case object GetStats extends Command >  > No, we don't explicitly put monitoring code in, it will be woven in (everyone has their own preference of stats collection). >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/1030/files#r2667283
It's what we discussed last week. How about using a special value instead of `null` for marking that you don't want an acknowledgement?
Don't we want to catch Terminated here?
We had it in there before but currently use the death pact to just die when the watched handler dies.
I understand, but some additional logging could be useful.
There is exactly one pending write in this state, so why is this needed? And if it's not guaranteed to have a pending write in this state, then why can we use pendingWrite on line 76?
Yes, ``require`` is good for detecting bugs :-)
At least you get the DeathPactException in the logging. If you want a custom message you would need handlers in the other states as well.
Because `doWrite` may change the value of it.
Ok, I see. We could rely entirely on the logging on "our side" and change this to a `case object ErrorClosed`. Sounds good to me.
Hmm, it would be nicer if we would be able to copy a part of the buffer, not the whole and then take a slice.
OK, I see. We are not guaranteed to be able to write in one go.
I think that's a bug. We don't have to use `take` at all because the `ByteBuffer` should have proper limits set and `ByteString.apply` should just copy the right amount of bytes. I'll fix that.
Or we just make it an immutable.Iterable[SocketOption] and then the user gets to decide if he wants to use a Set or just a List.
@jrudolph Yes, that's a much better solution, thanks. And do ``require(ack ne null)``
technically we just need to traverse it  (and I agree with the over-engineered comment)
If someone sends a Write(ByteString.empty, myAck), then it will be not Write.Empty (because of the non-null ack field). Maybe we should add protection against this?
Great, thanks :-)
Yes, great solution @sirthias 
You could always throw an exception in preRestart I guess.
it is the job of the supervisor to do this; the exception in here to work against that would have to go into postRestart, though.
of all threads from the dispatcher which TCP connection actors are using
channel.close can throw, right? Which would make this a zombie (if it isn't closed)?
I don't see why this match is needed
It may be nicer to pass the Settings in the constructor
depends on what you look at: `import Tcp._` (or `import Tcp.*;`) will then give you `SO.ReuseAddress`, which looks quite nice to those who actually use this API
What happened to the idea of making this an interface? As it stands it is impossible to include the Write itself in the ACK.
Great point Roland, that usage is slick
no, the user should watch this actor
yes, this should probably not be logged unless some extreme debug option is set
Shouldn't this be done first?
didnt we say that the write responses should go to the sender of the Write?
okay, Im assuming that all of those will go out, right?
Somebody loves this comment, but it is badass, so it should stay
do I understand correctly that one actor uses this buffer in a write(), then the next actor runs on that same thread and clears that buffer? Is that safe?
As I mentioned above, having a Write(ByteBuffer.empty, someAck) might break this (it depends on what should be the semantics of null writes).
what if we wrote everything which did fit into the `buffer`, should we not just try again?
we dont like allocations ;-) (seriously, though, reading some comment above this will be fixed by using a marker reference to detect the non-acked case
hmm, in case we terminate because of the Selector failing, should we tell the client or not?
they can of course always `watch()` us
AFAICS it does not matter, apart from sending one more message to deadLetters
better move this into the missing else clauses in order not to send multiple messages unnecessarily
Just pure curiosity: does it have any measurable effect to exchange lines 183, 184?  
so you want to call the actual selectors /system/IO-TCP/0/$a, /system/IO-TCP/0/$b and so on? I think Id prefer to name the router selectors.
Also, we do not need to extract the attachment unless the key isValid
btw, the else structures are a bit confusing. isWritable, isAcceptable and isConnectable are mutually exclusive, but isReadable is not. Is this intentional? If it is, a comment would be welcome :)
`case x @ (_: Connect | _: Bind) => selectorPool forward x`
I'd move this into:      val select = new Task {       val doSelect: ()  Int = ...       def tryRun() {         if (doSelect() > 0) {                 }  So you don't need to close over "this" to call the function.
Yes, I'll fix that.
So one needs to send multiple messages to register for more than 1 interest?
Always watch first. The fine thing about watch is that it returns its input, so you can write:      val child = context watch context.actorOf(Awesomeness)
where do we need to send more than one at a time?
since all the `retriesLeft` funneling is already done, why not decrement by one and forward to context.parent instead of replying to the sender?
You mean the debug logs without any information in particular? Yes, those can go.
Yes, good catch in the read case we already did that.
No, I mean anything which is logged per-message, which usually is not useful either way; it would make the DEBUG loglevel setting in the application.conf completely unusable, if you want to keep (some of) them please put them behind a special flag. Also think about what happens if someone decides to forward logging streams over remoting?
What do you mean exactly? Would the actor be a zombie if `close()` throws? Or do you mean the actor is gone and the channel still open? Wouldn't that case be caught by the additional `channel.close` in `postStop`?
the handler would not currently get a closedEvent, but the actor would be cleaned up thanks to the stoppingStrategy in its supervisor
I think this match can go in any case. But we should still think about the semantics of an empty write.
So this means that the configured MaxChannels is a soft limit, because we could potentially exceed it due to updating the count asynchronously, no? Im not saying that is a real problem, just something to be aware of.
hmm, not so sure it would actually help; Id leave this in as a marker for potential benchmarking sessions, though
it should be. writing should copy the buffer into the kernel buffer and afterwards the buffer can be reused. even if not everything could be written we currently don't store the remaining direct buffer for later but we will instead copy the remaining again into a direct buffer when the write can continue. i guess that's not yet the best strategy but the simplest.  
will the channel.close() then make sure to remove the key from the selector?
I'd put this one in a finally
I'd probably shave a couple of allocations and do:      val i = selector.keys.iterator     while(i.hasNext) i.next().channel.close()
which does imply that true zero-copy I/O does not exist for Java (just collecting a data point here)
http:docs.oracle.com/javase/6/docs/api/java/nio/channels/FileChannel.html#transferTo(long, long, java.nio.channels.WritableByteChannel)
`connectionHandler.expectMsgType[Received].data.decodeString("ASCII") must be("testdata")`
This shouldn't be here, but in the extension.
Nice catch. If we leave it a "weak" limit, then we should document it.
yes, that is the one existing special case; using Linux syscalls you can mmap a socket and avoid all copying between user-land and kernel for the general case
The benefits are more readable failure messages and two lines less; could be used through-out.
setup._ was imported
this test name suggests more than the test actually verifies
this is a neat test harness!
Is the Closed not sent by the connectionActor? If so then this test is racy (i.e. it could potentially take a while for that flag to return true). Better watch the actor and `expectMsgType[Terminated]`.
Yes, ``isTerminated`` is one of my Sins. Even though the semantics of it is extremely clear, people get it wrong.
why is no intervention from the selector needed ( la ChannelWritable) to slurp it through?
here `expectMsgPF` makes sense
yeah, thats a good one ;-) But Id rather open port 0, see which number comes back, close that one and use that port. You never know 
should this not have a within() with a lower bound to verify the timeout?
why not kill the userHandler? that would verify the watch() as well
How about 'Empty' rather than 'empty' so it matches ByteString.empty, Map.empty, etc?
Very nice test framework. We should enumerate which tests we need.  1) Verify that all options are applied as expected 2) ...
no fixed port numbers, please
It's often really useful to get some information about the cause of an error. What about making cause into a String?
Brief docs for some of these would be useful.
What about a case statement for that condition?  ```scala val RegisterTimeout = getString("register-timeout") match {   case "infinite" => Duration.Undefined   case x => Duration(x, MILLISECONDS) } ```
yes, that looks a bit cleaner, good one!
It looks like NrOfSelectors * MaxChannelsPerSelector can be less than MaxChannels due to rounding. Is that intentional?
Actually that code wouldn't quite work due to x being a String, but you get the idea!
Is including Write itself in the ACK the only use case for making it an interface? Just to be able to support this case it complicated the code quite a bit so we got rid of it. If you say we want to support this case I'll revert that change.
We discussed this and thought of `Duration.Undefined` et al. as examples for other constants being upper case.
Yes, we should make sure a user-given zero-byte-Write never ends up in `pendingWrite`.
I've got some ideas about how to support `transferTo` but didn't want to put into the first draft. Let's discuss this in the next round.
What would be the advantage? If you would put it into the extension it would mean that if two actor systems share the same threads they would use twice as much buffers per thread without being able to use both of them at the same time.
Yes, of course, that's much cleaner.
it's all one can test in a unit test but I agree the name is too strong.
This may be the cause of the instabilities we've seen with the tests. We've already got rid of most of the `isTerminated` but this instance was forgotten.
Apart from the messages being other ones on OS X as we found out
yes, I wonder where it has gone.
@sirthias used his certified in-brain unique port number generator to generate this port so how could anyone else use it?
Can you stop a TestProbe? Otherwise you would have to duplicate some code to use a real actor instead of a TestProbe for the userHandler. We did that before but this is so much shorter.
That's what `pullFromServerSide` is doing.
you can send it a PoisonPill or do it like in the test below
ah, of course
Hint: Buffer size
How expensive is accessing the extension?  Because that call would be in the hot path?
Or we spend a field in each connection actor to hold the thread local.
Also, we shouldn't use ThreadLocals for this, since the same underlying threadpool can be used for multiple systems.
I propose that we cache the Tcp Extension in the ConnectionActor:      val tcp = Tcp(context.system)     def registerTimeout = tcp.Settings.RegisterTimeout  Then the buffer can be checked in and out directly from the extension  Getting an extension is a CHM access, so on Java8 it will be completely lockless for reads
Ok, so I'd vote an `immutable.Traversable[SocketOption]`.
Sold! To the man with the awesome hat.
Yes, I like a `case class ErrorClose(cause: String)` even better. No problems with serializability and still transports some more details as to what happened to the user. Since this case should be rare the extra allocation over a case object does not matter.
So what's the recommendation here? Should we throw an exception in preRestart/postRestart or leave it be and rely on the supervisor (which we fully control) to apply the stoppingStrategy?
So should we change anything?
    @tailrec def extractMsg(t: Throwable): String =        if (t == null) "unknown"       else {         t.getMessage match {           case null | "" => extractMsg(t.getCause)           case msg => msg         }       }
The downside though is that the cause string becomes public api and people will try to match against it to determine whatever
Couldn't we just rely on postStop to close and publish? (store away closedEvent if need be for postStop)
that exception in postRestart would be the direct equivalent of a `require()`; Id include it to be sure
We have tried this but came across an interesting issue: How do you express the type of the `Tcp(context.system).Settings` object as a type annotation? The weirdness of this is probably a good reason to change the current design of `Settings` as an object. I'll propose a solution for this...
yes, that would solve it nicely
TcpExt#Settings.type should do it
Actually it doesn't. See [this discussion](https:groups.google.com/forum/?fromgroups=#!topic/scala-language/qqbDyxaU24E) on scala-language we started for this topic  --- mathias@spray.io http:spray.io  On 17.01.2013, at 11:57, Roland Kuhn <notifications@github.com> wrote:  >> +import java.nio.channels.ServerSocketChannel >> +import scala.annotation.tailrec >> +import scala.collection.immutable >> +import scala.util.control.NonFatal >> +import akka.actor.{ ActorLogging, ActorRef, Actor } >> +import Tcp._ >> + >> +class TcpListener(manager: ActorRef, >> +                  selector: ActorRef, >> +                  handler: ActorRef, >> +                  endpoint: InetSocketAddress, >> +                  backlog: Int, >> +                  bindCommander: ActorRef, >> +                  options: immutable.Seq[SocketOption]) extends Actor with ActorLogging { >> + >> +  val batchAcceptLimit = Tcp(context.system).Settings.BatchAcceptLimit >  > TcpExt#Settings.type should do it >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/1030/files#r2679188
Actually this is a bug. It should be:      context.watch(handler)  and should go first.
I think I found two bugs in the TestKit here:  1) TestProbe.watch() fails if the watched actor is terminated in the moment we try to watch it: ``` java.lang.AssertionError: assertion failed: expected Watch(TestActor[akka:TcpConnectionSpec/user/$$q]), found Watch(TestActor[akka:TcpConnectionSpec/user/$$p]) [info]   at scala.Predef$.assert(Predef.scala:179) [info]   at akka.testkit.TestKitBase$class.expectMsg_internal(TestKit.scala:295) [info]   at akka.testkit.TestKitBase$class.expectMsg(TestKit.scala:281) [info]   at akka.testkit.TestKit.expectMsg(TestKit.scala:641) [info]   at akka.testkit.TestKitBase$class.watch(TestKit.scala:155) [info]   at akka.testkit.TestKit.watch(TestKit.scala:641) ```  2) I've witnessed a deadlock in commit jrudolph/akka@5719aa0a67fe170ce2ae2bd4f3fdf5ef53e016ae (maybe when the actor has already died when `TestProbe.watch` is called?) ``` "TcpConnectionSpec-akka.actor.default-dispatcher-3": 	at sun.misc.Unsafe.park(Native Method) 	- parking to wait for  <0x00000000e101e558> (a java.util.concurrent.locks.ReentrantLock$NonfairSync) 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178) 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186) 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262) 	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:224) 	at akka.testkit.CallingThreadDispatcher.dispatch(CallingThreadDispatcher.scala:204) 	at akka.actor.dungeon.Dispatch$class.tell(Dispatch.scala:106) 	at akka.actor.ActorCell.tell(ActorCell.scala:306) 	at akka.actor.RepointableActorRef.$bang(RepointableActorRef.scala:154) 	at akka.actor.ActorRef.tell(ActorRef.scala:108) 	at akka.actor.dungeon.DeathWatch$class.sendTerminated$1(DeathWatch.scala:55) 	at akka.actor.dungeon.DeathWatch$$anonfun$tellWatchersWeDied$2.apply(DeathWatch.scala:71) 	at akka.actor.dungeon.DeathWatch$$anonfun$tellWatchersWeDied$2.apply(DeathWatch.scala:71) 	at scala.collection.immutable.RedBlackTree$.foreachKey(RedBlackTree.scala:84) 	at scala.collection.immutable.TreeSet.foreach(TreeSet.scala:151) 	at akka.actor.dungeon.DeathWatch$class.tellWatchersWeDied(DeathWatch.scala:71) 	at akka.actor.ActorCell.tellWatchersWeDied(ActorCell.scala:306) 	at akka.actor.dungeon.FaultHandling$class.finishTerminate(FaultHandling.scala:202) 	at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:159) 	at akka.actor.ActorCell.terminate(ActorCell.scala:306) 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:369) 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:256) 	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:226) 	at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:184) 	at akka.actor.dungeon.Dispatch$class.restart(Dispatch.scala:91) 	at akka.actor.ActorCell.restart(ActorCell.scala:306) 	at akka.actor.LocalActorRef.restart(ActorRef.scala:355) 	at akka.actor.SupervisorStrategy.restartChild(FaultHandling.scala:310) 	at akka.actor.OneForOneStrategy.processFailure(FaultHandling.scala:386) 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:284) 	at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:248) 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:306) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:400) 	at akka.actor.ActorCell.invoke(ActorCell.scala:385) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230) 	at akka.dispatch.Mailbox.run(Mailbox.scala:212) 	at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:501) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262) 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) "pool-2196-thread-2": 	at sun.misc.Unsafe.park(Native Method) 	- parking to wait for  <0x00000000e21a0da0> (a java.util.concurrent.locks.ReentrantLock$NonfairSync) 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178) 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186) 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262) 	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:224) 	at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:184) 	at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch.scala:113) 	at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:306) 	at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:351) 	at akka.actor.dungeon.DeathWatch$$anonfun$watch$1.apply$mcV$sp(DeathWatch.scala:21) 	at akka.actor.dungeon.DeathWatch$$anonfun$watch$1.apply(DeathWatch.scala:20) 	at akka.actor.dungeon.DeathWatch$$anonfun$watch$1.apply(DeathWatch.scala:20) 	at akka.actor.dungeon.DeathWatch$class.maintainAddressTerminatedSubscription(DeathWatch.scala:159) 	at akka.actor.dungeon.DeathWatch$class.watch(DeathWatch.scala:20) 	at akka.actor.ActorCell.watch(ActorCell.scala:306) 	at akka.testkit.TestActor$$anonfun$receive$1.applyOrElse(TestKit.scala:65) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425) 	at akka.actor.ActorCell.invoke(ActorCell.scala:386) 	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:238) 	at akka.testkit.CallingThreadDispatcher.dispatch(CallingThreadDispatcher.scala:204) 	at akka.actor.dungeon.Dispatch$class.tell(Dispatch.scala:106) 	at akka.actor.ActorCell.tell(ActorCell.scala:306) 	at akka.actor.RepointableActorRef.$bang(RepointableActorRef.scala:154) 	at akka.testkit.TestKitBase$class.watch(TestKit.scala:154) 	at akka.testkit.TestKit.watch(TestKit.scala:641) 	at akka.io.TcpConnectionSpec.assertActorTerminated(TcpConnectionSpec.scala:430) [...] ```  Do you think those are valid ones? I'll file issues then.
And it is meant to convey that we are relying on a `DeathPactException` rather than handling `Terminated` ourselves.
Yep, and it is a badass comment :-)
Yes. Good catch.
It seems strange to have to introduce another field to transport information to the `postStop` call. Couldn't we just wrap the sending with try/finally ?
I find it more brittle to duplicate the cleanup code.
It's either that or call doCloseConnection on postStop
We don't save anything by calling `doCloseConnection` on postStop because its parameters would have to be transported to postStop as well. So, I'll just introduce a field.
yes, please file ticket (dont have time to dig into them right now)
Yeah, I can see the current solution be sub-optimal. I'll apply some more love...
I see. Actually I wanted to name the selectors /system/IO-TCP/selectors/0, /system/IO-TCP/selectors/1, etc, but couldn't figure out how to do it. Is this possible?
no, unfortunately not; $a, $b,  does not look too bad
Yes, right, we talked about this. Actually this also allows us to get rid of the `Reject` message type.
Yes, it's a soft limit. I'll improve the docs in the reference.conf.
We now put a throw into postRestart.
This is now hidden behind a config setting `trace-logging`
There's now a dedicated case which directly acks zero-byte Writes.
Registered listeners or connections only close their channel before terminating themselves (or in postStop()). Termination triggers unregistration in the selector. So the current implementation should avoid any leaks here.
MaxChannels is a "soft" limit. I have updated the docs in the reference.conf to also note that the implementation might accept fewer connections than the configured limit (as well as more).
this should be switched with the empty line above
these two cases can only ever match when the and-check was pointless, because if either of OP_READ or OP_WRITE was set these case statements will not be reached
They will also be reached when, for example, `OP_READ & OP_CONNECT` is set, which is sometimes the case (depending on the plattform implementation).
missing copyright header
ah, I see, thanks; not sending the ChannelReadable (for example) right after ChannelConnectable makes it require one more round-trip while keeping the ConnectionActor simpler, or do we immediately try to read? (/me goes looking)
we _could_ potentially try to read immediately, which may speed things up on the server side (unless theres something I dont know about how sockets work, which is entirely possible)
I think the reasoning was that the connection actor has to wait for a `Register` message before it can do anything with incoming reads. So it waits before declaring `ReadInterest` for the `Register` message.
I agree we could try that. The code shouldn't change more than replacing that line with a `doRead` call.
No ifs were involved here...
ruined arrow alignment
this is an automatic formatting by scalariform (in eclipse), only when there is `;` there is no setting for disabling this so I think we have to live with it
quite possibly for the same reason as above
those `if`s can be one-liners
hmm, maybe it works with braces?
Why this name? Might just put them into the SystemMessage companion?
I made SystemMessage.next package private and put the value classes in the   same package to be able to access the field.
Actually Roland's recommendation :)
lot's of duplication in these 3, no way to reduce that?
we decided that this kind of `if` statement should go on next line
would it be more clean to have an operation that did `unlink` and `head` in one go perhaps returning both `head` and `tail`
when will `child` be null and what should be done then?
We were talking about it. Unfortunately it needs then an allocation of a   Tuple2.
Yes, I should add that method :)
ok, but `unlink` can at least return itself `val msg = messages.head.unlink()`
thanks, that improves readability, several more places than I pointed at
Ok. makes sense
+1.  I'd suggest something like:      message match {       case sm: SystemMessage if shouldStash(sm) => stash(sm)       case f: Failed => ...       ...     }  shouldStash can be very efficiently implemented for the normal case:      def shouldStash(sm: SystemMessage): Boolean =       (state: @switch) match {         case `DefaultState` => false         case `SuspendedState` => sm.isInstanceOf[Failed]         case `SuspendedWaitForChildrenState` =>           sm.isInstanceOf[Failed] || sm.isInstanceOf[Recreate] || sm.isInstanceOf[Suspend] || sm.isInstanceOf[Resume]       }    Wdyt?
That's a fair point. I'd like to see what improvements the MPSC queue I've been tinkering with could add.
This is a matter of taste. The current approach was chosen by Roland and   adopted by me, because it makes it more FSM like. Not super-important,   though, I like your approach, too.
Quite a lot source-wise. This approach is I think close to the best that   we can make out from the "SystemMessage as list node, minimal overhead"   approach considering code-niceness (excluding macros).
It's not only taste imho, code duplication is bad for maintainability. An alternative impl of shouldStash could be:      def shouldStash(sm: SystemMessage): Boolean =       (state: @switch) match {         case `DefaultState` => false         case `SuspendedState` => sm.isInstanceOf[StashWhenSuspendedState]         case `SuspendedWaitForChildrenState` => sm.isInstanceOf[StashWhenSuspendedWaitForChildrenState]       }
So, I recommend that we take the MPSC for a spin in this case (we might even be able to skip the stash since we can just store a pointer to our current node if we want to continue to scan forward.
Terminate and Suspend can also become case objects with that queue.
Then this PR should be closed.
no, you could just experiment, taking this as a starting point
Yes, I mean closed as in "not review ready -- come back later" :)
(makes Mad Scientist look)
yes, I agree with Viktor on this one, way cleaner
~~~ Scala if (isTerminated) dump(todo) else if (todo.nonEmpty) invokeAll(todo, newState) ~~~
it for sure is unexpected 
why can this happen? should this not be covered in systemInvoke?
that would be ` instead of {{{ or }}}
this should probably be called `reverse_:::` (as it is on the standard List)
So the guard can be removed?
I am not sure. This is a separate state then, or is covered by the others?
while failed we are always suspended, if not then that would be a bug AFAICSplease check if you come to the same conclusion
Please introduce a marker trait and make this a single `m.isInstanceOf[StashWhenWaiting]`; for symmetry Id prefer a `StashWhenFailed` then as well, even if it is only used by Failed right now.
I like it!
Envelope creation is outside here now (in the Cell trait) so any serialization exceptions will be thrown directly on message send, rather than published to event stream?
Yes, most likely, will check tmro morning
That's a strong demand. Shouldn't it be "Must not throw non-fatal exceptions"?
where did the noSender replacement go?
could this check be placed in Envelope instead?
ok, but why is it better to check in send?
shouldn't we use `noSender` instead of `null`?
It should actually be, "May only throw NonFatal Throwables".
We can, but then we violate the tell contract.
Absolutely! Good catch!
two times the same argument type? I dont really know much about this, but it smells fishy.
ActorContext.system is implicit (https:github.com/jboner/akka/blob/master/akka-actor/src/main/scala/akka/actor/ActorCell.scala#L108) so a mere:  import context.system would most likely suffice.
I'd probably do: IO.takeAny map socket.write
This looks pretty needed, right?
Probably shouldn't rely on that, what do you wish to test for?
What iz diz gud fo?
Yep, at one point ActorRef.stop stopped working, and forgot about this part after I figured out how to stop an actor again
I want to just start up a single shared IOManager. I believe ticket #1477 will take care of this
Trying out different methods of triggering a select. Right now there is an explicit Select message that it sends to itself, and also after N messages. I kept the SelectNanos things in there just for testing it out, but it doesn't seem like it adds anything and will probably be removed.
Isn't it possible to implement the IO stuff as an Akka Extension?
The original created a StackBuilder (https:lampsvn.epfl.ch/trac/scala/browser/scala/tags/R_2_9_1_final/srclibrary/scala/collection/mutable/Stack.scala#L26) which contains a ListBuffer.
Could be great to add as a comment, so no one tries to optimize a line of code ;-)
Probably, haven't looked at them too much. If it lets me lazily create and lookup an ActorRef then it would probably work better.
There's docs :-)
yep, that would work better. I had it cleaner before by having the methods look for an implicit ActorContext, but that seemed wrong.
Yep, good idea. I used that block for debugging at one point, which is why it is so verbose
I'll be sure to check them out :)
Oh yeah, I remember also why I wasn't concerned about this: if the IOManager is not managing any channels it stops looping until it is needed again, so there is little reason to shut it down since it will more then likely be needed again.
Yep, Extension is the way to go. Didn't realize they could be added to an ActorSystem on demand. Good stuff there!
My Iteratee-fu is not strong enough: is there a fundamental reason why write operations are not lifted in the same way as reads? It feels a bit strange that you can build your reader without passing around the `socket`, but that you need to change that for writing. Or is it actually desired as some kind of poor-mans effect system?
What happens during a restart?
What happens when the Actor stops?
Needs moar docs
Reading and writing to a socket is performed by sending messages to and from an actor (the IOManager). The iteratees aren't needed, and are completely optional. To read bytes, you receive the message in your actor. To write bytes, you send the bytes to be written to the IOManager (performed by 'socket.write(bytes)' so you don't have to worry about the IOManager).  It is very much the same reason why receiving messages and sending messages are handled differently with actors (the receive method vs. tell/ask).  Also, the only reason I moved all this into a separate method is to minimize the state that would be captured in the closure.
In this tutorial I haven't dealt with fault tolerance, but typically the 'postStop' method would contain a 'socket.close()'. One of the things I have left to do in the actual implementation is the ability to handle the IO.Failure iteratee.  With IO.Failure handling you would call 'state(IO Done Some(exception))', which should cause an IO.Failure(exception) to be propagated up the the nested iteratees until it is detected by the (unwritten) failure handler, which could handle writing to the IOManager any error messages that are needed, followed by a socket.close().  For now I should at least handle connection failures (with retries) in the test actors, so some of this is tested, and then we don't need the 'started' latch either.
So everything is fine if the Actor fails and is restarted?
In this actor, no, the the socket should be closed during the restart because something it had received more then likely caused the actor to fail.
Latest version has much improved fault tolerance in the tests
is this exception supposed to occur? then I recommend EventFilter.intercept and setting `occurrences = N`. 
The amount of occurrences is unknown. It will keep happening until the server port is open.
The 'started' TestLatch was used to avoid that condition before, I want it to happen now in order to test fault tolerance.
Before this is eligible for merging into master, definitely needs more docs. I'd _love_ to be able to ship this as a part of M3, it will hopefully be released in about 24 hours, so with a little bit of last minute effort, you might just make it.  Thanks, 
ScalaDocs done (at least to an acceptable level). Still have the reST docs to finish, so not sure if I'll make it for M3
I'm not sure prepending is that readable for Java devs.  a +: b  becomes  b prepend a  reverse order  I think it should be   a add/append b
Also, you have a bug. you a preprending that to that instead of this, which means that you're lacking a test ;)
Damn, I thought I can sneak the `that +: that` in without you noticing... :) I'll fix the `prepend` and add an `append` to `SimpleWriteCommand`. While the `append` is easier to understand you cannot say      simpleWrite.append(write1).append(write2)  because `append` returns a general `WriteCommand` and `append` is only defined on the `SimpleWriteCommand`. However, when using `prepend` you can say:      write.prepend(simpleWrite1).prepend(simpleWrite2)  The fact that Java devs then have to think backwards is something that they'll just have to live with... :smirk:
Yes, well spotted, thanks!
how can this message get lost?
u ask me ;-)
similar to the mystery of the Identify message
in this case the shutdown, start, shutdown sequence is extremely quick. see the timings here: https:jenkins.akka.io:8498/job/akka-local-repeat/3087/consoleFull
Add a comment about why this is needed, now that we know that it's needed.
I don't understand localMembers *-- changedMembers ++ changedMembers *-- removedUnreachable ?
equals is based on the address of the member, but the status might be changed; replace = remove + add `val newMembers = (localMembers -- changedMembers) ++ changedMembers`
what does "- m + m" mean? Quite confusing.
Yes, we need to as far as I see.
equals of Member is based on the address only, this is replace by remove followed by add I will add a comment
What happens on "changedMembers ++ localMembers"?
that is a good suggestion, thanks
this does not seem to be used in this method
this does not really prune anything; is this the bug you meant?
if this happens >500ms after the `"hello"` then the test will fail, right?
another interesting test would be to split the cluster in half, join a node to the one half and then down the other and verify that the joining works without fixing the partition.
no this was not *the* bug, but this is clearly not correct, it should do `records.filterNot(_.observer == observer)`
sounds like a good addition, will do
It would be restarted and a new "boom" is scheduled, but you are right that there is a small race, if the Resume from the supervisor doesn't arrive it will be stuck. Perhaps I can use one-way blackhole.
Is this really an error. I know that the other node has gone down in an "uncontrolled" way or we have decided that we can't talk to him any more. But it's not like our system has to fail. 
Very cool trick to make `second` become quarantined from `third`.
Do you have any guarantees here that you manage to set up the probe before `second` is quarantined by `third`? You have already done a blackhole on `second`.
Maybe be more precise saying that it is the move to `Down` that can be performed automatically or manually. To me it sounds like the reachability can be changed manually as well.
good point, fixed
very true, changed in: 36228fa
What's the impact on throughput due to this change?
There's no significant impact. Throughput benchmark results are at https:www.assembla.com/spaces/akka/tickets/3574-akka-persistence-prototype?comment=355843673#comment:355843673
Yes, will change to `mss.reverseIterator.filter(_._1.sequenceNr <= toSnr)`
Yes, will change to `processorId`
we have discussed alternatives, and will evaluate carefully, probably when you are back in town @viktorklang 
if this is to simulate code from inside an actor it should be `getContext().actorOf`
wonder if we need to qualify this name more? perhaps `processorId`?
I like the alternative, when would it be better with Message.create?
is `getCurrentMessage` same as `msg`?
should have the experimental note, see http:doc.akka.io/docs/akka/2.2.1/java/io.html
Does that mean that recover is mandatory? Is it possible to skip recover (for some reason)?
it should be clarified if these two alternatives are equivalent or when to use which
This is scary business, there is no real ordering guarantee between messages sent with ActorSelection relative to those sent with ActorRef. For example the last replayed message sends msg1 with ActorSelection and next msg2 is sent with ActorRef. msg2 may be received before msg1.
interleaved actorSelection and actorRef might result in wrong delivery order
I would prefer more describing parameter names
I mean longer
`Option(current)` does the same thing
yes, optional configurable dispatcher, such as cluster
or should it always be a dedicated dispatcher due to IO?
should we make it `def processorId(ref: ActorRef): String` to make it consumable for java, otherwise these should be INTERNAL API
I guess there is a reason for `private [persistence]`? It should then be marked as INTERNAL API (it is public from java). I think this will also pollute the namespace of java actor extending UntypedProcessor.
I suggest that it should be named `processorId`
might be more clear without overloaded method names `process` vs `processPersistent`?
can we afford the allocation of `Some`?
`_sequenceNr += 1`
not sure if this will ever be sent over the wire, but if there is a chance, so please add `@SerialVersionUID(1L)`
When would you not want to do this? I'm not convinced of the value of this trait. Could it be default behavior in `Processor` constructor, with possibility to override?
should it be `final`?
Do we see any alternatives to this trait? The life-cycle methods call chain are difficult enough to remember and this adds to the complexity for the user. Would an alternative be that the user has to do it themself from ordinary preRestart, but provide some convenience helpers for the unwrapping and construction of Recover from the `message: Option[Any]` 
Should be INTERNAL API
Should this create the `Props` only? Passing ActorRefFactory (if actor context) and potentially using it in the wrong place (outside the actor) feels scary.
Will fix it
Might be misleading to call this `props`, since that is used for other things. I think we have used `Settings` at several places for similar things.
Isn't it a bit redundant to have `processorId` on trait `Processor`? For that reason, I've chosen `id` on purpose.
Might be better to place the test config inline, and use `ConfigFactory.parseString`
In the Scala API, the idea was to have no difference in `Message` creation inside and outside a processor (i.e. using `Message(payload)`). The Java equivalent for message creation inside a processor is `Message.create(payload, getCurrentMessage())`. But I'm fine to recommend using `msg.withPayload(...)` inside processors (Scala and Java) and using `Message(payload)` (Scala) and `Message.create(payload, null)` (Java) outside processors. If someone by accident uses `Message(payload)` inside a processor, it wouldn't matter (because of implicit `Option[Message]`). In Java, `getCurrentMessage()` *must* be called instead.
I'm thinking of how it looks like in user code, it will often be overridden, it prevents the user to use a field `id` for other things. 
Will add it.
yes, this is worth thinking about
Recovery is mandatory, no possibility to skip. Otherwise it would be impossible to consistently recover a network of processors as they could receive new messages (from other recovering processor) before their own recovery. This would change the ordering of messages from previous application runs, for example.
I'm for recommending [this](#discussion_r6122425) and additionally mention equivalent alternatives and caveats. 
Maybe we should mention the tradeoffs more clearly here  - +: at-least-once delivery, conversation recovery - -: no ordering guarantees for unconfirmed messages  Also, receivers could use message sequence numbers (generated by processor) to re-order received messages.  Do you see other alternatives/mechanisms how this could be improved?
Same arguments as in [this comment](#discussion_r6122849).
Will fix it
Right, will change it
Having a dedicated dispatcher is a reasonable first step. We can make it configurable any time later, if needed.
The reason for `private[persistence]` is that it is used by the `RecoverOnRestart` trait. Depending on the alternatives to `RecoverOnRestart` (see below), `journal` will become either INTERNAL API or `private`.
Makes sense, let's name it `processorId`.
Will change it
Will change it
You mean var `_currentMessage` should have type `Message` and can be `null` if there's no current persistent `Message`? Should method `currentMessage` still return Option[Message]?
Can be on wire, will add `@SerialVersionUID(1L)`
This is what most processors may want to do. So having it as default behavior with opt-out makes sense. One example where one wouldn't do auto-recovery is when two processor A and B depend on each other. This means both must have been created before they can send messages to each other, and, message may already be sent during recovery. In this case an application would first create instances of A and B and then recover them.   What about moving the above `preStart` implementation to `Processor` and make it `final` so that concrete processors can only override `preRestartProcessor`? See also [this comment](#discussion_r6123602)
Similar to what I proposed [here](#discussion_r6123408), we could move the `preRestart` and `postRestart` implementations to `Processor` and make them `final`. This would recover on restart by default, and concrete processors should override `preRestartProcessor` and `postRestartProcessor` if needed.  To customize processor recovery, the processor pre-restart hook could return an `Option[Recover]` which lets implementors define custom recovery parameters and make recovery optional too. The same idea can be applied to `def preStartProcessor: Option[Recover]` too.  Reasonable default implementations for processor life-cycle hooks are possible.
This should create the actual `ActorRef`. `JournalFactory` is preliminary and will likely be replaced when I start working on the journal plugin API. I'll make it `private[persistence]` meanwhile.  I understand the issue of using an actor context in the wrong place but can you prevent that at all? For example an actor could reply the context to a sender, a `PromiseActorRef`, and the context is then used in the `onSuccess` callback.
Will rename to `Settings`
See [this comment](#discussion_r6123602)
I don't see any good alternatives, but we must think this through thoroughly.  A channel could perhaps "stop" delivering (buffering) until it has fully resolved the selection to a real ref (using Identify)
yes, that was my thought, I'm not sure it is needed my assumption is that currentMessage is normally not called (and only called once (or a few times for each message))
Sounds good to me, will try that
sounds good, reducing number of combinations would `RecoverOnRestart` be the default behavior? 
`currentMessage` won't be called if users follow the `msg.withPayload(...)` approach discussed [here](#discussion_r6122425). It will be called only if users create new outbound messages via `Message(...)`. This could be another good reason to recommend the `msg.withPayload(...)` approach inside processors.
Yes, would be default, same as recover on start.
Right but will be `def currentMessage = Option(_currentMessage)` once [this change](#discussion_r6123222) is implemented.
A few more thoughts on this. First, `akka-persistence` should completely free applications from dealing with `ActorSelection` directly to resolve potentially invalid stored sender references. Resolving sender references is anyway only needed in two channel usage scenarios (inside a processor).  1. when a processors forwards the stored sender of a replayed message to a channel: `channel forward Deliver(m, destination)`  2. when a processor replies to a sender: `channel ! Deliver(m, sender)`  This means we can always hide resolution of actor references (via `Identify`) for replayed messages inside `Channel` and let applications only specify when they want to have a resolution. This could look like  - `channel forward Deliver(m, destination, Resolve.Sender)` and - `channel ! Deliver(m, sender, Resolve.Destination)` (because `sender` is used as channel destination)  A resolution will only take place for replayed messages that have not been retained by the channel. This usually affects only a few messages after a crash, for example. The default is to do no resolution at all i.e. do not pass a `Resolve.*` argument to `Deliver.apply`.   This change would also make the public `Message.replayed` method obsolete (because `Channel` s could get this information via an internal API) and we could change `Message` pattern matching to `case Message(payload, sequenceNr)`.  Processors can still determine whether they are in recovery mode (i.e. if messages are replayed) by calling `def recovering: Boolean` on `Processor`.
A different aspect, that I think @rkuhn mentioned at some point, is if references to the processor itself should be stable after recovery (stop/start), i.e. the processor actor always gets the same uid.  To me it is not clear what implications that would have. E.g. watch/Terminated. I think it would also be complicated to implement, as the uid is assigned before the actor is really created.  Anyway, all this needs discussion, and we might not be able to make it perfect in the first iteration.
Yes, I proposed that in an email but then decided to start without relying on persistent actor uid. The alternative to persistent actor uids are the resolve strategies mentioned above which are IMO good enough for a first iteration. Fully agree that this should be further discussed.
indeed, a tricky one.
what happens if the leveldb.write(batch, ...) call fails? withBatch should probably accept a success and a failure thunk, and send Written only if that write call acually did its job. Btw, is leveldb.write synchronous?
use named parameters for clarity
while I like short variable names, here they are part of the API by being parameter names. I recommend using slightly more readable ones ("cid" particularly, but I always confused by "snr", too, I always think of "signal-to-noise-ratio" ;) )
Shouldn't byte order be explicitly set?
You might want to inline confirmTarget and confirmMessage since they are named parameters anyway.
Names "d" and "r" are rather cryptic
what does "nid" mean? Please use more descriptive identifiers
since the only valid sequence is (started = 0, ended = 0) -> (started = 1, ended = 0) -> (started = 1, ended = 1), isn't it more clear to use only one variable with symbolic values NOT_STARTED, STARTED, FINISHED, being slightly more readable (IMHO) and also disallowing the invalid (started = 0, ended = 1) state?
can this be lost? (e.g. write failed)
I am not sure it can happen, but might the user interfere with the internals by using the stash for his own purposes? In other words, can Processor actors use the stash they have?
If I understand correctly, the need for this is to maintain order between persisted and non-persisted messages by sending them through the same journal? Maybe it deserves a comment. Also please comment that the reason for separate Looped and Written messages (instead of a simple flag "persisted") is that one stores Any, while the other stores MessageImpl
Hm. I found this method a bit dangerous. Another approach would be to still save the message with a flag (e.g. "failcount = 1", maybe even saving some data about the failure) and optionally ignore the message during replay. WDYT?
Idea: do we want to support markers? Basically just named sequence numbers so upon recovery it is possible to go only up to a certain "checkpoint"? Might be another option/addition to deal with dangerous messages (see my comment on the delete method: https:github.com/akka/akka/pull/1700/files#r6155494)
What I actually wanted to say that it is easy to ignore a saved message (if there is appropriate metadata, like  "failcount" for example), while it is impossible to recover something that was not saved.
A small note on performance. As long as nobody is overriding `receiveAround`, the JVM will inline it and we will see no difference. Have we done any test where one class is overriding it? It doesn't have to be running the test as long as it's loaded and instantiated.
With the current implementation, the user can interfere with internals when using the stash. What is needed is to isolate the stash used by `Processor` internally from the stash used by the processor implementation class.   One possibility to support that is to have 'named' stashes, for example: `stash("my-stash")` and `unstashAll("my-stash")` where the `Processor` internally uses a special named stash and the processor implementation class continues to use the 'default' stash using `stash()` and `unstashAll()` (where only the default stash is part of the public API). This would require an extension to the existing `Stash` (or another `NamedStash` trait, for example).  Potential duplicates of replayed and unstashed messages (during a restart) can easily be filtered `Processor` -internally using sequence numbers. Duplicates may arise if a processor implementation stashes a persistent message and then crashes/restarts.
Already [changed locally](https:github.com/eligosource/akka/commit/15d2e487fec271c859cfbd4efa76898ba9c034a3). Will be part of the next pull request update.
Yes, `leveldb.write` is synchronous and it throws an exception on failure. This will restart the journal, receive the next `Write` and the processor will never see message for which writing failed.   If a client wants to have a confirmation that a write actually succeeded it should expect an (application-level) reply from a processor. What a journal could do is to publish a `WriteFailed` to the event bus in case of a `leveldb.write` failure.
Already [changed locally](https:github.com/eligosource/akka/commit/15d2e487fec271c859cfbd4efa76898ba9c034a3). Will be part of the next pull request update.
Already [changed locally](https:github.com/eligosource/akka/commit/15d2e487fec271c859cfbd4efa76898ba9c034a3). Will be part of the next pull request update.
Already [changed locally](https:github.com/eligosource/akka/commit/15d2e487fec271c859cfbd4efa76898ba9c034a3). Will be part of the next pull request update.
Good idea, will change it, maybe using `NOT_RECOVERED`, `RECOVERING`, `RECOVERED`
See [this comment](https:github.com/akka/akka/pull/1700#discussion_r6201299)
> If I understand correctly, the need for this is to maintain order between persisted and non-persisted messages by sending them through the same journal?  Exactly.  > Maybe it deserves a comment. Also please comment that the reason for separate Looped and Written messages (instead of a simple flag "persisted") is that one stores Any, while the other stores MessageImpl  Will do
Fully agree on this. I anyway planned to implement that as it is needed for an audit log.
I like the checkpoint idea. Will keep it in mind for a separate ticket.
Does this constant offset have any special meaning or is it just arbitrarily chosen?
Exactly how it is implemented is up to the journal implementation I guess. A journal that doesn't support update (or think it is too costly) could store the "failed marker" in a separate collection.
Maybe it's just me., but I prefer 'keyToBytes(idToKey(nidOffset))'
It's always BIG_ENDIAN by default.
This is an arbitrary choice to reserve space for some special-purpose keys.
Will change it
I think that the word _delete_ implies that it actually will be deleted from the journal. Is `Ignore` a bad word? Maybe I'm just splitting hairs.
As long as we document that this a logical deletion, I think `delete` is fine. It also depends if we're going to allow un-delete operations in the future. Need to think more about it and come back to it with a separate ticket ... 
> ... can Processor actors use the stash they have?  I meanwhile have a [solution](https:github.com/eligosource/akka/commit/0bd66c0005aed040aa38f327cd90e31e68def296) implemented for that which will be part of the next pull request update. It  - uses a special-purpose processor-internal stash ( *processor stash* ) that is isolated from `theStash` maintained by `akka.actor.Stash` ( *user stash* ). I didn't implement 'named' stashes as mentioned before. - unstashes all messages from the user stash on restart **excluding** messages of type `Replayed` and `Written` because they will be re-delivered by the journal (as `Replayed` messages). This is the proper way of avoiding duplicates during restart (mentioned in a previous comment). Checking sequence numbers to avoid duplicates doesn't work because `stash()`/`unstashAll()` re-orders the sequence. - also supports cases where a processor is recovered (restarted) between `stash()` and `unstashAll()` calls. [Another commit](https:github.com/eligosource/akka/commit/40508ef1f563328742e2b7388cd9795128c14f1f) (that adds support for re-running recovery when recovery fails) also verifies that several recoveries (restarts) between `stash()` and `unstashAll()` calls are supported. 
> ... move the `preRestart` and `postRestart` implementations to `Processor` and make them `final`.  Making them final limits composition with existing traits. For example making `postStop()` final on `Processor` prevents mixing in the FSM trait. I propose to leave the `preStart`, `postStop`, `preRestart` and `postRestart` non-final but still recommend overriding `preStartProcessor`, `postStopProcessor`, `preRestartProcessor` and `postRestartProcessor` if possible.
We decided that @bantonsson will evaluate the alternatives for aroundReceive and perform the benchmarks this week
make this little function statically available, e.g. on the Actor object
this reads a bit too generic: should we rename `Message` to `Persisted`?
this means that Processors cannot reasonably opt out of having their children terminated during a restart
And rename `Processor` to `PersistentActor`?
Staying in the metaphor, it would be more like registered mail instead of a plain letter. Processor is fine, I think, but Message is too generic.
can we enforce this and give good error messages?
I did enforce that in Scala (using implicit current message), couldn't find a way how to enforce that in Java. 
it would be a runtime check: Message.create() sets something to `false` which is set to `true` while persisting it (for example)
arguments used only in the constructor are prefixed with an underscore in Akka: `_channelId`
we usually put the guard before the arrow ;-)
Message.create() is not necessarily related to persisting it (when sending it over a channel). But maybe I'm not getting your point here. Can you please elaborate?
The situation here is comparable to forwarding in Scala and Java: `ref forward msg` where the context is implicitly set vs `ref.forward(msg, getContext())` where the user is responsible for passing `getContext()`as 2nd argument.
of course, thanks!
After the arrow is correct. Messages retained by the channel should not go to `deadLetters`, and most replayed messages will be retained on every replay.
Yes, that was clear; I was commenting only on the coding style: either use a guard to the left of the arrow or place the conditional on the next line, that makes it easier to see the difference between the two (which is exactly what you just explained).
I guess my question boils down to: It is not allowed means that something will go wrong if you try it nevertheless. What exactly will go wrong, and can we give a good error message in this case?
nice little actors :+1: only one small nitpick: `destination` pairs better with `source` than with `sdr`
Agreed on the preliminary nature, but Patrik is right in that this pattern `(ActorRefFactory => ActorRef)` should not be used, just create Props instead; of course people can point the gun at their feet and pull the trigger, but we should not show them how to do that.
since the `sender` is already part of the request, why not let it travel full circle? I have not read where this is used, but I can imagine that that would allow users of this interface to keep less state
the journal might later want to live somewhere below `/system/persistence` instead of `/user`
please add a link to UntypedProcessor so that Java users find it quicker, and copy/adapt the doc to make a nice javadoc out of it as well
The `sender` parameter will become necessary later when I'm going to implement reliable channels. Should I find another way to implement that, I'll switch back to the sender that is part of the request.
this line needs to go outside the `withBatch`
What's the advantage?
This would change the impl to something like       val sm = withBatch { batch        val ps = if (s.isInstanceOf[PromiseActorRef]) context.system.deadLetters else s       val sm = pm.copy(sender = Serialization.serializedActorPath(ps))       val pid = numericId(sm.processorId)       batch.put ...       batch.put ...       sm     }     p.tell(Written(sm), s)  What do I gain?
My assumptions about LevelDB API might be incorrect, but it looks like the gain is that the confirmation is now sent after the write has completed, not before.
Doh, of course :)
yes, Ive seen that you use the implicit sender now instead of an explicit one; I remarked here mainly because of the visual inconsistency between Write and Written (i.e. either use the implicit sender in both cases or not at all; I tend to favor the latter because it is harder to get wrong)
This is not used elsewhere, put it inside `nodeRing`?
Isn't this weird?
else slice? wut?
I suspect this is a bug
why not use `sendGetContacts` here as well?
why is this needed in state `active`?
Hmm, actually Im thinking about making this `pubSubMediator ? msg pipeTo sender` to allow the cluster client to operate without requiring inbound connections from all the other cluster nodes. The problem obviously are multiple replies, so instead wed need to create child actors for all clients; DeathWatch would take care of cleaning them up AFAICS.
making this a `TreeSet` gets rid of some more lines further below
we should package these features up as extensions which just do the right thing
yes, I suspect that too  ~~~ scala val slice = {   val first = ring.from(a).tail.take(numberOfContacts)   if (first.size == numberOfContacts) first   else first ++ ring.take(numberOfContacts - first.size) } sender ! Contacts(slice.map(  )) ~~~
this `"bonjour"` message should turn up at a `testActor` sooner or later, shouldnt it?
I guess you wanted to assert that you only possibly get `"hi again"`, but you might not. In that case `fishForMessage` might be more appropriate, since that actually fails when something unmatched is encountered.
That is an interesting suggestion. I wanted to keep the server receptionist stateless of connected clients, but one of the primary goals was to minimize number of connections on the client side. I agree that the replies defeats that goal. I will create child actor per client. Cleanup cannot be done with death watch. It's not reliable outside cluster. Client may also change receptionist. I think receive timeout based approach is fine.  It will still be possible for client to talk directly to an actor, but that decision is made by the client. Makes a lot of sense.
Yupp, this is indeed a bug, thanks.
Usage of receive timeout in this actor is a thinko.
Well, I was thinking ahead of time. InitialContacts will be ActorSelection, and contacts will be ActorRefs.
you mean like      fishForMessage(2 seconds) {         case "hi again"  true       }  That doesn't work. It fails with: assertion failed: timeout (2 seconds) during fishForMessage  This is a more strict receiveWhile:        receiveWhile(2 seconds) {         case "hi again"          case other       fail("unexpected message: " + other)       }
It's received by the testActor on the node that is just about to be shutdown. The purpose of the "bonjour" message is to find out which node the client is "connected" to, so I can shutdown that node.
Minor alternative:  ```scala Option(role).filterNot { _.isEmpty } ```
Why 4? I think this is used somewhere else too. Maybe extract to common val?
I just have to say it: I love for comprehensions. :+1: 
True, but that is 3 allocations instead of 1 ;-)
True. Conciseness has a penalty at times. :)
for(d <- pruneDeadline if d.isOverdue) context stop self
I'd check `if subscribers contains ref` before. since if you're trying to remove a non-member you'll reset the deadline every time. Or perhaps that is the semantics but it looks accidental.
require(r.forall(cluster.selfRoles.contains), s"This cluster member [${selfAddress}] doesn't have the role [$role]")
you could hide the `version` variable like so:  ~~~ scala val nextVersion = {   var version = 0L   () => {     val current = System.currentTimeMillis     version = if (current > version) current else version + 1     version   } } ~~~
message should include an explicit hint that this `Put` is ignored
add guard for `refs.isEmpty`
while `Put` is explicitly local, `Subscribe` may happen from remote, hence there should be an acknowledgement so that message loss can be counter-acted
`topic` should be any String, right? In this case it must be URL-encoded further below
this assumes that `version`  timestamp, which is only true if the `Put` rate is  (gossipInterval / 1ms); this could be fixed by not bumping the version in `put` if the current version has not yet been gossiped; or you could measure the time in s instead (simply multiplying by 1000)
ah, it feels good that certain decisions already pay off 
NICE! This is an extremely clear and concise push-pull gossip implementation!
`runOn(first, second)` ?
~~~ scala val names = receiveWhile(message = 2) {     case "hello all" => lastSender.path.name   } names.toSet must be(Set("u8", "u9")) ~~~
I don't get it. The version is bumped on `Put` and `Remove`. It would only be a problem if the sustained modification rate is > 1000/s. I don't understand why the gossipInterval would affect this.  The idea is that removed entries are first set to `None` and that is spread to all nodes, and thereafter the entries can be pruned (after a few minutes).  However, I see that I did a mistake to compare with currentTimeMillis. I should compare `bucket.version - version`.
yes, it would only be a problem if the sustained Put rate is rather high; but as I said you can retain the meaning of version == timestamp by not bumping it when that is not really needed, or am I missing something?
ok, then we are on the same page Deferring bump of the version would complicate things, and it's only a theoretical problem.
I LOVE IT
If undefined or if empty string?
I don't understand why we need to do all these allocations on every new actor. We should only need to allocate things if we need to change what the user provided.
isn't this sort of weird, that we call it "dispatcherId" in one place and "dispatcher" in the other?
This should be completely de-cluttered once we change `Props` to not hold anything but the `creator` and a `Deploy` anymore, which I thought should be part of this ticket as well. But lets discuss tomorrow, since there is significant overlap with [ticket 3081](https:www.assembla.com/spaces/ddEDvgVAKr3QrUeJe5aVNr/tickets/3081).
yes, me too!
I dont see why not: it holds a public default value.
I disagree. I don't want to removing the programatic API. Example, create child actor with preferred dispatcher the same as parent. I'm sure there are more dynamic scenarios as well.
no, please read the thread linked to from the ticket: it is a pure cleanup, no semantics are lost
ok, I misunderstood your comment
That is why it is in quotes, I guess. In code it is named `dispatcher`. Anyway, I will change the docs.
ok, I have changed here and in cluster reference.conf
thanks, foreach it is
@viktorklang I changed to only create things when dispatcher is defined in conf
when making genjavadoc I read that `<code>` is the usual mark-up for code; if thats not correct then we should correct that
since these kinds of code blocks are not so usual constructs in Java land (AFAIK) it might help to add a short example
since a class by itself does not really return anything an example would also be beneficial here
I'm not sure. I just looked at java.util.List
I think you are right, I will change to `<code>` http:stackoverflow.com/questions/14537311/tt-vs-code-elements-when-writing-java-docs
ok, I have added snippet
A Set is a predicate so you can write `allowedTransitions(oldStatus)(status)`
I know, but I find that less intuitive to read, but if that is a general opinion I will adjust
hmm, I would find `require(allowedTransition(oldStatus)(newStatus))` quite intuitive
So to recap: the only effect of the guards was to force these sloppy copies to be semantically correct transitions? Thatd be good news indeed!
that will be added by @bantonsson to all cluster events and related data
Nice. Perhaps put it as a ".. note::"?
I have no objection to that, but where is note:: documented?  I don't think I came across that directive in my skim of the pointed-to guidelines...
Okay -- figured it out and committed that.  (Still can't find it documented, though.)
Why this change?
Should we support datasize units in the config format
This one shouldn't exist anymore
Change this to a fixme so it's caught in the FIXME searches
Sexy. But why lazy val, objects are lazily initialized anyway
instead of path => path == "default" you can get away with "default" ==
What is this used for? (No docs)?
Compression doesn't exist anymore
1 was the default before also, but it was specified in code. 1 makes sense, because router = direct is the default  On Sat, Nov 19, 2011 at 12:50 PM, viktorklang < reply@reply.github.com > wrote:  > > >  #     available: "direct", "round-robin", "random", "scatter-gather" > > >  #                "least-cpu", "least-ram", "least-messages" > > >  #     or:        fully qualified class name of the router class > > >  #     default is "direct"; > > >  #     if 'replication' is used then the only available router is "direct" > > > > -        nr-of-instances = 3 > # number of actor instances in the cluster > > +        nr-of-instances = 1 > # number of actor instances in the cluster > > Why this change? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240574 >
The config lib supports bytes in various units. I can see if that can be used here.  On Sat, Nov 19, 2011 at 12:51 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -151,14 +150,15 @@ akka { > >          max-items = 2147483647 > >          max-size = 2147483647 > >          max-items = 2147483647 > > +        max-item-size = 2147483647 > > Should we support datasize units in the config format > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240576 >
The reason was to not load the defaultConfig if only referenceConfig was used. Do you see a problem with that?  On Sat, Nov 19, 2011 at 12:52 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -123,6 +102,48 @@ object ActorSystem { > > > >    } > > > > +  object DefaultConfigurationLoader { > > + > > +    lazy val defaultConfig: Config = fromProperties orElse > fromClasspath orElse fromHome getOrElse emptyConfig > > Sexy. But why lazy val, objects are lazily initialized anyway > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240579 >
It is to pimp the config with the additional methods getConfigOption getStringOption  Only used at a few places, and if you dislike the implicit I can rewrite that to ordinary inlined match conditions. See usage in for example Deployer.lookupInConfig  On Sat, Nov 19, 2011 at 1:03 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -0,0 +1,28 @@ > > +/** > > + * Copyright (C) 2009-2011 Typesafe Inc. <http:www.typesafe.com> > > + */ > > + > > +package akka.config > > +import com.typesafe.config.Config > > + > > +object ConfigImplicits { > > +  implicit def decorateConfig(config: Config) = new > ConfigWrapper(config) > > +} > > + > > +class ConfigWrapper(config: Config) { > > What is this used for? (No docs)? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240583 >
compression is used here: https:github.com/jboner/akka/blob/master/akka-remote/src/main/scala/akka/remote/Remote.scala#L133  On Sat, Nov 19, 2011 at 1:05 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -0,0 +1,55 @@ > > +##################################### > > +# Akka Remote Reference Config File # > > +##################################### > > + > > +# This the reference config file has all the default settings. > > +# Make your edits/overrides in your akka.conf. > > + > > +akka { > > + > > +  remote { > > +    # FIXME rename to transport > > +    layer = "akka.cluster.netty.NettyRemoteSupport" > > + > > +    use-compression = off > > Compression doesn't exist anymore > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240585 >
I don't mind, but it doesn't look like we have standardized on that. There is 103 TODO and 83 FIXME in the code base. Why not add FIXME to the search filter?  On Sat, Nov 19, 2011 at 12:52 PM, viktorklang < reply@reply.github.com > wrote:  > > -    val DebugLifecycle = getBool("akka.actor.debug.lifecycle", false) > > -    val FsmDebugEvent = getBool("akka.actor.debug.fsm", false) > > -    val DebugEventStream = getBool("akka.actor.debug.event-stream", > false) > > - > > -    val DispatcherThroughput = getInt("akka.actor.throughput", 5) > > -    val DispatcherDefaultShutdown = > getLong("akka.actor.dispatcher-shutdown-timeout"). > > -      map(time => Duration(time, DefaultTimeUnit)).getOrElse(1 second) > > -    val MailboxCapacity = > getInt("akka.actor.default-dispatcher.mailbox-capacity", -1) > > -    val MailboxPushTimeout = > Duration(getInt("akka.actor.default-dispatcher.mailbox-push-timeout-time", > 10), DefaultTimeUnit) > > -    val DispatcherThroughputDeadlineTime = > Duration(getInt("akka.actor.throughput-deadline-time", -1), DefaultTimeUnit) > > - > > -    val Home = getString("akka.home") > > -    val BootClasses = getList("akka.boot") > > - > > -    val EnabledModules = getList("akka.enabled-modules") > > +     TODO move to cluster extension > > Change this to a fixme so it's caught in the FIXME searches > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240578 >
and I mean, Why not add TODO to the search filter?  On Sat, Nov 19, 2011 at 3:41 PM, Patrik Nordwall <patrik.nordwall@gmail.com>wrote:  > I don't mind, but it doesn't look like we have standardized on that. There > is 103 TODO and 83 FIXME in the code base. > Why not add FIXME to the search filter? > > > On Sat, Nov 19, 2011 at 12:52 PM, viktorklang < > reply@reply.github.com > > wrote: > >> > -    val DebugLifecycle = getBool("akka.actor.debug.lifecycle", false) >> > -    val FsmDebugEvent = getBool("akka.actor.debug.fsm", false) >> > -    val DebugEventStream = getBool("akka.actor.debug.event-stream", >> false) >> > - >> > -    val DispatcherThroughput = getInt("akka.actor.throughput", 5) >> > -    val DispatcherDefaultShutdown = >> getLong("akka.actor.dispatcher-shutdown-timeout"). >> > -      map(time => Duration(time, DefaultTimeUnit)).getOrElse(1 second) >> > -    val MailboxCapacity = >> getInt("akka.actor.default-dispatcher.mailbox-capacity", -1) >> > -    val MailboxPushTimeout = >> Duration(getInt("akka.actor.default-dispatcher.mailbox-push-timeout-time", >> 10), DefaultTimeUnit) >> > -    val DispatcherThroughputDeadlineTime = >> Duration(getInt("akka.actor.throughput-deadline-time", -1), DefaultTimeUnit) >> > - >> > -    val Home = getString("akka.home") >> > -    val BootClasses = getList("akka.boot") >> > - >> > -    val EnabledModules = getList("akka.enabled-modules") >> > +     TODO move to cluster extension >> >> Change this to a fixme so it's caught in the FIXME searches >> >> --- >> Reply to this email directly or view it on GitHub: >> https:github.com/jboner/akka/pull/116/files#r240578 >> > > > >
FIXME == Thing that needs attention before release TODO == Thing that is intentionally left blank until need to take care of arises
But the DefaultConfigurationLoader should only be loaded if default configuration is used, right?
Is this "the Scala API" of the config stuff?
Ah, I think it's mis-named, since it doesn't allude to what gets compressed.
Ok, got it!  On Sat, Nov 19, 2011 at 6:55 PM, viktorklang < reply@reply.github.com > wrote:  > > -    val DebugLifecycle = getBool("akka.actor.debug.lifecycle", false) > > -    val FsmDebugEvent = getBool("akka.actor.debug.fsm", false) > > -    val DebugEventStream = getBool("akka.actor.debug.event-stream", > false) > > - > > -    val DispatcherThroughput = getInt("akka.actor.throughput", 5) > > -    val DispatcherDefaultShutdown = > getLong("akka.actor.dispatcher-shutdown-timeout"). > > -      map(time => Duration(time, DefaultTimeUnit)).getOrElse(1 second) > > -    val MailboxCapacity = > getInt("akka.actor.default-dispatcher.mailbox-capacity", -1) > > -    val MailboxPushTimeout = > Duration(getInt("akka.actor.default-dispatcher.mailbox-push-timeout-time", > 10), DefaultTimeUnit) > > -    val DispatcherThroughputDeadlineTime = > Duration(getInt("akka.actor.throughput-deadline-time", -1), DefaultTimeUnit) > > - > > -    val Home = getString("akka.home") > > -    val BootClasses = getList("akka.boot") > > - > > -    val EnabledModules = getList("akka.enabled-modules") > > +     TODO move to cluster extension > > FIXME == Thing that needs attention before release > TODO == Thing that is intentionally left blank until need to take care of > arises > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240675 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
I can make like that, by moving referenceConfig to another place, which I think is a good idea.  On Sat, Nov 19, 2011 at 6:56 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -123,6 +102,48 @@ object ActorSystem { > > > >    } > > > > +  object DefaultConfigurationLoader { > > + > > +    lazy val defaultConfig: Config = fromProperties orElse > fromClasspath orElse fromHome getOrElse emptyConfig > > But the DefaultConfigurationLoader should only be loaded if default > configuration is used, right? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240677 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
No, it was only a convenience thing I added. After thinking about it I have decided to remove it. It's not motivated. Stay tuned.  On Sat, Nov 19, 2011 at 6:56 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -0,0 +1,28 @@ > > +/** > > + * Copyright (C) 2009-2011 Typesafe Inc. <http:www.typesafe.com> > > + */ > > + > > +package akka.config > > +import com.typesafe.config.Config > > + > > +object ConfigImplicits { > > +  implicit def decorateConfig(config: Config) = new > ConfigWrapper(config) > > +} > > + > > +class ConfigWrapper(config: Config) { > > Is this "the Scala API" of the config stuff? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/116/files#r240678 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
ConfigImplicits removed in latest commit.  On Sat, Nov 19, 2011 at 7:00 PM, Patrik Nordwall <patrik.nordwall@gmail.com>wrote:  > No, it was only a convenience thing I added. After thinking about it I > have decided to remove it. It's not motivated. Stay tuned. > > > On Sat, Nov 19, 2011 at 6:56 PM, viktorklang < > reply@reply.github.com > > wrote: > >> > @@ -0,0 +1,28 @@ >> > +/** >> > + * Copyright (C) 2009-2011 Typesafe Inc. <http:www.typesafe.com> >> > + */ >> > + >> > +package akka.config >> > +import com.typesafe.config.Config >> > + >> > +object ConfigImplicits { >> > +  implicit def decorateConfig(config: Config) = new >> ConfigWrapper(config) >> > +} >> > + >> > +class ConfigWrapper(config: Config) { >> >> Is this "the Scala API" of the config stuff? >> >> --- >> Reply to this email directly or view it on GitHub: >> https:github.com/jboner/akka/pull/116/files#r240678 >> > > > > -- > > Patrik Nordwall > Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts > Twitter: @patriknw > > >
Rename to rootPath
system.Settings would be nice
Could be rootPath instead?
Docs for the methods bitte
Add FIXME comment to remove this
def tempName = "$_" + Helpers.base64(tempNumber.getAndIncrement())
Alternative to have one field and then a CAS to put in an immutable datastructure with all the values?
Add comment to make the default scheduler configurable
If this is called multiple times?
done (Ive de-capitalized it, though)
changed all to system now
I dont think thats worth it here: I still have to make the vars private, but then everything is controlled by the provider and cannot be f***ed with.
added an IllegalStateException in that case
Do you really want to force the size to know if empty?
Why a val?
Possible to use testActor instead and do an expectMsg("sum")? (to get rid of latch)
Why a val here?
No reason, I'll change to def.  On Mon, Nov 14, 2011 at 11:26 PM, viktorklang < reply@reply.github.com > wrote:  > > > >  class ZooKeeperBasedMailboxException(message: String) extends > AkkaException(message) > > > >  /** > >   * @author <a href="http:jonasboner.com">Jonas Bon&#233;r</a> > >   */ > > -private[akka] object ZooKeeperMailboxConfig { > > -  val zkServerAddresses = > config.getString("akka.actor.mailbox.zookeeper.server-addresses", > "localhost:2181") > > -  val sessionTimeout = > Duration(config.getInt("akka.actor.mailbox.zookeeper.session-timeout", 60), > TIME_UNIT).toMillis.toInt > > -  val connectionTimeout = > Duration(config.getInt("akka.actor.mailbox.zookeeper.connection-timeout", > 60), TIME_UNIT).toMillis.toInt > > -  val blockingQueue = > config.getBool("akka.actor.mailbox.zookeeper.blocking-queue", true) > > +class ZooKeeperBasedMailbox(val owner: ActorCell) extends > DurableMailbox(owner) with DurableMessageSerialization { > > + > > +  val zkServerAddresses = > app.config.getString("akka.actor.mailbox.zookeeper.server-addresses", > "localhost:2181") > > +  val defaultTimeUnit = app.AkkaConfig.DefaultTimeUnit > > Why a val here? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/110/files#r228592 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
What is the question?  On Mon, Nov 14, 2011 at 11:21 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -140,9 +142,11 @@ object QDumper { > >        System.exit(0) > >      } > > > > +    val app = ActorSystem() > > ? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/110/files#r228567 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Don't use symbols, they are registered in a global object
Use tryComplete instead of this bandaid
What if the statements above throw an exception?
Might want to document that this does a busy-wait
then match against them explicitly so that illegal states are not silently swallowed
document the busy waiting
Shouldn't "stop" make sure that result.isCompleted so that isTerminated shows the right stuff?
if `path` was never called, nobody can possibly be watching us; Id bet this can be omitted
should directly transition to RefStopped, no need for `busy`
yes, needs `try publish finally unregister`
transition directly, no need for `busy`
Id rather use a `@volatile private var _state: AnyRef` and update using `akka.util.Unsafe`.
Ok, no problem. Just out of curiosity: why are Symbols a bad thing here? 
We need to fulfill the `path` contract and always return a valid path, even after the ref has been stopped. Here I wasn't sure whether `provider.tempPath` could conceivably throw an exception and how we'd best deal with this case
you mean the problem lies in their implementation using WeakReferences and their associated extra GC cost, right?
The prevent a compiler warning on an incomplete match. We probably don't want to resort to a sealed trait based solution in order to have to wrap the "hot" case `ActorPath`.
This was not the case before and I didn't want to change the behavior...
Cool, even better!
Right, of course. At first I had the `state.set` after the `deathWatch.publish` and `provider.unregisterTempActor`, which results in a deadlock. Since it can be moved up the busy state can indeed be skipped, excellent....
This is also giving me the creeps: need to look more closely how long exactly it might be spinning.
Since temp path creation and temp actor registration were in the hot path before I assumed that they would be fast enough to not having to worry about the spinning here. But maybe some analysis is needed...
Not only that, it uses locks and it's a shared global structure. There is 0 value in using Symbols and a lot of cost, so if you need a flag, use a case object.
As far as my little REPL-experiments have revealed, Symbols are cached in static final fields in those classes which use them, so there should be no overhead after the class has been initialized. That is not to say that we always use case objects (for the other reasons), though.
Ewww. That was even worse. Also, Symbols aren't typesafe, so a slight misspelling can trip things up good. Just use case objects if you need states. KTHXBAI
this calls `provider.tempPath` while spinning: wasnt `Registering` meant to be used to prevent wasting names like this?
Yes, I'm optimizing for the common case (which is probably still quite uncommon in reality). In rare cases it might indeed happen that two threads want to get a previously uninitialized path of a Ref that has already been stopped at the same time. One is going to make it, the other one will have wasted a temp path. Actually there is no tempPath creation in a real spinning scenario here. In order to waste 10 temp path creations 10 threads need to get to this branch at the same time. Due to the rather limited "wasting" in quite rare scenarios I took the shortcut route.  Fully protecting this branch would require another state token or renaming of the "Registering" state, which I wanted to avoid for code readability reasons as well.
Why is the payload an array of bytes instead of just putting in an AnyRef and let Akka Serialization do the marshalling/unmarshalling?
Haven't looked at the akka serialization yet. Will incorporate this, thanks for the hint :-)
You're most welcome
Isn't this extremely unsafe?
Don't capture "this" in your callbacks. Which you will do if you capture "channel". I'm not keen about sending around closures either since it makes your program communicate using shared state.
So, create a message that you send to channelActor, that doesn't close over state, and then use AskPattern instead of creating your own Promise.
What happens if this actor is restarted before onAvailable is triggered?
This doesn't make any sense.
Uses shared-state concurrency, don't do it.
Just remove all the numerous imports of this and place it at top level, DRY
Right, I'll leave the callback only for specialized access to the channel, but will make the messaging part more explicit, by just sending the Message to the channel actor.
I'll rework this whole idea. Don't really like it myself too. :S
You mean restarted while being starting/created?
needs to be private[camel] - we talked about it
needs to be private[camel] 
needs to be private[camel] 
needs to be private[camel] 
Will this work, what if you have 2 subtrees with the same name that are then created remotely, then they'll conflict under /remote.
Yes, but youll have configured the conflict explicitly yourself. This is the same kind of conflict you get when trying to create the same child twice. BTW: we need a strategy for that one as well. My proposal is to return deadLetters.
Then we should make this a configuration error.
Yes, if we can detect it. Do we require that configs are in sync for collaborating remote systems? In that case we have the full picture.
If you by "collaborating" mean "where we support remote supervision", then yes.
Okay, got it.
I suggest "elements" or "parts" or something like that. path.pathElements feels a be repetitive
Is this user API?  I think I need convincing if it is. Also, the name of "LookupPath" is misleading, it doesn't lookup a path.
Should it really be "ActorRef.path"? wouldn't it make more sense to use URI? ActorRef.uri
Or aren't all: akka:system@host:port/xxx/yyy ? Perhaps a toURI method else?
Sweet API (and test)
Use an akka.util.Switch instead
Isn't this logically a write?
Can't we just do a mailbox status read before reading this? it's racy anyway, so we only need to ensure memory visibility.
name.charAt(0) == '$'
_:Terminate is prolly cheaper
That is a good check!
Why List and not Seq or Vector or IndexedSeq?
Isn't this racy anyway, and is null useful, is it only for internal usage?
What's up with the ActorPath.separator? Are we back in File.separator country?
Is this premeditated? Will eat memory
yes, elements is the way to go, will change that.
Doh! It's only for the root
No, its just a case class declared in the test.
Maybe collect the paths in collection and do mkString at the end to reduce string concatenation.
You guessed it.  Also, use pbhash :-)
All paths conform to URI when toString-ed, but they are different beasts internally and you should prefer to use them directly, even thought using their .toString also works in most cases. Adding a .toURI would be trivial (even without the intermediate String), please add a ticket if you want to see it done, because I dont have a use-case for it.
Isnt that quite a bit too heavy in this context?
no, the only thing which must be protected against concurrent enqueue() (which is a read) is shutdown().
true, will investigate.
check your javap: this compiles to exactly one instanceof test.
and it was the only reason why I touched HWT at all: to guarantee either execution or exception.
because the algorithms for traversing path structures rely heavily on head/tail, which is most effectively done using List. All others would have to allocate/copy objects during the traversal.
Well, its so internal that you can remove it and nothing breaks; meaning, that this is some residue of a discontinued solution to the problem ;-)
tempName is side-effecting, use tempName()
Okay, will change to hard-coded. Cannot really be changed due to URI compatibility anyway.
Isnt that included in the package of making the whole class final?
@tailrec would have told me already ;-) Seriously: inner methods are always final!
ah, right, oversight.
I will, as soon as you point me to the research detailing how to mix in a different value using PBs code.
Should this be overridable?
Should this be overridable?
Should this be overridable?
Should this be overridable?
Damn clean, Mr Kuhn, damn clean!
yes, e.g. TestActorRef
DO NOT USE THIS EXCEPT INTERNALLY IN AKKA
Are these methods seen from Java?
only if you downcast, but there is nothing we can do against that. We need these methods from FaultHandlingStrategy.
I have reviewed "all" parts of the shutdown, and it looks good
when will this happen? It will stop the other tasks from close to executed, but that is maybe fine
Thanks. I am currently chasing one slightly mysterious bug in this area, but its good that you agree with the approach.
yes, that was the intention: it has been argued that InterruptedException should be propagated upwards as fast as possible on a best-effort basis, and this is my interpretation in the context of forcefully stopping a timer service.
hashedWheelTimer.stop().asScala foreach execDirectly
yup, will do
yes, you can be sure that I'll check with the profiler after merge ;-)
size is an O(N) op, check !isEmpty && tail.isEmpty
I've seen some issues with this being null, why null?
I generally prefer to use unit explicitly: : Unit = ()
Users still cannot get to the provider, right?
Add an explicit return type here
Love the docs, great work!
Add ScalaDoc "Internal use only"
These two are :(
Should this really default to null?
Should this really pipe back an exception?
I think I prefer:  children.get(name.head) match {   case null => Nobody   case some =>     val t = name.tail     if (t.isEmpty) c     else c.getChild(t) }
this is "schedule" not "exec"
Shouldn't this verify that the IllegalState was from it being stopped?
my research indicates that this is the case: http:stackoverflow.com/questions/3961881/why-defining-class-as-final-improves-jvm-performance
Should we really mandate that all addresses are IP-based? (AMQP, 0MQ etc)?
Ive replaced it with a single StringBuilder instead and added a ticket about a possible further optimization if needed (which would cost memory).
Why return instead of else if?
Y NO MARKET?
Check the cause/message?
This name isn't really that awesomized.
||is a wonderful thing
This won't work, this is loaded from config in master
case RemoteActorPath(`remoteAddress`, elems) if !elems.isEmpty  ?
If this method is really invoked, there are no child paths to consider, so this should not be a problem. If there are children, the method is overridden.
because that is what it is on ScalaActorRef ;-) Seriously, though: it is converted to deadLetters in ActorCell.tell, and we cannot really require that implicitly because then everybody would need to have an implicit ActorSystem around.
well, usual answer: you can downcast an ActorSystem.
It doesnt really break anything, plus its `private[akka]` already, but I added a scaladoc comment.
Thats for another patch series 
is there an animated unicode character which somehow sparkles?
yes, because thats what ScalaActorRef does, but I have added a check to guard against humongous stupidity
Yes, the contract of actorOf is that it throws an exception in case of an invalid (e.g. alread taken) name, so this must be propagated to the true caller.
case `system.deadLetters` ??
Nope, our HWT throws ISE instead of RejectedExecution, is all.
well, thats why I left it open: hostPort is just that part of the URI where normally host and port are found, you can put in there anything which is valid for that part of a URI.
will see during merge
its only used on those two lines, but I can change it 
then Id have to `import system.deadLetters` as '.' does not play nice with back-ticks
On Mon, Dec 5, 2011 at 3:32 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -459,7 +605,9 @@ class DefaultScheduler(hashedWheelTimer: > HashedWheelTimer, log: LoggingAdapter, > >           Check if the receiver is still alive and kicking before > sending it a message and reschedule the task > >          if (!receiver.isTerminated) { > >            receiver ! message > > -          timeout.getTimer.newTimeout(this, delay) > > +          try timeout.getTimer.newTimeout(this, delay) catch { > > +            case _: IllegalStateException =>  stop recurring if timer > is stopped > > Shouldn't this verify that the IllegalState was from it being stopped? >  If not already done, the IllegalStateException should be well documented in the api of the Scheduler, because now we rely on that behavior for proper dispatcher shutdown, and the Scheduler is (in theory) replaceable.   > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/119/files#r269820 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
> > > > +    val path = rootPath / "bubble-walker" > > is there an animated unicode character which somehow sparkles? >  LOL
Like it a lot. Good work Davy Jones. 
Why not make 'procotol' a type?  Why not separate 'host' and 'port'? Using 'hostPort' relies on convention with :
The basis of my implementation is that an ActorPath should be representable as and represent a URI, meaning that the protocol part is a string and following the `:` there is something of the form `auth@server:port`, where the three parts are just named according to convention: we actually use the `auth` part as ActorSystem name and different transports may use `server:port` in different ways, but e.g. none of it can contain any slashes. If that is not okay, Ill have to change it. But I see great utility in having a well-defined string representation which is powerful enough to serve as serialization format.
Ok. I trust your design. Go with what you think is best.  On Tue, Dec 6, 2011 at 12:01 PM, Roland Kuhn <reply@reply.github.com> wrote: >> + */ >> +package akka.actor >> +import java.net.URI >> +import java.net.URISyntaxException >> + >> +/** >> + * The address specifies the physical location under which an Actor can be >> + * reached. Examples are local addresses, identified by the ActorSystems >> + * name, and remote addresses, identified by protocol, host and port. >> + */ >> +abstract class Address { >> + def protocol: String >> + def hostPort: String >> + @transient >> + override lazy val toString = protocol + ":" + hostPort >> +} > > The basis of my implementation is that an ActorPath should be representable as and represent a URI, meaning that the protocol part is a string and following the `:` there is something of the form `auth@server:port`, where the three parts are just named according to convention: we actually use the `auth` part as ActorSystem name and different transports may use `server:port` in different ways, but e.g. none of it can contain any slashes. If that is not okay, Ill have to change it. But I see great utility in having a well-defined string representation which is powerful enough to serve as serialization format. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/119/files#r272877    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
why not "60 seconds"?
oops, -0ms was a typo  On Mon, Nov 21, 2011 at 4:26 PM, viktorklang < reply@reply.github.com > wrote:  > >        core-pool-size-factor = 8.0      # No of core threads ... > ceil(available processors * factor) > >        max-pool-size-factor  = 8.0      # Max no of threads ... > ceil(available processors * factor) > >        task-queue-size = -1             # Specifies the bounded capacity > of the task queue (< 1 == unbounded) > >        task-queue-type = "linked"       # Specifies which type of task > queue will be used, can be "array" or "linked" (default) > >        allow-core-timeout = on          # Allow core threads to time out > >        throughput = 5                   # Throughput for Dispatcher, set > to 1 for complete fairness > > -      throughput-deadline-time = -1    # Throughput deadline for > Dispatcher, set to 0 or negative for no deadline > > +      throughput-deadline-time = -0ms  # Throughput deadline for > Dispatcher, set to 0 or negative for no deadline > > sexiest > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/118/files#r242134 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
you mean with implicits from akka.util.duration._ ? sure  On Mon, Nov 21, 2011 at 4:26 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -14,7 +15,7 @@ class TellThroughputPerformanceSpec extends > PerformanceSpec { > > > >    def createDispatcher(name: String) = > ThreadPoolConfigDispatcherBuilder(config => > >      new Dispatcher(system.dispatcherFactory.prerequisites, name, 5, > > -      0, UnboundedMailbox(), config, 60000), ThreadPoolConfig()) > > +      Duration.Zero, UnboundedMailbox(), config, Duration(60, > TimeUnit.SECONDS)), ThreadPoolConfig()) > > why not "60 seconds"? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/118/files#r242130 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
what if the cause is not serializable? endless loop?
java.lang  Class Throwable  java.lang.Object   java.lang.Throwable All Implemented Interfaces: Serializable Direct Known Subclasses: Error, Exception
Yes, but our world being Java, `Serializable` does not have anything to do with whether a class is actually serializable or not. So youre basically saying: if you have a factually non-serializable exception in there and happen to send RemoteClientShutdown over the network, then you should be prepared to handle the endless loop?
What are our options?
implement `private void writeObject(java.io.ObjectOutputStream out) throws IOException`, which tries `out.defaultWriteObject` and substitutes the exception by null if that fails
I don't think that will work, since what do you do if it is the message that cannot be serialized? Try to write your writeObject, you'll see what I mean :-)
This will allocate a new PF every call to it.
This will allocate a new PF every call to it.
final override val ?
final override val?
final override val?
You aren't using it as a PartialFunction so what's the purpose of having it return a partialfunction?
You aren't using it as a PartialFunction so what's the purpose of having it return a partialfunction?
should it just silently swallow messages?
I strongly advice to make it a method from Any to Any.
I strongly advice to make it a method from Any to Any.
The first thing the user would do in transform they would cast it so partial function seems to be good from the usability point of view. ie. override val transformResponse = { case msg: CamelMessage => msg.bodyAs[String] }   so if we change it to (Any) => Any we are forcing user to type more:  override def transformResponse(msg:Any) = msg match { case msg: CamelMessage => msg.bodyAs[String] }   I agree with the val vs def let's change it to val.
yes it is not silently - it has to be explicitly configured by user 
Please dont expose ActorSystemImpl like this, it will be abused and something will break: use composition instead of inheritance.
This will be used automatically if you use the factory method, dont pass in a loader and the context loader is non-null.
(undocumented) naming convention is that all args with shall not become fields (i.e. only passed to super constructor) get leading underscore; please add them (no such rule for secondary constructors, because their arguments dont become fields).
why not use MigrationSystem?
This facility is intended to be permanent, right? Then it should have very clear warnings that it is not suited for serious work, because you should properly design an actor system as a hierarchy, where top-level actors form the Error Kernel, i.e. creating them is not the rule but very much an exception.
Please put the same warnings/explanations/rationale as on the trait, people might overlook them.
    if (s ne null) s     else {       ...     }  or      tl.get match {       case null =>       case x => x     }
this wont fly: breaks horribly in all kinds of cases, not even thinking about remoting. ActorRefs MUST be properly registered with the provider, because they must just work, we cannot add crippled ActorRefs.
mutable queue and synchronized is not how we roll: this looks more like a ConcurrentLinkedQueue to me. The rest is thread-local, so what needs guarding?
this is begging for memory leaks, please adhere to in-order reception policy
this blows up toString and everything else; see comment above
this will be fixed once you switch to CLQ
please have a look at TestKit/TestProbe because that does nearly exactly what you want here
background to the question: you have smart actor creation here, but not on the MigrationSystem.  Also: all systems should be configurable, please read in something like `ConfigFactory.load().getConfig("ActorDSLSystem")`.
This trait is supposed to keep working once it moves into stdlib, right? Then it should be in the envisioned package already now, otherwise it will be incompatible. On the other hand: who would actually use this type on a value? Exchanging implementation will not be a source-compatible operation anyway, unless Im missing something.
this is not used anywhere, right? if it were, the my-dispatcher would not be defined 
I usually prefer to explicitly use `EventFilter[Exception](occurrences = N) intercept {}` around every such site, because it ensures that expected exceptions are thrown.
calling this self conflicts with many pre-existing uses, consider renaming it to threadActor or something equally descriptive/explicit.
why not use for-comprehension here also? while/var just smells bad
commented tests are not nice.  Please add tests for looking up threadActors (using system.actorFor()) and examining their path, printing them, etc.
Would you like to create delegate method for all public methods in the API of ActorSystem or just for actorOf that is used in migration?
I think the best compromise would be to directly have the ActorDSL on it and otherwise expose the rest via a `val system: ActorSystem`.
To clarify, you're suggesting `object MigrationSystem extends ActorDSL`, right? Sounds good.
Yes, and it would save quite some lines & effort to put the implementation inside that trait, with just an abstract `def system: ActorSystem` in which the two objects would differ.
Fixed. I replaced the queue with a `LinkedBlockingDeque`, leading to a dramatic simplification of `receive`.
I guess there will be some functionality coming later which uses this, right? Anyway, dont forget to null out the ref once done.
Id still argue for retaining that information by queuing `(Any, ActorRef)` and then exposing the sender like in TestKit, i.e. `lastSender`.
This line makes 0 sense to me
This probably will risk shadow "self" in Actor, right? Is that intentional?
Why have multple TLs instead of just one with different states? Keeping multiple TLs up to date is complecting things.
Why is start called here?
When are these cleaned up?
Stop the entire ActorSystem if not Exception?
This was "INFO" in your other pull request
mark this as @inline final
This is only an sample in the documentation. I only fixed the wrong (changed) configuration of it.
done, thx  On Mon, Nov 28, 2011 at 10:22 AM, viktorklang < reply@reply.github.com > wrote:  > > > >      case InitializeLogger(_) => > >        log.info("Slf4jEventHandler started") > >        sender ! LoggerInitialized > >    } > > > > +  def withMdc(name: String, value: String)(logStatement: => Unit) { > > mark this as @inline final > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/125/files#r253826 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Is there any sensible reason why we have 2 different log levels (stdout and the other?)
I think it would be good to clarify what the point of this option is.
I generally omit braces when there's only one line
I think we need to clarify "extensions" here, "Akka Extensions (<link to docs>)"
I think there's a ticket for restoring this functionality, if there isn't create it and assign it to 2.0
I feel that there are too many names at play here:  name, clustername, nodename
I suggest making this named "findExtension" and return Option (avoid NPEs)
this should be:  extensions.get(key) != null
Then problem here is now we've made internal API user-level API (for those users who write extensions), but perhaps that is ok?
Shouldn't this be a part of the serialization extension?
This should also be a part of the serialization extension?
The apply method seems like boilerplate for every and all keys
I dont think this is a good pattern in general. Extensions should be registered before/while initializing the ActorSystem and then only used. Otherwise it will be quite hard to debug (in user code, I mean) if something goes wrong during `registerExtension`.
Yes, this separation of init & key is a good thing.
will this fail horribly enough with clear indication if somehow the lazy vals are forced too soon? My stylistic preference would be to keep all that init() code within the actual init() method.
and then `extension()` would need to be changed to handle (i.e. await) the CountDownLatch case.
Hmm, we might well choose to introduce a new interface for this level of access, just so we can shuffle around the implementation just a little bit.
you could directly extract the TestEventFilterLeeway here
I would change all these to be just extractors, without the get-or-create semantics.
The CountDownLatch was the reason for x: Extension I think that if registration is in progress (CountDownLatch) then the extension doesn't exist yet and therefore this returns false. I think current impl is correct. Tell me if I'm wrong (I bet you will, but just saying).
I'm not sure what to do with these comments. I'll add a FIXME so we don't forget.
another view might be that while being under construction, hasExtension should return true, because (given no errors) calling .extension() will return a valid answer, maybe blocking for a bit.
yup, good enough.
Serialization is initialized from SerializationExtension. I could move all of it to SerializationExtension, but I think it is better to have it separated like it is now, but I don't have a strong opinion about that.
I don't agree. I think extensions should be registered on demand, when first used. Otherwise we have the dependency issue that ActorSystem need to know about all extensions, or users need to configure all extensions they use.  I can remove boilerplate if we add the by-name parameters to registerExtension.
it will be one line with by-name parameter. Yes or no?
lazy failure will throw exception and then it will try to initialize the lazy val on next access  I think lazy val is less ugly than all @volatile var, but I would prefer something totally different for all this init stuff, unfortunately I can't come up with a good alternative.
We should do the by-name param in any case, do it
ok, I'll merge in the minor fixes and merge it into master, then I'll do the by-name thingy separately  On Thu, Nov 24, 2011 at 2:31 PM, viktorklang < reply@reply.github.com > wrote:  > > +import com.typesafe.config.Config > > +import com.typesafe.config.ConfigFactory > > +import com.typesafe.config.ConfigParseOptions > > +import com.typesafe.config.ConfigRoot > > +import akka.util.Duration > > +import java.util.concurrent.TimeUnit.MILLISECONDS > > + > > +object BeanstalkBasedMailboxExtensionKey extends > ExtensionKey[BeanstalkBasedMailboxExtension] > > + > > +object BeanstalkBasedMailboxExtension { > > +  def apply(system: ActorSystem): BeanstalkBasedMailboxExtension = { > > +    if (!system.hasExtension(BeanstalkBasedMailboxExtensionKey)) { > > +      system.registerExtension(new BeanstalkBasedMailboxExtension) > > +    } > > +    system.extension(BeanstalkBasedMailboxExtensionKey) > > +  } > > We should do the by-name param in any case, do it > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/121/files#r250773 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
System.currentTimeMillis usually has about 30ms of inaccurracy, I'd definitely recommend switching to System.nanoTime  Also, bear in mind that our beloved Jenkins isn't the fastest car on the racetrack ;)
5 seconds  instead of the Duration(..)?
typo - I'll fix
Ok then :-)
I find it a bit weird to have a method called "toValue" that returns a key-value pair...
Does it throw anything if it cannot find any config, if so, can you add that to the ScalaDoc for this method?
Why no val? If not a val, and is captured, it may not be guaranteed to be visible.
application config is not mandatory, defaults are always available, and it's runnable with that  On Wed, Nov 30, 2011 at 11:05 AM, viktorklang < reply@reply.github.com > wrote:  > >    def create(name: String): ActorSystem = apply(name) > > -  def apply(name: String): ActorSystem = apply(name, > DefaultConfigurationLoader.defaultConfig) > > +  /** > > +   * Uses the standard default Config from ConfigFactory.load(), since > none is provided. > > +   */ > > +  def apply(name: String): ActorSystem = apply(name, > ConfigFactory.load()) > > Does it throw anything if it cannot find any config, if so, can you add > that to the ScalaDoc for this method? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/129/files#r260468 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
applicationConfig is not intended to be used outside the constructor, it's turned into val config  On Wed, Nov 30, 2011 at 11:07 AM, viktorklang < reply@reply.github.com > wrote:  > > @@ -273,7 +293,7 @@ abstract class ActorSystem extends ActorRefFactory { > >    def hasExtension(ext: ExtensionId[_ <: Extension]): Boolean > >  } > > > > -class ActorSystemImpl(val name: String, val applicationConfig: Config) > extends ActorSystem { > > +class ActorSystemImpl(val name: String, applicationConfig: Config) > extends ActorSystem { > > Why no val? If not a val, and is captured, it may not be guaranteed to be > visible. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/129/files#r260472 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Add return type
Add return type
Add return type
So it's impossible to transform the AnyVals? and should be a MatchError?
I guess so. What's your thinking?
Is there an elegant solution for this problem or we should drop it on the floor like we do now? Poor java guys...
just do: message.asInstanceOf[AnyRef]?
This method only returns Unit, not Any.
fair enough - it will just box the primitives. Wouldn't it?
This still is wrong...
Sorry for this ping-pong.
Viktor, do we want to keep it that way or should we just use curly braces instead?
I think we can leave it as it is.
This shouldn't return Any. It's Unit
/**    * Should return the current number of messages held in this queue; may    * always return 0 if no other value is available efficiently. Do not use    * this for testing for presence of messages, use `hasMessages` instead.    */   def numberOfMessages: Int
I would have found that if it was also on Mailbox.numberOfMessages  will copy the comment.
Also if there was support for unsigned integers in Java :-)
I think this is not needed. If you specify anything in the nat-whitelist you're saying that you accept nat-whitelisting?
This should most definitely not be a list, since multiple entries which are equal should be discarded, and since it's only used for lookups it should most naturally be a Set, which usually have a very fast test for inclusion.
I think the natAddress should be passed into natOK and not host & port
Ouch, undocumented behavior that an empty whitelist makes ALL unknown addresses whitelisted, surprising behavior.
This is probably not what we want. Can't we just check if the Scheduler is Closeable?
Great changes here
Good point, I'll make the DefaultScheduler Closeable  On Fri, Dec 2, 2011 at 10:19 AM, viktorklang < reply@reply.github.com > wrote:  > > @@ -400,8 +403,11 @@ class ActorSystemImpl(val name: String, > applicationConfig: Config) extends Actor > >    } > > > >    protected def stopScheduler(): Unit = scheduler match { > > -    case x: DefaultScheduler => x.stop() > > -    case _                   => > > +    case x: DefaultScheduler => > > This is probably not what we want. Can't we just check if the Scheduler is > Closeable? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/134/files#r266233 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Why not 1 SECONDS?
changed to 1 second
I'd be surprised if this is the way to create RoutedActorRefs. Jonas?
No this is cheating by using low-level API that shouldn't even be exposed.  It should be configured to be a routed ref.  But that will change when Roland is done. 
Same here. Should not be used. 
Ok, I'll wait for Rolands fix then.
Already fixed, remove this! :-)
Is this correct?
Is it also used somewhere?
Remove this fixme
Is this used somewhere?
I don't know how it is intended to be used. The system is passed in so I can agree to that it's not really the responsibility of BootableActorLoaderService to stop it. I'll remove the system.stop here, if no objections.  On Wed, Nov 30, 2011 at 4:05 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -58,8 +58,7 @@ trait BootableActorLoaderService extends Bootable { > >    abstract override def onUnload() = { > >      super.onUnload() > > > > -     FIXME shutdown all actors > > -     system.registry.local.shutdownAll > > +    system.stop() > > Is this correct? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/131/files#r261063 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Nope, I removed it.  On Wed, Nov 30, 2011 at 4:06 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -8,8 +8,7 @@ > >  akka { > > > >    remote { > > -    # FIXME rename to transport > > -    layer = "akka.cluster.netty.NettyRemoteSupport" > > +    transport = "akka.cluster.netty.NettyRemoteSupport" > > Is it also used somewhere? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/131/files#r261066 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
nooo it should get used somewhere :-)  I'll add a ticket to make use of it.
alright I'll put it back :-)  On Fri, Dec 2, 2011 at 9:35 AM, viktorklang < reply@reply.github.com > wrote:  > > @@ -8,8 +8,7 @@ > >  akka { > > > >    remote { > > -    # FIXME rename to transport > > -    layer = "akka.cluster.netty.NettyRemoteSupport" > > +    transport = "akka.cluster.netty.NettyRemoteSupport" > > nooo it should get used somewhere :-) > > I'll add a ticket to make use of it. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/131/files#r266161 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Perhaps clarify when an actor is started.
That is not true, it's not a synchronous call.
I'd replace "logged" with "thrown"
I'd add that the start is executed asynchronously.
What does "Actor Internal API" mean?
imports1 is just a name of a block of code in ActorDocSpec what will be included.  On Wed, Dec 7, 2011 at 2:27 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -46,21 +41,22 @@ along with the implementation of how the messages > should be processed. > >  Here is an example: > > > >  .. includecode:: code/ActorDocSpec.scala > > -   :include: imports,my-actor > > +   :include: imports1,my-actor > > imports1? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/137/files#r276414 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Open a ticket for this
Add that "sender" should never be closed over, since it's not valid outside of handling a message.
"[...] actor reference." => "[...] actor reference by default."
This is not true, if you want to propagate exceptions, you need to do so manually, but you can do so by:  try {   operation } catch {   case e: Exception => sender ! akka.actor.Status.Failure(e); throw e }
You tell me, I haven't changed. I also stumbled over that heading for a while because I think it makes one think of internal implementation of akka, which is totally uninteresting. Maybe simply change it to Actor API.
I love this
Worth documenting that it's not a synchronous operation.
s/of current/of the current
Should document what happens to those messages.
What does "the actor object" mean? "object Actor" or "the instance of the Actor"
s/is lost/isdiscarded  s/it in life-cycle callbacks./it in preRestart/postRestart
Yeah, drop "Internal"
Ticker for describing 'Identifying Actors': #1448  On Wed, Dec 7, 2011 at 2:33 PM, viktorklang < reply@reply.github.com > wrote:  > > > > > >  Identifying Actors > >  ================== > > > > -An actor is identified by its address. If no address is associated with > an actor > > -then a unique identifier is used instead. The address of an actor can be > > -accessed using ``self.address``. > > +FIXME Actor Path documentation > > Open a ticket for this > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/137/files#r276428 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
oops, should be Props[MyActor]
sure, but what happens? Nothing, they will be garbage, if the mailbox isn't durable or something.
or, are they sent to deadLetters?
That's not very intuitive, is it? I think it worked like I described in Akka 1.2.
Yes, it worked like that when we had both senderFuture and sender
does it work with simply importing it? (could be a def and not a val)
I think we should write the tests as we want the users to write their code, I recommend "import.context.become"
Very good, thanks for the excellent docs!
How about "UntypedActorContext"?
Could you make ActorCell a final class?
It should be noted that it currently doesn't support sub-millisecond timeouts
If I make ActorCell a final class, then it's not needed on the methods.  On Tue, Dec 6, 2011 at 11:37 AM, viktorklang < reply@reply.github.com > wrote:  > > @@ -120,6 +196,11 @@ private[akka] class ActorCell( > >    @inline > >    final def dispatcher: MessageDispatcher = if (props.dispatcher == > Props.defaultDispatcher) system.dispatcher else props.dispatcher > > > > +  /** > > +   * JavaActorContext impl > > +   */ > > +  def getDispatcher(): MessageDispatcher = dispatcher > > final > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/136/files#r272818 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Agreed: because that means that our tests break when we change something that would break user code.
So, if you make this `implicit`, then `import context._` just fixes that. Same goes for `system`.
But then again: who does?
I don't think the import is a big deal. I think some users will actually prefer to use context.become() instead of import context.become become()  Depends on how much of the context that is used in that case. Being explicit is actually rather pedagogic. ;-)  BUT we have a lot of convenience methods in AkkaSpec, such as actorOf that shouldn't be there because they provide delegation to something that is not available in plain user code.  On Tue, Dec 6, 2011 at 12:04 PM, Roland Kuhn < reply@reply.github.com > wrote:  > > @@ -37,7 +37,7 @@ class HotSwapSpec extends AkkaSpec { > >            case "init" => > >              _log += "init" > >              barrier.await > > -          case "swap" => become({ > > +          case "swap" => context.become({ > > Agreed: because that means that our tests break when we change something > that would break user code. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/136/files#r272889 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
I like UntypedActorContext
But is it needed to have implicit dispatcher and system? Where is it needed in user code?
I agree with the sentiment, but the point remains that we should have test cases covering all use cases to see what breaks during a change.  wrt. AkkaSpec: https:www.assembla.com/spaces/akka/tickets/1439
currently needed for creating Futures, which is why you had to make your `implicit def dispatcher` in some cases
Fixed. Documented and impl.
I made system and dispatcher implicit in ActorContext and changed to imports
I have changed system and dispatcher to be implicit in in ActorContext, so now it works with import instead.
Changed to UntypedActorContext, thanks.
it might be nice to remove the default argument and add `.withFallback(testConf)` before passing on.
Why is this commented out?  Either it should be a FIXME or it should be deleted
I am working on it, but it is not done yet.  On Tue, Dec 6, 2011 at 2:55 PM, viktorklang <reply@reply.github.com> wrote: >> @@ -0,0 +1,134 @@ >> + package akka.remote >> + >> + import akka.actor.Actor >> + import akka.remote._ >> + import akka.routing._ >> + import akka.routing.Routing.Broadcast >> + >> + object GossipMembershipMultiJvmSpec { > > Why is this commented out? > > Either it should be a FIXME or it should be deleted > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/135/files#r273195    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
Why is this a val?
Why is this a val?
Why is this a val?
What do you mean?
I mean "why is it a val and not a def"?
I don't care. You tell me. 
If it doesn't need to be retained in the object it should be a def, if not, a val.
Not a biggie at all since it's in the test. I was just curious why it was a val.
Where? (hint: URL)
Every single messagequeue has it's own settings? hideously wasteful
Pass the settings into the constructor instead
which one? I could generically link to akka.io/docs.
Boy-scout rule might be nice in principle, but I have no idea what to document. Shall I start researching?
see the "Testing Actor Systems" section of the Akka Documentation at http:doc.akka.io  sounds ok?
Add TODO docs :-)
yes, thats what I had in mind
Fixing this would mean passing `system` to `MailboxType` in the constructor, which would potentially make sense. Yes, I think I will do that.
requires a change to how mailbox types are instantiated, see above.
hmm, not sure: this will have to be turned into a system property somehow, to enable Jenkins to use it, right?
Sounds great, open a ticket about it also, so it goes into the changelog.
Something similar to:  parallelExecution in GlobalScope := System.getProperty("akka.parallelExecution", "false").toBoolean,
created http:www.assembla.com/spaces/akka/tickets/1863 instead
doesnt work: there is no system available while constructing MailboxType :-(
Then we need to rethink this structure. Settings should be created once, in the MailboxType, then it can be passed into the MEssageQueue (def create)
Ill change the objective to just include `system.settings` in DispatcherPrerequisites, which fixes your complaint.
Soon Prerequisites will be ActorSystem. Full circle.
Nope, it only includes those items which are constructed and ready at the point when the default dispatcher is created; settings were the last omission.
Famous last words. :-)
This name is a bit misleading, it should be test-durable-mailboxes, I'll fix it.
one missing backtick
What's the purpose of this test, to verify that messages are not dropped during normal circumstances?
If it's under normal circumstances I think that should be included in the test name as to not give readers the wrong impression.
please make the tags different, just for kicks
what about making this 1,000,000?
changed to 300000, which takes 5 minutes on my machine we should change back to something smaller if we decide to merge this to master
thats 5000 msgs/sec which go over remoting, which does not sound like an awful lot  we should benchmark this more
yes, but this is not a benchmark, it waits for each round, with expectMsg and its delay
Shouldn't we change both sides here?
Why the change at all?
yes, it should be changed on both sides, of course the reason is that it saves a line break, otherwise it must be:      var membersByAge: immutable.SortedSet[Member] =        immutable.SortedSet.empty(ageOrdering)
would it work on both JVMs if we passed in the spec itself and just used `spec.getClass`?
I have no idea, and I don't plan to test that on the PPC box today if I can avoid it ;)
could you give it a short try on MacOS? If it works, then it must work on all JVMs unless Im severely mistaken
does this actually help? (what does it do?)
why 1.3.0 only here?
ah, forget it
Yes and no, it forces maven to check for updated dependencies, but since we don't publish any real maven metadata for the snapshot repo, it won't pull down the new files.
what if there's an exception raised here?
This logic should be in the ChildrenContainer.
please avoid anonfuns, explodes the core lib
Could we try to address the code duplication here?
children foreach stop
I'm pretty sure it created a new TreeMap for every empty since it needs the Ordering. use a cached value for the empty.
I'd suggest:  childrenRefs.getByName(name) match {   case None => _actorOf(props, name)   case _ => throw new InvalidActorNameException("actor name " + name + " is not unique!") }
if (childrenRefs.isInstanceOf[TerminatingChildrenContainer]) dispatcher suspend this
Why this check here?
What happens if there's a failure here?
is on its way out anyway
you mean to the children? because otherwise its like a normal failure
then we should make sure that this actor gets suspended; good catch!
which means that it would need to be a nested class; is that worth it?
your fix is a nice one ;-) ah, if only we could compile with `-optimize` 
no, it isn't
Because otherwise NPEs and other funny things occur. We should only ever call preRestart() once on one Actor instance.
this should go inside `RemoteWatcher.props` same thing at other places that use props factory method pattern
you changed this to throw, please explain
returning an EmptyLocalActorRef from actorOf is not consistent with the other error cases, where we bail out with an exception
`akka.actor.serialize-creators` is defined in several reference.conf files Is the merge order of reference.conf files defined? hmm
it is in classpath order, but youre right: Ill add a test which just verifies that this setting was applied (in each of the projects)
ok, I looked at what kind of exceptions could be thrown here, and it is mostly from `localAddressForRemote` and that is in the IllegalArgumentException category, so all fine. Good!
yes that is a good solution
that is not necessarily a good idea in all cases
Ill leave it as is for now, because I dont see a clear down-side to my approach and the change is somewhat cosmetic, so it can also be done after RC1
well, for these internal actors, that are not intended to be remote deployed it must be better to define all props in the props factory, why not? of course I didn't mean "all" as in those in contrib
Good samples, but add the typical ask with onResult callback thing also
Which one do you mean?
Wrong as in "3chars" or wrong as in "the code block has wrong indentation"?
why calling postStop(), which is supposed to be called only exactly once in an actors life?
why calling preStart(), which is supposed to be called only exactly once in an actors life?
maybe add a paragraph here explaining that callbacks on that Future need to be careful not to close over the actors state
oh, I noticed that you just copied from Actor. That is wrong, but Ill create a ticket to fix that then. That means that your change is currently okay since it is consistent with the Scala side of things. Sorry for the confusion.
so, Viktor convinced me that writing that ticket is not actually the right thing to do.
It's an exact copy of what is done in Actor trait. Check with Viktor why it's done like that.  I guess the reason for having override of these methods in UntypedActor is for documentation, and make it clear what the the API of UntypedActor is.  On Fri, Dec 9, 2011 at 11:06 AM, Roland Kuhn < reply@reply.github.com > wrote:  > >     */ > > -  override def preRestart(reason: Throwable, lastMessage: Option[Any]) > {} > > +  override def preRestart(reason: Throwable, message: Option[Any]) { > postStop() } > > why calling postStop(), which is supposed to be called only exactly once > in an actors life? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/143/files#r282430 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
All fine then :-)
Isn't ActorCell an implementation detail that shouldn't be seen in log (exception) messages?
no, need to subclass in remoting
Make this return true/false if succeeded?
Make this return true/false if succeeded?
What does this do?
What does this do?
What does this do? 
What does this do? 
I'd probably avoid the extractor and instead in the default case do the check  case other => extraNames.get(s).getOrElse(super.getSingleChild(name)) or avoid the allocation of getOrElse using pattern match
"Null-object pattern" sounds more academic
Shouldn't we delegate this to the LocalProvider?
This path mangling logic seems to be rather spread out, couldn't we try to put it in one place?
I cannot imagine any meaningful use of this information: if it fails, something is wrong. Should I throw an exception instead? The distinction would be between favoring the old or new contents.
same as above
it pipes the exception back to the guy who was actually ordering that new actor, see `ActorSystem.actorOf`.
mine does the very same allocations and looks nicer  Shall I change it to avoid the allocation?
no docs, yet. docs would be for other ActorRefProvider implementers. do we care, yet? Ill add ScalaDoc anyway.  no racy: supposed to happen in ActorRefProvider.init(system). will be in docs.
Id call it net negative ;-)
thats part of the routing changes, to be completed.
hmm, I didnt do that because that one always gives the child the path `parent / name`, which is not right in this case.
well, its all here in this method, unless Im overlooking something. how can it be more local?
if !elems.isEmpty && elems.head == "remote" Never check size when the size is irrelevant :-)
refactored `ActorRefProvider.actorOf` to always expect an `ActorPath` and did the delegation.
Okay, wrote a whole novel in a comment explaining this.
Why is this turned off if all is fine and dandy?
Nice, but wouldn't it be more appropriate with "withRouter"?
Shouldn't this be dead letter queue?
Is there any JAva test for verifying that these work from Java?
easy: I just forgot about that ;-) all fine now.
I assume that the recipient will know how to deal with it, given the default just above.
none besides javap and common sense
Await.ready and Await.result is sweet!
What exception? Document.
what is the .history file?
Any exception detailing why the T couldn't be produced.
do we have a policy of repeating type parameters in initializers? (just askin)
lol, I can change that if you want to drop 6 chars here ;)
I guess this timeout does not need to be implicit anymore.
no, it really was just a question how much intention was behind this.
I'm not sure about the semantics (or perhaps just the naming) of orElse here.  The name 'orElse' suggests something similar to 'recover' but that obviously doesn't match up with picking the first completed future. It'd be nice if this were renamed, or perhaps modified to have the same semantics of Option.orElse, with the firstCompletedOf operation moved to a different method - maybe call it "race"? :).
I'm curious as to why the signature of ? still includes a timeout. Since it returns a Future, which will no longer time out, isn't the timeout superfluous?
Why does `Future` extend `Awaitable`?  I thought the goal was to make it actively harder for users to block on `Future` results?  With this mixin, it's impossible to have a `Future` which is *not* blockable.
You do realize the weirdness of not being able to wait for something to happen?
val l = new CountDownLatch(1) future onSuccess { case _ => l.countDown } l.await(Long.MaxValue, TimeUnit.DAYS)
That's a good point.  This really isn't the monadic `orElse`, nor can it be as `Future` does not have a zero.  This implies that you *cannot* modify this method to have the same semantics as `Option#orElse`.  With that said, I'm not sure what the name should be.  The idea of calling it "race" is amusing, and not entirely facetious.  Technically, that's what it is, and so "race" is more accurate than "orElse".
For those of us who just *need* the good old days of C++ pipelining
The implementation will likely be tweaked in M2 to be the first successful result or if both failed, the LHS exception as failure
What's wrong with this?  ```scala val f: Future[A] = ... Await.result(f, timeout) ```  I don't have a problem with providing a semi-convenient API for waiting (though I would rather if it mentioned kittens), I just don't want to make it trivially easy to call that function on a `Future` directly.  It's almost a psychological thing.  It's way too easy to block right now.  You want to carrot people to find a better way.  The few who really do need to block will dig deeper and find `Await.result`.  Those who don't need to block will find an alternative within `Future` itself (like a callback, or a `flatMap`).  The path of least resistance should be the most performant path.
I believe the proper name was `blockOnResultAndKillSevenInnocentKittens`.  :-)
It's already like:      val f: Future[A] = ...     Await.result(f, timeout)  Also, you cannot call "block" directly since it requires a private witness.
Why would there be a memory leak?
That doesn't make it any more `orElse`-ish.  :-)
Because we need to hold onto the backing Actor that gets created to serve as the sender of that message. And we need to hold onto it to service the reply message. So when do you throw that reference away? It can't be kept in a WeakHashMap since if the message gets forwarded or sent to a remote host, how would you ever know when it is eligible for destruction?
Depends on your definition, now doesn't it?
> Also, you cannot call "block" directly since it requires a private witness  Ah, I missed that part!  You may want to make `CanAwait` extend `NotNull` to avoid anyone sneakily (from Scala anyway) passing null for that parameter.
Yeah, I really cannot avoid people from aiming at their feet In any case you always have the issue with Reflection... ;)
> Depends on your definition, now doesn't it?  I think almost anyone who sees this and has *any* prior Scala experience is going to think of `Option#orElse`.  (that is, the monadic `orElse` function)  It's a reasonable name for what you're doing, the problem is that there is a prior squatter on that name and it has been given a specific semantic meaning.  People fiddling with Church numerals rarely call them "Int".  Names should convey meaning, and the meaning of names is very dependent on reference and context.  In this case, the context is Scala and the most immediate reference is `Option`.  This is analogous to what would happen if I were to write a charting library in Scala with geographic capabilities with a method called "flatMap" on the `GeoChart` class that smooths the topography.  It would be accurate, but highly deceptive.
Ah, that makes sense!  Ok, timeout it is.
We could always rename it to "or".
> We could always rename it to "or".  I would certainly prefer that.  It's going to throw the Lift guys for a loop.  Then again, Lift's naming conventions are quite inconsistent, so the association between "or" and "orElse-like semantics" is probably much weaker than with `Option`.  In any case, the Lift community is a strict subset of the Scala community, so "or" is an improvement over "orElse".
How about an alternate mechanism then: actor ! \[M[_], T\](m: M[T], toFulfill: Promise[T]): Future[T] - where the Future is simply the Promise that was passed in?   On second thought, I guess that doesn't really avoid the issue because you can't actually send the Promise to the remote host for fulfillment. It would have to be a different type of actor that was guaranteed to reply (probably with the result of the PF that forms the actor's handler.) 
Though, given that this proposed method would require two parameters, it really should be `sendPromise` or something like that.  Operators and n-ary functions don't mix.
Unfortunately, the more I think about it the more it wouldn't help my use case anyway because I'm using ? to send explicit blocking flushes to an actor that is responsible for managing a cache where non-flush messages are sent via !. If you wanted to mix the two on the same actor, you'd have to have an actor with two separate handlers - a responding handler and a fire-and-forget handler. The queue would then be of Either where Left requires a response and Right cannot be replied to.
Nice, I'd missed the private witness too. But I agree that using NotNull would be appropriate - for someone coming to the API without a full understanding of why blocking is bad, they may well try using null to circumvent the check.
Not only that, but I don't believe that Lift uses akka futures anyway. "or" is definitely better than "orElse" though I still like "race." 
Trust me, I have spent quite some time pondering these issues. The ask/? we have for 2.0 is the least of all evils.
Doesn't help at all:  scala> trait T extends NotNull defined trait T  scala> val x: T = null.asInstanceOf[T] x: T = null  scala> val x: T with NotNull = null.asInstanceOf[T] x: T with NotNull = null
The fact that they have to cast will at least be a signal that they're doing something wrong.  As you said, you can't prevent people from aiming at their own feet.  You can helpfully hint in the right direction though.  Inheriting from `NotNull` is essentially free, and it does a better job of declaring the intent of `CanAwait`.  I don't see any reason *not* to do it.
In Scala people already know that nulls are bad, which they don't know in Java-land, and your proposed fix doesn't deal with that anyway. I understand your point, but to me it's like wearing a helmet while swimming, it's just not worth the effort.
Feels more like wearing a swim suit that says "remember to breathe".  Certainly extra effort, but only slightly.  In this case, the extra effort is 16 characters.  Seems worth it, if only for the declarative aspect.
note to self: remove logLevel, it's wrong
Change: "Set to 1 for complete fairness." to "Set to 1 for as fair as possible"
Perhaps clarify why it doesn't matter
When are entries removed from here?
What does this do?
No, that is not the way. A PinnedDispatcher enforces that only one Actor can use it. "Dispatcher" does not do that.
Perhaps include a link to .j.u.c.BlockingQueue
This is not true anymore. You cannot specify any RejectedExecutionPolicy manually.  What will be used is: SaneRejectedExecutionHandler, it's a non-broken version of CallerRuns.
Is 5 still the default? (just making sure)
The new version of the BalancingDispatcher is not work-stealing, it's work-sharing, so all actors that use the same BalancingDispatcher will share message queues.
They are never removed, until Dispatchers is garbage collected together with the system. I don't think it's a problem to keep them around, since a Dispatcher is shallow, the actual ExecutionService is shutdown and removed.
ok, so we don't have a way to configure a PinnedDispatcher. I guess it's not much to configure. I'll change this to code sample instead, with newPinnedDispatcher
done, and several other links to j.u.c
Ok, then I remove all that and instead write:  When using a bounded queue and it has grown up to limit defined the message processing will run in the caller's  thread as a way to slow him down and balance producer/consumer.  correct?
yes, 5 is default in reference.conf
I think it's really bad, especially with PinnedDispatchers, since they create a new Dispatcher for every actor. We must fix this
ok, changed, in several places
AFAIK there's no way to configure a pinned dispatcher from conf. We might want to create a PinnedDispatcherConfigurator
PinnedDispatchers are not configured and therefore not placed here. It's only used for the lookup method. We might want something more advanced when we complete the #1458 fully.  I can unregister when the dispatcher is shutdown. Shall I add register and unregister methods that can be used for dispatchers created programmatically also?  On Tue, Dec 13, 2011 at 2:16 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -62,9 +69,30 @@ class Dispatchers(val settings: ActorSystem.Settings, > val prerequisites: Dispatc > > > >    val defaultDispatcherConfig = > settings.config.getConfig("akka.actor.default-dispatcher") > > > > -   TODO PN Shouldn't we fail hard if default-dispatcher is wrong? > > -  lazy val defaultGlobalDispatcher = > > -    from(defaultDispatcherConfig) getOrElse > newDispatcher("AkkaDefaultGlobalDispatcher", 1, MailboxType).build > > +  lazy val defaultGlobalDispatcher: MessageDispatcher = > > +    from(defaultDispatcherConfig) getOrElse { > > +      throw new ConfigurationException("Wrong configuration > [akka.actor.default-dispatcher]") > > +    } > > + > > +  private val dispatchers = new ConcurrentHashMap[String, > MessageDispatcher] > > I think it's really bad, especially with PinnedDispatchers, since they > create a new Dispatcher for every actor. > We must fix this > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/149/files#r289597 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
No, it's probably for the best that pinned dispatchers aren't registered there.  Open a ticket about making sure  that it gets cleaned up and assign it for 2.0, we might want to use a WeakHashMap or equivalent.
This also aligns with the ticket to specify dispatchers by id in props
WeakHashMap is bad for gc, but ticket is created #1494 I leave it with a FIXME refererence to that ticket.  On Tue, Dec 13, 2011 at 2:32 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -62,9 +69,30 @@ class Dispatchers(val settings: ActorSystem.Settings, > val prerequisites: Dispatc > > > >    val defaultDispatcherConfig = > settings.config.getConfig("akka.actor.default-dispatcher") > > > > -   TODO PN Shouldn't we fail hard if default-dispatcher is wrong? > > -  lazy val defaultGlobalDispatcher = > > -    from(defaultDispatcherConfig) getOrElse > newDispatcher("AkkaDefaultGlobalDispatcher", 1, MailboxType).build > > +  lazy val defaultGlobalDispatcher: MessageDispatcher = > > +    from(defaultDispatcherConfig) getOrElse { > > +      throw new ConfigurationException("Wrong configuration > [akka.actor.default-dispatcher]") > > +    } > > + > > +  private val dispatchers = new ConcurrentHashMap[String, > MessageDispatcher] > > No, it's probably for the best that pinned dispatchers aren't registered > there. > > Open a ticket about making sure  that it gets cleaned up and assign it for > 2.0, we might want to use a WeakHashMap or equivalent. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/149/files#r289614 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Add a sentence to understand from where the Cancellable comes from.  When you ask the ``Scheduler`` to run things in the future you get a handle to the scheduled task as a ``Cancellable``.
I think we should have a short Scala and Java example here.
Add:  You can schedule sending of messages to actors and execution of tasks (functions or Runnable).
uummm, what, don't you need to check the transport address? What if people write their own transports? I also see a big risk here, if people can't get it to "just work", it means that their code will be hardcoded to the transport being used.
Or rather, how can a LocalActorRefProvider have an external form with a protocol?
is that true? What if the transport just doesn't exist?
I think we must support omission of the desired protocol, mainly because having to manually specify protocol in code means that your application is no longer transparent in the remoting.  I propose to change so that it is configured with transport protocol to use by externalization, then the logic can be:  First, check currentTransport, if there is none, fallback to configuration default external protocol.
filter+map = collect (but it doesn't matter here, and now)
collect that sh!t
love that one! I also like this one: find + map = collectFirst   On Thu, May 30, 2013 at 1:07 PM, Patrik Nordwall <notifications@github.com>wrote:  > In project/Publish.scala: > > > @@ -102,6 +103,11 @@ object Publish { > >    def akkaCredentials: Seq[Credentials] = > >      Option(System.getProperty("akka.publish.credentials", null)) map (f => Credentials(new File(f))) toSeq > > > > +  def pluginPublishLocally(default: File): Option[Resolver] = > > +    Option(sys.props("publish.plugin.locally")) filter (_.toLowerCase == "true") map { _ => > > filter+map = collect (but it doesn't matter here, and now) > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1515/files#r4465613> > . >
One question I have:  Why do we stop the reader AND disassociate the handle, shouldn't that be the same thing? (Disassociate message going to the reader?)
This felt a bit safer than the original, does anyone disagree?
So the rationale here is that I assume that calling disassociate on the handle will send the Disassociate message to the reader...
Don't use this one, use com.eaio.Uuid
Why do we need AMQPActor at all? It could send itself the Connect message in its constructor or in preStart
Why have Settings extends Settings?
If you scrap AMQPActor, fold class Settings into object Settings and rename it to AMQP you can hide the:  context.actorOf(Props(new FaultTolerantConnectionActor(cr.connectionParameters)), "amqp-" + UUID.randomUUID.toString)  Take a look at the TypedActor extension for more inspiration.
How do they use Seq[Address] from Java?
Stopped here for the day, will continue tomorrow :-)
Great, thanks again for the review, will address them ASAP.  On Mar 13, 2012, at 10:14 AM, viktorklang wrote:  >> +   Needed for Java API usage >> +  def this(exchangeParameters: ExchangeParameters, channelParameters: ChannelParameters) = >> +    this(Option(exchangeParameters), None, Option(channelParameters), None) >> + >> +   Needed for Java API usage >> +  def this(exchangeParameters: ExchangeParameters, returnListener: ReturnListener, channelParameters: ChannelParameters) = >> +    this(Option(exchangeParameters), Option(returnListener), Option(channelParameters), None) >> + >> +   Needed for Java API usage >> +  def this(exchangeParameters: ExchangeParameters, returnListener: ReturnListener, channelParameters: ChannelParameters, errorCallbackActor: ActorRef) = >> +    this(Option(exchangeParameters), Option(returnListener), Option(channelParameters), Option(errorCallbackActor)) >> + >> +   Needed for Java API usage >> +  def this(exchangeParameters: ExchangeParameters, channelParameters: ChannelParameters, errorCallbackActor: ActorRef) = >> +    this(Option(exchangeParameters), None, Option(channelParameters), Option(errorCallbackActor)) >> +} >  > Stopped here for the day, will continue tomorrow :-) >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/370/files#r552733
Good question...  The short and probably incorrect answer is that they could use scalaj-collection, create a list and use .asScala.  I made this change based on an earlier comment.  Originally this was an Array[Address], but the mutability of an Array was distasteful.  Can you suggest a happy middle ground?  
Use java.lang.Iterable[Address] and use scala.collection.JavaConverters to turn into Seq
Java, why do you hate programmers so much?
Will do.  Thanks!  I'm glad I rejoined the world of software dev after Scala came onto the scene.  I basically went from C 20 years ago, skipped everything in between, then Scala :-)
I started programming professionally in C, like 14 years ago. It's a fun little language :-)
You'll need to add a constructor that only takes the message, there'll be no constructor permutations added.
Why a Settings for each FaultTolerantChannelActor?
Remove replyTo here
replace line 28-34 with:      val shutdownListener = {       val replyTo = self       new ShutdownListener { def shutdownCompleted(cause: ShutdownSignalException): Unit = replyTo ! ChannelShutdown(cause) }     }  No need to store replyTo as a field
I'd avoid the for comprehension here and instead do the check in the guard of the match.
What happens with messages that are in the mailbox between the current message and the new Start message?
you're closing over "self" here.
Just make sure you're not leaking it to another thread.
where are you delegating to the parent class?
What happens with messages in the mailbox between the ChannelShutdown message and the Start message being processed?
What happens with messages in the mailbox between the Failure message and the Start message being processed?
Why not delegate to super.postStop?
What's this used for?
Why get the extension every time and not cache it if you need it 3 times?
I still think "Settings" is a too vague name, and that it should be named AMQP
Why not instead create children that represents each channel? Then the ConnectionActor can be in charge of supervising them
Is this method safe to close over?
I'd strongly suggest you Let It Crash(tm) and have postRestart and preRestart set up the connection.
Why not delegate to super?
You are closing over sender here, in a future callback
No kidding.  At least my 11 years at Sun were mostly on the infrastructure side :-)
this is pretty much cleaned up I think.  I've got a newConnection method with a bunch of Java supporters.  It may be nice to add methods for newProducer and newConsumer as well.  Will wait for feedback on the current work.
to avoid this compilation message.  I'm probably missing something obvious here, but I'm not sure what it is:  [info] Compiling 1 Scala source to /Users/jstanford/devel/github/akka/akka-amqp/target/classes... [error] /Users/jstanford/devel/github/akka/akka-amqp/src/main/scala/akka/amqp/FaultTolerantChannelActor.scala:126: could not find implicit value for parameter executor: akka.dispatch.ExecutionContext [error]       channel = Some(Promise.successful(ch)) [error]                                        ^ [error] one error found 
Well, adding another field in the class is not optimal at all. And also you're saying that the Promises' callbacks should be executed on the default dispatcher, which is probably not what any user would expect.  just do:  Promise.successful(ch)(context.dispatcher) if it's only needed in one place,  or do:  import context.dispatcher  at the most appropriate places otherwise.  If that's the only place that needs 
fixed in next push
Now AMQP, but not clear on what I would do.  Pull the settings out into an object?
Yes, I think I'm going to tackle this next.  Hopefully this will address a few of the questions about what happens to messages when channels restart.  
fixed in next push
fixed in next push
Do you mean something like this?  case Acknowledge(deliveryTag)  channel match {       case Some(cf) if cf.isCompleted  cf.map(c => acknowledgeDeliveryTag(c, deliveryTag, true))       case _          sender ! Status.Failure(new AkkaAMQPException("consumer " + self + " could not acknowledge message - channel not available"))         self ! Start     }  One of the reasons I was doing it the way it's done currently was to compensate for the slow channel setup.  I think this will all get better when the channel is wrapped in an actor and we can start it in prestart/prerestart.  As long as the actor is alive, the channel should be there...
oh, good question.  seems like we might have a bunch of failures and start messages.   This is another case of channel missing, so channel in an actor should help.
cleaned up in next push
comment corrected in next push
was concerned about the mutability of channel.  maybe was overkill.  removed.
going to rework channel into an actor
going to rework channel into an actor
fixed in next push
will look into this
will fold the resolution into the LetItCrash(tm) approach
What is this needed for?
What is this needed for?
Add explicit return type
{   val p = Props(new FaultTolerantConnectionActor(params))   if (name.isDefined) context.actorOf(p, name.get) else context.actorOf(p) }
I don't agree that:  delivery.payloadAsArrayBytes  is better than  delivery.payload.toArray
You might want to put these messages inside the AMQP object, and instead of having the getInstance method, offer a java method:      object AMQP ... {         case object Start ...         def start(): Start.type = Start     }  which in Java allows for:  import static akka.amqp.AMQP.*;  if(msg == start()) ...
Don't use JavaConversions, use JavaConverters
is this java.lang.Iterable? If so, import it at JIterable so it's obvious that it's not scalas Iterable.
How do you provide this from Java?
use explicit return types for _all_ getInstance() methods please
I think once the channels are children, this will clear up a lot of code.
Isn't it weird that the user cannot control which dispatcher is used? And that the same dispatcher is used for all AMQP consumers?
Same here WRT dispatcher id. Why isn't it in the ProducerRequest?
Btw, what's the reason for not connecting right here?
you could use `with ActorLogging`
why not getContext().getSystem()? I know its a slippery slope, though 
up to four only
Good tip. I added that as an alternative.  On Tue, Dec 13, 2011 at 12:42 PM, Roland Kuhn < reply@reply.github.com > wrote:  > > @@ -0,0 +1,59 @@ > > +package akka.docs.event > > + > > +import akka.actor.ActorSystem > > +import akka.testkit.AkkaSpec > > +import akka.actor.Actor > > +import akka.actor.Props > > + > > +object LoggingDocSpec { > > + > > +  #my-actor > > +  import akka.event.Logging > > + > > +  class MyActor extends Actor { > > you could use `with ActorLogging` > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/150/files#r289403 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
We don't have getSystem in UntypedActorContext. Duplicating all methods feels wrong.  On Tue, Dec 13, 2011 at 12:44 PM, Roland Kuhn < reply@reply.github.com > wrote:  > > + > > +  @Test > > +  public void useLoggingActor() { > > +    ActorSystem system = ActorSystem.create("MySystem"); > > +    ActorRef myActor = system.actorOf(new UntypedActorFactory() { > > +      public UntypedActor create() { > > +        return new MyActor(); > > +      } > > +    }); > > +    myActor.tell("test"); > > +    system.stop(); > > +  } > > + > > +  #my-actor > > +  class MyActor extends UntypedActor { > > +    LoggingAdapter log = Logging.getLogger(getContext().system(), this); > > why not getContext().getSystem()? I know its a slippery slope, though  > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/150/files#r289411 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
why different order of the parameters compared to `apply`?
No good reason, apart from just adding one parameter here, and `apply` mimicking the single class version.
what precisely is this good for?
Are you asking about `me` or why I made it `private`?
I'm asking whether this line has any effect, because if it has then it lacks a comment.   The private val me =  is redundant unless some other code accesses this field using reflection. 
So the `me` is a field in the `TypedActor ` class that holds the user implementation of the typed actor.
When is this GCed/released?
In actor `postStop` just like a local `TypedActor`
oh, I see now which line I missed; everything is fine
why do you use Runnable, instead of ordinary function?
ouch, in case parent is or becomes a final field this will not work. VERY strange things will happen. I don't like.
Looks like you forgot to do the same change in UntypedActor. This is second time this happens. What shall we do? Remove the 4 overridden lifecycle methods in UntypedActor? The only reason I see that they are there is for documentation of the UntypedActor API, but it's not worth it when it is so easy to do misstakes like this.
why not match on path.parent instead of if-else
because I wanted to have a class I could give a name to ;-)
okay, I am changing it to volatile var: https:www.assembla.com/spaces/akka/tickets/1504
LOL, I have heard that functions can have names also  On Wed, Dec 14, 2011 at 11:34 AM, Roland Kuhn < reply@reply.github.com > wrote:  > > +    def run = { > > +      val iter = heap.entrySet.iterator > > +      while (iter.hasNext) { > > +        val soul = iter.next(); > > +        deathWatch.subscribe(Locker.this, soul.getKey)  in case > Terminated got lost somewhere > > +        soul.getKey match { > > +          case _: LocalActorRef =>  nothing to do, they know what they > signed up for > > +          case nonlocal         => nonlocal.stop()  try again in case > it was due to a communications failure > > +        } > > +      } > > +    } > > +  } > > + > > +  private val heap = new ConcurrentHashMap[InternalActorRef, Long] > > + > > +  scheduler.schedule(period, period, new DavyJones) > > because I wanted to have a class I could give a name to ;-) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/151/files#r292649 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
why not just make them all call super.<whatever>?
yes, good point
On Wed, Dec 14, 2011 at 11:47 AM, Roland Kuhn < reply@reply.github.com > wrote:  > >     */ > > -  def preRestart(reason: Throwable, message: Option[Any]) { postStop() } > > +  def preRestart(reason: Throwable, message: Option[Any]) { > > why not just make them all call super.<whatever>? >  That's fine, but scala docs needs to be updated anyway.   > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/151/files#r292680 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Add comment explaining why this is here
Same as above
isn't this an interval and not a period?
might be faster to do:  val newRefs = childrenRefs - child.path.name if (newRefs ne childrenRefs) {   childrenRefs = newRefs   ... }
Is this method documented to be potentially blocking?
I dont fully follow, but changed it to `reaper-interval` anyway.
premature  whatever.
now it is
this is what I get for using `while`
here as well
this is the only thing indicating that the server has hit its capacity for accepting incoming connections, so it should be logged at warning level (the one above is fine, though)
ok, I thought that the signal on the next line was enough
this is similar to the above: there is nobody on our side which asked for it, so nobody gets notified
should it then be logged with full exception (stack trace)?
connections can be opened by client request (which is covered by CommandFailed) or upon incoming connections (which is not)
hmm, thinking a bit more, this can probably also fail if the connection was reset by peer in the meantime; not so sure anymore
changing it back to `log.error(e, "Accept error: could not accept new connection")`
good to get rid of that
FYI, we have ticket #1390, for later
I think this is wrong, we should use default location of configuration, as defined by the config lib, i.e. application.conf, which will be used if you create the ActorSystem with plain ActorSystem(name)
ah, you want to read the name from config before creating the system. Is that setting really necessary? Could it be an option main arg instead?
when we use extensions instead of Bootable, I think the idea is to not have shutdown hook in extensions, but let the extension registerOnTermination callback if needed
Perhaps include -XX:OnOutOfMemoryError='"kill -9 %p"'  
Agree with Patrik here
Could this be tackled as well here? https:www.assembla.com/spaces/akka/tickets/1085
Yes, just copying the existing setup for now
Yep, was going to ask if we were switching to the default application.conf. Saw that there was still an akka.conf in config dir so just kept it compatible with previous microkernel. Can switch over.
Not necessary. Just easy to put it in the config as akka.conf was being read first anyway. Think easiest is to just move the creation of the actor system into the boot class - user can do whatever there, even have multiple actor systems.
The runtime shutdown hook is for catching interrupt signals (like ^C on command line) and then doing a graceful shutdown by calling ActorSystem.shutdown.
For Microkernel this might make sense. Worst case it's preventing things from being GC:d
I would expect the booted actor systems to stay up for whole the lifetime of the JVM when using the microkernel.
Okay, good idea. Will add the out of memory handler.
Yes, can look at #1085. It's easy enough to change the jvm opts but we could detect the jvm first.
Spend at most 15-20 mins on it, if it takes longer it can stay in the backlog for M2
yes, use default application.conf  On Wed, Dec 14, 2011 at 8:07 PM, Peter Vlugter < reply@reply.github.com > wrote:  > > +trait Bootable { > > +  def startup(system: ActorSystem): Unit > > +  def shutdown(system: ActorSystem): Unit > > +} > > + > > +object Main { > > +  val quiet = getBoolean("akka.kernel.quiet") > > + > > +  def log(s: String) = if (!quiet) println(s) > > + > > +  def main(args: Array[String]) = { > > +    log(banner) > > +    log("Starting Akka...") > > +    log("Running Akka " + ActorSystem.Version) > > + > > +    val config = ConfigFactory.load("akka.conf") > > Yep, was going to ask if we were switching to the default > application.conf. Saw that there was still an akka.conf in config dir so > just kept it compatible with previous microkernel. Can switch over. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/152/files#r294207 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
I'll leave it for the user's boot class to start the system, giving complete control there. Will change empty config/akka.conf to config/application.conf.
ah, it is an interesting idea with the boot classes, then boot classes are not same as extensions, as we sketched (#1390), but that is fine, it makes a lot of sense to be able to start several systems  On Wed, Dec 14, 2011 at 8:31 PM, Peter Vlugter < reply@reply.github.com > wrote:  > > +trait Bootable { > > +  def startup(system: ActorSystem): Unit > > +  def shutdown(system: ActorSystem): Unit > > +} > > + > > +object Main { > > +  val quiet = getBoolean("akka.kernel.quiet") > > + > > +  def log(s: String) = if (!quiet) println(s) > > + > > +  def main(args: Array[String]) = { > > +    log(banner) > > +    log("Starting Akka...") > > +    log("Running Akka " + ActorSystem.Version) > > + > > +    val config = ConfigFactory.load("akka.conf") > > I'll leave it for the user's boot class to start the system, giving > complete control there. Will change empty config/akka.conf to > config/application.conf. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/152/files#r294307 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Actually, #1085 is for the windows script as well. I don't have a way to try things out on windows right now. Won't look at it now.
I think every ActorSystem instance should have its own counter, right?
if you make it a final val without type ascription it'll be the constant instead.
Instead of the "magic value" put a field in Timestamp that has a more proper name for the semantics of this value
well, that would be difficult, but your question got me thinking; we don't need a counter at all a17cb07
bonus points for remembering osgi, please update config to 1.0.2
very good to have this check in place
that's an useful operator
was this fixed in new protobuf version?
Sure, I'll do that.
It seems so. It no longer generate the `private Builder(BuilderParent parent)` version.
looks correct to me:      def fromLessThan[T](cmp: (T, T) => Boolean): Ordering[T] = new Ordering[T] {       def compare(x: T, y: T) = if (cmp(x, y)) -1 else if (cmp(y, x)) 1 else 0
Doesn't getOrElse allocate a thunk for each invocation?
That is an interesting question. One would think so, but the jvm does some awesomeness here. I think it is escape analysis that does its job.  Here is a Thyme run with the two ways (only difference in code is replace of getOrElse with get):      scala> :paste      Entering paste mode (ctrl-D to finish)          import akka.cluster._         val addresses = (1 to 1000).map(n  Address("akka.tcp", "sys", "node-" + n, 2552)).toVector         val rnd = new scala.util.Random(0)         val shuffledAddresses = rnd.shuffle(addresses)          import scala.collection.immutable.TreeSet          val addressOrdering2: Ordering[Address] = Ordering.fromLessThan[Address] { (a, b)             cluster node identifier is the host and port of the address; protocol and system is assumed to be the same           if (a eq b) false           else if (a.host != b.host) a.host.get.compareTo(b.host.get) < 0           else if (a.port != b.port) a.port.get < b.port.get           else false         }          val th = ichi.bench.Thyme.warmed(verbose = print)         val w1 = th.Warm {           val sorted = TreeSet.empty[Address](Member.addressOrdering) ++ shuffledAddresses           sorted.head         }         val w2 = th.Warm {           val sorted = TreeSet.empty[Address](addressOrdering2) ++ shuffledAddresses           sorted.head         }       Exiting paste mode, now interpreting.      Creating Thyme instances and warming busywork methods...done in 2.94 s     Warming up benchmarking...done in 21.36 s     Warming up head-to-head benchmarking...done in 51.52 s     Warming up computational complexity benchmarking...done in 100.56 s     import akka.cluster._     addresses: Vector[akka.actor.Address] = Vector(akka.tcp:sys@node-1:2552, akka.tcp:sys@node-2:2552, akka.tcp:sys@node-3:2552, akka.tcp:sys@node-4:2552, akka.tcp:sys@node-5:2552, akka.tcp:sys@node-6:2552, akka.tcp:sys@node-7:2552, akka.tcp:sys@node-8:2552, akka.tcp:sys@node-9:2552, akka.tcp:sys@node-10:2552, akka.tcp:sys@node-11:2552, akka.tcp:sys@node-12:2552, akka.tcp:sys@node-13:2552, akka.tcp:sys@node-14:2552, akka.tcp:sys@node-15:2552, akka.tcp:sys@node-16:2552, akka.tcp:sys@node-17:2552, akka.tcp:sys@node-18:2552, akka.tcp:sys@node-19:2552, akka.tcp:sys@node-20:2552, akka.tcp:sys@node-21:2552, akka.tcp:sys@node-22:2552, akka.tcp:sys@node-23:2552, akka.tcp:sys@node-24:2552, akka.tcp:sys@node-25:2552, akka.tcp:s...     scala> th.pbenchWarm(w1)     Benchmark (140 calls in 2.860 s)       Time:    582.1 us   95% CI 565.0 us - 599.3 us   (n=19)       Garbage: 2.344 us   (n=3 sweeps measured)     res0: akka.actor.Address = akka.tcp:sys@node-1:2552      scala> th.pbenchWarm(w2)     Benchmark (140 calls in 2.911 s)       Time:    593.3 us   95% CI 563.7 us - 623.0 us   (n=20)       Garbage: 2.344 us   (n=3 sweeps measured)     res1: akka.actor.Address = akka.tcp:sys@node-1:2552      scala> th.pbenchWarm(w1)     Benchmark (140 calls in 2.891 s)       Time:    577.9 us   95% CI 559.1 us - 596.7 us   (n=19)       Garbage: 2.344 us   (n=3 sweeps measured)     res2: akka.actor.Address = akka.tcp:sys@node-1:2552      scala> th.pbenchWarm(w2)     Benchmark (140 calls in 2.986 s)       Time:    592.5 us   95% CI 571.6 us - 613.4 us   (n=19)       Garbage: 2.344 us   (n=3 sweeps measured)     res3: akka.actor.Address = akka.tcp:sys@node-1:2552      scala>  w1 is the original code w2 is the "optimized"  Escape analysis is enabled by default in Java SE 6u23 and later. This is the result of the same code with escape analysis disabled (-XX:-DoEscapeAnalysis):      scala> th.pbenchWarm(w1)     Benchmark (300 calls in 3.486 s)       Time:    652.2 us   95% CI 631.9 us - 672.5 us   (n=20)       Garbage: 3.125 us   (n=4 sweeps measured)     res5: akka.actor.Address = akka.tcp:sys@node-1:2552      scala> th.pbenchWarm(w2)     Benchmark (140 calls in 2.970 s)       Time:    588.0 us   95% CI 565.5 us - 610.5 us   (n=20)       Garbage: 1.563 us   (n=2 sweeps measured)     res6: akka.actor.Address = akka.tcp:sys@node-1:2552      scala> th.pbenchWarm(w1)     Benchmark (300 calls in 3.485 s)       Time:    648.4 us   95% CI 629.5 us - 667.3 us   (n=20)       Garbage: 2.344 us   (n=3 sweeps measured)     res7: akka.actor.Address = akka.tcp:sys@node-1:2552      scala> th.pbenchWarm(w2)     Benchmark (140 calls in 2.989 s)       Time:    593.2 us   95% CI 570.1 us - 616.2 us   (n=20)       Garbage: 1.563 us   (n=2 sweeps measured)     res8: akka.actor.Address = akka.tcp:sys@node-1:2552 
Interesting: never trust _anything_  (okay, Akka team excluded ;-) )
What happens if this is actor is Restarted?
Can we get an ABA problem here?
An idea: create a child actor to autoDown for each node to watch, then you don't need the scheduler not keeping track of things here. Wdyt?
for a restart it will just start over, the worst thing that can happen is that the downing is delayed  I have not seen a problem, but please clarify if you have a scenario in mind  What would such child actora do, except waiting for receive timeout?
I suspect spawning a child might make the code smaller and with less vars.
I explored the idea (wrote code), and it just gets more complicated, thanks anyway
this assumes a deprecation PR against 2.2, right?
This doesn't explain what it's used for, what is a router and what does it do?
This I do not like, we shouldn't special-case this. it should be always   implicitly[ClassManifest[T]].erasure.asInstanceOf[Class[_ <: RouterConfig]].newInstance
Specialcasing is bad
This isn't true, you use the same actorOF, but different Props
only needs one "configuration"
only needs one "configuration"
Actors are created by passing in a ``Props`` instance into the ``actorOf`` factory method.
I just copied the docs from the method above. I think you have to go through ScalaDoc for the whole class then...
Tried that first. It doesn't work. You can't instantiate a Router class like using empty constructor. 
So what do you suggest? That gives us: withRouter[RouterType]?  I could add a special set of case classes for this config usage only, if you want. One for each router type. 
I didn't write it. Fix it, boy scout. 
Right. Bad english. 
Can change it.
I can of course fix it for you. I'll do it. 
Check if it has a zero-args constructor and invoke that, if it hasn't throw the exception. You can use ReflectiveAccess for this
Can't you just use the FQN? Or provide the same type of iface as the DispatcherConfigurator?
It's not super-important, but there isn't any way right now to specify your own router impl in the config, right?
Boy scout rule says that the first one to see it fixes it. I think it was you :-)
??? They don't have a zero arg constructor. I know that. Don't want to mess with ReflectiveAccess for this. Then matching on class is much nicer. 
No, boy scout says that you should leave the code in better shape than how you found it ;)
It was in my old impl, but not anymore. Roland removed that. 
But this hardcodes routers, which makes it impossible for users to create their own routers, no?
FQN? You mean I should get the name of the class and match on the string? Are you serious? 
No, I'm saying don't pattern-match on the class, take the manifest, get the erasure, check if the class is assignable from Router, if it is, try to find a no-args constructor, if there is one, invoke that one and return the new instance, if there isn't, throw an exception, done.
But thought it was fine. If don't like it then change it to something you like better.  On Wed, Dec 14, 2011 at 3:25 PM, viktorklang <reply@reply.github.com> wrote: >>   */ >>  def withRouter(r: RouterConfig) = copy(routerConfig = r) >> + >> + /** >> +  * Returns a new Props with the specified router config set. > > No, boy scout says that you should leave the code in better shape than how you found it ;) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/155/files#r293162    --  Jonas Bonr CTO Typesafe - Enterprise-Grade Scala from the Experts Phone: +46 733 777 123 Twitter: @jboner
these should be parentheses instead of braces
what is this good for?
We should open a ticket to move this option into the actual dispatcher conf
We should have a line of whitespace between each attribute, because icky to read otherwise
should DEBUG be the default??
it's a bit contradictory, at first it says 2552 and then 2562?
Misstake, thanks.  On Wed, Dec 14, 2011 at 3:41 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -51,6 +51,7 @@ class DeployerSpec extends > AkkaSpec(DeployerSpec.deployerConf) { > >    "A Deployer" must { > > > >      "be able to parse 'akka.actor.deployment._' with all default > values" in { > > +      println(system.settings.toString) > > what is this good for? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/156/files#r293212 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
This isn't default values. It's an example of a custom user specified configuration.
same thing here, read it in context
Is this a sensible default?
Hey, this has nothing to do with default values. It's examples of user defined configuration.
My point is that we must expect users to copy-past config at times, so having examples which make for good defaults might be preferable.
This default is about as good as any, probably. Most people will get four to eight threads, and if you are running on a big box you definitely WANT to tune this.
yeah, that sounds about right
But that bike-shed has still some bare wood shining through in the bottom left corner: what about indenting the comments to match up with the equals sign (or whatever) and this way make the actual settings stick out?  Doesnt need to be done now, of course. Just a thought.
We should make the TestLatch an Awaitable
perhaps change all 2000 milliseconds to either "2 seconds" or reuse some "test-timeout"-value?
Good idea, I'll create a ticket for it, since Awaitable isn't in master yet.
use:  .. note::
See the line below. 
akka-amqp doesn't exist in 2.0-M1
akka-amqp doesn't exist in 2.0-M1
Or is this a reST comment?
Yeah. .. is comment. So I commented them out just because it is not in 2.0-M1. 
Sure. I'll fix. 
Also add that we might backport it to 2.0
Add also that it might get backported.
second Alright sounds boring
an Akka Actor
If you do want italics with the `_will_` I think it needs to be `*will*`. Or `**will**` for bold
Just tried the actual generated docs. This example comes out a little strange because of the different indenting. I think as two blocks is better for this:      .. includecode:: code/TypedActorDocSpec.scala#typed-actor-supercharge      .. includecode:: code/TypedActorDocSpec.scala#typed-actor-supercharge-usage
prefer log.info to avoid outputs when running tests
Perhaps use val r: ActorRef = ... similar to what you have done below
Might be good to clarify that a typed actor instance will only execute one method invocation at a time, in the same way as actors process one message at a time.
Why lazy val?
because I need to get this only once, and `system` is not available in the constructor
This creates a val, no? i.e. is retained until the ref is GC:ed?
Doesn't an extra level of indirection cost?
I.e. both in changed interface for users and in overhead?
This can be made public now IMO
Hmmm, doesn't deadLetterMailbox belong in Mailboxes?
Yes, just like about every other object we create; why are you so worried about this? If someone leaks Akkas objects then theyll leak Akkas objects. Every ActorCell links to its parent  Id much rather remove those ugly hacks and not muck with final fields than introduce one more such hack.
Passing in the Configurator makes sense for the API, it was either this or passing in the Config in addition. The prerequisites are used only during construction of things, not per message, so saving one level of indirection sounds premature. We can put the individual things into fields if you prefer.
who would want to call it?
hmm, maybe it does, will investigate
is this line within the pdf line width?
two space, one at the end of previous line and one here
Why do we have defaults here that doesn't do anything with the `mailbox-type` above?
Users that create their own MessageDispatcher implementations?
they should call it from within their dispatcher then, no?
This section contains the default values for all mailbox types we provide. It also happens to double as the description of the default mailbox.  I discovered the need for default values when writing the tests for requirement-driven mailbox selection.
perhaps define `""` as a constant somewhere, `NoMailboxRequirementGiven`
should it be public?
Hasn't some config properties moved? Should that also be mentioned here?
yes, you are right
Are these special mailbox-types explained in the docs somewhere?
Good point Bjrn, perhaps would make sense to scrap them now when we have a lot of different mailboxes and instead require putting the FQCN instead?
+1 Very good writeup.
these are compat bridges for existing configs, will add a TODO to remove them for 2.3 (and add them to migration docs)
Is this sound?
nobody raised their voice, you liked it, tests pass 
no matter whether its true: I still dont _feel_ guilty for scalariforms crimes; fixed anyway
can also (better?) be written as `if`
This `systemService` flag, was it used before the `LocalScope` change? What does it mean?
its the difference between actorOf and systemActorOf; among other things it bypasses remote deployment
why with and without `EndpointWriter.` prefix?
Would it be possible to use FlushAndStop + Death Watch instead of StopReading + StoppedReading (with death being a more robust way of detecting that reading has stopped?)
is `ep` endpoint? then I think you should spell it out, since same is used two lines below
what happens if we are in Handoff right now?
why the round-trip? AFAICS this is not synchronized with anything else, hence it feels like sleep 100ms to make sure that all sheep are in
Are you gonna blame Eclipse for this hideous indentation? :wink: 
no, this comes from scalariform in our build
Ah, sorry about that. Not your change initially. It would be nice to be able to fail the PR-validator if scalariform did any changes during the build.
Where is this used in this method?
I am not sure I understand this formula.  Isn't seenSize is the number of nodes potentially sending to the seenDiff amount of potential candidates?
thx, residue after refactoring
I started with the formula you suggested in the ticket, which corresponds to seenDiff = 1 (one candidate left) Do you prefer `seenSize / seenDiff`?
Yes, that is probably better. This is the probability that the node will gossip to exactly one random straggler, or multiple ones?
ok, makes sense, thanks
Can you refactor it like this: GossipDifferentViewProbability * stragglerCount / max(upToDateNodes - smallClusterBias, 1) where stragglerCount = seenDiff and upToDateNodes = seenSize I feel a bit more comfortable with this form. (Sorry for changing my mind all the time ;) )
I don't think that is good, the result should be less than GossipDifferentViewProbability (< 1.0) e.g. 3 stragglers in a 10 node cluster
Then you get P * 3 / 7 (ignoring the bias factor now). This keeps the expected amount of gossips to any individual straggler at constant. But yes, this can actually increase the resulting probability if there are more stragglers than up-to-date nodes. We can cap it though. Maybe @rkuhn wants to comment on this? 
(in other words, my mind is melted down and I trust my basic arithmetic skills even less than before)
to be strict, shouldn't host and port be optional fields? now None is serialized as "" and materialized as Some("")
JavaConverters should be used instead of JavaConversions JavaConversions is too much invisible magic 
here is one example of the unwanted magic; here you should probably iterate the java way instead of converting the collection
Yes, that is correct, I just didn't question the way it was before. Let me look at it.
Is the collection really converted? I thought it just pulled out a Scala iterator wrapping a Java iterator.
Ok, didn't know we had that policy. Will change.
please verify that that is the case (and use `.asScala`)
So we really shouldn't get an Address without host/port here, since you need remoting to be part of the cluster. Now throws an exception on failure.
How do I obtain a reference to an actor on another machine?
You should also mention that you need to add akka-remote as a dependency
There are quite some examples in the tests for the remoting, this for example:  system.actorFor("akka:remotesys@localhost:12346/user/echo") ! "ping"
could you instead create another actor, watch it, send it a PoisonPill, and awaitTermination?
why not speed it up by using 1.second?
`uid.exists(_ == receivedUid)`
Yes, sorry, originally I had a 1s gate installed so the retry interval needed to be higher.
Yes, that could work!
trying to understand this so this is a trick to create an EmptyLocalActorRef to be able to test invalid refs? what defines an invalid ref? can that be created in some other way? I don't like that we use internal api, if not needed.
what caused this to change?
> so this is a trick to create an EmptyLocalActorRef to be able to test invalid refs?  yes  > what defines an invalid ref?   an actor ref that doesn't point to an existing actor instance. For the test it's only important that the empty ref's path is the same as that of an actor created immediately afterwards, so that it can be resolved by a channel.  > can that be created in some other way?  before, I created it with `actorOf` but the problem here is that I cannot stop that actor and wait for its termination as it will not let me reliably create an actor with the same name. I could run all these tests inside a special purpose actor but think that it complicates the test much more than needed. Can you image another way of creating an invalid actor ref?  > I don't like that we use internal api, if not needed.  agree, but I felt it was the best compromise in this case.
That's a mistake, will undo.
ok, I have no other suggestion than using another actor that creates/stops the child I agree that it would complicate the test. I think this is alright.
Await.result(democratsResult, 1 seconds) must be === 3
Await.result(republicansResult, 1 seconds) must === 2
Then you can scrap these
should probably document that it replies on the same thread, might be useful to know that ask() does not actually lead to blocking.
Yes, I will add that. Thanks.
Why are you using the name only? Shouldn't it be the whole path, or the ActorRefs? It's not mandatory to use same parent for the router as the routees, or is it?
oh, good catch: why not just _routees?
I used the name instead of the ActorRef because I wasn't sure which one was better. We have both representations internally in the RemoteActorRef and I was unsure which one would make most sense to expose. I'll update to return the refs instead.
why is this faster than match? (just trying to learn)
I think the naming here is a bit strange, this should be named v1 (it is a VectorClock)
rename one => seen1, and another => seen2
Because in this new version I just want to check Some and then v1 or v2 depending on the value, so my options would have been:      v1 tryCompareTo v2 match {       case None                    Do nothing (pointless branch       case Some(x) if x > 0  builder += ((node, v1))       case _                         builder += ((node, v2))      }   or        v1 tryCompareTo v2 match {       case Some(x)  builder += ((node, if (x > 0) v1 else v2))       case _               Pointless branch, do nothing      }  or      v1 tryCompareTo v2 match {       case Some(x) if x > 0  builder += ((node, v1))       case Some(x)              builder += ((node, v2))       case _                           Pointless branch, do nothing      }   And I didn't want to use Option combinators to avoid allocation of closures.  Like:      v1 tryCompareTo v2 foreach {       case x if x > 0 => builder += ((node, v1))       case _ => builder += ((node, v2))     }  or      v1 tryCompareTo v2 foreach { x => builder += ((node, if (x > 0) v1 else v2)) }  Prefer this one, but allocates closure? :(
I mean v2 for this one and v1 for oneNode 
I see, makes sense
So the idea is to run the polling on another dispatcher and run each connection with it's own thread? Do the connection actors do any blocking ops? Otherwise it might make sense to have a dispatcher with only 1 thread that runs a set of connection actors.
afaik the only blocking operation was the polling, so it can be a dispatcher with 1 thread that runs a bunch of socket actors.  This duplicates some functionality that is already in zeromq,  `params.context` is a zeromq context that actually holds the I/O threads and that one also takes a thread already.  So with the current solution for a single socket we actually occupy 3 threads, which leaves a bit of a bad taste. if a dispatcher is shared it would already be a somewhat less dirty
Yeah, that's no cool.  Why not do the polling on the same dispatcher as the connection actor, since that's already got a dedicated thread?
because that thread would block until the poll expires or a message comes in so you wouldn't be able to send messages while polling for incoming messages? 
But the socket can only be used by one thread anyway? so you can't send to it from other threads?
you can poll it from an other thread but you can't read/write to it from another thread.  So the creating thread needs to be used to read/write from/to the socket. But polling is a separate operation in zeromq which blocks and when that completes you get a result code that needs to be 0 to indicate success. Once you know there is a message you can read it in the creating thread. But the actual polling happens somewhere else so that the actor wouldn't be occupied.  I've had no issues in my tests anymore since i used this pattern; well no more issues with zeromq plenty of other ones.  
Alright, I'll merge into release-1.3 so it will be a part of RC5.
Sorry for commenting on this before. I haven't had much time lately to put into Akka stuff.  I think the changes done in this changeset, specifically #newEventLoop, should be scrutinized a lot more, if not reverted completely. I prefer the latter one.  1 - The code is much more complex than it used to be and I am far from being convinced that these changes improve the code. It is clear that  new messages cannot be sent until the polling for new incoming messages completes, but I do not really see the issue here. If there is an issue, e.g. performance, then show me some hard numbers.  2 - ZeroMQ is clear about what's and what's not thread-safe, and therefore, the socket must be used in the very same thread that it was created in. Let's keep it that way, across polling, as well.  3 - I hate this commit because not only does it do a lot of things (incremental changes with good commit messages would be preferred), but introduces zero tests and doesn't even try to explain how to reproduce this issue (or issues).  Before I am willing to move on with the forward port for Akka 2.0, this code must be either reverted or cleaned up along with adding appropriate tests.
Hi Karim,  1.3 has not been released yet, propose a fix and I'll have a look at it for the next RC, which should be out sometime later this week.
 Writing a test for this would result in a non-deterministic test and if that's not bad enough, it will actually exit the jvm that's running the test if it turns out the issue does occur resulting in an unstable test suite that can't be run reliably automatically all because of a *logic* error.  But the error occurs frequently enough in our own code, because we have a larger number of tests (which we do have in an in-house project that has 200 tests exercising zeromq)  The bug that would trigger the assert is resolved by using a ThreadBasedDispatcher (Pinned in akka 2.0). This is the *only* type of dispatcher you can use with the zeromq socket, there is *no* other dispatcher that guarantees that the socket will be accessed by the same thread that *created* it. One of the rules from zeromq is that you need to access the socket from the **same thread** that **created** the socket. And there is exactly 1 dispatcher in akka that gives that guarantee.  Then to justify the Future based polling loop  The problem: Say I have a throughput configured of 1 and a receive timeout of 5 seconds. Considering that there are no *incoming* messages on the socket. I can only send **1** *outgoing* message every **5** *seconds* because that is how it was configured.   By using the future you don't block the thread that **created** the socket by polling an idle socket, leaving it free to process its own mailbox.  Any solution that blocks inside the actor thread will significantly impact throughput for the socket 
This line is big no-no, spotted it for the 2.0 stuff as well. Is it needed?
not really it only gets there when the actor is stopping so that logic can be moved
Great, thanks, then I'll strip it out.
Great explanations Ivan, thanks
the idea here is to run the tests against a different scala version than was used for compilation, to test binary compatibility of scala-library; please revert or redo this part
Is it the responsibility of Akka to test Scala binary compatibility? And in any case, why wasn't this fact important enough to add as comment on that line?
I think it is reasonable to expect that people read a comment even if it is a multi-line comment directly above the section they are editing.  Another use of this sub-project is to run tests without compiling them, IIRC Bjrn was working on something using that at some point (or set up a jenkins job, I dont remember).
I'll fix, and add comment
But, thinking about it, isn't my change more appropriate? According to the multiline comment the version is intended to be set from the outside anyway, which would override. It's going to be much worse to try to keep things in sync manually in the AkkaBuild otherwise.
With your change this sub-project does not serve any purpose any more (AFAICS); it only adds value if the `scalaVersion` differs from the main one or if the artifacts being tested are taken from an external repository (which is not implemented here).
not true, it will soon be used by atmos trace project
defaulting to same scala version would be good
that is what I meant earlier, but it is not yet visible in the source
okay, so we leave the customization comment for testing against a different Scala version in and change the default; which means that Viktors change is exactly what is needed?
Maybe another property, for testing against a different scala version, that defaults to `requestedScalaVersion`?
Not a fan of this, looks hacky
How about just wrapping IOExceptions with new IOExceptions with the same message as the original but with scala.util.control.NoStackTrace mixed in?
so if you want to, you can do getCause.getStackTrace etc
The problem is that the user doesn't get to see these exceptions. All they do is crash the connection actor.
I was looking for a way to reduce unhelpful logging output without disabling stack traces completely so that for certain debugging cases users can turn it back on, ideally without needing to recompile.
Yeah, my point is that it should ideally be configured for the logging library and not the akka configuration. (in fact, we already have too many discrete logging options (we should reduce these imo since it is hard for the user to know what affects what)) Since the stack trace won't help the user in any way, and it is possible to get the cause exception + stack trace then one can decide if one wants to pull out the cause trace or not.
is there a substantial benefit to this change? I personally find == more obvious, `eq` seems like an implementation detail
please include remote address
True, actually I didn't want to include it and will change it back.  Just for the sake of discussion: By using `eq` instead of `==` (on a case class) we could trade an `instanceof` against a simple comparison. Since `ConnectionClosed` is a type with 5 sub-types the simple comparison is going to be (a very small amount) faster. However, doing this is probably only warranted in the hottest of all loops, which this site is certainly not.  Side question: `Aborted == closedEvent` would be (a tiny bit) faster than `closedEvent == Aborted`, no? (however, I find the latter more natural)
I would not worry about any of these as the precise evaluation order (for example) is an obscure implementation detail which might change. The performance difference of `==` on a case object will only in rare cases justify deviations from the most obvious (and therefore correct) syntax.
Can't we just avoid allocating these? (cache them)
Sure, the extra bitfield is really not an issue here.
I'll do that.
is this better, or the same as `super.hashCode`?
not much value in a case class any more?
What happens if scala.util.hashing.MurmurHash3.productHash(this) ever returns 0?
@patriknw that is what's done inside a case class. The super is `Object`.  @drewhk There will be no caching. I didn't want to special case that. It will be fixed by changing to a lazy val.
Pretty printing and equals?
Great work Bjrn!
``Timestamp.zero`` becomes a thunk, we should do this:      val cmpVersions = versions withDefaultValue Timestamp.zero     version match { case (n, t)  t <= cmpVersions(n) }
No need for this method anymore, just inline the code in the if-else?
we should use withDefaultValue here as well
can you use these constants in `receiveGossip`? would make things more readable
would it make sense to always use withDefaultValue for the `VectorClock.version`? (in that case, remember to add it in merge also) 
I'm not sure about that, so I didn't want to change it.
I see why you use a var, but I'm not sure that I like it
didn't you define constants for these?
perhaps match on receiveGossipType in the gossipStats match
Yeah. we should remove PartiallyOrdered and use constants for comparison instead.
Yes, you're right. Should use them here as well.
should we make VectorClock private[akka] so that we can change it? In that case it feels a bit over-designed to have Node, NodeImpl just to wrap a string for our internal implementation of vector clocks wdyt of using `type Node = String` instead
Great idea Patrik. Lets offer services on top of it, but not expose it directly.
I'd match on Older as the trait is sealed
Is this the most likely case? (i.e. why test it first?)
new String? Really?
This is still a 3-pass solution with lookups?
This is going to be slower than the replaced version as it still uses lookups, why not iterate and merge?
why allocate a new String instance?      def apply(name: String): Node = hash(name)      def fromHash(hash: String): Node = hash
Yes, it is. Those optimizations were deferred until after 2.2.0.
is this more efficient than `status.version.versions.keys.toVector`?
I don't think that I understand what you mean. This does what the replaced version does, only it doesn't do replacements for equal times. And yes, this could also be rewritten with iteration using the `TreeMap`, but RC2 is coming up.
Nice catch. You are right, it probably is the least common case.
No apparent reason. Will clean up.
Nope, just got fooled by the old code and IntelliJ IDEAs broken highlighting.
This method has a _terrible_ performance profile. It creates so many intermediate collections I can't even start to count it. I sincerely hope it's only called once every blue moon but I have a nagging feeling that's not the case.
It's called once for every gossip message that we send out (once a second is the default). If the cluster stabilizes we change to `GossipStatus` instead.  Yes, the performance is bad for much of the code in here. I've opened a ticket for optimizing it. https:www.assembla.com/spaces/akka/tickets/3456
I dont see why the var is needed here, why not explicitly return the result from each branch?
this will (hopefully) give an exhaustiveness warning; I'd prefer introducing a common sealed trait for the non-Ignored statuses to get around this without ugly comment-only cases
was this not Same|Before previously? (I don't know what is right, just commenting on the diff)
please add `override`
Same is now the first case we check for and it really doesn't matter if we take v1 or v2 if they are the same.
okay, thanks. I dimly remember corners where it matters which of the equal ones we take ;-)
I just think that having the value returned in the 5 first `if` statements that are just 1 line loggers just to keep it a `val` looks way uglier, but I can change.
Shouldn't this respect the remote dispatcher setting?
I though about that but I couldn't create a `HashedWheelTimer` from an `Executor`, and you can't pull the `ThreadFactory` out of the `Executor`.  Also people might be confused if one of the threads in their `Executor` is hijacked by the `HashedWheelTimer`.
Was there a HashedWheelTimer created implicitly before?
Yes. In the depths of the `NioClientSocketChannelFactory` it created a `HashedWheelTimer` with no arguments and that used `java.util.concurrent.Executors.defaultThreadFactory`
I backtracked the constructors of `HashedWheelTimer` through Netty into our code, and this was the only place (except for the TestConductor, but that one is just for testing and uses the `Executors.defaultThreadFactory` for everything anyway.)
Is there a possibility to reuse HWTs? I bet we use more than one for the netty parts?
Yes, @viktorklang We can probably reuse one `HashedWheelTimer` for the whole `NettyTransport`.  So @drewhk correct me if I'm wrong. This seems to be per outbound association. Are we creating this for every connection to a new remote system?  
Isn't the factory assigned to a val? I will check. Btw, NioDatagramChannelFactory cannot be fed with a HashedWheelTimer?
@bantonsson I think we should do that to keep thread count down.
No, I checked, this is created when the driver is loaded. Shouldn't you apply the same to serverChannelFactory?
Thanks, then we only create one. The `NioServerSocketChannelFactory` doesn't create one, neither does  the `NioDatagramChannelFactory`
One per driver, to be exact, but that is fine I think. Can you add a comment explaining that serverChannelFactory and datagram channel factory do not create a HWT? Otherwise next time we will look at this code will be confusing.
Yes, that's what I meant, and I think that's fine too. I'll add comments.
rename to getInstance, and ScalaDoc it as JavaAPI, you can reuse the same text.
final @inline ?
final @inline ?
Add an empty line between all of these, for readability
What if members is empty?
Shouldn't we include the ActorSystem name here?
Can't be empty. It will at least have the node you are currently talking to. 
It is used by the 'ping' command to just check that a node is up and fine. Could be improved to send more info than "pong". But it will do for now I think. 
Good question. Bigger question. Discussed with Roland some time ago. Currently a cluster can include multiple actor systems, but this should be changed to only allow a single one. 
But why is it a public method that returns a String? Isn't this an implementation detail?
I can use the 'member-status' instead, and remove this method. 
Is the delay really necessary? I know that it existed in the old code as well but it has a really bad smell and is quite arbitrary.
Should we send to dead letters? Or too inefficient?
Ok, I see, so sending to dead letters is not in NoRoutee but here. Probably makes sense, maybe add a comment to NoRoutee that references this place.
This class is a nice one.
Intentionally not sealed? I like the idea of keeping it open, but then it might need some documentation.
Not necessarily a connection.
Negative value is allowed?
I have a relatively shallow understanding of the original routers, so my question might be totally stupid. Given that we have the aroundReceive interception API, is this strictly needed? I'm a confused soul :)
Again, a question just to educate myself. Why is the specialized cell needed?
This deprecation message is slightly confusing...
What is the relationship between RouterPoolActor, RouterActor and RoutedActorCell?
yes, I have been pondering about that as well. The reason is explained in the comment. It is only a "trying to do my very best effort". Since it now is possible to add and remove routees more easily users might complain if some messages are routed to deadLetters due to a removal of a routee.
yes, it is only a note for ourselves, since this is private[akka], but I can improve
RouterPoolActor is more specific than its subclass RouterActor, adds some more mananagement. Both types have the RoutedActorCell as "context" (cell).
yes, all classes in this file needs more api docs
this code was only moved so I assume so
but isn't that in Actor, not in the cell? or what do you mean?
it's an optimization
This will want to be documented: a router implementation cannot assume that all routed messages are properly recorded (since we create a new instance here and messages going through between the copying and the write-back will be routed by the old instance and not seen by the new).
yes, it should, all management messages go one side of normal messages, since they are also processed by the head actor
why not `context.system.scheduler.scheduleOnce(10.seconds, self, ReceiveTimeout)`?
this might want to become a wildcard, possibly ;-)
you don't need the double-quotes
case valid @ ("whitelist" | "blacklist") => valid case invalid => throw new IllegalArgumentException("akka.remote.nat-firewall is set to "+invalid+" and not to 'whitelist' or 'blacklist'")
val NATFirewallAddresses = getStringList("akka.remote.nat-firewall-addresses").asScala.toSet
Nice!  The following version has 0 allocation for the all-or-nothing cases (which includes the default setting) and doesn't repeat the natAddress validity check for both branches.      private def allow(natAddress: Address): Boolean = {       import provider.remoteSettings.{ NATFirewallAddresses, NATFirewall }       if (natAddress.host.isEmpty || natAddress.port.isEmpty) false Partial addresses are never OK       else NATFirewall match {         case "whitelist" => NATFirewallAddresses.nonEmpty && NATFirewallAddresses.contains(natAddress.host.get + ":" + natAddress.port.get))         case "blacklist" => NATFirewallAddresses.isEmpty || !NATFirewallAddresses.contains(natAddress.host.get + ":" + natAddress.port.get))       } }
To aid in debugging, might be nice to know why the message didn't get through  case ActorPathExtractor(natAddress, elements) =>    if (allow(natAddress)) system.actorFor(elements).tell(remoteMessage.payload, remoteMessage.sender)   else log.error("Firewall: dropping message {} for non-local recipient {} at {} local is {}", remoteMessage.payload, r, address, provider.transport.address) case r => ...
Nice!  had to add this   val settings = provider.remoteSettings   have to do this to do the import or else err "stable identifier required"  import settings.{NATFirewallAddresses, NATFirewall}
Can't we move the `if` to the case line?
but then there must be another case, or?
yes, the `(None, None)` can move to the bottom and become `_`
I'm not totally convinced that that is easier to understand, and None, None is the normal, most frequent, case
I think it is clear enough as it is; perhaps add a comment to the (None, None) case that it is the most frequent one
please add comment here that `super.preRestart` is called is called from `preRestartDefault` already, so they dont duplicate the call
Classes overriding `preRestart` should be able to call `super.preRestart` as shown in [this example](https:github.com/eligosource/akka/blob/wip-3618-cleanup-lifecycle-hooks-krasserm/akka-samples/akka-sample-persistence/src/main/scala/sample/persistence/ProcessorFailureExample.scala#L21) (first delete message then run recovery as defined in `Processor.preRestart`). A subclass calling `super.preRestart` will only call `Processor.preRestart` whereas the `super.preRestart` call in `preRestartDefault` calls `Stash.preRestart`  as it should be. No duplicate call of any `preRestart` definition. 
@rkuhn ... or do I misunderstand your comment?
ah, yes, you are right
now, if the eventStream would also preserve the sender instead of always using deadLetters, then I would probably say: send from self, include original sender in message. Does that qualify as a use-case?
Update all java doc samples that use UnhandledMessageException. The idea was to illustrate a good default behavior in onReceive, i.e. throw  UnhandledMessageException, as scala Actor does. Now that should be changed to publish to event stream, I guess.
missed sender, here will fix
It's also in TestKitDocSpec "demonstrate unhandled message" in {
It's also described in testing.rst, search for unhandled
this method is overridden in UntypedActor also, needs fix, invoke super.unhandled or remove it from UntypedActor, since it is not really part of UntypedActor API (never called by akka)
Do we need UnhandledMessageException any more? Remove class?
Thanks, will fix
Yeah, good point
Removed override in UntypedActor
oh, thats weird: github did not show me the latest, then. but it seems that you just fixed UnhandledMessage instead of preserving sender in EventStream.publish; there is this ticket where I stipulate that that might be useful  think about using an EventStream for pull-routing of messages to actors, then having the sender right would definitely help.
Not included in this ticket :-)
Alright, so Ill add it to 1542.
Wow, this is going to be expensive, but it's only done at ActorCell creation, so open a ticket under Performance to make this more efficient, with low prio
If this isn't being done in REflectiveAccess we should add it there, makes no sense rethrowing InvocationTargetException
What's the purpose of having this here as a def instead of using the createInstance that takes a FQN-string?
Why do we still need these? Can't we drop them?
I'd like to drop these as well
Great stuff Patrik! I like the approach
Done, separate ticket, #1555, and commit, but pushed to this pull request.
This introduces an extra allocation, try marking "withErrorHandling" as final and @inline
Yes! It's always possible to use new CustomMailboxType("akka.actor.mailbox.RedisBasedMailbox") in case of programatic definition.
ah, thanks, fixed
have you observed thread-pool-executor to be better than fork-join for these dispatchers?
I think this should be aligned with akka serializers. They can not be the same because of the stream vs byte array, but they should be as close as possible. For example the classloader should be grabbed from the `system.dynamicAccess.classLoader`. See `akka.serialization.JavaSerializer`  That will also make it possible to create a SnapshotSerializer that delegates to an akka serializer, converting the stream to bytes.
These are used for blocking IO which is the bottleneck, not the dispatcher type.  - the write dispatcher is a dedicated thread used for sequential blocking writes to LevelDB - the replay dispatcher are dedicated threads used for concurrent blocking (potentially long-running) reads/scans over the journal.  
yes, that feels important
how are write failures propagated to the processor?
Good idea, will do.
BufferedOutputStream and BufferedInputStream?
My plan is to work on journal IO failure handling when implementing the journal plugin API (i.e. with the next PR)
Makes absolutely sense, will add it.
what is the advantage of this `Base64` (which adds another dependency) over java.net.URLEncoder/ULRDecoder ?
`URLEncoder` should do it as well and even has the advantage that one can read the processor id in the snapshot filename. Will change it.
Why not pipeTo?
Maybe a more descriptive name?
can PID contain dashes?
Because I find       saveSnapshotAsync(md, snapshot).map {       case _  SaveSnapshotSucceeded(md)     }.recover {       case e  SaveSnapshotFailed(metadata, e)     }.to(self, p)  less readable, although it's less redundant. Anyway, I should change `Success(m)` to `Success(_)`.   
Will change to `load`
Yes, but this is still working then. These extra dashes are correctly matched by `(.+)`
I think Timeout has to go. Or we need to create: Timeout.create(t, TimeUnit.SECONDS)
Perhaps add a link to the wikipedia article for ACID?
Since this is the Java version, you might want to clarify that we're talking about Scala scala.collection.immutable.Map and s.c.i.Vector so the reader doesn't confuse it with Java j.u.M and j.u.Vector
What kind of datastructure? a normal mutable, a transactional or immutable?
Okay, I'll add Timeout.create for java usage. Using an implicit or explicit Timeout for Coordinated lines up with other timeouts.
Okay. Thanks. Just copied this over from previous example.
This is straight from the original docs (from Jonas I guess). I read it as: needing to share a datastructure (in the general sense of datastructure) across threads is a good use-case for STM.
Yes, this is just a copy of the Scala docs. Originally we provided our own persistent datastructures with Scala and Java interfaces. Not even sure if Scala vector and map are that usable from Java. Will remove this section.
Actually, I see there is already this constructor:  https:github.com/jboner/akka/blob/v2.0-M1/akka-actor/src/main/scala/akka/util/Duration.scala#L548
Great, use that :-)
Note the withCreator is used in many of the java examples. Will create separate ticket for this.
Should be a very simple regex to replace:  new Props().withCreator(x) with new Props(x)
Yes. But I'll just keep this pull request focussed to transactors.
Why a List in particular?
Perhaps implement akka.dispatch.Awaitable?
Yes, it's been a long time since I last used java.util :)
Yes, an Array is more straightforward, though I guess it doesn't make much difference.
Not sure what it gains here.
I was thinking more along the lines of perhaps Seq
Uniformity of API?
This is not a part of akka API, just a common interface for local and distributed barriers. I don't see the reason to burden with extra API here. 
IndexedSeq since it should allow for a constant-time apply(index). Array should be fine as well.
Not to be extra picky, but for collections I really prefer not to have concrete implementation leak out into user API :-)
Changed that. Didn't expect it to become part of the user API:)
I'd probably use Seq(...)
 while the actor system is shutting down. 
I guess you mean `> 0` here?
which means that since preRestart is not overridden this actor will no longer be subscribed after a restart
yes, of course, thanks
what do you mean, the subscription is not removed in case of restart (self ref is the same)
ah, I'm so stupid -- or this api is so stupid I got it
it was an existing actor that was exactly what I needed for this test
"Dead letters does not have to be a problem" --> existence of dead letters does not necessarily indicate a problem
that is better, thanks
We should definitely not require people to set this to be able to use remoting, it should be some UUID or something if not speificied.
Are they in the same package?
I'd add "actorFor" somewhere here
And add actorOf somewhere here
We desperately need to add the remote-mounting ASAP, so people don't loiter their code with absolute URIs
There is a ticket for that: http:www.assembla.com/spaces/akka/tickets/1564 But I guess we have to have it in there until that ticket has been implemented.
No, I'll add the whole package to the example
please do not abbreviate, we must establish the term, and the best way is repetition, repetition, repetition, 
very nicely showing off our shiny new configuration :-)
Okay, I will key my new keyboard... :-)
Yeah, it's really slick isn't it
We should add so that if there's any options that we don't expect in the config (could be typo etc) should be outputted at startup
What does this config mean? is it valid to omit the core-pool-size / core-pool-size-factor?
I think we should call it "id" and not Key, to align with things like ExtensionId etc..
Is this threadsafe?
Or needs to be threadsafe?
I vote to rename dispatcherFactory to "dispatchers"
Perhaps we'll have to require that people subclass the UnboundedPriorityMailbox and override a method to provide the ordering, and then they just use the FQCN of that class in the config?
I suggest just "default" instead of "default-dispatcher"
Shouldn't this check be done in "lookup"?
Don't think we should have it here anyway. Perhaps put it in ActorSystemImpl for Extensions to hook into.
Should perhaps clarify that it might create a new instace for each call, so it is not a guaranteed lookup
CAn't we scrap this now and just make a configurator and a lookup?
What does it do if the key does not exist anywhere?
Might be possible, but we must consider that user's can subclass in various ways (incl mailbox). Create a separate ticket for it.
we don't have core-pool-size in config. This means that the core pools size will be exactly 60, independent of  core-pool-size-factor. It might be nice with core-pool-size config property for this use case. Create ticket if you think it is useful.
I used key, because it corresponds to a config property key, but I can change it to id, np.
It doesn't need to be threadsafe because it is a test, and individual test methods in same instance is not running in parallel, or in different threads, as far as I know, but I can change it to AtmomicInteger to be on the safe side. More important is that all this is a hack for the MessageDispatcherInterceptor. Can't we do something better? 
sounds good, you have my vote as well
Might work, I'll try. The way I did the mailboxType config is that the fqcn is the class name of the actual Mailbox. That works nice for the durable mailboxes. But here we need to support the fqcn of the MailboxType. Maybe we should support both, load the class, and then check if it's a MailboxType or a Mailbox and then instantiate appropriately. WDYT?
Yeah, the config lib rocks!
This name will be used for threads, so I think default-dispatcher is very good.
good idea, I'll move it too lookup and return defaultDispatcher if id == Props.defaultDispatcherKey
Oh, I think it should always be a MailboxType.
defaultDispatcherKey is a null, so it must be treated special somewhere
Let's say that we solve the prio mailbox without it, we still have the ActorModelSpec, which creates it's own Dispatcher. I would really like to remove the method, so it's not that.
but that depends on the impl, PinnedDispatcherConfigurator returns new instance each time, but not DispatcherConfigurator
Can't ActorModelSpec just require that the tests create a class that extends DispatcherConfigurator and just have that in the ActorSystem config and look it up there?
That's my point, the contract of the method is not to return the same dispatcher instance each call. Can be good to document that at the interface level.
That might work, I'll try.
as it says: "or if not defined it uses the default dispatcher"
Could be a source of hard to diagnose problems if it just silently uses the default. Perhaps log a warning
then I have to write one MailboxType for each durable dispatcher, lot's of work ;-) just joking, I'll change it to MailboxType
I think the problem is that it's not top level class there. I'll see if I can rewrite it, but I leave that for last, so if run out of time you can complete that part.
ok, I'll clearify that. I thought "an instance" was fuzzy enough to mean any of the alternatives, but I agree, better to clarify.
On Wed, Dec 21, 2011 at 5:08 PM, viktorklang < reply@reply.github.com > wrote:  > > > >     FIXME: Dispatchers registered here are are not removed, see ticket > #1494 > > -  private val dispatchers = new ConcurrentHashMap[String, > MessageDispatcher] > > +  private val dispatcherConfigurators = new ConcurrentHashMap[String, > MessageDispatcherConfigurator] > > > >    /** > >     * Returns a dispatcher as specified in configuration, or if not > defined it uses > > Could be a source of hard to diagnose problems if it just silently uses > the default. Perhaps log a warning > > That's why I log a Debug, but I can change it to Info. Don't think it's a Warning, because I think as a start default-dispatcher is a good default, but still with possibility to tune a specific dispatcher, if needed.   > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/182/files#r309168 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
No, my point was, if I say that "foo" should be my dispatcher, and there is no "foo" registered, and it falls back to the default, I want a WARNING
On Wed, Dec 21, 2011 at 5:24 PM, viktorklang < reply@reply.github.com > wrote:  > > > >     FIXME: Dispatchers registered here are are not removed, see ticket > #1494 > > -  private val dispatchers = new ConcurrentHashMap[String, > MessageDispatcher] > > +  private val dispatcherConfigurators = new ConcurrentHashMap[String, > MessageDispatcherConfigurator] > > > >    /** > >     * Returns a dispatcher as specified in configuration, or if not > defined it uses > > No, my point was, if I say that "foo" should be my dispatcher, and there > is no "foo" registered, and it falls back to the default, I want a WARNING > > Alright, I have another point, but I'll change it to Warning.   > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/182/files#r309229 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
The thing is that it might be that it's using the wrong config, or a name is misspelled in the config/code, so it's good to know what's happening.
changed to id
dispatchers it is!
FQCN is to the MailboxType, that solved the issue with priority mailbox
The register method is left, but only used from ActorModelSpec and CallingThreadDispatcherModelSpec. I created separate ticket for rewrite of them. #1563 With that the register method can be removed.
Yes! That worked out fine. Thx.
Shouldn't this be the Props.defaultDispatcher?
Props.defaultDispatcher = null for some reason. Probably an optimization of mem size or serialization size. I don't know.  If it wasn't for that I would have done it the other way, I think. Props.defaultDispatcherId = Dispatchers.DefaultDispatcherId  Then the special case in lookup wouldn't be needed.  On Wed, Dec 21, 2011 at 11:10 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -29,251 +30,196 @@ case class DefaultDispatcherPrerequisites( > >    val deadLetterMailbox: Mailbox, > >    val scheduler: Scheduler) extends DispatcherPrerequisites > > > > +object Dispatchers { > > +  /** > > +   * The id of the default dispatcher, also the full key of the > > +   * configuration of the default dispatcher. > > +   */ > > +  final val DefaultDispatcherId = "akka.actor.default-dispatcher" > > Shouldn't this be the Props.defaultDispatcher? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/182/files#r310193 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
My point was, why not just make Props.defaultDispatcher = "akka.actor.default-dispatcher" and scrap Dispatchers.DefaultDispatcherId?
Yes that, or the other way around could work, removing one of them. but still, do you know why it was = null ? Setting something to null must be done for to a good reason, or is it just accidental?  On Wed, Dec 21, 2011 at 11:23 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -29,251 +30,196 @@ case class DefaultDispatcherPrerequisites( > >    val deadLetterMailbox: Mailbox, > >    val scheduler: Scheduler) extends DispatcherPrerequisites > > > > +object Dispatchers { > > +  /** > > +   * The id of the default dispatcher, also the full key of the > > +   * configuration of the default dispatcher. > > +   */ > > +  final val DefaultDispatcherId = "akka.actor.default-dispatcher" > > My point was, why not just make Props.defaultDispatcher = > "akka.actor.default-dispatcher" and scrap Dispatchers.DefaultDispatcherId? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/182/files#r310224 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
It was null because it couldn't be an instance of a Dispatcher, since there has to be an ActorSystem to get the default dispatcher instance.
ah, forgot that it was a MessageDispatcher :-) fixed!  On Wed, Dec 21, 2011 at 11:34 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -29,251 +30,196 @@ case class DefaultDispatcherPrerequisites( > >    val deadLetterMailbox: Mailbox, > >    val scheduler: Scheduler) extends DispatcherPrerequisites > > > > +object Dispatchers { > > +  /** > > +   * The id of the default dispatcher, also the full key of the > > +   * configuration of the default dispatcher. > > +   */ > > +  final val DefaultDispatcherId = "akka.actor.default-dispatcher" > > It was null because it couldn't be an instance of a Dispatcher, since there > has to be an ActorSystem to get the default dispatcher instance. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/182/files#r310259 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Great!  I'd love for someone else but me to have a chance to have a look at it before merging it in, any volunteers? :-)
too late, sorry I will not work tomorrow, that's why it felt good to close it now  On Wed, Dec 21, 2011 at 11:51 PM, viktorklang < reply@reply.github.com > wrote:  > > @@ -29,251 +30,196 @@ case class DefaultDispatcherPrerequisites( > >    val deadLetterMailbox: Mailbox, > >    val scheduler: Scheduler) extends DispatcherPrerequisites > > > > +object Dispatchers { > > +  /** > > +   * The id of the default dispatcher, also the full key of the > > +   * configuration of the default dispatcher. > > +   */ > > +  final val DefaultDispatcherId = "akka.actor.default-dispatcher" > > Great! > > I'd love for someone else but me to have a chance to have a look at it > before merging it in, any volunteers? :-) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/182/files#r310305 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> - Enterprise-Grade Scala from the Experts Twitter: @patriknw
Alright, merge it in squashed so it's easy to revert if someone finds anything explosive in there :-)
should probably change range to `0..2GiB`
which unit is this in?
same here: whats the unit?
I'll add a description here
I'll add a description here, it's number of connections.
this is the only deferred option parsing Im aware of: why not put it in ActorSystem.Settings?
Oh, no, it's perfectly recommended to put config parsing in Extensions as they may or may not be used. See Serialization for instance.
ah, okay, wasnt looking at a large enough piece of context here.
Are we sure that we're calling ``clearAll()`` at all times when the Thread cannot be recovered? (so we don't grow the heap indefinitiely if the scheduler thread dies)
Why do we need the companion?
a mutable toString?
Missing `@tailrec` annotation?
add a comment here as well that this is peekNode() inlined
Is this performant enough?
Does this error get escalated somewhere, or only logged?
yes, this is done only once at shutdown
there is no escalation possible, which is what ships going down! is supposed to convey
But it actually brings down a system, or just "lets it crash"?
just one case would be enough
please use a subtree of the default config here, otherwise there will be conflicts between this and real actor systems      ConfigFactory.load().getConfig("migration")
No, this is not going to fly, but well discuss alternatives offline.
Having a separate ActorSystem for all DSL usage is not that useful: if the DSL is nice, people will want to use it for their real systems. Therefore, every action of the DSL must take an `implicit system` and this special thing must go.
Thread-based communication should reuse existing infrastructure, I see no need to do this special dance with containers and refs.
This should throw an exception if used from an event-based context, i.e. within an actor of future.
this shouldnt be necessary
any reason why you didnt port `futureFlowLoops` from the old tests?
this file belongs into the `akka/dataflow` directory (for us poor eclipse users)
Oh, since Derek didn't document it I deleted it.
I'll try removing it ;-)
I always get that wrong! argh
an alternative would be to throw a PersistenceFailureException and let it be handled by the supervisor, but that is perhaps not what we want. Pros/cons?
what is returned, BoxedUnit for java api?
are these defaults used (automatically) if the plugin-dispatcher is not specified?
processors *and* snapshot
is the mutable map needed for performance?
INTERNAL API marker
perhaps collapse into one if line
`PersistenceFailure` is a notification about a problem in the journal actor, so a restart of the processor by a supervisor wouldn't help much here. It's actually the journal that must be restarted by a supervisor (as done by [`SyncWriteJournal`](https:github.com/eligosource/akka/blob/wip-3641-storage-plugin-apis-krasserm/akka-persistence/src/main/scala/akka/persistence/journal/SyncWriteJournal.scala#L28) if `write` throws an exception). `AsyncWriteJournal` OTOH delegates fault handling to implementation classes (e.g. this could be even located in the async storage backend driver) - I'll remove the [erroneously placed](https:github.com/eligosource/akka/blob/wip-3641-storage-plugin-apis-krasserm/akka-persistence/src/main/scala/akka/persistence/journal/AsyncWriteJournal.scala#L45)`throw e` in `AsyncWriteJournal` with the next commit.   A processor can still decide to throw an exception (for whatever reason) when receiving this failure message. But it likely may want to inform the sender to re-send the message, or trigger some application-specific compensations. The latter is also the reason why a generic failure reply from the journal to the sender is not flexible enough.  
This should be `Void` of course.
`plugin-dispatcher` must be specified, at the moment. Will change it to make it optional. 
We agreed with Mark that all files that have a significant amount of existing eventsourced code should keep the Eligotech license header. This is the case for most journal/snapshot related files. In case of this file however, a Typesafe header would be more appropriate. Will change it.
I had the same discussion with @drewhk already and my argument was that I find this a bit more readable, although it introduces more redundancy. Will change to map-recover-pipeTo as that style is used in many other places in the Akka code.
I didn't measure that. I implemented this algorithm several years ago using a mutable map and then never changed the implementation. 
NO, shutdown of dispatchers are done by themselves, as you know, and Roland and I have spent considerable time to make it work together with shutdown of scheduler. They can't be shutdown here.
Ok, then there is no solution to this. I'll clean up and invalidate the ticket.  Cheers, V
ok. The dispatchers are shallow and doesn't consume much memory, when the actual ExecutorService is shutdown, so it should not be much of a problem to keep the DispatcherConfigurators in the map. Might be a problem for pinned dispatchers if a lot of actors are created with pinned dispatchers, but I guess that is a rare use case.
What is the purpose of this? If it is only for defining default value to "undefined" that should be done in reference.conf, instead of ""
Good point, I first had newUuid.toString here. Thx
Perhaps using the logging instead of stdout?
final here as well
use:      new Props(JCreationActor.class)
use:       new Props(JAdvancedCalculatorActor.class)
Why is this here?
use:      new Props(JLookupActor.class)
We use println in the Scala example and I wanted them to be similar.
It's to prevent this sample from bombarding the actors. We only want to show how to communicate, not that we can load the crap out of our things. It's the same in the Scala sample.
I know I'm late, but anyway... I think that in our Java samples we should send unhandled messages to deadLetters, to promote that as a good convention. See samples in Actors Java doc.
Yes, a bit late but I can fix this anyway :-)
Thx for pointing this out btw!
I noticed that it is maybe not correct in the Actors Java docs else throw new UnhandledMessageException(message, getSelf());  I think the correct way should be else unhandled(message);  I'll create a separate ticket for this.  /Patrik  On Dec 28, 2011, at 18:52, Henrik Engstrom<reply@reply.github.com> wrote:  >> +package sample.remote.calculator.java; >> + >> +import akka.actor.UntypedActor; >> + >> +public class JAdvancedCalculatorActor extends UntypedActor { >> +    @Override >> +    public void onReceive(Object message) throws Exception { >> +        if (message instanceof Op.Multiply) { >> +            Op.Multiply multiply = (Op.Multiply) message; >> +            System.out.println("Calculating " + multiply.getN1() + " * " + multiply.getN2()); >> +            getSender().tell(new Op.MultiplicationResult(multiply.getN1(), multiply.getN2(), multiply.getN1() * multiply.getN2())); >> +        } else if (message instanceof Op.Divide) { >> +            Op.Divide divide = (Op.Divide) message; >> +            System.out.println("Calculating " + divide.getN1() + " / " + divide.getN2()); >> +            getSender().tell(new Op.DivisionResult(divide.getN1(), divide.getN2(), divide.getN1() / divide.getN2())); >> +        } >  > Thx for pointing this out btw! >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/194/files#r317294
It was ok in master  else unhandled(message);  /Patrik  On Dec 28, 2011, at 18:52, Henrik Engstrom<reply@reply.github.com> wrote:  >> +package sample.remote.calculator.java; >> + >> +import akka.actor.UntypedActor; >> + >> +public class JAdvancedCalculatorActor extends UntypedActor { >> +    @Override >> +    public void onReceive(Object message) throws Exception { >> +        if (message instanceof Op.Multiply) { >> +            Op.Multiply multiply = (Op.Multiply) message; >> +            System.out.println("Calculating " + multiply.getN1() + " * " + multiply.getN2()); >> +            getSender().tell(new Op.MultiplicationResult(multiply.getN1(), multiply.getN2(), multiply.getN1() * multiply.getN2())); >> +        } else if (message instanceof Op.Divide) { >> +            Op.Divide divide = (Op.Divide) message; >> +            System.out.println("Calculating " + divide.getN1() + " / " + divide.getN2()); >> +            getSender().tell(new Op.DivisionResult(divide.getN1(), divide.getN2(), divide.getN1() / divide.getN2())); >> +        } >  > Thx for pointing this out btw! >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/194/files#r317294
I would have written this as two separate tests "verify creators" and "verify messages".
Is this for messages? Do we have something similar for creators?
Sure, I can do that
Ah, here was the answer to my prev question.
Don't like the word empty. That word does not exist in the actor vocabulary. I'd rather have TerminatedActorRef
Shouldn't this validation be in ActorPath?
Are all MinimalActorRefs Local? Always?
now they are. (in both meanings)
Why does that look so cludgy? can't / just skip nulls and ""s?
well, terminated implies that it once was, which is not guaranteed. empty means that it does not actually contain an Actor
Does this work for Java?
ActorPath does not care about what its toString means, and also I prefer to terminate the stack trace for the exception in this obvious place rather than deep in the LocalActorRefProvider (which could even be remote, I guess).
There should probably be a FIXME somewhere here where Props is to be used.
yes, I also came to that conclusion, just not yet pushed.
Why remove that? People have been asking me to put it back whenever I remove it.
oops, you are right, will fix.
we already have a ticket for changing this (1461, should be moved from performance to M3)
added it a few lines above without the extra allocation and clumsy deserialization etc. will make this send variant private (not referenced anywhere else) to document the fact.
can this actor be restarted, and is the scheduled AttemptSysMsgRedelivery valid for a restarted instance?
how do we know that this loop has an end?
This actor is never restarted
because we get the StartupFinished message from above. This is a very short window, btw.
jet lagged code is jet lagged
There's already a SHUTDOWN CommandType, why not just add another one called QUARANTINED?
I am not sure what other type of information we want to provide in the future, so I left it open.
btw, SHUTDOWN should be DISASSOCIATE, the name is a remainder from the old remoting.
Nice catch. That was a subtle bug.
Yes, it did not cause message loss just unnecessary retransmits.
Just rename it? (Protobuf uses ordinals and not names anyway)
Regarding DisassociateInfo I still think we should just add a new commandtype. DisassociateInfo is an enum anyway so there's not really that much info we could add in the future without breaking things.
Not one, but two. The idea was that the DisassociateInfo is optional, if not set, then the reason for the Disassociate is Unknown.
True, but it creates an invariant that has to be enforced in code, that DisassociateInfo shouldn't be present when CommandType is not SHUTDOWN (DISASSOCIATE).  I would also argue that if we get an AkkaControlMessage, then we know the reason, and if we know the reason, we can put it as the CommandType. I.e. DISASSOCIATE, QUARANTINE etc. Wdyt?
There is also such a code enforced invariant with handshakeInfo (and I wanted to be more or less symmetric with disassociateInfo), but I see what you mean. 
What do you think about DISASSOCIATE, DISASSOCIATE_SHUTTING_DOWN, DISASSOCIATE_QUARANTINED? It is more verbose but it conveys the proper meaning.
If we can fix that issue with handshakeInfo I'd love that as well.
Unfortunately that is a proper record, not an enum.
this is a very necessary message, we would have had quite some questions on the mailing list otherwise
Id rather say that this may spin for an arbitrary number of times; it would be better to schedule it 10ms into the future to keep the load down
That would work as well.
no reason? (I dont usually like default arguments)
The reason would be Unknown here. I think if there is a special reason that should be expressed explicitly. Also, disassociate is an API by the AssociationHandle, providing a reason is Akka Protocol specific (which we know to be accessible here, but I need a default version to implement the interface).
why not require a reason everywhere? that would ensure that it is not easily lost
thanks! That means that we should enable more things, remoting is `/system/endpointManager` and cluster is `system/cluster`, and `/system/remote-watcher`, .... Will that enable tracing for all actors below that path or do we have to add wildcards      /system/cluster     /system/cluster/*  Can we enable all system actors? `/system` `/system/*`
Yes, that we enable tracing for all system actors. The defaults that make these actors untraceable can be found here: https:github.com/typesafehub/atmos/blob/master/monitor/trace/core/src/main/resources/reference.conf#L17-L28
That **will** enable tracing for all system actors.
I'm a tad skeptical to the name "mainbus", how about some alternatives? "events", "eventbus", "eventstream"
Use intercept here for brevity:  intercept[Throwable] { (actor ? "Failure").get }.getMessage must be ("Expected exception; to test fault-tolerance")
No unsubscription tests?
Shouldn't this just ignore IOException?
What's the difference between these two?
We definitely need to get this initialization order and visibility/safe publication issue sorted
Really, really like this
This logic should be in the "start"-method of AkkaApplication so it's not spread out inside the constructor.
Really catch Errors here?
This is racy
Is this what we fall back on?
Are there any Java API tests for the logging?
What's the purpose of the reaper?
What is this useful for? Why no return type?
You can have any number of event buses, but the app has only one which it uses for logging (and which users may use for other purposes, as long as the listeners are actors). Hence I was reluctant to call it eventbus (although Im not opposed specifically) and borrowed the term from spacecraft electrics (ignoring thatas with all thingsthere usually are two of these on a spacecraft).  Long story short: feel free to chose a different name.
 well, ahem, yes, okay, I know: boyscout rule, right?
test suite is by no means even started, was waiting for general go on the design.
I have no idea, declaring you as master boyscout ;-)
you are right: documentation is still missing. The very limited stdout-logger has its own level setting because users may want to silence STDOUT.
well, in this case I started with the sorting in that I did not pass this but only the parts which are needed. This transformation must also happen for the other cases, where I would like the provider to have complete power for creating actors (currently lacking at least the dispatcher).
Hmm, then we cant use vals, right? The start-up is somewhat complicated, so I would like to present it as linearly as possible within the code.
We could consider switching to lazy vals, volatile reads are cheap  On Mon, Oct 31, 2011 at 6:03 PM, Roland Kuhn < reply@reply.github.com>wrote:  > > > >     TODO think about memory consistency effects when doing funky stuff > inside constructor > >    val deadLetters = new DeadLetterActorRef(this) > > > > +  val deathWatch = provider.createDeathWatch() > > + > > +   chain death watchers so that killing guardian stops the application > > +  deathWatch.subscribe(systemGuardian, guardian) > > +  deathWatch.subscribe(guardianInChief, systemGuardian) > > Hmm, then we cant use vals, right? The start-up is somewhat complicated, > so I would like to present it as linearly as possible within the code. > > -- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/101/files#r199627 >    --  Viktor Klang  Akka Tech Lead Typesafe <http:www.typesafe.com/> - Enterprise-Grade Scala from the Experts  Twitter: @viktorklang
I believe there is another ticket which covers this question, no need to mix it in with the logging changes.
true. as its not a hot path Ill fix it with a normal lock.
this is used to cover the start-up and shut-down periods, where normal actors cannot be used for logging. I discarded the Future idea because then start-up failures would never make it to the user: better be reliable here.
what do you mean? Getting the logger? That is indeed one thing I forgot. The logging interface itself looks very Java friendly to me already.
getting rid of dead listeners. that death-watch thing you wrote looked really nice for doing this. and traffic on this channel should really be low.
used to initialize the reaper above; will add more doc and return type.
I like EventBus or EventStream. Probably EventStream best. 
E.g. do not use the name MainBus. 
Don't format for comps like this, one line. Harder to read. 
typo: Thurough => Thorough
Hmm, should ```spuriousSelectWakeups``` also be declared @volatile? I'm not sure if select() could be called by multiple threads and thus suffer from not having a happens-before via @volatile.
Hmm, maybe a little @tailrec love?  ```scala def terminate() {   @tailrec def terminateNextChannel(it: JIterator[SelectionKey]) {     if (it.hasNext) {       val key = it.next()       try key.channel.close() catch {          case NonFatal(e) => log.error(e, "Error closing channel")        }       terminateNextChannel(it)     }   }   try {     val selector = current     try terminateNextChannel(selector.keys.iterator)     finally selector.close()   } catch {     case NonFatal(e) => log.error(e, "Error closing selector")   } } ```
No, select will only be called from one thread. So it's fine. The volatile is primarily for wakeup, as described in the class scaladoc for AkkaSelector
Yeah, good suggestion, it resonates well with the other code in the PR
Ah, makes sense. I missed that in the scaladoc for AkkaSelector. Nice!
This means that the Selector will be alive for a _while_ after postStop finishes. If the old behavior was intentional, it should've been documented and hopefully tested. I can switch back and use the same stunt as with `wakeup` for `stop` but I'd rather keep it as is now. Thoughts?
`(System.nanoTime - start) / (timeout.toNanos / 4) < 3`
I find that less clear to read (and it's 2 DIV + a toNanos :-) ) Any reason in particular?
What about returning `0` here to handle potentially enqueued commands? It is not immediately clear to me that this cannot be an infinite loop in the worst case.
Im always uncomfortable using the FPU, thats all; could make it  ~~~ scala (System.nanoTime - start) * 4 / timeout.toNanos < 3 ~~~
If it's imperative I could always switch to strictfp
I'll add a retries parameter
Actually, this is the right calculation (thanks for reminding me): def returnedtooEarly(startNs: Long, finishNs: Long, timeoutNs: Long) = timeoutNs.toDouble / (finishNs - startNs) <= 0.75d
The FPU version is also easier to tweak as the ratio is changed in a single place
Several points:  - AFAICS all connections and listeners registered on the selector close themselves in their own `postStop` methods.   I don't remember whether we already had this discussion, but is it really required that the selector also closes all the channels via its `postStop`?  - Since the selector is the parent of all connections and listeners they will have been already shut down when the selector `postStop` runs, i.e. it shouldn't matter whether we run the clean-up code synchronously (as before) or asynchronously (as with this patch).  - When I call `system.shutdown()`, in which order are the actors and dispatchers stopped? I know the "child actors first" rule, but how do dispatchers fit in the picture? I.e., if I schedule something to a dispatcher in my `postStop`, can I be sure it still gets run before the ActorSystem is shut down?
Hi Mathias,  1) I'll add wakeup-detection to differentiate between spurious returns (Epoll bug) and wakeup requests 2) I'll add stop detection as well if we are to terminate the selector from the outside 3) When the system shuts down, there is no guarantees for tasks that are separately scheduled  I just pushed some new code for this PR, please review if you have time!
if someone called wakeup() within the last 7 lines then the selector will already wake up, will it not? this could benefit from a comment 
or rather: I usually use those which batman does not have
so we bounce a cache line (hopefully not shared with other stuff) across cores to eliminate unnecessary system calls; sounds fair at first sight
It depend on whether the other `wakeup` was called before or after the `select()`, `selectNow()` or `select(timeout)`. 
Yes, here we sacrifice an extra wakeup as not to miss a wakeup
`if ((currentOps & op) != 0)`
is != 0 faster than == op?
no (should be the same, right?), but !=0 is always correct whereas yours assumes single bits only; less assumptions => profit
True, I'll switch to != 0 and change the name to "ops"
But if it is intended to support _enabling_ of multiple interests, this is wrong " if ((currentOps & ops) == 0) key.interestOps(currentOps | ops)", I'll have to change it to be " if ((currentOps & ops) != ops) key.interestOps(currentOps | ops)" i.e. if not all bits are set, set them.
Thinking some more about it, I find this easier to maintain:      def enableInterest(ops: Int, connection: ActorRef) =     new Task {       def tryRun() {         val key = childrenKeys(connection.path.name)         val currentOps = key.interestOps         val newOps = currentOps | ops         if (newOps != currentOps) key.interestOps(newOps)       }     }      def disableInterest(ops: Int, connection: ActorRef) =     new Task {       def tryRun() {         val key = childrenKeys(connection.path.name)         val currentOps = key.interestOps         val newOps = currentOps & ~ops         if (newOps != currentOps) key.interestOps(newOps)       }     }
we've got a winner!
use must matchers instead of asserts, for fun and profit!
side effecting `time.cancel()` `timers.clear()`
you dont really need to wrap things inside an object like this
That was so that fsmref and assertTimersActive could refer to each other. Do you think I should do something else?
does it not work if you just remove the enclosing `object`?
That gives a "forward reference" error. Local values can't refer to names that haven't yet been defined.  Using an object is one solution. I can also make one value lazy to delay its evaluation until the other is defined. Maybe I'll do that.
ah, okay; if theres some non-obvious reason the usual policy is to add a one-line comment (unless it is really non-obvious and/or inside the dungeon, then make it a novel)
OK! I'll neaten it up as much as possible (I think a lazy val might read nicer) and then comment whatever I can't neaten.
I thought we had decided to drop this naming convention
braces not needed in case clauses :-)
Where can I find out about the new naming convention?
It's one of those horribly undocumented things :/  I think "isStateTimerActive" is a good name for a Yes/No question :-)
agreed; we should deprecate that other method in there, Rich, could you do that please?
So, deprecate "timerActive_?" and make a new method "isTimerActive" in both FSM and TestFSMRef?  (I'm guessing I can't just rename the method in TestFSMRef because it's also a public interface.)  Also I'll change all references to the deprecated methods in code and docs.  Should I do the deprecating/renaming work in a separate pull request? Otherwise this pull request might get quite noisy.
precisely: new ticket and pull request as you just outlined
Rich, what Patrik means is that side-effecting calls should always use parens
Can the lookup and underlying values change during the evaluation of the &&?
The same question as below. I don't have enough context, so I assume it is the right thing, but please confirm :)
That's a really good question, and yes the value of lookup and underlying can change, but the critical change of underlying (swapCell) is protected by the the lock in the unstarted cell. And this test is only used from inside that lock.  Maybe there should be a comment that `cellIsBeingReplaced` and can only be used from inside the lock? 
We could always just inline it?
Inline is fine with me.
having names for things is also nice: Id propose to add a comment at the use site then
Is this guaranteed to exist on all major JVMs + Dalvik?
What's the use-case?
This will be terrible, both for performance and memory usage.
classOf[JavaLogging].getName didn't work?
Why is this needed at all? How does one discern a companion from the class?
Instead of this I'd recommend just logging the LogRecord as a LogEvent to the system.eventStream, then in your JavaEventHandler, unpack messages that are LogRecords and just use them straight off the bat.
it is optional, with the default being off. Note that this is *not* used by the ActorLogging framework it's only if someone wants to have the AkkaLogging API available in a non-Actor context (and here it will be implemented by Java Logging)
I tried that and got a weird exception. I can look into it again but it was hard to diagnose.
Feel free to remove it I frankly didn't understand why the $ sign was there. If it is the distinguisher between trait and companion, then it makes sense.
@viktorklang this part of the code is not used by the ActorSystem it's just to make your Akka Logging API available to non-Akka parts of the application.  The only part of `JavaLogging` used by the `ActorLogging` is the `logger` field.
IMO it doesn't make any sense to add a logging API that doesn't integrate with the Akka Logging backend. My suggestion also removes the need for synchronous and Future-based logging.
You might want to check AkkaSpec.getCallerName for a quicker way to obtain this information. 
message is already a string
I still say that it should log LogEvent an EventStream and not use Futures or ExecutionContexts.
ok, cool, this is the bit you mean?  ```scala def getCallerName(clazz: Class[_]): String = {     val s = Thread.currentThread.getStackTrace map (_.getClassName) drop 1 dropWhile (_ matches ".*AkkaSpec.?$")     val reduced = s.lastIndexWhere(_ == clazz.getName) match {       case -1  s       case z   s drop (z + 1)     }     reduced.head.replaceFirst(""".*\.""", "").replaceAll("[^a-zA-Z_0-9]", "_")   } ```  Is there any profiling that shows it is faster to use the Thread instead of the Throwable?
The aforementioned function is more important IMO  that there's a JulLoggingAdapter that ships LogRecords to the JulEventHandler that logs them.
I'm happy to remove the Future if you really want, but I think this it is a perfectly reasonable approach for many sensible use cases (and I've turned it off by default anyway). I can only see there being a problem with Future creation if the underlying handlers are unable to write to disc quickly enough. I understand Futures are very cheap.  The way I look at it, if somebody's logging requirements become so intensive that they really need to use the power of Akka to overcome limitations in their legacy backend choice, then at least they'll not need to change their log calls. All they'll need to change will be the name of the trait they imported to get logging support.  I'd love to see some performance studies of where Java Logging breaks down  do you have any numbers on this?   I don't think it makes sense to have the `JavaLogging` trait send information to the EventStream, since it is supposed to be a logging implementation of the Akka Logging API without depending on any ActorSystem being in place.  I'm surprised that what you're suggesting  a trait which exposes a `log` field (exactly like `ActorLogging`)  but doesn't depend on an `ActorSystem` being in place  doesn't already come with Akka. Did I miss that in the docs? I really think it would be a good thing to have. Developers would then have 3 logging choices  1. ActorLogging (when I'm coding an Actor). 2. "AkkaLogging", which uses the EventStream as you suggest. 3. Implementation specific, e.g. JavaLogging. Useful for applications that don't have an actor system.  But the API remains exactly the same in all cases an `akka.event.Logging` field called `log`. 
that's what I get for copying and pasting!
OK, I'm now using the Thread. It is cleaner to keep the rest of the code the way it is because I need to grab several entries from the `find` result.
Oh, sorry, mixed that up on the phone: I actually meant ActorSystem.findClassLoader, which uses `sun.reflect.Reflection.getCallerClass` if available. I did some profiling before resorting to that extreme measure: getting a stack trace takes 50s while each `getCallerClass` takes 700ns (on some popular hardware) and you need only a handful working your way up the call chain.
I can see that you have had a long discussion, and I admit that I have not read all of it, but my instinct tells me that this specialized LoggingAdapter doesn't belong in akka. If you want to log directly to j.u.l. then you should use that api directly. Exactly what does this `JavaLoggingAdapter` solve that the ordinary `LoggingAdapter` together with the above `JavaLoggingEventHandler` can't handle?
It allows the Akka Logging API to be used outside of Actor systems.  My argument for why it should be included in Akka is that it consolidates usage of various adhoc Logging APIs to be the Akka Logging API.
My vote for JavaLoggingAdapter: :-1:  I don't think that is something akka should try to do. Maintenance burden. Why should only j.u.l. be supported?  
I think `JavaLoggingEventHandler` is a good thing. An advantage over slf4j with j.u.l. is that it doesn't require any dependencies, but I guess it solved some problem also.
That is the epoch milliseconds. Have you verified that it can be easily formatted in logback configuration? Otherwise I suggest that you format it here.
True, but I don't want to waste CPU formatting something that might not be used, and then the user can't really reformat easily. Wdyt?
I could not find anything about reformatting MDC values, so this will just print a mildly meaningless 10-digit number, I fear. What we can do is to print the time of day in UTC, side-stepping the expensive calendar formatting issues. This will usually be well correlated with the SLF4J timestamp because the delay between the generation of the Event and the call to `log()` should not be that long.
I'm sorry, which format function do you suggest? Should we have it configurable (the date format) in the akka config?
It's possible to create a custom logback converter, which can format the millis. http:logback.qos.ch/manual/layouts.html#customConversionSpecifier  That is not something we can provide, since it is logback specific, but possible to document if we prefer that.  If we don't want the overhead of formatting it in Slf4jLogger we can output the millis diff to the log %date. I guess that can only be an approximation, by calling System.currentTimeMillis again.
I guess the downside would be that it would require to re-parse the string to an int and then convert it into a date or something. If we call System.currentTimeMillis _again_ we're calling it 3 times per log message...
For slf4j the usage will be as a "sorting column", since I don't think it can replace %date. Therefore the raw millis is not totally bad. I vote for that, and documenting what the number is.
+1 for that
hmm, I still like the idea of doing ~~~ scala val timeOfDay = millis % 86400000L val hours = timeOfDay / 3600000L val minutes = timeOfDay / 60000L % 60 val seconds = timeOfDay / 1000L % 60 val ms = timeOfDay % 1000 val timestamp = f"$hours%02d:$minutes%02d:$seconds%02d.$ms%03dUTC" ~~~ because it is more useful as a printed value than the millis themselves, and it still is reasonably cheap; maybe put both in the MDC?
MOD isn't cheap and generating a large(r) string introduces more GC pressure. Then it's much cheaper to do currentTimeMillis and set the drift...
well, let me explain where I come from: this is for logging, and the output should be useful for the one seeing it. raw millis are not useful, offsets are also not really useful because all those calculations in the head are not easy. Hence my conclusion that if we include something then it should be human-readable
the formatted timestamp is fine by me, place it in an protected method and it is super easy to subclass and override if something else is preferred
please include a hint what this is about
This will print the time of day (in UTC) when the log entry was originally created.
    val serverSocket: GeneralSocket =        if (udp) DatagramChannel.open().socket()       else ServerSocketChannel.open().socket() 
And it nicely aligns, too! :)
Can you explain why you need this type? Aren't those used from different separate tests? Wouldn't it be enough with one temporaryServerAddresses variant for tcp and one for udp?
I wanted to be DRY, and the cost of reflection does not matter here.
It's a bit over dry in my taste, but ok, fine.
I'd just have one for TCP and one for UDP and avoid the boolean parameter (which is also defaulted so if you aren't aware of the difference it'll silently fail).
Switch to using Akka Logging instead, it's in master
Switch to using Akka Logging instead, it's in master
Switch to using Akka Logging instead, it's in master
Switch to using Akka Logging instead, it's in master
Switch to using Akka Logging instead, it's in master
Switch to using Akka Logging instead, it's in master
Switch to using Akka Logging instead, it's in master
Drop all constructors except for this one
What is this needed for? Might want to look into using ConcurrentSkipListSet (which works just like this, but is concurrent)
If this is only used for the logging, just use Akka Logging instead
If this isn't absolutely critical, this can be removed.
If this is only used in one place for one usage I'd rather just use ConcurrentHashMap and make sure that whatever gets put into it only has identity equality defined
I'm guessing that this can be dropped if we use Akka Logging instead
When using Akka Logging this is no longer needed
Looks much better!
What should be the semantics if receiver ! message throws an exception?
The org.jboss dependencies shouldn't be present in the general interface
This could preferrably be placed in the org.jboss.netty.akka package, as to not "pollute" the interface with an implementation detail.
Remove this val, use app.scheduler explicitly
Should we put this in the base class so we don't need to override it and type the same thing every test?  We can still override if needed. 
What if a null is passed in here last? Shouldn't we have a Queue that records all updates or similar?
I don't think that is possible, because roles comes from `import SunnyWeatherMultiJvmSpec._`, i.e. the configuration object, but perhaps there is a way?
members will never be null, that must be part of the MembershipChangeListener contract (i.e. tested elsewhere if needed)
This didn't work out in field test, and when thinking on it we are mixing two different things here. I think we have two alternatives 1. notify listeners when there is a *change* in convergence, but then it's more like a ConvergenceChangeListener, which should pass the Gossip to the listener 2. notify the listeners when there is a *change* in members, including there status  I'm playing with number 2 now. WDYT? 
it's possible in MultiNodeSpec, but @rkuhn  has written scaladoc about that so I guess there is a thought that it should be user defined
I have fixed this, for the cluster tests, initialParticipants are by default same as number of registered roles, https:github.com/akka/akka/commit/56735477b8758c51ed762629ca1afac7dcbbb96d
The idea was that it may not always be necessary that all nodes are there to begin the test (in general, as in for the TestConductor). It may well be that we always want that line for cluster tests, though.
I still think it would be good to have a default. Now we have the same line in 20 tests. Don't see any value in that. 
as I said, I have fixed it for cluster tests  On Fri, Jun 8, 2012 at 9:26 AM, Jonas Bonr < reply@reply.github.com > wrote:  > > +    """)) > > +} > > + > > +class SunnyWeatherMultiJvmNode1 extends SunnyWeatherSpec > > +class SunnyWeatherMultiJvmNode2 extends SunnyWeatherSpec > > +class SunnyWeatherMultiJvmNode3 extends SunnyWeatherSpec > > +class SunnyWeatherMultiJvmNode4 extends SunnyWeatherSpec > > +class SunnyWeatherMultiJvmNode5 extends SunnyWeatherSpec > > + > > +abstract class SunnyWeatherSpec > > +  extends MultiNodeSpec(SunnyWeatherMultiJvmSpec) > > +  with MultiNodeClusterSpec { > > + > > +  import SunnyWeatherMultiJvmSpec._ > > + > > +  override def initialParticipants = roles.size > > I still think it would be good to have a default. Now we have the same > line in 20 tests. Don't see any value in that. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/523/files#r949416 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
yup, looking good
Missing "must be (true)"
this diff looks weird, the only change is this if statement
Nah, it looks like you've changed indentation
If we want to guard execution like this, then I think that `if (executeMultiJvmTest)` should wrap the whole `if (multiNodeEnabled) ... else ...`
yes, but I was thinking that multiNodeEnabled is something that is actually enabled explicitly (with system prop), but I can change to that, np
    case ((_, sensibleName), (_, otherSensibleName)) =>       val newSensibleName = sensibleName ++ otherSensibleName
I'm just thinking that I want the difference to be minimal.
That's not Patriks fault, it's mine. You're right, something like this would be nicer. ``` case ((_, testResults), (_, multiNodeResults)) =>   val results = testResults ++ multiNodeResults ```
It's no ones fault, it should've been caught in review, but now it falls under Boy Scout Rule :-)
of course :-)
bindings.collectFirst { case (c, s ) if c.isAssignableFrom(clazz) => s }
can Identity work here instead of "(c: Class[_])  c"?
I tried that first, but compiler complained, and said that I had to specify type, ugly
yes, that works
find-map is collectFirst  ser.bindings.find { case (a, b) if a == addr.getClass => b.getClass) }
the java binding should already be known from akka-actor/reference.conf, right? same below. Would it make sense to remove proto stuff from akka-actor and properly describe it in the docs?
by properly I mean mentioning that depending on akka-remote brings in protobuf
that is a huge margin, 20 should be enough
Ok, if you think it is enough.
otherwise the failure detection is broken, I have other tests that use 15 s
please add return type here (to avoid bridge method)
probably better to document a custom dispatcher here instead of changing the global default
please remove `thisActor` name
`private[this]` has some special side effects, and the instance-only semantics are not needed because actors are encapsulated anyway, so Id prefer a plain `private` here.
exception message should require only DequeBasedMessageQueue but maybe give hint about how to configure the unbounded one.
shouldnt this be `super`?
please add a test with a restart to verify no messages are lost and everything is working
Who do you handle capacity violations? (In case of bounded Deque)
that reminds me: there should also be a configurable limit for the stash size.
Good point. Fixed.
Yes, this is now `super`. Previously, I couldn't use `super` because only the self type was `Actor`, but `Stash` didn't actually extend `Actor`. Now it does.
Should this really be able to grow unbounded? There should at least be a config option to set a limit which when reached will clear the stash and throw a StashOverflowException so that the supervisor knows what happened.
I would prefer to declare the super-trait before the sub-traits so the reader knows whats coming.
remove superfluous braces
lock is not needed here, because capacity cannot shrink
ouch, this hurts! What do you gain by making unstashAll() atomic? In case of overflow there will be an exception anyway, which should probably be used to clear the mailbox, stop the actor, call system.exit() or something like that (depending on the context).  Background: we switched to ConcurrentLinkedQueue in order to decouple head and tail and avoid parking threads (which is insanely expensive). This would be the only lock in an actor hot-path.
this importalbeit usedis not really needed, right?
an alternative would be      implicit val sender = testActor  which would be default with ImplicitSender     boss ! slaveProps     val slave = expectMsgType[ActorRef]
technically its more a bounded mailbox, the bounded stash is not yet implemented ;-)
forget about this one: I missed the required constructor arg for reflection
I've added a test with a restart to ActorWithStashSpec.
Calling `unstashAll()` now throws a `MessageQueueAppendFailedException` in case of a capacity violation (this is also properly documented now). Moreover, the stash size limit is now configurable using `stash-capacity` (an int in the dispatcher section of the config).
This is fixed now with the config option (see above). Calling `stash()` will throw a `StashOverflowException` when the limit is reached.
Actually, I had taken the order from the existing `QueueBasedMessageQueue` traits. :-) But, I agree and I have changed the order now (both for `QueueBasedMessageQueue` and `DequeBasedMessageQueue`).
I've removed the lock after giving up on the atomicity of `unstashAll()` (see below).
The original thought was to make the semantics of `unstashAll()` as easy to understand as possible. But, I agree that the price for making it atomic is probably too high, so I've removed the lock.  The behavior of `unstashAll()` is now to simply try and un-stash as many messages as possible into the mailbox, and throw an exception as soon as that's no longer possible.
Yep. I kept the name, though, since the bounded stash is now implemented and has its tests in the same file/class.
this is now redundant, isnt it?
This way you encode a default value directly. We have made an effort to put all defaults into `reference.conf` to keep them central (and testable).      val dispatcher = context.system.settings.config.getConfig(context.props.dispatcher)     val config = dispatcher.withFallback(context.system.settings.config.getConfig("akka.actor.default-dispatcher"))     config.getInt("stash-capacity")  which will never fail because there is a default in the default-dispatcher section.
With this setup (which ensures the right linearization order, right?) users would probably write      class MyActor extends Stash {}  which looks a bit weird. Perhaps rename to StashingActor?
You should most definitely check that stash isn't called twice for the same message.
what happens if you call unstashAll twice in a row?
I'd recommend not adding def enqueueAllFirst(receiver: ActorRef, handleIterator: Iterator[Envelope], size: Int): Unit and just instead do:  for(e <- theStash.reverseIterator) enqueueFirst(receiver, e)
That's completely racy.
Why extend Actor at all. There is a self-type to Actor already. Then user could write:       class MyActor extends Actor with Stash {}  Which is more clear I think. 
OK, I added the default to `reference.conf`. Thanks for the snippet.
I agree `enqueueAllFirst` is not really necessary, since `unstashAll` is no longer atomic. Removed in a recent commit.
Is now always configured (see above).
Makes sense. I added this check. `stash()` now throws an `IllegalStateException` if the same message is attempted to be stashed twice.
It is, but at least the race was harmless: fail fast if you don't have a chance to succeed anyway. (By the time we do the rest the remaining capacity might be even smaller and the subsequent enqueueFirst would fail.)  This method is gone now anyway (see above).
I agree that it's best to allow only one way of using the `Stash`. Having only the self type looks nice indeed.  However, there are two things that this requires: first, I had to duplicate the implementation of `Actor.preRestart`. The reason is that calling `self.preRestart` leads to an infinite loop when `preRestart` is overridden in a user class that extends `Actor with Stash` (like in the test in `ActorWithStashSpec`). The other thing that has to be kept in mind is that the `Stash` trait has to be mixed in right after the `Actor` trait. It's not possible to write `Actor with MyActor with Stash` if `MyActor` overrides `preRestart`.  Removing the self type and only extending `Actor` would remove these two issues, but it would have the problem that Roland mentions, namely that users could write `class MyActor extends Stash {}` in which case we should probably rename to `StashingActor`.  The current solution uses only the self type, adding a note saying that `Stash` has to be mixed in before any trait/class that overrides `preRestart`.
It's true that calling this variant of `unstashAll` twice in a row is problematic, since it calls `become`. So, I think the simplest solution is to remove this version of `unstashAll`. It doesn't add much anyway. If someone likes it, they can write it themselves, and then they are aware of what's going on.  In contrast, the semantics of the parameter-less version of `unstashAll` should be completely clear (calling it twice in a row is the same as calling it once).
Yes, I fixed it (see below).
How do users use this from Java?
If unstashAll throws an exception, if for instance the mailbox is bounded, then preRestart will throw an exception....
This needs to be updated to latest master: this(settings: ActorSystem.Settings, config: Config)
This needs to be updated to latest master: this(settings: ActorSystem.Settings, config: Config)
I added an abstract class `UntypedActorWithStash` that can be used from Java in a way analogous to `UntypedActor`, but with the additional stashing methods. There is an example in the JavaDoc for the class. I also added tests for the Stash Java API.
Good point. In commit f4b7993 I made `preRestart` more robust. It now catches `AkkaException`, but avoids to swallow other exceptions like `InterruptedException`.
hmm, this method now has a rather large discrepancy between what it says and what it does. I think it should just unstashAll() and super.preRestart(), in that order. Then if the queue is full the supervisor will have to decide whether to kill the guy or not. I dont think it should silently swallow exceptions here, and special-casing AkkaException sounds rather arbitrary.  (in case you wonder: ActorCell will catch any exception thrown from here and wrap it inside ActorInitializationException before telling the supervisor of this failure)
I agree Roland. After that change things are good to go into master/release-2.0
`immutable.IndexedSeq` as api type? Perhaps this is just internal?
no exc message?
no exc message?
I trust you, you trust the spray.io guys who trust the Netty guys :)
Are you sure this is the best possible name? :D
I have to trust you on this part, I have no experience with SslEngine
its pretty internal
[{}], also below
well, it is quite precise 
I have no idea either, I just trust the spray guys
pretty ;-) OK
Does this need to be immutable?
Yo, Dawg, I put an ack in your Ack, so you can acknowledge while you acknowledge.
What does this mean?
ticket? If there is, link it from the comment.
No separate client code for Scala?
no, Java must be better served somewhere :-)
So the handler forwards the events to this actor?
I mean people will copy-paste from examples, and it would be not the nicest pattern. Can be fixed later though.
what do you mean? events are sent here (after being decrypted) and `become` is what makes sense when the connection is established, no?
I know where the 16665 and the first 1024 come from, but the extra 1024 for compatibility I have no clue. Seems to work for them :wink:
This is the connection manager actor, no? Shouldn't there be separate actors for each inbound client handling the Received events?
Added Bjrn to trust list :)
Should we use `[{}]` here as well even though it's a test?
Ok, but create a ticket then :)
no, this is the per-connection actor only
Is that your most modestly priced receptacle?
you know it
Why is this using its own ByteBuffer allocations instead of using the/a pool?
these are user writes of arbitrary size, why would we want to spend the effort without benchmarking?
Btw, why don't we just store the Writes?
I can only guess (probably to reduce object allocation counts); @sirthias?
For the record: this is the source: https:github.com/netty/netty/blob/netty-3.6.5.Final/src/main/java/org/jboss/netty/handler/ssl/SslBufferPool.java#L38  However, a `git grep 16665` on the current Netty 4 master turns up empty. Maybe @normanmaurer has access to the latest Netty wisdom as to the required buffer size for preventing BUFFER_OVERFLOWs from the JDK SSLEngine?
Yes, exactly. The `Send` class here is intentionally mutable (through the ByteBuffer), because we want to work with a single instance until its buffer has been encrypted completely (which might take several `encrypt` calls, each of which drains a chunk of the ByteBuffer). Working with ByteStrings here would potentially entail several "slicings" and repeated copies to a ByteBuffer.  It *might* be however, that in the large majority of cases a write can be encrypted in one single go. If so, it could be more efficient to get rid of the `Send` intermediate structure and copy the write ByteString to a ByteBuffer from the pool in the `encrypt` method itself. We could then save the creation of a new HeapByteBuffer at the cost of ByteString slicing and repeated ByteString -> ByteBuffer copies if the write *cannot* be written in one single `encrypt` call. Profiling real-world applications would provide an answer here.
would it be conceivable to construct a heuristic which switches strategies, e.g. based on size? or would it also need to consider history?
One thing that we can be sure of is: large writes will always be broken down and encrypted in several passes. From the SslBufferPool sizing question above we know that there is a max number of output bytes the SSLEngine will generate with a single `wrap` call. This size is < 20K. So, if no compression is involved we can assume that every write larger than the max output buffer size will not be written in one go. One could assume that writes which are smaller than the max output buffer size (minus a safety margin) will *always* be encrypted in one go. So, provided this hypothesis proves valid in testing, we could use a threshold size of maybe 15K as a heuristic for switching strategies here...
I'd suggest removing Send() and just cache the Write for now, then we could see if using a pooled ByteBuffer is faster or slower than allocating a new Send and a new ByteBuffer for every thing going thru.
Yes. Thinking about it again the best approach might be to remove the `Send` and, in the `encrypt` method, copy only the next `maxBufferSize` bytes from the write ByteString into a ByteBuffer from the pool and have the SSLEngine `wrap` these bytes into another ByteBuffer from the pool (which we already do). We can then `drop` the actually wrapped number of bytes from the ByteString and recurse until everything has been written. This costs us two allocations per subsequent iteration: one for the ByteString `drop` and one for the iterator performing the copy to ByteBuffer, but saves us the creation of a HeapByteBuffer per write.
According to discussion in another PR we should use `Duration.Undefined` for "disabled"
shouldn't there also be a case for `Inf`?
Does that work with FiniteDuration?
document "off" and possibly "infinite"
Do we want support for infinite quarantining?
perhaps check for "disabled" and don't do anything here?
That is handled by the caller. This class has no access to settings.
Then it must be a `Duration` and you pattern match when you use it. That is at least how I did it. That is a good indication at the type level that "disabled" is supported.
Perhaps not important, can always be set to a long duration. Inf is a potential memory leak. I agree, skip that.
Would that mean that RemoteWatcher also must check for this setting?
No, no, this class is an internal structure: EndpointRegistry. This is only used by EndpointManager who does the check, and who gets the Quarantine command from the external worlds
Oh, I missed to hook in that command... yikes. Thanks :)
ok, I just double-checked. this is where the feature will be completely disabled when configured to "off", since Duration.Undefined is not a FiniteDuration. Same thing at one other place. 
Thanks! I checked Undefined too
Shouldn't the title be more general, like other concurrency abstractions or something like that?
Cluster before remoting? Is this because we prefer people using clustering to plain remoting?
This might later go into a Deployment section?
Will you remove the test, too?
ooh, yeah, right
I liked the sound of this better, more hands-on
I hate that test, you know :)
no worries, its history ;-)
Shouldn't this be in some external file?
the doc ends early (it can be used to access)
..instance of _the_ Camel class..
Creates _a_ new instance of ..
Is the rethink/rewrite still necessary?
.. of _the_ Camel subsystem ..
little ambiguous "it" in It has to be shut down by the user. (Camel or actorSystem). Rather "The actorSystem has to be shut down.." Maybe remove "in typical scenario" part of the sentence.  
Is this TODO still valid?
nice catch (no pun intended), as well as the one below
no other issues.
It is better than it was, but I am still uneasy about the way this thing works and there is something wrong with names of the methods which is a smell of something else.
I've played with the proxy class loader idea but it doesn't quite work (I suspect ObjectInputStream).  The main difference is that with the Java Hack the code operates directly with the targeted class loader i.e. Remote or Original. But with the Proxy it operates with Proxy instance. In theory they should be equivalent but...  I get the classic RCL deadlock, CL waits for Bytecode, Bytecode comes over the wire but waits until the CL request timeouts before being deserialized.  Because of this RCL communication has it's own thread group and uses the original actor system class loader. While RCL is done with RCL Class Loader and on the primary thread group.  But with the Proxy CL it deadlocks anyway. I suspect Java Serialization does some strange things under the hood. I looked at the Java sources but they look ok. I started looking at jvm.cpp if Class#forName0 is doing some strange things. I surely missed something.  I'd prefer to just keep the Java Hack.
Why is this Java?
Can't be null: http:docs.oracle.com/javase/6/docs/api/java/lang/Class.html#getDeclaredField(java.lang.String)
Don't use return
Can't be null: http:docs.oracle.com/javase/6/docs/api/java/lang/Class.html#getDeclaredField(java.lang.String)
Never use return
Because if I did it in Scala the Scala Compiler said Method classLoader needs to be a stable, immutable value. But I explicitly intended the ClassLoader to be ThreadLocal and very much mutable (but thread safe).
A lot of repeated code here, please abstract and reuse (DRY)
Use collectFirst instead of find
Problem is if the Scala Compiler inlines the classLoader if/when "-optimise" is enabled, then this class will be broken.
What's the rationale for having two OrderedMemoryAwareThreadPoolExecutor ? Do the settings make sense?
Why a lazy val?
We never use braces after case-statements, please remove all occurrences
entire case can be rewritten as:      case someOrigin: ByteString          threadLocalDynamicAccess.dynamicVariable.withValue(getClassLoaderForOrigin(someOrigin)) { super.receiveMessage(remoteMessage) }
arp.getMessage. getMetadataList exists { _.getKey == "rclChatMsg" }
afaik this are non local returns, return is required. The whole ReflectionUtil is quick and dirty copy paste from Java. I was tired. Just wanted to have something that is working. Will clean it up.
The Scala compiler cannot give the same guarantee for Java code as it can for Scala code. I cannot imagine that this will get inlined. Will compile and look into the generated bytecode.
Use @tailrec and write it as a recursive lookup instead, then no need for return.
The first one should just be MemoryAwareThreadPoolExecutor. The first one is the one that exclusively handles the RCL stuff so it doesn't get blocked by us blocking in the classloader.
Because the address we depend on is loaded on demand. We are guaranteed to call this lazy val only when the address is initialized.
The new way of remote tests is incompatible with RCL tests because both work as a custom remote transport. One whould have to be a trait to be mixed in or something along those lines.  I copied the guts of the old remote tests infrastructure to be used for RCL tests. I mention this in the description.
Hm...gave it some more thought. The content of ThreadLocalReflectiveDynamicAccess#classLoader is  return dynamicVariable.value();  java code therefore return ;)  even if this gets inlined it is still dynamicVariable.value() which is fine. I did some compiles and decompiles and failed to see this optimization in action.
No, I meant what happens if Scala decides to use the field (the val) instead of the accessor method?
You're right. With the val in ReflectiveDynamicAccess(val classLoader: ClassLoader) we've just given a public guarantee that the classLoader is stable and immutable.  But:   * We are saying we are dealing with DynamicAccess and not ReflectiveDynamicAccess  private val _pm: DynamicAccess = createDynamicAccess()  def dynamicAccess: DynamicAccess = _pm  Here we have a mutable guarantee with the highest API being DynamicAccess#classLoader()  The compiler cannot change anywhere a dynamicAccess() is used to use a field of RDA. In runtime the JVM can.  If it would be def dynamicAccess: ReflectiveDynamicAccess then sure we've just given a more robust guarantee.    * Our extended class was compiled from Java that has no notion of vals and thus cannot respect our Scala guarantee.  All this goes in favor in Scala should not decide to use the field. ;)  I just can't see this happening.  But ok, I don't like this either. It's a hack because I didn't want to duplicate RDA code and I explicitly didn't want to modify any existing akka code. It solved both.  What do you recommend we do?
How about creating a ClassLoader that just delegates to the DynamicVariable?
And pass _that_ ClassLoader into the ReflectiveDynamicAccess's constructor?
Ok, yeah this satisfies both conditions. I am a bit uneasy since we've just created a proxy Class Loader that delegates to a Thread Local ClassLoader. I don't like proxies. We have an additional method call that can't be optimized.  But yeah I can totally see this work. If there are no other alternatives. Will make it soo.  
I should have known :) I took it across as is. Will fix it.
yes, sure, not a big deal :)
I guess protected[this] would be ok here as well as in the Consumer
nice catch, was there before, removed both, can't think of a reason.
I'd probably not check superclass here, and just check the passed in class first, then interfaces, then recur over getSuperclass until null:      @tailrec def resolve(c: Class[_]): Option[Serializer] = c match {       case null => None       case some => lookup(some) match {         case x @ Some(_) => x         case None => c.getInterfaces collectFirst { ... } match {            case s @ Some(_) => s            case _ => resolve(c.getSuperclass)         }       }     }
and then if this returns None, you just map the class to serializers("default") to make future lookups faster.
all interfaces must be resolved recursively as well, so that doesn't work.  My first approach was to try to get collectFirst working but I didn't find a good way, and it is really only some extra HashMap lookups that are performed (initially).  I would be glad to see a tailrec version, but I don't think it is necessary, since inheritance depth is limited
can't we do, right here:  ... :Serializer = if (bindings.isEmpty) serializers("default") else {   resolvethat shit }
With the benefit that "def resolve" is defined in the logical branch that uses it.
Downside of approach is that the first failed lookup in "serializerMap.get(clazz.getName) match {" will lead to yet another "serializerMap.get(c.getName)" in resolve.  Not a real problem though, just a wasteful operation first time it fails.
(c.getInterfaces.iterator ++ Iterator.single(c.getSuperclass)) flatMap resolve headOption
I think I tried that but headOption doesn't exist for Iterator I can of course do two steps and use hasNext if its better with iterator Seriously, I think the stack based solution is rather slick. It stops on first find as well.      @tailrec     def resolve(stack: Stack[Class[_]]): Option[Serializer] = if (stack.nonEmpty) {       val c = stack.head       serializerMap.get(c.getName) match {         case null            resolve(stack.pop.pushAll(Option(c.getSuperclass)).pushAll(c.getInterfaces))         case x  Some(x)       }     } else {       None     }
I just think the stack bacsed solution uses quite a bit of memory and complicates the code. Would rather see something based on "collectFirst" instead then (and no stack)
The problem I saw with collectFirst was that I had to call resolve 2 times like this      classes collectFirst { case x if resolve(x).isDefined  resolve(x).get }  but there is maybe a trick around that? 
perhaps classes.view map resolve collectFirst or something?
yeah, that is a nice case for view, I have changed to that. Thanks for feedback.
Np mate, great work!
Wow, that's really mean
Couldn't this just be `.foldLeft(0.0)((x, y)  x + (y.toDouble - newMean))`
    (0.0d /: newIntervalsForConnection) { (mean, interval) => mean + interval.toDouble - newMean }
import math.{ sqrt =>  }  val newDeviation = (newVariance)
This constant `1000.0` is it related to the `1000.0` in the `failureStatsFirstHeartbeat`? Then we should maybe name it something.
oops, no that was first attempt, will remove that change
you have to do that commit yourself man :-)
That is definitely an improvement. 
So we use / for division but now  for sqrt? WTF?
use another duration here (20), because inital delay is already 30, and it will remain 30 even if you don't do this
here I guess you have forgot `context.setReceiveTimeout(Duration.Undefined)`  but I think it is confusing to do that on receive of ReceiveTimeout, combined with throwing exc,  clear the timeout when receiving another ordinary message instead
for java it should be `Duration.Undefined()`
this basically begs to be automated in some form
 which is a weak form of said automation ;-)
yes, I don't think it's that simple to automate, the sample code is committed source ScalaVersion shouldn't change that often
What is the reason for those new `from` methods? Java API? Should that be annotated in the docs, as we normally do.
That documentation comment is an implementation comment, and should not be visible in public API docs.
I might miss something, but where is the encoding (utf-8) specified here? What serializer is used here? Isn't it JavaSerializer (java.io.Serializable)?
what is this? how is protobuf involved here?
It isn't visible in public API docs since it isn't public ;-)
Good point, it is Java API, will fix
   ByteString loadTopic =  	 230	 +                ByteString.fromArray(ser.serialize("health.load").get());
Remnant from the old API which also handled ProtobufMessages. Should be removed.
in the javadoc that we might have some time... it's simply not something you should write in /**
yeah, but I mean here, when deserializing bytes to String it looks like you do it in a different way in the scala sample (utf8String)
Fair enough, I'll add the extra comment as a normal comment below
this could in principle break source compat (while keeping binary compat); hence it could not be done in 2.1.x (x > 0)
`v.major() == 2` would be sufficient in the second check
different ticket, possibly: would be nicer with proper extractors
How about offering an (optionally imported) `AnyRef => ByteString`? Then you could do      import ZMQMessage.fromAny     ZMQMessage("mytopic", payload)
Yep, that's been the plan all along.
True, do you foresee an issue with it? :-)
Absolutely, and I'll open a ticket to add that.
add JAVA API
yes, that's better
this header line is wrong (remove)
header line here?
This test is wrong. Will fix.
I don't understand this one, what does it mean?
Is this message helpful to the user? I don't understand how I know what I put and when.
something wrong with this sentense
I'll update this with some describing text/example like in the docs. Something along the lines of:  If your Actor implements akka.dispatch.RequiresMailboxQueue[T] then when you create an instance of that actor it's mailbox type will be decided by looking up T in this mapping.
Yup, cut'n paste issues. Will fix.
feels backward, but there might be a reason      bounded-mailbox = "akka.dispatch.BoundedMessageQueueSemantics"
The reason is that it's a mapping from type to mailbox, and not mailbox to type.  You say that your actor requires `akka.dispatch.BoundedMessageQueueSemantics` and then we look up the mailbox configuration to use.
together with that sentence just proposed I think it is fine (apart from the wrong its)
`e.toString` is the same as `e` here
`parseString("id=" + id)` ?
the `if` expressions could now be inlined (I think that was not previously the case because everything was on two lines)
awesome name :-)
the outer braces can go
perhaps clarify the order of things (who wins), or is that done somewhere?
this means that if  * Props did not configure a mailbox * deployment section did not configure a mailbox * and dispatcher DID configure a mailbox  then the dispatcher section trumps the automatic look-up.  This might be what we want, I just wanted to spell it out clearly (and it should be in the docs as well).
this file might want to have its lines shortened
then you'd need to properly quote and escape
Id suggest to name this `lookupByQueueType`
yup, has a bit of a macro-code-ish feel ;-)
is it conceivable that we might want to cache the result? this might take a while for larger class hierarchies 
more succinct would be: ~~~ scala val q = expectMsgType[MessageQueue] types forall (_ isInstance q) ~~~
consumes creator functions and is typed  I like it
I don't like this. This is run for every actor and is going to generate a lot of garbage.
as I commented below: we might want to cache the `getRequiredType` call since there will be only a finite amount of different actor classes; but even without caching it is not actually that bad for actors which do not inherit from the `RequiresMailboxQueue` trait (it is basically one List node and one call to `.isAssignableFrom`
I think you're right, but we shouldn't even need to call it unless actorClass.isAssignableFrom(classOf[RequiresMailboxQueue[_]]) right?
I dont think the potential double-evaluation is worth saving one List allocation, but we can sure benchmark it.
those lists want reverse_::: instead of ++
Since we are just iterating, isn't it cheaper to keep it lazily evaluated?
Not only that, but the allocation of the foreach closure as well.
This method seems wasteful to me, higher-order functions, option-allocations + getRequiredType. What can we do to improve on this?
I'm not sure I like caching failed lookups, just do like in Dispatchers and return the default? Could we also try TrieMap instead of CHM (as Java 6 & 7 CHM locks on reads)?
Good catch. Meant `e.getMessage`.
Yes, the lines are a bit long.
Good name change.
The `classOf[RequiresMailboxQueue[_].isAssignableFrom(actorClass)` should be blazingly fast, and generate no garbage.
Thanks. Great catch. We shouldn't cache failed lookups.
Wanted more feedback (hence the original version). Turned it into.  ``` val q = expectMsgType[MessageQueue] types foreach (t  assert(t isInstance q, s"Type [${q.getClass}] is not assignable to [${t}]")) ```  
Absolutely. We could store things in a WeakHashMap to not leak classes & class loaders.
noo, not WeakHashMap
Yes, it give me this feeling as well :scream:, but unfortunately the class is the key.  Maybe we should go with some fixed size LRU cache with a max age instead. Does anybody have any good ideas?  And, yes this is for later.
hmm, good point; so it should be a `List[Type]` and here we just prepend `c.getSuperClass`
I made it into an `Iterator[Class[_]]`. Is that ok? 
yes, that would be even better; why do I never think of iterators?
hmm, this order is wrong: the deployment is more specific than the dispatcher and should hence win; I had the impression that that was also how it is actually implemented
So if you define a mailbox and a dispatcher with a configured mailbox in deployment then we shouldn't get the mailbox from the configured dispatcher?
yes, which I think is also what happens: if the `deploy` has a mailbox configured which is successfully looked up, then that is taken; the Props are then tried next (by virtue of being the fallback of the `deploy`), then the dispatchers mailbox type, then the requirement; I think it is just wrong in this list
Yes, you are correct.
Currently it's comma+space separated
we don't want List[String]?
What is the format of said address String?
ok, I will change it to comma only
I think that is problematic from the command line script
Address.toString, I'll add more docs about that
clusterView.leader.fold("")(_.toString) ? :-)
actually it is more like `!isTerminated`, which should be made more clear here (otherwise it could be interpreted as this node is UP).
I'll rename `isRunning` to `isTerminated` in the cluster extension, and remove this attribute from jmx, because it doesn't make sense there, the MBean is deregistered when the cluster extension is shutdown :-)
No need to redo the reduceLeft for every r, right?      for(c <- Option(config.reduceLeft(_ withFallback _)); r <- role) _nodeConf += r -> c
No need for a 2-pass:      nodes foreach {       case (`myself`, t) => thunk(t)       case _ =>     }
    nodes collectFirst { case (`myself`, i) => yes(i) } getOrElse no
See my comments on the other PR
If we decide that removing fNode` should be removed existing usages (in tests) should be updated together with this PR.  My opinion is that ordinary scala control structures (`if-else`) should be used instead `ifNode`. We will discuss this next week, when Roland is back.
This might want to be a `reduceRight` instead, since that would allow short-circuiting (not a huge effect for sure).
looks great! would it make sense to abort the loop if too many `discarded`?
@patriknw I don't think we should abort. Or if we should it needs to be configurable in a setting.
ok, skip the abort, then this is ready to be merged
a small readability improvement would be to place the init and start calls inside the scope of the val creation, i.e.      private val actorCell: ActorCell = {       val actorCell = newActorCell(_system, this, _props, _supervisor)       actorCell.init(ThreadLocalRandom.current.nextInt(), sendSupervise = true)       if (actorCellShouldStart) actorCell.start()       actorCell     } 
Would it make sense to place the call to `init` and `start` inside `newActorCell` instead? Would that remove the need for the `actorCellShouldStart` flag? 
Yes, I thought that too, but it's not possible since the object field `actorCell` needs to be written to before the code in `init` and `start` is run to ensure proper visibility since we basically can start using it at any time after we send the `Supervise` message in `init`.
See comment above. Not possible due to visibility and concurrent access.
Should probably comment this in the code.
 I'm not sure if the `actorCellShouldStart` call is nicer since it's only overriden in TestActorRef, or if we should simply do `underlying.start` in `TestActorRef`. I'm afraid that the `underlying.start` call will be forgotten at some point during a rewrite, while this is more explicit.
What about passing a "start = true" into init and remove "def start" and the flag altogether?
I really can't remove the `start` methods since I need to start the cell separate from creation/init in the normal case, see `Children.scala` line 195, right after `initChild` (which is not this `init`, but is related to the child container). The path to this constructor call is a couple of lines higher up.  Maybe just a constructor argument `_cellShouldStart` with default value `false`, that can be overridden in TestActorRef, is nicer.
Is this guaranteed to only be invoked once?
The only guarantee is that it's only called in one place and only once for every new ActorRef in that place, if that's what you mean.
clients.exists(_.fsm == sender)
Does it matter if the barrier is in an `after` block or inside the test method? I did like this in my test.  ```scala var afterBarrierNumber = 0 after {   afterBarrierNumber += 1;   testConductor.enter("after_" + afterBarrierNumber) } ```  
As I wrote in the commit comment: * It's no problem using after, but scalatest will output the test method   as completed (green) before running after, so it looks confusing in the logs * Using unique barrier names adds extra traceability in case of failures.
Sorry. That is definitely confusing will not use `after`.
I think this comment is misleading, high frequency = short delay, increase frequency => do more often do you need special config for this test?
Yeah. True. Should be 'decrease'. 
remove unused `with ImplicitSender with BeforeAndAfter`
we just decided to not use `after`, see https:github.com/akka/akka/commit/e4104cfd0687ca09943a64e8a42706e1a97ebf1c
do we need a barrier after starting first? most tests have that, but I'm not sure if it's needed, what happens if second tries to join before Cluster extension of first is started?
I'll continue review later
isn't frequency the wrong word? "Frequency is the number of occurrences of a repeating event per unit time." - http:en.wikipedia.org/wiki/Frequency
That piece of code is not obvious what it's good for
why the exists?
Shouldn't this be some form of assertion?
I think this is misnamed, should be "interval" or similar: http:en.wikipedia.org/wiki/Interval_(time)
cluster.convergence must be('defined)
it is, the latch must be countdown within the timeout (otherwise it will fail with an exception thrown)
I agree, frequency is not the right term
Well, of course, it just doesn't give any nice error message on what is asserted.
This config comment is confusing
Just any node needs to be leaving? Can it be more deterministic?
I don't like that syntax at all. It's not type safe, and therefore doesn't prevent silly misstakes or help in refactoring      "foo" must be('defined)     cluster.convergence must be('definod) 
I think there is a race here. If first leave runs first and that is completed on third before this registerListener happens you will never get the notification. need some more barrier.
why do you need long (1s) gossip-interval? timing issue?
What does this test compared to NodeStartupSpec? Do we need both?
Should I use 'interval' instead?  E.g. rename the config option to ' leader-actions-interval'? 
ah, you removed NodeStartupSpec 
It is calling the cluster() method to trigger it to start. In reality you would just do Cluster(system).
To check that there is a member in the node ring with the status Exiting. Is that not clear? 
isn't it possible that convergence can be "lost" immediately after the latch.countDown, i.e. race here?
This two-step await for Joining and Up feels timing sensitive (racy). Another way would be to have only one registerListener and collect all notifications and afterwards check that you got both Joining and Up notifications.
If you know what the 'reaper does, what 'reaping' is and 'unreachable node set' is then I think it is very clear.  If you don't read up on the config option and code. 
Sure. I can add a check for the node address as well. But it can only be one. 
I agree, I don't like it either. I hate symbols in general. 
To be able to check for JOIN before checking for UP. 
Right. Will fix.
I'm assuming that the comment is to aid, and as such shouldn't be confusing, if you need to read the code to understand the comment, then I think the comment should be removed.
Adding a utility method with clear name. 
I suggested that in the other pull request where I used the same scheme but then you said I should keep it as is.  But I can split it up, I'll do that in both tests then. 
renamed all options
Changed the option name. Splitting up the tests so no need for the extra long interval.
Ok. I just didn't think it was clear why it was set to 30 seconds. 
No, I meant, it doesn't matter which node is Exiting nor that it is only one exiting?
Added check now. 
I know. I'm older and wiser now. Actually it was not at all this split into two test classes that I was thinking of now. You want to test that transitions Joining and Up are notified to the registered listener. Use one listener, and inside that send something representing the notification to `testActorRef` and then afterwards use `expectMsg`. No need for latch. You don't need to change this, but it can be a technique to keep in mind.
That is a good idea. More actorish. But on the other hand I think a latch is easier to understand. 
Does this empty line somehow contain secret information, is it some side channel? 429 == 3*11*13, hmm, what do these primes tell us? ;-)
why keep these commented lines?
OHNOES! Good that theres `scripts/fix-protobuf.sh`, try it!
Encode this as a kind of ChildRestartStats instead of a separate Set
There's no apparent semantics tied to this method
Whoa, this section needs some comments-love.
Why include this case at all then?
Same goes here
This seems like duplicated code, collapse and reuse?
I think this shouldn't be enforced here, why don't we make the guardian and systemGuardian LocalActorRefs in the Impl?
Doing that will greatly simplify the rest of the code in this pull request so I'm holding the review here.
Looks flaky, I'd prefer try actor.systemInvoke(msg) or try { actor systemInvoke msg }
The general semantics of this method should be documented here I think. Right now I think you're silently ignoring problems instead of reenqueueing on failure?
I think this should be broken out to a method so it can be reused in cleanup, no?
The enclosing braces can be removed
The encosing braces can be removed
or some move-the-ROBERT-out massage  (done)
because that is the only way to shut up the compiler, I fear
will fix when completely changing this part anyway
Ah, so much nicer. Great. 
Why pass the extension around? 
worth noting is that (this.members ++ that.members) was wrong, because it is a Set you don't get the duplicates that you want
needed to change that to make it testable (using subclass of Cluster in test)
needed to change that to make it testable (using subclass of Cluster in test)
to be able to make it testable, override that in subclass of Cluster in test I don't know if we prefer private[akka], private[cluster], protected[cluster] for this.
Got why the protected modifier. 
Ah, good catch. Thank you. 
Ok. Don't really like it when testing pollutes the code, but no other way to do it I guess. 
I think private[akka] should be good enough. 
ok, I'll change to private[akka]
Great. Probably much more probable to be right. No weird rounding errors.
Three is a magic number? Why 28?
:-) 28 is a nice number I changed from 30 to 25 yesterday. Saw a failure today. 30s is the barrier timeout, and I would like to be less than 30, but still long. I think there is something else that is broken. We have several tests that sometimes fail when we shutdown.  
This is nice.  What happens if you do both startCluster and awaitClusterUp won't the cluster move the node to JOINING again?
I hope not, that is an interesting test, to join when already joined, please create a ticket
Why the watch?
Shouldn't this be      case Terminated(`child`) =>
How can this be right?
Also, assigning vars inside a transformation?
    for(msg <- lastMessage; aref <- lastSender) context.parent ! (aref, msg)
killing the child with stopping strategy and I dont want a deathpactexception. (Let me know if I'm mistaken there)   On Sat, Dec 8, 2012 at 12:14 AM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/test/scala/akka/camel/ProducerFeatureTest.scala: > > > @@ -254,6 +267,47 @@ class ProducerFeatureTest extends TestKit(ActorSystem("test", AkkaSpec.testConf) > >  } > > > >  object ProducerFeatureTest { > > +  class ProducerSupervisor(childProps: Props) extends Actor { > > +    override def supervisorStrategy = SupervisorStrategy.stoppingStrategy > > +    val child = context.actorOf(childProps, "producer-supervisor-child") > > +    val duration = 10 seconds > > +    implicit val timeout = Timeout(duration) > > +    implicit val ec = context.system.dispatcher > > +    Await.ready(CamelExtension(context.system).activationFutureFor(child), timeout.duration) > > +    context.watch(child) > > Why the watch? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933/files#r2354974>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
On Sat, Dec 8, 2012 at 12:17 AM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/test/scala/akka/camel/ProducerFeatureTest.scala: > > > +    override def oneway = true > > + > > +    var lastSender: Option[ActorRef] = None > > +    var lastMessage: Option[String] = None > > +    def endpointUri = uri > > + > > +    override def transformOutgoingMessage(msg: Any) = msg match { > > +      case msg: CamelMessage  if (upper) msg.mapBody { > > +        body: String  > > +          if (body == "err") throw new Exception("Crash!") > > +          val upperMsg = body.toUpperCase > > +          lastSender = Some(sender) > > +          lastMessage = Some(upperMsg) > > +      } > > +      else msg > > +    } > > How can this be right? > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933/files#r2355013>. > >  Its maybe a bit of a strange test but I want to see that I can capture a sender at an ok message (the reason for the test, the sender ref did not reflect correctly), then crash the actor with a bad message and then send the last message to the last sender, if I receive a msg on the last sender (the test actor ref) then it works.  --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
On Sat, Dec 8, 2012 at 12:19 AM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/test/scala/akka/camel/ProducerFeatureTest.scala: > > > +    override def oneway = true > > + > > +    var lastSender: Option[ActorRef] = None > > +    var lastMessage: Option[String] = None > > +    def endpointUri = uri > > + > > +    override def transformOutgoingMessage(msg: Any) = msg match { > > +      case msg: CamelMessage  if (upper) msg.mapBody { > > +        body: String  > > +          if (body == "err") throw new Exception("Crash!") > > +          val upperMsg = body.toUpperCase > > +          lastSender = Some(sender) > > +          lastMessage = Some(upperMsg) > > +      } > > +      else msg > > +    } > > Also, assigning vars inside a transformation? > Yep. just to see if the sender ref is ok during transformOutgoingMessage.  >   > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933/files#r2355028>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
Sure   On Sat, Dec 8, 2012 at 12:14 AM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/test/scala/akka/camel/ProducerFeatureTest.scala: > > > @@ -254,6 +267,47 @@ class ProducerFeatureTest extends TestKit(ActorSystem("test", AkkaSpec.testConf) > >  } > > > >  object ProducerFeatureTest { > > +  class ProducerSupervisor(childProps: Props) extends Actor { > > +    override def supervisorStrategy = SupervisorStrategy.stoppingStrategy > > +    val child = context.actorOf(childProps, "producer-supervisor-child") > > +    val duration = 10 seconds > > +    implicit val timeout = Timeout(duration) > > +    implicit val ec = context.system.dispatcher > > +    Await.ready(CamelExtension(context.system). > activationFutureFor(child), timeout.duration) > > +    context.watch(child) > > +    def receive = { > > +      case msg: CamelMessage  > > +        child forward (msg) > > +      case (aref: ActorRef, msg: String)  > > +        aref ! msg > > +      case Terminated(_)  > > Shouldn't this be > > case Terminated(`child`) => > >   > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933/files#r2354977>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
true   On Sat, Dec 8, 2012 at 12:20 AM, Viktor Klang () <notifications@github.com>wrote:  > In akka-camel/src/test/scala/akka/camel/ProducerFeatureTest.scala: > > > +    var lastMessage: Option[String] = None > > +    def endpointUri = uri > > + > > +    override def transformOutgoingMessage(msg: Any) = msg match { > > +      case msg: CamelMessage  if (upper) msg.mapBody { > > +        body: String  > > +          if (body == "err") throw new Exception("Crash!") > > +          val upperMsg = body.toUpperCase > > +          lastSender = Some(sender) > > +          lastMessage = Some(upperMsg) > > +      } > > +      else msg > > +    } > > + > > +    override def postStop() { > > +      lastMessage.foreach(msg  lastSender.foreach(aref  context.parent ! (aref, msg))) > > for(msg <- lastMessage; aref <- lastSender) context.parent ! (aref, msg) > >   > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/933/files#r2355032>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg book: http:manning.com/roestenburg
I have no clue; will remove. Weird.
how do you know that 300 is enough?
those should be `def preStart(): Unit = {`
INTERNAL API marker
we dont care on Jenkins and this is only optimizing the test output when running on my machineand possibly others, but I dont care about those as much ;-)  So: it works on my box.
might want to bump this one
Is this for JavaDoc? Do we need to add that in all places that we have declared `@deprecated(` as code annotation?
List instead of Vector?
that was the idea, but before we go overboard someone needs to figure out which IDEs display what how, since I could not get Eclipse to show me these in a half-way decent form; it seems that Eclipse does not actually use the -javadoc artifact, instead it parses the attached source if present.  So, in conclusion: Id like to keep this in an figure out later whether that can fill the gap left by Javas weaker deprecation support (i.e. without message).
well, the purpose is calling `.toArray` on it sometime, which is why I wasnt sure thats a win
`s"unknown actor creator [$clazz}]"`
sender? use `null`?
I think this read funny right now. Can't it just be `... it is recommended to create ...`
Same thing here.
ok, I was hoping that it was only a cleanup thingy
good catch: must be `getSelf()` of course
oh, this crept in as well; will add RST docs for that.
Name of the test case might be improved :)
I know this message is private, but the name "Watch" might be a bit confusing
Very nice tests
So why don't we remove the interestedInResume list then? Isn't fail fast better?
What do you mean by "completeness of delivery"?
Maybe a note here, that contrary to the simple NACK case, this allows duplicateless non-lossy delivery (to the network interface of course) if proper user-side buffering is implemented.
that is a looong line
eclipse formatting due to the `;` break it to two lines, or enclose in `{}`
Maybe a note that the reader should have some spare buffer space when sending the SuspendReading othewise it will be overrun because of the async nature of back-pressure notification.
sequence charts would be very helpful. Maybe not in this PR, but a ticket at least.
java constant: `MAX_STORED`
it would not be fail-fast, it would result in stuck actors (because people are allowed to assume that the `WritingResumed` message is not lost on a local system)
that every single message must be delivered
yes, good point
what do you mean, I cannot even see that it ends!
I like this name change!
Just looking at the property name I thought it was something else. I was thinking timeout on the sender side, that it expects the ack within this time. Could we use something else than timeout? flush?
no equals and hashCode for this one? why not case class?
It can be a case class, it is almost always boxed anyway.
No, flush would be misleading. I am thinking right now that I might have   to change the timing logic of acks and resends.
perhaps `immutable.IndexedSeq` as the api type?
`TreeSet` vs `immutable.SortedSet`? I don't know if this will be public api, but if it will I think we should use `immutable.SortedSet` as api type.
Yes, make sense, also for IndexedSeq above.
I like those classes. We should open source them :-) I mean, they will be useful for implementing similar things at the application level.
don't save characters
INTERNAL API marker
You can encapsulate the `var`:      val nextSeq = {       var seqCounter: Long = 0L       () => {         val tmp = seqCounter         seqCounter += 1         SeqNo(tmp)       }     }
Ah, nice! Thanks!
looks like you always go back to receive from idle and gated, then you can use discardOld = false, and unbecome. Don't know if that is more idiomatic or better in any way.
just forget it, I missed gated->idle
I prefer explicit state transitions, unbecome makes it a bit harder to see   where we transition. This could be an FSM btw, but I wanted to remain more   lightweight in this actor.
named parameter for the last param?
Definitely. IntelliJ even warns about it...
place the timer name in a val
`[]` around variables in log and error messages
if it is None, then it is a bug, and we should throw an exception. I should have added a comment here.
So why is it an Option? There is probably a reason. Then I suggest that you add a `require` in the constructor to detect it as early as possible.
Because the Send envelope carries non-system messages, too, and they have   no sequence number.
what is the key? add comment?
Hmm, this is a common pattern of localAddress -> remoteAddress. It might   make sense to create a case class for this purpose.
missing space at end of line
`require` in constructor?
ok, it might be used together with reliableDeliveryEnabled
No, because None is a valid value. The only thing that is invalid is using   it in an acked buffer if it is None.
`val ackAndEnvelopeBuilder = AckAndEnvelopeContainer.newBuilder.setAck(ackBuilder(ack))`
why not chain these?
if (msgPdu.hasSender) Some(... else None
pattern match, since you are grabbing the values anyway
not that the secure cookie adds that much security, but is it really good to log the cookie?
ThreadLocalRandom?  Does it matter here? That testcase is single-threaded.
what is `info`?
use log or remove
This is used when the test fails to get some debug info, similar to how   SupervisorHierarchySpec does it. Log would work too, but will cluster the   output with timestamps and other things.
don't write new tests with `actorFor`      def there = {       system.actorSelection(root / "user" / "temp" / sysMsgVerifier.path.name) ! Identify(None)       expectMsgType[ActorIdentity].ref.get.asInstanceOf[InternalActorRef]     }
Ok, I'll fix them.
code is better than documentation :-)
it doesn't matter, but why not use the good random
Could this be solved with recursion to avoid the duplication? Something along the lines of ```case DeadLetter(m, _, _) if m.instanceOf[Send]  !(m)```? Thx.
No, that would unwrap arbitrary deep nesting, which is not what I intended.
It helps debugging, and no security is violated as there is none :)
Adds additional output to the test case (colored green if everything OK). Useful in stress stests.
not cluster but clutter....
why is this required? and why is it a number?
in akka.serialization.Serialization:     Try(serializerByIdentity(serializerId).fromBinary(bytes, clazz).asInstanceOf[T])
yes, but there is no mechanism for synchronizing these IDs on both sides AFAIK; I sure think that it would be a good idea to implement that, but even then the ID should be `optional` here
my guess is that this number is too small, but well see that during testing
so in case of no traffic flowing back we will see four resends even if the message was already received at the first attempt?
Log files tend to be sent around, so I don't we should do this. The received cookie can possibly be logged, but not the expected.
thanks, didn't know
Yes, one more zero is needed
Only if the immediate ack is lost. I will clarify the comments.
Btw, most of the default settings should be fine-tuned after benchmarking carefully. 
should this not be a `<=` to enforce strict monotonicity?
please use parentheses for such statements, no braces
Why? I don't want to allow buffering the same message twice
Ah stupid me, I see! Thanks, this sia bug
this code optimization has de-optimized the value class: `Function0` returns T which means boxing; it might well be that sending system messages is rare enough to justify this, I just wanted to have said it
It is a case class already. There is too much boxing anyway, so it did not make sense at all to keep it a value class.
Although we can change that back, but that needs some care then.
this sends to deadLetters in an order which was not the original one; I guess we should honor the implicit assumption that deadLetters falls into the same recipient category
this does not do anything: such code needs to go into `postRestart` so that it fails again (and `context stop self` before throwing)
What do you mean by that? I mean the order is not violated as far as I see, only there are holes.
this `System.nanoTime` call is unnecessary if the deadline was just moved three lines above
I need to correct myself: since the result of `nextSeq()` always goes into a `Some()` boxing will happen immediately anyway; an optimization would be to encode the optionality of a SeqNo by reserving `Long.MinValue` as undefined.
If we get rid of overwrap handling, than we can just go with Long, and then MinValue can be used safely
why not send the `Ack` directly to the grand-parent?
if the `SeqNo` is completely out in the woods (i.e. 1e9 greater than what we know about) then this will generate weird errors; I think there should be a maximum permissible distance here, and it should be waaay less than a million (say)
It does not always exists. For read-only inbound connections we don't allocate a ReliableDeliverySupervisor. But it can be optimised, I just have to pass down the ref in an option if present. 
So it is always guaranteed that all `nacked` are sorted before all `nonAcked`? It seems that that would be the common case, but I was not instantly sure that it is a given.
we can also leave it as is for now and think about optimizing once we have benchmarked ;-)
please move comment before `if` to make it align properly
I think this was changed to an Extension on Patriks branch
It is. nonAcked is always the messages with larger seq than the cumulative ack, and nacked ones are the explicitly nacked, which has to be below the cumulative ack.
I trust you
okay, then its fine
Let's be extra-safe then. I will add a configurable limit. If we detect such a stray seq, we should quarantine.
I wanted the pseudo-else statement to be below. Scalariform changed the identation though.
Ok, I will port it. 
yes, I know; having Scalariform is a huge overall win, so we have to live with its warts
In fact, the sender should abort too, if the gap between the lowest buffered and highest ones is too large. Unfortunately this is not the same as the buffer size.
perhaps `else ()  ...`
If we make it optional, then we would have to require either an id or a manifest, to be able to deserialize the message.
Problem solved:       protected val counter = MTF.tfCounter
right; I wanted to keep the keep em separated semantics which we had previously; not sure who could possibly depend on that, though
I'd prefer my solution over using a seed-approach (sine the seed approach doesn't solve the problem of similarly named threads)
Normally we always add      /**      * INTERNAL API      */  @viktorklang mentioned something yesterday that I had not thought about before, `private[akka]` doesn't even show up in scaladoc, so why why do we have this ceremony with INTERNAL API? One argument would be to make it super clear for Java developers looking at the source code that this is not supported API - and that is good enough argument for me, but then we should use that convention consistently.   Sorry for the novel, I was just thinking aloud, and it was not meant as criticism to this minor oversight here.
yes, good points, will add the doc comment.
Hmm, which problem exactly is not solved? If my proposal solves the issue at hand and is a tad less invasive than yours then for 2.0.x Id say we go with the smaller fix. @henrikengstrom should confirm that this works before we go forward with this back-port.
    val m1 = MonitorableThreadFActory     val m2 = m.copy()     val t1, t2 = m.newThread(someRunnable)     val t3 = m2.newThread(someRunnable)  Your solution gives same name to 2 of the threads, right?
yes, but that is not what happens in an ActorSystem: the .copy() is made per dispatcher, and dispatchers have distinct names, hence the same number may be appended to different strings
I should probably add that I verified that within our code-base copy() is only ever called with a new name argument
It smells like quick and dirty since it makes assumptions on usage rather than semantics. It is also more code than the other solution, which also incidentally solves the aforementioned issue.
Of course thats all correct; however my base assumption is that BC fixes for old-stable releases do not work the usual way, since a lot more weight is put onto keeping existing semantics.
True, we aren't expecting many/any other changes to the 2.0.x release track once 2.1 is out anyway.
do you have a better idea?
our convention is to enclose variables in square brackets in log and error messages: s"bound to unknown SocketAddress [${socket.getLocalSocketAddress}]"
move up 1 line
So if this message is never delivered, we leak in terminatedQueued?
I don't understand this line, why use an O(N)?
you mean join the lines? the result will not be aesthetically pleasing (I did this to make the arrows align)
all the others are on the same line. consistency
well, there is a conflict here, a choice to be made: perfect consistency is impossible
when will that happen? feels wrong
ok, it's back as auto receive, then note to self: "don't forward Terminated, use a WrappedTerminated"
this will be removed once `actorFor` is gone: if you call `context.unwatch(context.actorFor(...))` we should actually remove all watches at the given path, which was not previously done, and which unfortunately will need to be O(n)
ok, true, then it is no problem, just another motivation to do the right thing
yes; the alternative is to deliver `Terminated` immediately, i.e. with system message semantics, and nobody liked that  Ill argue that an actor for which this leak is a problem has bigger design issues that this.
... actor _had_ terminated ...
I like the one-liner, too.
How will the user know if the callback fails?
How do I remove a callback?
logging is a godd suggestion, thanks
you don't, and I don't think there is a need for that, this is similar to `ActorSystem.registerOnTermination`
So if I have an actor-service that wants to "unload" at runtime after a while, it cannot?
yes, you can, that is in your control, since you started it this actor is just a listener that runs the registered callback at the right time, once if you change your mind and don't wan't to do anything in your callback that is also fine
But the callback will never be garbage-collected, right?
it will when it has been called, this actor is stopped
Alright!  How do I specify which ExecutionContext the callback will be executed on?
Do you really think that is needed? The purpose is to kick off other things, like starting some top level actor. Wouldn't default dispatcher be good for that? I can of course add a ec parameter that without problem, but not just because I can. We don't have that for terminationCallbacks.
How about user passing in a Promise[CurrentClusterState] as the callback, and then they can just add callbacks to that with whatever EC they need.      val p = promise[CurrentClusterState]     p.future.onComplete {  }     cluster.registerOnMemberUp(p)  Then it hooks in with existing Future/Promise machinery, and it is clear that the same rules for closing over state applies to the callback/thunk passed into the registerOnMemberUp call.
Or even having registerOnMemberUp be:      def registerOnMemberUp[T](f: CurrentClusterState => T)(implicit ec: ExecutionContext): Future[T] = {        val p = promise[CurrentClusterState]        val f = p.future map f        currentClusterStateListenerActor ! AddListener(p)        f     }
if go down that path we could just expose a       def onMemberUp: Future[CurrentClusterState]  and users can register `onComplete` callbacks on that.  I'm not sure what api is best. Will think about it, and expect other team members to join the discussion.
Very good suggestion!
syntax error  The callback will receive X as argument?
this would better be done as      getInt("...") ensuring (_ > 0, "... must be ...")
yes, using the `Future` API for callbacks sounds like a good plan!
created ticket for that, since ensuring is an assert, https:www.assembla.com/spaces/akka/tickets/2798
I discussed the api design of this callback thing with @rkuhn, and while the Future suggestion is a nice idea it doesn't fit in with other similar things, such as registerOnTermination. Therefore we keep it as a simple thunk callback. We decided to simplify by not providing the CurrentClusterState in the callback. Now it is a plain thunk/Runnable.  In Rollins this will be replaced by passing in Props of a top level actor to ActorSystem or Cluster extension.
One is a class of implementations, the other is a scope; these two are not any longer aligned (since there may be non-local cells).
WDYM? Nothing changed, apart from using named parameters for `Boolean`.
then I might as well `c.get(name)`
Hey, atleast I gave you options ;-)
no, but `addChild` is a spin-CASing bastard and we dont _need_ that volatile write to ruin everyones day ;-)
perhaps a good comment? ;-)
there is a comment three lines further up which explains this in great detail
okay, will add bounds to breadth of printed tree; zipper is beside the point because a String is what is needed in the end
Sorry man, must've been tired
Thinking a bit more, I just added big warnings to the docs. It only serves its purpose if it prints the whole tree, nothing else.
I'm not sure I agree there, a signle String is not needed at all, which is my point, perhaps better to chunk it into multiple Strings that are pulled out instead of generated strictly, Stream[String]?
Its part of the illusion that there might be a different implementation of `ExtendedActorSystem`. But I suppose I should change the guardians type.
 which Ive done
I really do hope that nobody tries to call this method who has trouble deducing that from the existing ScalaDoc. But Ill add a dare in any case.
will add a comment explaining expected semantics
yes, it looks a lot more funny outside of Eclipse.
Theres nothing being inferred, and Unit is not actually an object when used as a return type. `{}` is an empty block, which is precisely what I mean (in this case Id actually prefer `def suspend() {}`)
`null` never was allowed, but the check was not actually executed (because `null.isInstanceOf[AnyRef]` returns `false`)
To avoid an object creation: the result of `c get name` is an `Option[ChildStats]`.
 and here
because of the bounded waiting time for `tryLock` which has not yet been implemented here ;-)
apart from that `super` is an ActorCell while `p.recipient` is an ActorRef, and unfortunately the real method is `!` in one case and `tell` in the other. Hmm.
Why the distinction between ActorRefWithCell and RepointableRef?
(c get name) plz
systemService popped up again?
why check isEmpty prior, does addChild throw up if already exists?
Needs some introduction
Needs some introduction
say what? Is it wise to publish this prior to being constructed?
I do not like this one. It's better if it was a Zipper or something, you risk having millions and millions of actors being put into this string, creating a huge blob that OOMEs
Shouldn't this be caught earlier? Make systemGuardian: LocalActorRef
Perhaps add that it shouldn't be called twice?
Soooo this is UNGUARDED and shouldn't be called in more than one thread at one place?
Wow, this does a bit more than just creates a new Cell, it creates and Activates a new cell
wow, unorthodox to say the least, add a dot please:  try cell.tell(envelope.message, envelope.sender) 
Unit == (), use that instead of getting it inferred as when using {}
is null not allowed any more, reason for change?
why is the cast needed?
why use ReentrantLock, why not synchronized?
tell vs. ! for two adjacent lines of code I think you should stick to the same
This is equivalent to `nextStateData.deadline.timeLeft`, which is available by implicit conversion from `nextStateData.deadline`.
This is included in the next line automatically.
hmm, maybe add compare ops to Deadline (looks like an omission to me, and Im allowed to say that ;-) )
please dont use default arguments for internal convenience: they are a PITA when refactoring (due to missing errors), and thus should only be used in stable user interfaces when they make the API _that_ much more slick.
    val stop = Deadline.now + timeout.duration  and then use `stop.timeLeft` and friends.
the scaldoc problem? need to run the scripts/fix-protobuf.sh
we should add the scripts/fix-protobuf.sh to the instructions, or better, make a script `generate-protobuf.sh` that does both proto and the fix
might want to rename this to `enterBarrier` for clarity
Yup, they where called that at one point. Will change.
As this is part of the build, Id prefer having one script which does it all, and which can be invoked by a task from SBT. Obviously this would not be run by default as that would require everyone to install `protoc`.
I've added instructions to the comments. Changing how this is generated is ticket #1798.
Id rather include that in the Data so that it is available during error handling.
and this would need to effect (possibly) a shortening of the timeout; the easiest solution would probably be to keep a Deadline inside the Data object and calculate a new deadline here and reschedule the StateTimeout in case this client wants to fail earlier.
Hmm, youre placing part of the failure handling inside the actor, while all other failure handling is done in the supervisor. Probably better to just keep it as it is and do `failBarrier()` in the supervisorStrategy for this case.
see my comment below: this is replicating the supervisors fault-handling, so Id simply throw an exception in this case.
given the points above Id leave this method as it was.
two bools with bad names: while youre changing this, I think it would make sense to switch to an `enum BarrierOp` here which has four cases (enter, fail, succeeded, failed). In principle these could all be mapped onto a single bool, because the different meanings occur on different message directions, but that would be ugly for no gain (the enum takes exactly the same space).
ah, good one
it would simplify the code, then, to have `required BarrierOp op = 2` (i.e. non-optional)
why not j.u.c.TimeoutException?
why not wrap this and include the failed barriers name?
Maybe I should have added a comment: this call is somewhat equivalent to a barrier when done for the first time, which is why I chose that. On the other hand our tests all require all participants to have joined before doing anything else, which makes this moot. Hmm.
So how would I get hold of the value during the transition?
First one in wins was a design choice ;) Sure that would be nicer. 
sayjust out of curiositydid you have some debugging trouble inspiring this change?
Yes, all the logs come first, and the test results afterwards, making it kind of hard to read. (For me at least ;)
yes, you are right: better make it configurable separately, defaulting to 5sec (slow multi-node Jenkins can override using system properties, right?)
yes, this looks like a good one to me!
yup, good one!
Yes, I'll untangle it and clean it up.
You can remove this line. Terminated is no longer sent as normal message, and this is a synthetic actorref, so it does not generate it either.
We discussed it and agreed on int.
yes, and it is supposed to always be used together with an `Address`
Ill call the newline police!
Id prefer `"disabled"` or some such to turn this feature off; can internally also be represented as `Duration.Undefined`
You mean that the property should be allowed to be both a string and a duration? Isn't that confusing? We have similar at other places:            # Use 0s to skip delay.           stop-delay = 1s        # negative number means infinite timeout. It is only used for type=Dispatcher       # and only if mailbox-capacity > 0       mailbox-push-timeout-time = 10s
yes, we have not been consistent from the start, but recent additions (e.g. IO layer) have been done like I suggested; maybe I only reacted to your choice of magic value, because `0s` could well mean retry immediately (whatever that entails)
ok, then I look at IO layer
sometimes I miss good ol C: `(a > b) - (a < b)`
I think the UID should be in an Extension instead and everyone just references it through that, keeping the API footprint of RARP one tiny bit smaller.
ok, that will probably be the smallest extension ever :-)
I think it would be good to harmonize this so it's clear when something is used or not, i.e. using Undefined internally and "off" or similar in the config file.
It always depends on what constitutes an intuitive value in the given context; Id also not like to break peoples working config by reinterpreting it behind their backs, hence my proposal to do it like this for all new additions; maybe we can slowly migrate the old settings where needed, but the cost-benefit ratio is prejudiced heavily in the cost direction on this one.
If we want to change this for 2.2 what would potential solutions be?
That depends on the problem, which remains unstated; in other words: what does this mean in your question?  In particular: the skip delay example above is completely fine, since a delay of 0s means no delay. `mailbox-push-timeout` could grow an additional infinite syntax without breaking anything.
extension added in 0270e90
fixed in cdf9fbb, note the cleanup of io settings
yes I can't understand that `;` is so popular just because it is in a match case. :-(
it has never been released. we added serialversionuid in cluster last week (or something)
should also say that joining only works once for a given system
ok, and I see that I should update this `0s` thing to `off` :-)
`configure` sounds pretty imperative given that the return type is `Option[_]`  (and the method definition is 95% types)
removed and added to migration docs
Should we not run this spec with both the FDs? 
Fold config into a one liner. 
This will break if we ever change the node ring sorting from plain name to something else right?
Well, this spec does nothing but gossip, and the intention is to test real healthy cluster. Testing with puppet would tests nothing at all, so I don't think that is necessary.
You can use just `cluster.status`, it is the same thing as `cluster.self.status`.  Same throughout the file. 
no, it is actually using the real cluster Ordering via Ordering[RoleName] which is defined in MultiNodeClusterSpec
Ah, got it. 
Just curious why we want max of these values? Is it so you can disable each of them individually by configuring the intervals very high, or is there something else going on?
That line answers my own question I think. Disable tasks individually it is.
It's those settings:      periodic-tasks-initial-delay     gossip-interval     heartbeat-interval     leader-actions-interval     unreachable-nodes-reaper-interval  For tests we need a simple way to disable all, or one, of these. Disable all is no problem, but disabling one was not possible, because it was run "once" due to the initial-delay.  Also, I think it's rather logical to not run the first task earlier than the specific interval.
yeah, you got it
I'd definitely recommend against stringifying a Message since that could potentially be _huge_
ah, thats a good point, will take out.
I'd avoid using origCause in message
nooooooo please cant we have this somewhere else?
It has a certain odor
Make it a one-liner, for teh eyes
false == Directive.Resume?
Can't we detect this so the need for the boolean disappears?
Always put unlock in finally clause.
How many times is the standard import statement repeat factor?
Clean up these, NonFatal should be grouped wit h the others from util
Isn't this method wrong? Don't you need to check getByRef every tailcall?
Why create the intermediate collection?
Since you're doing removeChild anyway, just do it at the top of the method.
Why assumed to be normal/terminating, leave as abstract
not case object?
not case object?
why not view here?
get the imports cleaned up
clean up imports
When should the non-recursive be used and when not?
Needs explanatory comment
Needs explanatory comment
comment this line
When does restart throw an exception?
Can't we make this 2 different messages instead?
Is this still true?
no, `false` means not in response to failure; in this case it means in response to the `suspend` five lines up
WDYM? I dont think its a problem.
I asked myself that very same question and came to the conclusion that the detection mechanism would qualify as black magic, at least from the users perspective, and I dont like making user-facing API magic. If you have a better suggestions, shoot!
Why? In case that is actually relevant, were blown out of the water already anyway. This can only fail when interpreted and we hit a SOE for calling `suspendCount()` (in which case the actor will be defunct afterwards) or when facing a VME. I can of course change it, but please give me a reason for doing so.
As high as Scala IDE likes to roll it. Last I heard they used a D20 for that.
No, because the change cannot happen in that direction as a race: the only entity removing a child is the actor itself, and the only entity which could be racing is somebody who calls `attachChild`, and there we are guaranteed that that child cannot yet have died (since it has not yet been created).  Ill put the above in a comment.
Doesnt gain much, will need override anyway. Its not about `assuming`, its about not repeating.
Thats a LOT of ceremony in this case. Still want it?
because it was not documented that the result shall be lazy. done that now, also for `children`
there is a comment nine lines down which explains this
whenever e.g. sendSystemMessage feels like it
We could, but I couldnt come up with a good name which was not `Resume`, and that one is obviously taken. Dispatch within ActorCell is also a bit more concise like this.
That was a good one: with this change, no actor system stops anymore ;-) So, no, this is not a good idea  (I did have some memory of having a reason for this; since every single test fails, does this really need a comment or should people who change this not find out by themselves?)
Just a spontaneouos reaction to the name mangling. We have tests that will cover this.
I didn't think this was user-facing?
Some day someone will either add some code in there or refactor it to be a method call instead... Putting try-fainally is just a very cheap way to ensure structural integrity and harmonize the way locks are handled. It would also make me sleep better :-)
Thank God it's not a D100 ;-)
Great. It's one of those things that could just silently break
`SupervisorStrategy` is not currently `private[akka]`, and I introduced these methods specifically to make it possible to meaningfully subclass it. The comment at the top makes it very clear who is NOT expected to roll their own.
Can it be both terminating and normal? Otherwise I'd recommend just having the isTerminating (and perhaps implement the isNormal as a final method that returns !isTerminating. To avoid the true-true or false-false case.
I thought the standard was to use case-object if it is an ADT
Yeah, it'll allow us to aggressively remove methods that are factored out. less code == more joy :-)
We have to double-check that stats isn't use in a strict way then.
Add a comment that points to the 9-lines-down comment :-)
hmmm, should we allow that? Ideally a sendSystemMessage failure should put it on resend-duty for the "guaranteed delivery" of system messages, right?
Resume and TryResume?
I thought we had the suspend count in a var? (I thought I saw that somewhere in the PR)
Wow, what happened? (I assumed that "val n = removeChild(..)" was moved to the top and that the second removeChild was removed?)
Ties into this: http:www.assembla.com/spaces/akka/tickets/2205
yes: ca. 90% fail if this is wrong ;-)
They cannot both be true, but both can be false (during a restart).
`toString` is overridden in all cases and pattern matching not done, so I dont see the value it brings. On the negative side we have the problem that `NormalChildrenContainer` cannot become a case class as is because of the resulting duplicate `apply`.
okay, checked again, result still the same.
the question is not about allowing, the try-catch here is just for robustness.
Nope, there is not trying involved in either case. Its a Resume, for two different reasons.
I am still convinced that the mailbox status is the intuitive place to put this logic: suspend two times, resume once, no go. If you put a counter somewhere else, it will only get more messy. The thing you saw is in UnstartedCell, to keep track of `suspend` before actual creation.
`removeChild` may well change the type of ChildrenContainer (thats the whole point of it). Added a comment.
Yes. The methods mentioned in that ticket have been removed (inlined in `cell.FaultHandling`), and these methods added to complete the picture. Do you think this fixes ticket 2205? I would not like to put an actual example with description into the rst docs, because that is too prominent.
I have corrected the comments and renamed the `becomeOpen`/`becomeSuspended` methods below to make this more clear.
msg.getClass will be None or Some right?
okay, will make this smarter and then merge it in.
Are we sure that suspend&resume is _only_ used in restarts?
Yes, but your question triggered some fortunate neuron which made me realize that without keeping count of how many times a ref was suspended we cannot really be sure that everything remains correct. Ill whip up a supervision stress test and cross-check.
Should this extend AkkaException? 
This smells quite heavily of dead code.
I think you want to return `true` here?
Fixed both. Thanks. 
No it is not ok. Damn. How can we fix it?   I wanted to avoid the more complicated solution (that I had first in mind) in which the first leader: 1. moves himself UP -> LEAVING 2. steps aside as leader somehow - don't know how 3. new leader is "elected" 4. new leader moves does handoff, moves old leader to EXITING and to REMOVED 5. old leader received REMOVED message and shuts himself down  This is rock solid I think, but how to handle 2) ?  
Ok. I'll change it. I just like to fold things. 
Ok. I think we solved it during lunch. 
I like folding also :-)      (localSeen /: removedMembers) { (seen, removed)  seen - removed.address }
I also like folding, but not for such a simple thing like removing a bunch of entries from a Map. I think the built in high level functions should be used when appropriate and fold when appropriate.  It's all about readability and how quickly  one can see the intent.  /Patrik  18 jun 2012 kl. 15:24 skrev viktorklang<reply@reply.github.com>:  >>                 member copy (status = Exiting) >>               } else member >>  >>             } >> -          localGossip copy (members = newMembers)  update gossip >> + >> +           removing REMOVED nodes from the 'seen' table >> +          val newSeen = removedMembers.foldLeft(localSeen) { (seen, removed)  seen - removed.address } >  > I like folding also :-) >  > (localSeen /: removedMembers) { (seen, removed)  seen - removed.address } >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/547/files#r1001080
Yep. I agree. I fixed it to your favor. Along with all the other stuff. Take a look. 
Don't need the MemberStatus prefix here
Would it be possible to set self.status to removed and do away with isRunning?
No. Since the node is no longer part of the node ring (or any other part of the gossip state). 
Well then technically self should be an option or status should be an Option, if there can exist such a state where it doesn't exist.
No. REMOVED is the tombstone state. Never present in the gossiping exchange but part of the semantics and the model.  A node can only have status REMOVED if if is removed and not part of the cluster any more. I am only returning REMOVED now to the user since that is what the user would expect from a REMOVED node.  'self' should not be an Option since then we would have to maintain the node's state in two different places.  The current impl is the way to do it. 
That's a great explanation to put in a comment above this method!
Ok. I'll do that :-)
Adding this:     /**       * Member status for this node (`MemberStatus`).       *       * NOTE: If the node has been removed from the cluster (and shut down) then it's status is set to the 'REMOVED' tombstone state       *       and is no longer present in the node ring or any other part of the gossiping state. However in order to maintain the       *       model and the semantics the user would expect, this method will in this situation return `MemberStatus.Removed`.       */      def status: MemberStatus = {        if (isRunning) self.status        else MemberStatus.Removed      }
This one looks pretty ugly to me, multiple vars and side-effecting in filter
why map + foreach?
why map + foreach?
Why toSeq here?
The only alternative I see is to duplicate basically the same logic one more time to search for the ones I have moved from EXITING -> REMOVED (in this section) and from LEAVING -> EXITING (below). And if you want to get rid of the 'hasChangedState' var then I also need to duplicate the code for the JOINING => UP section. That is both less efficient and move verbose. Or do you have a better solution? 
I found it more clean than the alternatives. 
I don't know. I think Patrik wrote that line. Can perhaps be removed. 
But I can change it to this if you like it better:       removedMembers foreach { member         val address = member.address        clusterCommandConnectionFor(address) ! ClusterLeaderAction.Remove(address)      }
What about:      val (newMembers, removedMembers) = localMembers partition (_.status == Exiting)     val hasChangedState = removedMembers.nonEmpty     newMembers map {   
this is incorrect, if Member already exists in localGossip.members it will not be replaced by the new member (LEAVING)  test coverage?
also, what happens if the member doesn't exist in localGossip.members, for example moved to unreachable, then you get it a two places
That's nice. Forgot about 'partition'. Thanks. Still need to repeat the logic but it won't be that bad I think.  
yes, the order is not needed for this compare
Isn't this the same as:      seen -- removedMembers.map(_.address)  I think that is easier to read than foldLeft for simple removal.
ah, but what if the gossip fails, e.g. picks a node that happens to just become unavailable? is that still ok?
so for unreachable there is a check only for self, but not for unavailable just asking so that it was a intentional change
is this same as `unreachable-nodes-reaper-interval`, then it should be read from config
should values in this cache expire? a node could go down, start a new remote transport and try to rejoin, could it not?
I second this concern. Why do we need a cache? Is the performance of node lookups critical?
Does it make sense to use the same role name for a node started on another address? If that is the case we should definitely not add this cache.  As next step I would prefer that we use role names in the tests instead of mix of address and role names, that's why I added the implicit conversion as suggestion towards that. Lookups can be done in tight loops, such as awaitCond. Of course one can cache that locally in a val, but knowing that it's no cost to do `node(role).address` means that you don't have to think about that part.  If you think it's a bad idea, and I have no problem with that, we simply don't merge this.  
Okay, I just wanted to raise this point so that everyone is clear about it: reconnecting the same JVM with a different Address is simply not possible (there is currently neither a test which does it, nor is there any facility which would support restarting the RemoteTransport, but the restriction on the test side should be clearly documented nevertheless). One thing which could be done but wont work is having a test case which creates multiple ActorSystems, one after the others failure.
I'll move the cache to MultiNodeClusterSpec to not put any constraints on the general testing framework.
    addresses foreach failureDetector.remove
dead foreach failureDetector.markNodeAsUnavailable
dead foreach failureDetector.markNodeAsUnavailable
make it a one-liner
If we make nr-of-nodes a section we can cut down on the BP a bit
I usually use `val idCounter = Iterator from 0`
it is not obvious how this could ever be false; maybe explain the mechanics of this test driver in its class comment
that makes each of the trees local-only (since these Props would not be serializable); just clarifying my understanding that only the top Worker router is going to be spread out across the cluster
why shutdown after start of successor?
nice, will try that here
ok, will do, it's simple possible flow control the scheduled SendBatch messages will be ignored if not enough acks received
yes, that was the plan, and not that unrealistic
For each iteration I want to start one member and shutdown one, therefore I shutdown the one that was started in the previous iteration. Do you see a problem with that? Is there a better way?
I meant to ask why this line is not moved three lines up
yes, that would look better, thanks 
So it looks like we added back the member (two lines down) when we received a `MemberDowned` before this fix. Is that correct?
@bantonsson That is correct! @rkuhn the problem is that we never go from Down to Removed, which will be fixed in another existing ticket
ah, okay, good to know
first test run on jenkins was successful, which doesn't mean that it's fixed, but anyway a good start I'll keep it running I'll fix review comments also
thought some more about this, you are right, that was a safe net that was not needed, removed, thanks
Nice catch! Sorry about the broken merge. 
np, it was a mess with fw/back porting, and not back porting this change
now we also have a test that actually catch it :)
this looks suspicious, but you only use it in the internal diff functions, so that's fine
double import of SortedSet
shouldn't this be removed?
Thanks, IntelliJ seems to go for `collection.immutable` by default. Will clean that up.
Nice catch. It should. Not referenced from anywhere.
this is more elegantly solved by changing the supervisors strategy (which is where the logging happens)
and thanks for pointing me towards [this issue](https:www.assembla.com/spaces/ddEDvgVAKr3QrUeJe5aVNr/tickets/3222)
Oops. I wasn't aware of this behavior. Good to know...
Right, good idea. I'll update the PR.
`cause match` ?
I had a match before, but it really doesn't add any value since I don't need the cast anywhere. So I found the single `isInstanceOf` actually a bit more readable...
Anything here that is likely to throw anything?
Not sure. I simply copied the logic from the `OneForOneStrategy` superclass: https:github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/actor/FaultHandling.scala#L334 (which unfortunately is private and therefore not directly useable from this derived strategy). We could mark the `publish` method of the `OneForOneStrategy` protected if there is no argument against such a step...
yes, making that `protected` sounds good for me
is this sent over the wire? serialVersionUID not likely to change, but
I don't see any scenario where we'd bump that number :/
If you don't define it, it will be computed at runtime based on methods and fields of the class, so if the implementation of a case object changes, then it will change "magically". Maybe getting in the way of rolling upgrades.
    private case object NullResponse {       @override def toString: String = "null"     }
I don't think we should add that toString, then log messages will be very confusing, is it "null" or the "NullResponse"
Will add the serialVersionUID
>       @override def toString: String = "null"  Wouldn't that be confusing?
Oh, sorry, can't do that for 2.0 as SerialVersionUID isn't fixed in 2.9:  TODO add @SerialVersionUID(1L) when SI-4804 is fixed
My intention was not to suggest that you should add toString, it was an example scenario where we would add a method, and break serialVersionUID. We concluded at the dinner the other day that there is no such thing as a perfect crystal ball :-)
I don't like this, conceptually, as we're truning something we need "guaranteed" delivery on to something that doesn't provide any such guarantees. Thoughts?
~~~ scala @SerialVersionUID(1L) case object Heartbeat @SerialVersionUID(1L) case object HeartbeatRequest @SerialVersionUID(1L) case object EndHeartbeatRequest ~~~ ?
You're right. That is unreadable without syntax highlighting.
due to our own watch this will never be true ;-)
not true, forall returns true if empty ;)
@rkuhn I want to understand what you mean. Do you refer to the watch that is done between the `RemoteWatcher` actors that is added when receiving `HeartbeatRequest`? That is in the other direction. That is added to `watching` Set at the other side, not here.
that other watch will trigger a counter-watch, leading to the two watchers watching each other
I see what you mean, good catch, will filter that here
after #1335 this must be a WrappedTerminated again
I don't think we have to be that strict. `d.name.startsWith("akka-kernel")` should be enough. Those name schemes tend to change, as we have seen.
Yeah, but if we add another dep that starts with "akka-kernel" then we've accidentally messed things up
please add a newline
lowercase name Also needs docs that it defaults to NoAck
is position allowed to be negative, is count allowed to be negative?
Why does it need its own dispatcher? Can't it use managed blocking?
so if this call fails we leak a buffer?
as I just commented: `blocking` is less managed in a sense
if drop(copied) fails, we leak a buffer?
so if this is violated we leak a buffer?
hasRemaining (never use size() for isEmpty)
Alright, but by default it is going to use the default-dispatcher, right? I want to keep thread creation down so we don't end up with Akka eating 1000 threads because everyone felt they needed their own thread.
the default (given somewhere above) is a PinnedDispatcher, meaning that for this purpose we allocate one thread per actor system; that might want to be a lazy val, though, so people not using WriteFile dont suffer
But that means that you can only transfer one file at a time. So then instead I suggest using a SerializedSuspendableExecutionContext over some dispatcher and use scala.concurrent.blocking. This will serialize transfers but won't mandate a thread of its own.
No, this pending write is still registered and will be cleaned up when the actor stops.
Are you saying you want all code that could potentially leak a buffer (through a bug in this class or in e.g. ByteString) to be protected against leaks?
I think we discussed this matter before and agreed that it was more consistent to give a constant an upper case name. But I agree that especially for `empty` the precedent is definitely on writing in lower case. So, I will change it.
under which circumstances will copyToBuffer throw an exception?  My point being that an Akka application should be able to run for months, if not years, so we need to parry for leaks, especially large ones.
Ok, great! :)
> under which circumstances will copyToBuffer throw an exception?  It doesn't as far as I know (it's a method of `ByteString` btw). If it would suddenly start to throw there could be greater troubles than this memory leak. So, that's why I'm asking. If you say we should protect against any leaks even those that shouldn't happen to the best of my current knowledge, I'll accept that and look out for it.
Ok, cool. I just know that there was a regression in 2.10.1 regarding copyToArray, so paranoia is warranted
Would you still be able to configure a bigger thread pool if you decide that it would help with your particular setup? I don't think we should completely remove the option to specify higher levels of concurrency.
Ok, so let's be paranoid.
How long will these tasks actually block? From my (limited) understanding the task should not block in the usual case, unless the disk is slower than the network; but that assumes that it returns early whenever the network buffer is full (doing the WriteInterest round-trip). If the blocking periods are only short, then we should just leave all code as it is and just change this default to `akka.actor.default-dispatcher`.
You are basically right with your assumptions. A task will only block if the disk is actually slower than the network. However, if this happens, it might block for the complete data in the worst case (I tried piping a file from an USB stick to localhost, where 400MB were transferred during one blocking call with 20 MB/s).  So, what I would additionally propose is a configurable limit on how much data is attempted to be transferred in one blocking call. That's also what nginx seems to do with its [`sendfile_max_chunk`](http:nginx.org/en/docs/http/ngx_http_core_module.html#sendfile_max_chunk) parameter.
yes, that sounds good!
this was previously `private[akka]` not possible any more? perhaps add INTERNAL API comment?
It looks like this doc section is duplicated in two places now. Keep it DRY.
Well, the implicit conversion is returning AskableActorRef so it's quite weird to have that class be package protected. What we could do is to make the constructor package private, but if we change the class to have multiple parameters it cannot be a value class anymore so I don't see the need.  What do you think?
It's always been duplicated. Where do we put it, on the trait or in the value class?
Could it fit into `AskSupport.ask`, since that is what the user imports?
true, it's not really internal api
I totally agree, thanks Patrik!
I'd recommend not using + for a destructive update
if it's configured and fails I think we should throw instead of defaulting
isn't there a risk that you have moved the node(role).address lookup to after the shutdown?
yes, for this unit test it is perfect to use FailureDetectorPuppet
I think this can be rewritten to use FailureDetectorPuppet
move to src/test
I think any real failure detector would need some config, (and logging) so I think we should say that the constructor takes a system as parameter
what do you need `connectionsToStartWith` for? Remove that and we can use configuration to instantiate FailureDetectorPuppet also.
Sure. I can remove it. Not used. 
Or I just change it to +=
Not used in this impl but needed in real FDs. 
I used to just throw, but then I changed it to log an error. I can change it back. 
Can't see why. In normal mode it will take a while for the real FD to detect failure. 
It does (takes (ActorSystem, ClusterSettings) and it is well documented in config already. 
Ok. I'm not using that ctor arg anyway. Will fix. 
because you changed it to lazy val, previously it was  `val secondAddress = node(second).address` that was performed before the "before-down-second-node" barrier
Removed it instead. 
Removed from interface. 
Yeah, but it should be fine now right? 
Simpler alternatives:  Just grep for "1.6." in the first line: ```bash java -version 2>&1 | head -1 | grep -qF "1.6." || fail "Java version is not 1.6" ````  Split using cut: ```bash java -version 2>&1 | head -1 | cut -d ' ' -f3 | cut -d '.' -f2 ```
Thanks for the bashing, Pete! I think number two looks like the winner
I would personally go for the simple F grep. Doesn't assume any particular columns, just that the literal string "1.6." must be somewhere on the first line.
Will that work for the initial 1.6 release? I'd rather avoid assuming anything else than looking at the 3rd char of the version no.
You mean if you had exactly ``java version "1.6"``? That wouldn't match, but I think java -version would always have 1.6.0 for the initial version.
Whatever works is good.  Thinking about the motivation for this... this check doesn't necessarily mean that the build is using the java 1.6 on the command line. It's possible that the sbt start script is pointing directly at a different version. If you want to be more cautious then parsing the result of ``sbt 'eval System.getProperty("java.version")'`` or similar would check the version running the build.
Yeah, I thought about adding it to SBT initially, but we only want to prohibit a new release being pushed to our repo. When/if we switch to some other payload delivery mechanism we will want to revisit this.  WDYT?
Since htis is essentially the only place where behaviorStackPlaceHolder is used, how was it deemed safe to remove it?
so if someone does:      context become Actor.emptyBehaivor  they mean       context become receive  ???
Ah. What if I use a different placeholder (null?) to represent inst.receive?  sRp On Jun 12, 2012 10:39 AM, "viktorklang" < reply@reply.github.com> wrote:  > > @@ -521,10 +521,7 @@ private[akka] class ActorCell( > >        if (instance eq null) > >          throw new ActorInitializationException(self, "Actor instance > passed to actorOf can't be 'null'") > > > > -      behaviorStack = behaviorStack match { > > -        case `behaviorStackPlaceHolder` => > Stack.empty.push(instance.receive) > > -        case newBehaviors               => > Stack.empty.push(instance.receive).pushAll(newBehaviors.reverse.drop(1)) > > -      } > > +      behaviorStack = behaviorStack.map { b => if (b == > Actor.emptyBehavior) instance.receive else b } > > so if someone does: > >    context become Actor.emptyBehaivor > > they mean > >    context become receive > > ??? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/534/files#r967890 >
That was what behaviorStackPlaceHolder was doing until it was retired ;-)
what do you mean? pulse-interval? heartbeat-pulse? we use interval for everything else heartbeat is the normal term for this kind of messages, I think
Lol, yes, I agree ,but the common terminology for the interval of heartbeats is known as the pulse ;-)
Why not Protobuf?
we will do protobuf when everything has stabilized, avoid dev overhead
This is expensive: SortedSet.empty[Member] + member  Cache the SortedSet.empty[Member] in an object
Also in case you need it somewhere else
Is the system scheduler configuration tuned for the heartbeats? (given the way the default scheduler works)
No, when running I have a pulse of 150 beats / minute, i.e. pulse is a frequency, not interval ;-)
Haha, you're right, my bad :)
We will turn all these into proto once the protocol have stabilized. Not now. 
Ok, but then there should be a FIXME so intent is preserved if someone else is to do that task.
we have ticket for it
So the ticket includes which classes need to be converted?
No I don't think we should. 
trust me, we will not miss that :-) I can sprinkle FIXME if that makes you sleep better, but I wouldn't trust the fixmes when doing the ticket anyway
I will sleep better if "quick and dirties" are clearly flagged ;-)
Kind of confused here. We are trying to heartbeat the unreachable members on the lines below. What is the FIXME referring to?  I think we need to try to talk to unreachable nodes until they are downed and removed.
good point, I'll change to scheduleOnce and adjust the delays
Change all the scheduled tasks. 
Should we cache all messages that are broadcasted?
Why would we need to continue to talk to unreachable nodes?  If it is a convergence on the unreachable set then none of the nodes in this set will ever be allowed to join the cluster again. But have to go through regular joining phase.  It is *not* needed to communicate with a node when it is moved to DOWN, but that can happen on any node (followed by a convergence).  So to sum up, if a node is moved to the unreachable set then it is considered DEAD. If it re-JOINs then it is considered a NEW node.  Considering this, why should we need to continue sending heartbeats to him? What am I missing? 
Could be a good optimization. One for each address.  Will be the same every second - forever. 
Nothing, I was probably a bit confused about reachability and overlap between failure detection and cluster gossip.
OK. So @patriknw, if you agree, will you change the code to reflect this? 
Sure  11 jun 2012 kl. 16:52 skrev Jonas Bonr<reply@reply.github.com>:  >> @@ -878,6 +887,25 @@ class Cluster(system: ExtendedActorSystem) extends Extension { clusterNode  >>  >>   /** >>    * INTERNAL API >> +   */ >> +  private[akka] def heartbeat(): Unit = { >> +    val localState = state.get >> + >> +    if (!isSingletonCluster(localState)) { >> +      val liveMembers = localState.latestGossip.members.toIndexedSeq >> +      val unreachableMembers = localState.latestGossip.overview.unreachable >> + >> +       FIXME use unreachable? >  > OK. So @patriknw, if you agree, will you change the code to reflect this?  >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/533/files#r960119
Any member in Joining or can we be more precise?
yeah, original author seems to like .empty ;-) I'll change
.empty[T] rockar fett!!
The question is if REMOVED should ever be in the unreachable Set.  Probably not. It is part of my the REMOVED status ticket. So leave it for now. 
"..due to unreachable *nodes* [{}]"
ok, yes, I just added it because it was treated like that in `convergence`, but all that will be revisited in the other ticket
Should the log be more semantic than technical?   "Cluster Node [{}] - No cluster convergence, since not all nodes have seen the same state yet [{}]."
fixed here and in other similar tests
save allocation by using `behaviorStackPlaceHolder` instead of Stack.empty
behaviorStackPlaceHolder instead of Stack.empty
Whoa, nice catch, completely unsuspecting:  scala> import scala.collection.immutable.Stack import scala.collection.immutable.Stack  scala> Stack.empty eq Stack.empty res1: Boolean = false
Would it be better (faster) to only call pop() once?
I think we should switch to just using an immutable List, immutable.Stack is ludicrously implemented.
no, eq/ne needs parens, and I don't want to make it look like the RHS has priority over the LHS, thereof the balanced parens
could use case callback :: more instead of c.tail
ok, I was thinking about (contextStack.isEmpty) I don't understand why this would be less readable, but it's up to you      if (contextStack.isEmpty || (contextStack.head eq null))
(Set.empty[Member] /: (a ++ b).groupBy(_.address)) { (acc, members) => acc + members.reduceLeft(Member.highestPriorityOf) }  I.e. no need for the Vector, turn algo from N2 to N
This line should be changed to wipe out everything (yourself will be added at the end). 
Would be nice to have a linter for stuff like this. 
reduce is sort of an implementation detail, can't we just rename it to "getHighestPriorityMembers"?
Might make sense to import MemberStatus._ in this file so we can have: m.status != Down etc
yes, I'll name it `pickHighestPriority`
Whoa, nice catch, bad IDEA
I'm not sure that this is as efficient as clazz.newInstance see java.lang.Class newInstance0 (some caching going on) you might want to do this as fallback, if clazz.newInstance fails
isn't it good practice to change it back to original value when done, or is the Constructor object a throw away instance?
If it's a throw-away instance we don't care, and if it isn't, we don't want to race with others doing the same thing.
Good point, fixed and pushed so that it is a fallback
move down "such as" also
move "this" down
move down "(if using"
move down "(if using a"
I'd move down "The nodes to join at startup if"
I'd move down the "gracefulStop(actorRef,"
Capitalize the "run" and "just"?
I'd move down "no matter"
I'd move down "if off"
I'd move down "if off"
At least capitalize "just" since at the beginning of a sentence. ;) And change the order of "to" and "below"
will add docs
ah, forgot to check license (but this is szeigers)
I'd consider:   final def noMatch[T](): T = throw NoMatch  since that's on an object, it becomes a static method, which there is limited type inference for. So in the Java code you can write:  } else { return noMatch(); }
java code?, then missing ;
this is junit, right? Then you can use:      @Test (expected=Exception.class)
This is too broad for my taste: you cannot assert that the exception originated from a specific action. (this is a consequence of not having function literals sucks)
Ah, well, this is not actually Java code; people are supposed to know that Akka is written in Scala ;-) Will add one here, though.
Pondered it, shifted it around at the back of my head, and I still lean towards making it explicit that a `throw` is happening here. Ive added one sentence above, WDYT?
What was the reason we couldn't use Typesafe Config?
So this is jus t way to make the configuration of the build and tests from Jenkins more manageable and readable. We currently use properties to control a number of things in the build, and this allows us to read them in from a file as well as from the command line.  We also use properties to _communicate_ with the sbt-multi-jvm plugin, if we don't want to pull in the config library there as well.
Great, thanks for the clarification
why not `new InputStreamReader(new FileInputStream(file), "UTF-8")` ? 
not production quality, but good enough for build, I guess
amazing that `sys.props ++` actually update System.properties
Yes, `sys.props` is specialized to be a two way mutable mirror of `System.properties`.
wow... just wow...
Yup, I had to publish it before I could use it. It also includes the cygwin fixes. 
this can't be correct, the test should verify that gracefulStop throws AskTimeoutException, now you have changed it so that the test is ok if the Await.result times out (200 millis) Wasn't the failure because of to small margin (200 ms is not much)
Yes, Patrik, you are of course right. I shouldnt trust my reviews today; thanks a lot for catching this!
Patrik saves the day. I'll fix
You're welcome, sorry to not being able to keep up with your PR rate today.
:-) Sorry for messing up the fix, did you see the new PR with the fix 2.0?
Doing 2 lookups instead of one is pretty expensive.
Alternative is to allocate an instance of Option. Btw, the map is really small. 
    try serializerByIdentity(serializerId) catch { case _: NoSuchElementException => null } match {       case null => Failure(new NotSerializableException(s"Cannot find serializer with id [$serializerId]. The most probable reason is that the configuration entry akka.actor.serializers is not in synch between the two systems.")       case some => Try(some.fromBinary(bytes, clazz).asInstanceOf[T])     }
I have to admit that I didn't dare to write the above code.
"null" is a valid value in a Map, so treating it equal to NoSuchElement covers both cases (null value and absent value)
now `deserialize` can throw anything but NoSuchElementException, which is not expected for a method returning `Try`; Id recommend something along the lines of `Try(serializerByIdentity(serializerId)).transform(_.fromBinary(bytes, clazz), ex => Failure(...))`
It can throw NoSuchElementException since only the lookup is protected by the try...catch. What is your point here, I am a bit confused I think.
previously everything was wrapped in `Try`, meaning that no NonFatal exceptions would ever be thrown from this method; this is not the same anymore, but the return type still suggests this behavior
My first approach was to test that serializerId is actually in the map, and depending on the result returning an appropriate Failed(...) or Try(...). This was rejected for performance reasons (double lookup). I don't like allocating an Option, neither creating an unnecessary Try with transform(). Do you have another idea?
Also, how realistic is serializerByIdentity(serializerId) throwing any other NonFatal exception than NoSuchElementException?
I prefer the code to be correct and readable first; when we see that that is a performance problem well fix it (but not before)
The solution is both simple and easy, NonFatal (t) instead of matching against NoSuchElement... then I suggest putting 't' as cause in the newly created exception.
in that case please make it a normal try-catch thing instead of this baroque try/Try mixture
I prefer code to be the best possible tradeoff between performance and readability. We are a library and people cannot make our code faster. For end-user code I agree with Roland, as the end user at any point in time can tweak their own code.
I understand Roland's view as well, but I agree with you here. Also, making this a normal try-catch thing will break the API. And all that for just adding an error message that makes more sense, and was also reported by a user. I am fine with any solution, but we should arrive to a consensus.
~~~ scala Try {   val deserializer =      try serializerByIdentity(serializerId)     catch {       case e: NoSuchElementException => throw new NotSerializableException(..., e)     }   deserializer.fromBinary(bytes, clazz) } ~~~
Ok, that makes sense to me.
Looks great, very nice suggestion @rkuhn!
I think that it's harder to read with a comment at the beginning of the line. Could we keep the _extra_ `val`?  And the test doesn't test the same thing `(2000 + 399) !< 2000`. Is that intentional?
Same thing here with comment at beginning of line.
I disagree regarding the val, imo it's just pointless repetition.      val foobarbaz = expr     foobarbaz must be awesome  vs      /*foobarbaz*/     expr must be awesome
Should we not have an `Echo` lying around somewhere?
didnt we like `Actor.noSender` better?
ah, no, we didnt; forget it.
Maybe be explicit about that `off-for-windows` means `on` for all other platforms.
What would be the difference between off and off-for-windows if it wasn't on for everyone else? :-)
Yes, I know that and you know that. I'm just thinking mainstream.
faith in humanity - 1 :-)
The normal behavior for an Akka actor is to drop the message it is currently processing if an exception occurs when processing it, and then continue with the next message after it has been restarted.
It would be interesting with a Retry Directive
Why the braces?
Why the braces?
what would that mean for the children of the failed actor, or when the failure had been escalated?
Same thing as Restart
import PeekMailboxExtension.ack ?
for(m <- msg if m == "DIE") context stop self
I think this section could benefit from a small detour of at-most-once, once-and-only-once and -atleast-once guarantees
may also discuss bounded mailboxes
I think we can leave facebook out of this
I think the use-case for dead letters should be described _before_ the explanation how one hooks into it.
I think it is important that we reiterate that the EventBus is not clustered, so that deadLetters will be the ones that are known to be dead locally.
What does this mean? The failed message is not available in the supervisor strategy. Do you mean that all messages proxy through the parent?
I think doc demo code belongs to `src/test/scala`, or `akka-contrib/docs/*/code`
Reading a sentence that starts with _The normal behavior of an Akka actor is to drop a message_ just gives bad vibes.  I would like the first sentence to to be flipped around to something like:  When an Akka actor is processing a message and and exception occurs, the normal behavior is for the actor to drop that message, and then continue with the next message after it has been restarted.
I agree. But maybe we should flesh it out in _The General Rules_ where we now only mention _at most once_?
- if the receiving actor fails while processing the message or is already terminated
Shouldn't we validate the max-tries value?
Why is 0 a special case here? How does it differ from "case n => ..."?
what does it mean if ack() is called and tries == Marker?
Thanks, Bjrn, your version won on the least-convoluted score.
hmm, people can do that, Id like to keep it dead-obvious
yes, good one
added a discussion point of the three terms
hmm, any native speakers with an opinion?
boundedness does not change ordering AFAICS
There should be a warning like that which follows, right? Do you have a better suggestion for the title of that paragraph? or shall the title just go?
changing to non-error condition tests
yes, good one
Is that not covered by the previous sentence in this paragraph?
hmm, I liked how you can run this with `sbt akka-contrib/run`, will see whether that can be done otherwise
yes, we should
I wanted to group 0 with Marker because they conceptually are similar (last was ACKed explicitly or implicitly); you are right that handling 0 there is not really needed logic-wise.
it means that it would be a duplicate ack(); good point, BTW: if tries == 0 it should not poll()
I'm a bit uncertain about the word `implementation-wise`, more important is performance tradeoffs
perhaps also add something about "don't pay for what you don't need", I mean at-most-once is cheapest, and other things can be added on top when needed, but not earlier
I think it is worth repeating "enqueue requests from different senders" here, to avoid misunderstanding
idempotent receiver is the important thing, and that can be achieved with discarding duplicates, or having no side-effects, or side-effects ends with the same result
ok, here it was
you are of course right
yes, good one
Native speaker here... IMO "will" is better, if the intent is really to indicate "always". Another suggestion would be to use the terminology given in RFC 2119 (http:www.ietf.org/rfc/rfc2119.txt) which in this case would mean using "shall".
Sorry, I was looking at the wrong word... yeah, for the "may" in line 82, I'd leave it as-is.
This should most definitely not be here. AMQP should create actors in the system that the extension belongs to.
What kind of timeout is this and why isn't it confgurable?
Arrays are mutable. Use something like a Seq
Add return type
Add return type
So there is no constructor for Java API usage that allows the Map? (I'm assuming Java would use j.u.Map)
Might want to drop this approach and instead using the immutable builder approach. ConsumerParameters(essentials).withFoo(foo).withPigdog(pigdog) ...
You shouldn't create all of these as top-level actors. Create an amqp-actor that is top-level and spawn the kids below that one.
This method is strange in this case since it does not verify that the connection actually belongs to the internal system (which should be removed anyway). I'd suggest dropping this method altogether and just tell people to send a PoisonPill
Uncool to do blocking here, just return a Future[ActorRef]
Same as above here
also, always have return types on your methods. Trust me, will save everyone involved a lot of headache and will add documentation.
Use Akka Serialization Extension for all serialization, that will give you awesomeness for free and streamline things
I'd suggest removing all these hard-coded StringProducer and ProtobufProducer and whatnot since it both pollutes the namespace, is hardcoded and doesn't really provide any value from my perspective. When using Akka Serialization I don't even see the need for them. Wdyt?
Is this method needed? And why?
Drop this and use Akka Serialization
Roses are red, violets are blue, arrays are mutable, they go into the loo
Don't you want Option(properties) here since it's Java and they like to null?
Add return type
over here as well
what's the difference between "message" and "body"?
What are people going to do with all this info?
Why pull out the deliveryTAg, then import envelope._ and then call getDeliveryTag again at line 57?
Not recommended in general, see akka.util.NonFatal
private def acknowledgeDeliveryTag(deliveryTag: Long, remoteAcknowledgement: Boolean): Unit =     for(c <- channel) {         c.basicAck(deliveryTag, false)         if (remoteAcknowledgement) deliveryHandler ! Acknowledged(deliveryTag)     }
use: log.warning("Consumer is rejecting delivery with tag [{}] - requeue [{}]", deliveryTag, requeue) since the current code will create the message anyway even if warnings are not enabled.
Why log and rethrow?
That doesn't make any sense. preRestart is called on the old instance. and the old instance will be orphaned.
All children will be stopped when parent is stopped anyway
for(tag <-listenerTag; ch <- channel if ch.isOpen) ch basicCancel tag
All copyright notices need to be updated to the new ones.
A bit too magic for me, what's it used for, why isn't it pulled from the config?
Why blocking here?
newChannel foreach setupChannelInternal
Also, why can it reply with None?
Can it have multiple Channels?
use the logging formatting instead. log.info("{} got normal shutdown", this.toString)
You might want "self.toString" though
I'd suggest using normal Restart logic (crash yourself, do restart in preRestart and postRestart) reinventing wheels is so 2011 :-)
for(cp <- channelParameters; sdl <- cp.shutdownListener) ch.getConnection.addShutdownListener(sdl)
private def closeChannel(): Unit = channel = channel.flatMap( c =>   if (c.isOpen) c.close()   notifyCallback(Stopped)   log.info("{} channel closed", self)   None )
private def notifyCallback(message: AMQPMessage): Unit =   for(cp <- channelParameters; cb <- cp.channelCallback if !cb.isTerminated) cb ! message
You can drop this method and in your constructor do:  self ! Start
I'd do:  val connectionFactory = {   val c = new ConnectionFactory   c setUsername username   c setPassword password   c setVirtualHost virtualHost   c }
Can such a scenario exist such that both connection and reconnectionFuture is Some(...) ? If not, you might want to encode it as Option[Either[Connection, Cancellable]]
Can you connect when already connected?
Kill will simply crash the actor and make it eligible for restart, is that the desired behavior or is "context stop self" what's desired?
val reply = connection match {   case Some(c) => Some(conn.createChannel)   case None =>     log.warning(...)     None } sender ! reply
Why this Exception?
    case cr: ConsumerRequest =>       val reply = if (connection.isDefined) Some(newConsumer(cr.consumerParameters))                      else {                         log.warning("Unable to create new consumer - no connection")                         None                      }       sender ! reply
Using smae approach as above you can scrap half the LoC
Not safe, use akka.util.NonFatal
This should most probably go into a finally
Children are automatically stopped when parent is stopped
don't use vars in guards.
Use TestLatch (it's an Awaitable so you can use Await)
I definitely agree.  I almost took them out, but opted for as close to a "pure port" as possible while I was getting familiar.  I'll start pruning this stuff out.
HI Viktor,  Regarding this comment, I'm trying to determine which timeout value from the config would be most appropriate.  Any guidance would be appreciated.  Is there a timeout config param that is meant to control how long to wait for a response to an ask?    Thanks, John  On Feb 23, 2012, at 2:11 AM, viktorklang wrote:  >> +abstract private[amqp] class FaultTolerantChannelActor( >> +  exchangeParameters: Option[ExchangeParameters], channelParameters: Option[ChannelParameters]) extends Actor { >> + >> +  protected[amqp] var channel: Option[Channel] = None >> + >> +  implicit val timeout = Timeout(5 seconds) >  > A bit too magic for me, what's it used for, why isn't it pulled from the config?
removed.  this is now implemented as a ConsumerRequest message to the connection actor.
removed. this is now implemented as a ProducerRequest message to the connection actor.
removed.  Using connection ! PoisonPill
fixed.  There is now an AMQPActor that accepts a ConnectionRequest message.
Removed.  Bring your own system and fire up an AMQPActor.
removed.  now you can either PoisonPill your AMQPActor or shutdown you system.
If you make the AMQP an Akka Extension just use the same pattern as for the otehr extensions, add your own config option in your reference.conf for the akka-amqp module.
Twas overkill.  Stripped out all the unnecessary info.  message was a string representation of all the other params.  I left message and removed everything else.  It might have been structured that way to allow the sender to reconstruct and resubmit the message, but I suspect there's a better way to do that than pulling it all out of an exception.  Will wait to see if anyone screams...
Removed all but message.  It might have been structured that way to allow the sender to reconstruct and resubmit the message, but I suspect there's a better way to do that than pulling it all out of an exception. Will wait to see if anyone screams...
I don't know.  Cleaned up.
don't know.  fixed.
added extension setting for akka.amqp.timeout.
changed to:  context.parent ? ChannelRequest map (_.asInstanceOf[Channel]) map setupChannelInternal  That's pretty slick!  
It appears not.  I'm not completely sure what's going on inside rabbit, but this case gets hit during various recovery scenarios.  Basically the method checks to see if the channel actor already has a channel defined, and if so, it closes the one sent in the message if it is open.  I'm not sure I want to tamper with this since it appears to work.  I would like to get a better understanding of it at some point.
Cleaned up.  I was a little confused about whether to do self ! Kill or self ! Failed(cause), but the results appeared the same, so I went with Kill.  
for some reason, that didn't work out so well.
nice.  updated.
done.  This was fun.  I have used both options and eithers, but never together.  Is there a nicer way to express this?  if (connectionStatus.isEmpty || connectionStatus.get.left.toOption.map(!_.isOpen).getOrElse(false))   
no.  this was handled by a condition in connect, but I moved the condition to the case for clarity.
fixed.  not sending back options any more.  either you get the connection or you get a timeout when you try to get it.
fixed, now uses akka.amqp.timeout
What type do I get back here?
The example should possibly avoid blocking, as users generally mimic the documentation.
Way too much internal blocking here. Use Future Composition instead. Instead of timing out on failure, I suggest replying with an akka.actor.Status.Failure if akka.util.NonFatal so the client can react to errors.
if (message == Disconnected.getInstance()) ?
Use Akkas Logging facility instead of SYSOUT
This should most definitely go into test/resources and not main/resources
Why context.system.actorOf? That creates top-level actors all over the place. Should most definitely be context.actorOf
Needs explicit return type, and you can use simply "this" instead of "Settings"
Don't you want default broker etc here?
still needs fixing
Add explicit return types for all of these
The defaults should go into the configuration file, we have no defaults in code.
Should be Option(...) and not Some(...)
Should be Option(...) and not Some(...)
Should be Option(...) and not Some(...)  Same goes for all Java constructors using Some(...)
case Some(Left(l)) => sender ! l.createChannel case Some(Right(_)) => what do you do when it's a Cancellable? case None => ...
    connectionStatus match {       case None                       => connect       case Some(Left(r)) if !r.isOpen =>  connect       case _ => all good     }
Use pattern matching like the above examples
Avoid capturing self here. add  val replyTo = self  and then close over replyTo instead.
you most likely don't want this since this will be executed every time. just pass in addresses
Wouldn't it be better to store an Option[Promise[Connection]] and then register all inbound channel requests as callbacks so that when the Promise gets completed all waiting guys can get a new channel?
for(s <- connectionStatus; c <- s.right) c.cancel
Wayyy too much blocking in this demo.
Don't catch anything, the test will fail if there's an exception thrown
use explicit charset
Don't put code in docs, look at the other Akka docs for how to include compiled&tested code into the docs.
Multiverse should definitely not be added, we use ScalaSTM.
This shouldn't go in here, use your ~/.sbt/plugins/build.sbt for that
I still need to go through all of the Java stuff.  I think that's on the agenda for today/tomorrow.  On Feb 27, 2012, at 2:32 AM, viktorklang wrote:  >> + >> +import java.util.concurrent.CountDownLatch; >> +import java.util.concurrent.TimeUnit; >> + >> +@SuppressWarnings({"unchecked"}) >> +public class ExampleSessionJava { >> + >> +    Timeout timeout = new Timeout(Duration.parse("2 seconds")); >> +    ActorSystem system = ActorSystem.create("ExampleSessionJava" , ConfigFactory.load().getConfig("example")); >> + >> +    final SettingsImpl settings = new SettingsImpl(system.settings().config()); >> +    Timeout timeout = new Timeout(settings.Timeout()); >> + >> +    ActorRef amqp = system.actorOf(new Props(AMQPActor.class)); >> +     defaults to amqp:guest:guest@localhost:5672/ >> +    Future<Object> future = Patterns.ask(amqp, new ConnectionRequest(new ConnectionParameters()), timeout); >  > What type do I get back here? >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/350/files#r490574
This example class is cleaned up and has been moved to the test package.  Run with test:run-main.
Lots of blocking avoided, though currently when we create a producer or consumer, there is no guarantee that the underlying channel has been started.  We return the actor, and it is up to the caller to verify that the channel is up by using a channel callback to monitor the state.  So, in the examples and tests, we potentially block before sending  a message from the producer.  I'm open to other alternatives, but the only one that comes to mind is blocking within the AMQP extension when we make the request for a new consumer/producer.  I don't think that's a great idea since these things are supposed to be fault tolerant, and it seems reasonable to get one that will eventually start when an underlying fault has been resolved. 
Lots of blocking avoided, though currently when we create a producer or consumer, there is no guarantee that the underlying channel has been started. We return the actor, and it is up to the caller to verify that the channel is up by using a channel callback to monitor the state. So, in the examples and tests, we potentially block before sending a message from the producer. I'm open to other alternatives, but the only one that comes to mind is blocking within the AMQP extension when we make the request for a new consumer/producer. I don't think that's a great idea since these things are supposed to be fault tolerant, and it seems reasonable to get one that will eventually start when an underlying fault has been resolved.
I don't think so.  There are already native default values in the rabbitmq client code.  See ConnectionFactory (http:www.rabbitmq.com/releases/rabbitmq-java-client/v2.7.1/rabbitmq-java-client-javadoc-2.7.1/).  The default broker then would be localhost.  I'd rather let the user of the extension define their own app params if they want to have those type of defaults.  Sound reasonable or am I crazy?
fixed, all tests consolidated to AmqpIntegrationTest.
I think this is pretty cleaned up now.  What do you think?  Still doing some blocking, but basically to ensure the channel is started.
fixed, all tests consolidated to AmqpIntegrationTest.
fixed, all tests consolidated to AmqpIntegrationTest.
fixed, all tests consolidated to AmqpIntegrationTest.
fixed, all tests consolidated to AmqpIntegrationTest.
fixed, all tests consolidated to AmqpIntegrationTest.
As a sidenote, why the System.exit(0)?
Saw your similar comment below.  Seems like you would prefer having the defaults in the config.  I'll add it to the list for next commit.
Will do this next commit.
I think I fixed this and other one way sends to self.  Could you explain why this is important?
Because closing over seal actually means: close over this (this.self()), and since "this" is the Actor, that instance could've been replaced (restarted) between now and the shutdownlistener triggering.
if (channelParameters.isDefined) ch.basicQos(channelParameters.get.prefetchSize))  Shorter and doesn't create an anon inner class :-)
ummm, why is replyTo = self and then only used to do replyTo ! Failure(cause)? that essentially means "self ! Failure(cause)" which will not restart self, if you want to restart self, just don't bother catching the exception at all.
shouldn't this be logged?
delegate to super.postStop or not?
I don't understand the replyTo pattern
I think what you mean to do was to move "val replyTo ..." _before_ the shutdown listener
What's the purpose of the code here above? (just for my understanding of the use case for this logic)
don't understand the above, why not just sender ! self
why is this here?
I recommend:  if (connection.isEmpty) connect
well, technically it's not terminating, it's throwing an exception, which will be handled by the supervisor.
instead of match just use:  if (connection.isDefined) {} else {}
why?  just use: consumer.tell(Start, sender)
why?  just use: consumer.tell(Start, sender)
replyTo _must_ be outside of the closure, otherwise it has zero effect
You're swallowing the exception
you're referring to "sender" inside a Future callback, big no-no
I'll clean all of these replyTo things up.  I was confused about an earlier comment about referring to self inside a closure.  I think I have a better understanding now (we'll see). 
logged, but I just realized why I was ignoring the exception.  when we get here because the connection actor was stopped, it was spitting out a "connection clean shutdown" exception which we don't really care about.  I put in some logic to ignore just that scenario and log anything else.
yes, fixed.  I have a better understanding for the reason behind this now.  Thanks!
request a channel from the connection actor, then handle the setup of the channel by sending the returned channel to self, then depending on the result, send back the ActorRef or the failure.  It ensures that the channel is started and ready to process messages when the ActorRef is handed back to the sender.  That minimizes the need to have a channel callback listener in the client, though you may still want to have one in case the channel bounces later.
leftovers.  fixed. 
I reworded the message to be more meaningful (I hope).
fixed (in new branch)
fixed, I don't think this was even restarting the channel as advertised.  Removed notification of sender.  They can create their own return handler if they want notification, set the channel to None, and then send a failure message to re-establish a channel.
wanted the response back from Start to ensure that the channel has been set up and is ready to accept messages.  Avoids the need for a channel listener (at least initially).   Using tell, in my tests, I would get the actor back, then block waiting for the channel listener Started notification to fire, or risk the chance of sending a message to the actor before the channel was available.  I would really like to some ideas on how to handle this better.   I can probably do fire and forget on the producer side, and leverage then do the basicPublish when the future is completed.  The consumer side seems like a different story since fire/forget would result in an ActorRef, but the consumer not ready to receive messages yet since the channel may still be in the process of being constructed.  I'm not sure how applying a future channel will help when it's purpose is to consume incoming messages.    That puts me back at using a channel callback to let the client know the state of the channel, which isn't a bad thing.   I'm very interested in your thoughts...
it is this barrier that fixes the problem
yep, I've run the test locally hundreds and hundreds of times without being able to reproduce it.
no need for volatile here, there is only one thread
No, that's not really true, "target" is captured by the props:      proxy = system.actorOf(Props(new ReliableProxy(target, 100.millis)), "proxy")
ah, good point, I would make it a `lazy val` instead, and use actorFor 
Yes, that's a good suggestion which I used initially, but then I thought that the code became brittle (avoiding to force evaluation of target until it's supposed to). I can switch it back to lazy val if there's a consensus on that. 
+1: the volatile var jumps into your face in an appropriate way, should be obvious enough
Agreed; though it would have been a smaller fix without volatile vars to just put the barrier into the original code. The sendN/expectN is an improvement regardless.
hmm, maybe add something like `replies.values count ( _ > 0 ) must be > (connectionCount - 2)`
Increasing the count of replies helps, too.
ok, will do
Is this user-level API?
Are you expecting to create many of these? In that case I recommend/strongly suggest that you create an actor that has the ActivationTracers beneath it, since otehrwise it will create a very shallow but broad tree.
use () for Unit
Clean!  Is this user-level API?
Not recommended. Avoid creating tons of top-level actors (just as you wouldn't create tens of thousands of top-level files on your harddrive)
Might it make sense to be a real URI?
Add return type
You also need to do this:  override def get(system: ActorSystem): CamelExtension = super.get(system) to get the Java API (return type will be barfed for Java otherwise)
Is this idempotent? (Since it will be called every time the Actor is Restarted)
Should this really be code config? I'd suggest pulling the config from the applicaiton config, falling back to akka-camel's reference.conf
Wouldn't it be possible to drop "blocking" and just check if replyTimeout is > 0?
Should be a FIXME
Java doesn't respect Scala access control, so add comment/scaladoc on all such and write that they are for internal use only.
Why is this trait needed? Seems like namespace pollution at a first glance
This does not delegate to super, is this intentional?
Shouldn't this be CamelMessage(%s, %s) otherwise I don't think anyone will be able to distinguish it from any other Message.
use "addHeaders" instead of "plus" since plus is not set-add. Also, method does not mention how it handles duplicates.
Needs careful documentation since it is a mutable class with non-threadsafe members. Also RichMessage doesn't really say anything. What is its usecase?
Is this to be done every time the Actor is Restarted? I.e. is registerProducer idempotent?
doneSync isn't used, intentional?
I would probably write this as:      def done(doneSync: Boolean): Unit = producer.tell(          if (exchange.isFailed) FailureResult(exchange.toFailureMessage(cmsg.headers(headersToCopy)))         else MessageResult(exchange.toResponseMessage(cmsg.headers(headersToCopy)))         , originalSender)
Is this user-level API?
Should these survive a Restart?
No delegation to super needed?
All of these needs docs
That doesn't sound good
I'll hold off commenting on this class until I know if it is residue, or will be replaced or how it is supposed to be used.
Since this class requires an Activation, and Activation already has a "def system: ActorSystem" etc, is this really needed?
I didn't think CamelContext was thread-safe?
also, why match on the ref if you don't use it?  case msg: EndpointDeActivated => ...
remove, pointless exercise in reinventing negation.
What's the logic here?
Can't use URI and get the scheme from that?
Are we expecting anything but one Camel?
Also, how is this class used? Is it threadsafe?
Why is this a ConcurrentHashMap?
Why is this registered on top level?
Not recommended. Have a look at the uses of akka.util.NonFatal
do a match instead
Do you want to publish prior to register?
Why is this Unsupported?
Why does this have vars?
Is this user-level API?
add return type
Is this user-level API?
Should be documented as blocking.
Why not just use Duration(...)?
Not big on the "rich" thing. Needs a better name
Is this thread safe?
Why is a method needed here?
Instead of having to override methods I'd prefer something like:  override def consumerConfig = ConsumerConfig(activationTimeout = 10 seconds, endpoint = "file:data/input/CamelConsumer")  That means that ConsumerConfig can be evolved in the future to add moer fields without risking method name clashes in the actor.
all of these on-hold  things should be removed.
Use TestLatch instead
why semi-colon, just have 2 lines
Thinking about it. What's the point of awaitActivation when you can just do Await.ready(camel.themethodthatreturnsthefuture(actorref), timeout)?
no need for protected
You can do:  intercept[CamelExecutionException] { ... }.getCause.getClass must be ...
use TestLatch everywhere you use CountDownLatch as TestLatch is an Awaitable
This can be too small for a slow box, write the tests so that the timeouts won't be giving me a headache when we run them on a slow integration test box.
Do you verify that this exception is indeed thrown?
case `expected` => All good
I see no test for concurrent registrations.
yes, it is part of the Camel trait.
There is only one per ActorSystem. (ActorSystem to Camel/CamelExtension is always one to one)
It is part of an implicit, the user-level API is onRouteDefinition on the Consumer, which allows you to customize the Camel route for the consumer at definition. http:akka.io/docs/akka-modules/1.1/modules/camel.html#intercepting-route-construction (and the configuration of routes uses the from/to so it uses it but you don't see it) so you can do from("uri:some/camel/endpoint").to(myActorRef)
removed def system from Camel trait, it is not needed in the end for the user. (you can always get to the system using the normal routes) Added a comment that Camel is shutdown when the associated ActorSystem that uses a Camel extension is shut down.
done override def get(system: ActorSystem): Camel = super.get(system) 
done. Should akka.camel.Message be renamed to akka.camel.CamelMessage to be consistent?
protected, because you might want to access it from the Producer Actor. yes it is idempotent.
intentional. we always send back async, irrespective of doneSync in camel processor.
no its internal
wait, addHeaders suggests modification in place and mutability of the message. I've intentionally chosen plus as for +=.  As far as I am concerned plusHeaders clearly states that we are not modifying current object but creating new. Please revert unless vetoed by the BOSS :)
"Clean!" as of you like it or "clean this mess up you moron!!!" :) ?
this is left by accident - there is no longer "blocking" mode
it's used for tests only - let's move it there and mark private
I don't mind getting rid of it
yeah, if we want to avoid confusing users toString should be consistent with the class name
This class needs more love - if I only had time...
BIG NO!!! It is not mutable. At least it was not intended to be so if I miss something tell me, otherwise this class is immutable.
read the package names :) No but seriously I was learning package visibility rules for scala when I moved it there and I didn't realise hierarchies apply so we could do private[camel]
no need - it is protected by try-catch_all below - or maybe I am naive and we need to review it
why? - it's not meant to be extended
well this is a legacy code I had no time to beautify so I am leaving this remark "for those who will come after us..." :) It is internal so users won't see it - resources versus time battle...
please comment in the code or change it - it works - it's just not pretty
yeah, but its a driveway coupling(http:www.theregister.co.uk/2008/04/18/emergent_design_part_three/print.html). ConsumerRegistry doesn't need to know that Activation actually provides it. It wants a system by itself.
In this case one could argue that the only thing that's needed is "withHeaders" and then you can do:  msg.withHeaders(msg.headers ++ newHeaders)  Because otherwise you'll have to add minusHeaders(), etc etc
I was waiting for this :) You win!:)
error handling made simpler. Did I break any of the actors principles? Teach me :)
legacy - ported as is
It is not TS but it wasn't intended to be as it is used in the extension only, which I am assuming is TS.
Intentional -> when you want to execute multiple dangerous statements at all cost.
There's nothing Akka can do to make arbitrary Extensions TS. The only thing Akka can guarantee is that there'll be at most one instance created of the Extension. You'll need to make sure that the Extension is TS.
I guess this would be for the case:  from(actor)...to(somewhere else)  but I think it is covered by the Producer  Ray, comments?
It has to - it's a camel bean
no -see my previous comment about visability
I swear I changed it to use Duration.parse - dunno what happened...
agreed - any ideas?
to support poor java user. Scala users get implicit conversion for free.
I did a spike and tried to do this - it created horrible experience for java users
tell me about it... :) Any ideas how to rewrite this test?
No need - as header is set by camel only if previous call failed
No WordSpec here?
If we follow what scala collections are doing I am happy to have minusHeaders. What is wrong with it?   On 25 Feb 2012, at 10:20, viktorklang wrote:  >>    */ >> -  def setBodyAs[T](implicit m: Manifest[T]): Message = setBodyAs(m.erasure.asInstanceOf[Class[T]]) >> +  def plusHeaders[A](headers: Map[String, A]): Message = copy(this.body, this.headers ++ headers) >  > In this case one could argue that the only thing that's needed is "withHeaders" and then you can do: >  > msg.withHeaders(msg.headers ++ newHeaders) >  > Because otherwise you'll have to add minusHeaders(), etc etc >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/344/files#r488547
What about system.registerOnTerminantion(...) ? Is this TS. If so combined with above, this is good enough guarantee. 
What? No. IF DefaultCamel isn't TS, and is called by multiple Actors at the same time, then it's not TS. At all.
Use akka.util.NonFatal. New decree
then it should be removed
But it's not plus or minus. that's my issue with it.  Plus: 3 + 3 == 6 Map concat: scala> Map("foo" -> "bar") ++ Map("foo" -> "baz") == Map(foo -> baz)  So it shouldn't be called plus, because it doesn't have plus semantics.
Isn't CamelContext mutable?
Cool, but it should be commented since it might trip someone up
Then it should be final?
Open a ticket
So it only needs _some_ methods from Activation? What's so special about system?
Prolong the time.
Only start/stop isnt TS, so that's ok.   - Piotr Gabryanczyk  On 25 Feb 2012, at 13:35, viktorklang<reply@reply.github.com> wrote:  >> +import org.apache.camel.CamelContext >> +import org.apache.camel.impl.DefaultCamelContext >> +import akka.util.Duration >> +import scala.Predef._ >> +import akka.event.Logging >> +import akka.camel.Camel >> + >> +/** >> + * Creates an instance of the Camel subsystem. >> + * >> + * @param system is used to create internal actors needed by camel instance. >> + * Camel doesn't maintain the lifecycle of this actor system. The actor system has to be shut down by the user. >> + * In the typical scenario, when camel is used with akka extension, it is natural that camel reuses the actor system it extends. >> + * Also by not creating extra internal actor system we are conserving resources. >> + */ >> +private[camel] class DefaultCamel(val system: ActorSystem) extends Camel { >  > What? No. IF DefaultCamel isn't TS, and is called by multiple Actors at the same time, then it's not TS. At all. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/344/files#r488621
That's a pretty funny design decision, but if that's true, then you're fine.
when I think about it even if multiple threads call start/stop it will be fine so in a sense it is TS even though wasn't designed to be. This methods are only used by CamelExtension in controlled environment so we are safe here. I wanted to avoid adding "started" guard as the assumption was this class is going to be used by camel extension only.
Created a ticket.
it is only used in test, moved it there
I'll pick it up.
it is. I'll think up something new, there is a strange smell here.
done (private and final)
Zooming out:  DefaultCamel has the same lifetime as the actor system it is associated with.   There are a couple of internal actors used in akka-camel (mainly idempotentConsumerRegistry, activationTracker, producerWatcher), all started by start of DefaultCamel lifetime, but there is no overall supervisor for them.   All of these are mainly for register/unregister state and notifications of ready state of register/unregister.  consumerRegistry and activationTracker have some state, producerWatcher has no state so survives restart automatically (but ProducerRegistry has state, which has lifetime of system and defaultcamel).  DefaultCamel shuts down on system termination, but does not stop the internal actors (which is not correct I think, they should be stopped, will add this).   Since this is all so similar (and now in quite a thought through state) I'm going to take a crack at refactoring it to one generic solution.  IF for some reason outside of akka-camel, the 'internal actors' are restarted and camel and system keep on going, we lose information about registration of producers and consumers, and activation status (so listeners will timeout, and you could get double registrations).   (I am assuming that now or in the future 'akka' can restart anything without warning, I have a dream where I suspend my akka cluster, 'serialize it' and switch it to another cloud provider and flip the switch ;-) but that's for another day)  If that is something that needs to be fixed, @viktorklang how do you recover state across pre/postStart/stop from within an actor?  
it is threadsafe in implementation (of DefaultCamelContext) when it comes to registration of endpoints and components. I count 39 synchronized statements in that code........  http:www.jarvana.com/jarvana/view/org/apache/camel/camel-core/2.9.0/camel-core-2.9.0-sources.jar!/org/apache/camel/impl/DefaultCamelContext.java?format=ok  (That could change of course, it is not documented on the CamelContext interface, but the intent clearly seems for it to be threadsafe :)  
it's there: 1677
Because it needs to be safe, but will change this in refactor of register unregister activation for both producers and consumers.
will change that in refactor of register/unregister/activation, will add a top level supervisor per Camel and put everything underneath that one.
Done. Nice :-)
moved it a line down.
changed line to   "case NonFatal(e) => log.warning("Safe operation failed. Swallowing exception [{}]", e)"  Is that what you meant?
You have 2 options:  1) Store it somewhere safe and then read it back in on postRestart, think EventSourcing or similar 2) Delegate all risky operations to children
The Actor Endpoint ( part of the  Camel Actor Component 'plugin') right now only supports receiving messages from Camel. The createProducer (not to be confused with producer actor) is used to send messages into the endpoint. The ActorComponent is only there to send to actors registered through an actor endpoint URI. So you can use an actor as an endpoint to send to in a camel route. so from(someuri) to (actoruri), but not 'the other way around'.  supporting createConsumer would mean that you could consume messages from an Actor endpoint in a route, which I think doesn't make sense, because an actor does not (necessarily) send out messages (and it is not fixed to where). It can always receive messages of course.  Producer Actors can be used for sending stuff to some other uri/ component type registered in Camel, so yes that is covered in a sense. 
If anyone has ideas to make this make sense, or if I missed something, I'm interested.@krasserm?
This is all nice, but Camel won't survive this serialize/deserialize events especially if users add there own custom routes. In my opinion doing this would be unnecessary complexity.  I think Viktor is picky on this point by principle :) In this context it doesn't make sense to preserve the state.  Also, if the actor system dies, everything needs to be re-registered.
why this complicated dance? why not just `implicit def camel = `?
Since the message itself is untyped (`Any`), this method is giving a false sense of type-safety. It should either be `Any => Any` or you need to attach a type parameter to `CamelMessage` (which can get out of hand quickly).
This type parameter does not help anything, why not take a `Map[String, Any]`?
Ah, I see: no covariance in Java-land. Carry on.
same comment as in Consumer
I hope Camel cleans this up if nobody ever replies?
Why not make this an FSM? This gives you onTransition stuff, external observability, if you want a rolling event log, 
youre leaking registrators since you didnt call super.preRestart()
why invent this special thing? looks baroque  and it does not fundamentally improve upon `try  catch finally`.
    try template.start() catch { case NonFatal(e) => context.stop(); throw e }  This has the advantage of not having to look up which exceptions are caught and what else happens.
Should some of this not go into ScalaDoc for this method and/or class?
I assume that you have thoroughly researched that modifying the exchange from some random other thread is okay, right?
ouch: why allocate an object in a default argument?
dont use `Any` in Java-facing signatures, Paul warned that there is pain lurking on that path
Thanks for the review Ronald, I'll pick up your comments tomorrow morning!
Youre welcome (Im still called Roland, though ;-) )
haha, sorry about that, maybe early signs of dyslexia ;-)
done. I have also removed the Try object.
added ``` super.preStart() ```
private, made it private[camel], added note for internal use
True. good catch. Did think about it when I first saw the code to mention, but thought FSM would mean more dependencies, but now I see it is  in akka-actor.  
Done already :)
this is only called from within the processor, which is safe.
Changed to AnyRef
added a separate ticket for this, #1923
What do you mean with 'this' ? The callback? the exchange? at timeout?  Looking at the camelk code, if the exchange is transacted, the callback is always called. Otherwise the processor implementation is responsible for calling the callback. I can't immediately find an issue.
on hold removed
getCamelContext is thread safe
I removed the rich stuff. java users need to pass in a CamelContext to the methods of CamelMessage that need it, Scala uses an implicit CamelContext (please review in next pull request)
Added some text on the method
created a separate ticket, #1924
I've removed the comment, the code works fine, nothing wrong with it in my opinion.
code is fine.
added ticket #1925
created ticket #1926
no problem in corresponding java file?
no, no problem there 
ah, because there is no old io for java :)
yes, that's the way
I think I'd understand this better if it was written like:  ```scala     val newMember = Member(node, Joining)     if (localUnreachable contains newMember) failureDetector.remove(node)      val newMembers = localMembers :+ newMember ... ```
not needed, use `roles` instead (it is in MultiNodeSpec)
`with ImplicitSender with BeforeAndAfter` not used, remove
you can use roles instead of addresses here
... then this will be handled by implicit conversion from RoleName to Address
`private[testkit] def roles`
... and this written as      cluster.latestGossip.overview.unreachable.map(_.address) must be(others.toSet map address)    
use address(victim) instead, or rely on the implicit conversion
`cluster down victim`
I think you look at the wrong place, L151 in MultiNodeSpec:      /**      * All registered roles      */      def roles: Seq[RoleName] = _roles
oh, you meant `roles`, I thought you meant `roles` ;)
Yes, I forgot that you had added that you had added that after I wrote this test.
Yes, I see what you mean. It's kind of contrived. I wanted to skip doing the same work twice, but I probably ended up iterating through the collections three times instead.  Is this any better?   ```scala   remove the node from the 'unreachable' set in case it is a DOWN node that is rejoining cluster val (newUnreachableMembers, rejoiningMember) = localUnreachable partition { _.address != node } val newOverview = localGossip.overview copy (unreachable = newUnreachableMembers)   remove the node from the failure detector if it is a DOWN node that is rejoining cluster if (!rejoiningMember.isEmpty) failureDetector.remove(node) ```
that's nice, perhaps change order and the negation      val (rejoiningMember, newUnreachableMembers) = localUnreachable partition { _.address == node }  !isEmpty => nonEmpty
Explain why you wipe the state in the comment
good catch indeed
No you are both wrong, it is `roles`. 
This is nice. 
Cool, some BSR to give RelativeActorPath some Scaladoc?
ok, for 2.2 it might be possible to use `def apply(anchor: ActorRef, elements: immutable.Iterable[String]): ActorSelection`
I'm not sure I follow, can you illustrate with a test case? shouldn't ActorSelection("/user/foo/bar") have rootGuardian as anchor perhaps this case is not "/..", but then what about "akka:MySys/user/foo/bar"
RelativeActorPath means relative URI (where "/user/a" is an example), not path relative to some actor; hence this patch should do exactly what you say and what the ticket says.  Ill add doc to RelativeActorPath to make this more clear.
yeah, I always mix that up, relative for me is something that not starts with "/", but you are of course right
this is effectively a minimized back-port, 2.2 does what you say
Magic value is magical
Isn't it cheaper to test version prior to set membership?
I'm not sure about that, equals of version is comparing a map of VectorClocks
So this is only used temporarily? It's only for the testing right?
Good point, thanks
I needed the counter for stats in LargeClusterSpec. Can probably be removed when we start micro-optimize.
well, it has at least a name (gossipToDifferentViewProbability) ;-)
How is this tuned? Do we have empiric evidence that this is optimal? Should it be configurable? 
... and I just wanted it be in symmetry with the other probability we have, gossipToDeputyProbablity
Yeah, but is it 0.8% or 80% or whathaveyou? :D
I doubt any user will need to tune that. It's only a way to say that we should sometimes gossip to any random node and most of the time to nodes with different view (old or newer version). I can of course move it to config if that is what we prefer.
0.8 == 80% in this case. we use random.nextDouble
If the actual percentage value has no practical impact on the gossiping then leave it (but I assume this is not true).  If it has impact then I think it should be configurable, different clusters have different needs, better to be overly tunable. 
fixed, moved the probability to config
Is it always 5 seconds?
Why is the implicit val needed? Also, shadowing the parameter "system" should be avoided.
Of course not. Silly me. To fast cut'n'paste.
It's needed for dilated. It's only inside this block for that exact reason.
it might be easier to pass this argument explicitly instead of implicitly
you might want to name the `system` parameter something else to make it clear that is not the same as `system` in AkkaSpec
no actorSystemResource here?
This is part of the docs, and the `AkkaJUnitActorSystemResource` is internal to akka.
ok, I see, that explains my questions
Remove this sentence, it is repeated below:  "+One ``CamelExtension`` is only loaded once for every one ``ActorSystem``, which makes it safe to call the ``CamelExtension`` at any point in your code to get to the  	 110	 +Apache Camel objects associated with it."
shutdown and "shut down" in the same sentence
What does this mean in practice? Or, rather, what is intended to be communicated with it.
Does Akka create producer actors?
Is it wise to link to trunk?
When I read "Acknowledgements" I expected a list of names. Perhaps something like "Delivery acknowledgements"?
Why the line break?
Should we revisit this URI pattern to harmonize on actor identifiers instead?  akka:system@host:port/asdasd ?
I'm all for it. That was originally something I asked for, but at that time someone (can't remember who or why) was opposed to it. It is far more universal if we use the standard akka actor path syntax. Don't know how users are going to use it, but one less translation would be nice. The only thing is the options of course, which is the camel way of passing in options, but the options we actually support (autoAck and replyTimeout) are not really something I think the user would set on the path. So we could change it to just be the actual actor path in akka syntax, or the actor path in akka syntax with the options at the end, which is less 'compatible', what do you think?
If we make the change in an isolated commit then it's easy to revert it if there's any problems.  Cheers, 
added a ticket, #2326, so we can do an isolated commit later.
Added: You can access the ``CamelExtension`` inside a `Producer` or a `Consumer` using the ``camel`` definition, or get straight at the `CamelContext` using the ``camelContext`` definition.
Akka creates all actors. Don't you know that? ;-P  The actors are created like any other, so maybe the wording is a bit confusing. I have changed it to:  Actors are created and started asynchronously. When a `Consumer` actor is created, the `Consumer` is published at its Camel endpoint (more precisely, the route is added to the `CamelContext`_ from the `Endpoint`_ to the actor). When a `Producer` actor is created, a `SendProcessor`_ and `Endpoint`_ are created so that the Producer can send messages to it.  
probably not. I'll link to version 2.8.0 since that is now the dependency in akka.  BTW, Camel 2.10.0 just came out 3rd of july, which I think has breaking changes. I'll add a ticket for upgrade. 
Opened a week ago: http:www.assembla.com/spaces/akka/tickets/2322
Ah ok. I've added a commit to this pull request based on your review, you can merge it I think.  
Alright. Tackling Java documentation now?
Yep :)  Sent from my iPhone  On Jul 12, 2012, at 2:05 PM, Viktor Klang ()<reply@reply.github.com> wrote:  >> + >> +.. includecode:: code/docs/camel/Introduction.scala#CamelActivation >> + >> +The above code shows that you can get a ``Future`` to the activation of the route from the endpoint to the actor, or you can wait in a blocking fashion on the activation of the route. >> +An ``ActivationTimeoutException`` is thrown if the endpoint could not be activated within the specified timeout. Deactivation works in a similar fashion: >> + >> +.. includecode:: code/docs/camel/Introduction.scala#CamelDeactivation >> + >> +Deactivation of a Consumer or a Producer actor happens when the actor is terminated. For a Consumer, the route to the actor is stopped. For a Producer, the `SendProcessor`_ is stopped. >> +A ``DeActivationTimeoutException`` is thrown if the associated camel objects could not be deactivated within the specified timeout. >> + >> +.. _Camel: http:github.com/akka/akka/blob/master/akka-camel/src/main/scala/akka/camel/Camel.scala >> +.. _CamelContext: https:svn.apache.org/repos/asf/camel/trunk/camel-core/src/main/java/org/apache/camel/CamelContext.java >> +.. _ProducerTemplate: https:svn.apache.org/repos/asf/camel/trunk/camel-core/src/main/java/org/apache/camel/ProducerTemplate.java >> +.. _SendProcessor: https:svn.apache.org/repos/asf/camel/trunk/camel-core/src/main/java/org/apache/camel/processor/SendProcessor.java >> +.. _Endpoint: https:svn.apache.org/repos/asf/camel/trunk/camel-core/src/main/java/org/apache/camel/Endpoint.java >  > Alright. Tackling Java documentation now? >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/562/files#r1148983
Why this level of indirection? I can only see it used once. 
well, I just wanted to "document" that the AskTimeoutException is related to the join procedure
I'm not sure I understand, what i the selection refers to multiple actors?
doesnt cut it: you missed the .toString calls
Looked too expensive...
An equals that allocates a lot of garbage is a baaad equals :)
Is the underlying assumption that toString will always be cheap for the implementations?
akka.util would be a much better place for Murmur, just sayin
no, the assumption is that `toString` is the only way to structurally compare two `Pattern` instances; am I missing something?
Here, the elements are String or Pattern, so toString is cheap. It's to bad that Pattern doesn't implements equals.
`target` is the anchor of the selection; you get the first reply, as discussed on the ticket
can't we change this Array to immutable.IndexedSeq - this bug is just another sign that the small performance advantage of array when parsing the original string is not worth it. It would have been a bug anyway, but only for wildcard selections, ...
I agree with the sentiment, but since Pattern does not play nice wed have to keep the same comparison dance (which Im currently tailrecing), meaning that Array is still the thing with least overhead
please verify the correlationId also, which in this case is the actual `selection`
you need to override toString, since this is used in ActorSelection.toString
ah crap, thanks!
this TODO can be removed now
indicates that a test is needed ;-) 
can this be extracted to a method?
interestingly the TODO never was covered by the ticket it referenced 
yes, it can
    byteSerializer.toBinary(ba) must be theSameInstanceAs ba
those two are not equvivalent, but I would say Class.forName is correct for our usage, but I think @bantonsson should confirm  http:stackoverflow.com/questions/8100376/class-forname-vs-classloader-loadclass-which-to-use-for-dynamic-loading http:blog.bjhargrave.com/2007/07/why-do-classforname-and.html
Yes, the change will force early "resolution" of the class which tries to ensure that the class is well behaved, and initialization by running the <clinit>.  I assume that the idea is that the class will be used just after calling this method, which will trigger those things anyway, so we might as well do it here. 
wow, that one is ugly, but I'll change
I can turn off the initialization. I needed to change to forName otherwise [B does not resolve (loadClass failed)
Doesn't work ;-)  [error] /Users/viktorklang/Documents/workspace/akka/akka/akka-actor-tests/src/test/scala/akka/serialization/SerializeSpec.scala:189: overloaded method value must with alternatives: [error]   (notWord: SerializeSpec.this.NotWord)SerializeSpec.this.ResultOfNotWordForArray[Byte] <and> [error]   (haveWord: SerializeSpec.this.HaveWord)SerializeSpec.this.ResultOfHaveWordForSeq[Byte] <and> [error]   (rightMatcher: org.scalatest.matchers.Matcher[Array[Byte]])Unit [error]  cannot be applied to (SerializeSpec.this.BeWord) [error]       byteSerializer.toBinary(ba) must be theSameInstanceAs ba
ok, because of Array, keep it as it is, np
I 1up:ed it:      for (a  Seq("foo".getBytes("UTF-8"), null: Array[Byte], Array[Byte]()))        byteSerializer.fromBinary(byteSerializer.toBinary(a)) must be theSameInstanceAs a
this looks unnatural/unorthodox.  I'd prefer to reuse the nested protocol notation: akka:tcp:.../...
`:` is not a valid character in URI path elements, and we need to use protocol descriptors in there
Yes, name mangling was the reason to choose "." as a separator.
After a heated and fun debate with Roland we agreed that "+" would be more awesome, but can't be used because it needs to be a valid path element as well. so "." it is.
newbie mode: where do I get this EnclosingActor thing?
What do you propose?
this needs to be more clear, since we do not recommend using `getSelf()` within a closure that is executed elsewhere: I guess you are talking about the `schedule` variant which does not take a `Runnable`.
In practice this means that you should not call methods on the enclosing Actor from within the Runnable.
Yes, that is why I said "schedule a message". I clarify the wording then.
the first to on this line is too much (sorry for not noticing during first review)
too long line, doesn't fit pdf
in other Settings we have named the constants with first as capital letter
Cool!  On Thu, Sep 27, 2012 at 10:26 AM, Viktor Klang () <notifications@github.com > wrote:  > In akka-camel/src/main/resources/reference.conf: > > > @@ -12,18 +12,22 @@ akka { > >      # enable/disable streaming cache on the Camel Context > >      streamingCache = on > >      consumer { > > -       # Configured setting which determines whether one-way communications between an endpoint and this consumer actor > > -       # should be auto-acknowledged or application-acknowledged. > > -       # This flag has only effect when exchange is in-only. > > -       auto-ack = on > > +      # Configured setting which determines whether one-way communications between an endpoint and this consumer actor > > thanks! > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/751/files#r1704156>. > >    --  Raymond Roestenburg  code: http:github.com/RayRoestenburg blog: http:roestenburg.agilesquad.com twtr: @RayRoestenburg
This will be an excellent spotlight!
Good idea, I added the Spotlight.
that trailing dot might want to fight and replace the dashes, possibly 
I think it would be better to just use the same basic text as for Scala, replacing trait with interface (and fixing some syntax). What is missing in both cases is to mention UnrestrictedStash (and its hopefully existing Java companion) which do not force this mailbox selection so that derived classes can be more specific in what they require.
Also, do we really want to go into UnrestrictedStash here? I think it is better to mention it in the ScalaDoc of Stash.
btw, is this usable from Java: "trait UnrestrictedStash extends Actor" ?
UnrestrictedStash is not for java
+1 for adding these links between traits to the ScalaDoc; and we are indeed lacking two classes in UntypedActorWithStash.scala (i.e. versions without the restriction), could you please add those as well? Thanks.
Ok, I will try to figure out how that works from Java :) Now lunch and then I update this.
you forgot `UntypedActor with` somewhere ;-) (and a test, I guess)
Thanks, true. I added tests for all 3 Java base classes now.
Maybe put into a Warning block?
Hmm, I think info level is enough.
can you have a diff but found == false? Perhaps return an Option[Changes]?
this makes no sense.
This is a bit too opaque.
This doesn't return Changes anymore right, it returns all subkeys
Great catch, it's just residue from an intermediate solution. Will revert it.
It never returned Changes. It used to return the subkeys of the current node (not used anywhere) now it returns all the subkeys of the newly integrated node. 
You're right. Will rewrite.
why isn't "v" used here?
It's the curse of the multiple inheritance. The diff we get back is only indicative of which keys have changed, and we then need to find all values for those keys in the whole tree, since a key can exist in multiple places and have different values in those different places.
Alright so if the "v" is not supposed to be used here, name it "_" as per convention.
Will do. Didn't know about that one.
Coolio. "_" is the placeholder for "don't care" in pattern matching.
well, more precisely it is a fresh name every time you use it, and those names are guaranteed not to be referable to  it boils down to what viktor said ;-) (sorry, couldnt resist)
Total misunderstanding. I know what "_" is.
this might be good enough, but it's not perfect it highlights the version number in all search results I think it would be better to use `site`, as described here: http:w3bguy.wordpress.com/2012/01/26/google-custom-search-restricted-to-specific-directory-and-filetype/  I understand that you don't want to avoid hardcoding the url and do the snapshot replacement, but it would give a better user experience, in my opinion `http:doc.akka.io/docs/akka/<version or snapshot>/`
More important than the highlight, it doesn't limit the search to the exact version. For example "2.0" will also find "2.0-RC" and "2.0.1"
I also think site is the way to go. The stable and milestones already have nice versioned directories and `-SNAPSHOT` can just be point to `snapshot`.  Can't we use the doc string replacement tool for this?
I'll see what I can do, perhaps just take the url of the current page and add as site
Of course not the entire url, but the appropriate part of it.
That would probably do the trick.
Do we ship the html docs in the download? In that case it doesn't work to take the url.
Search is only generated for the online docs :)
ok, no search bar for local docs, good
Why is this called a Mailbox when it is not a Mailbox?
This is not a mailbox either
That was one of the questions I expected to come up: users are used to the Mailbox term, does it have more value to use the less-gripping term MessageQueue because it matches our internal abstraction? Its a serious question. Either we rename all of them, including the config file entries, or none.
Users will only deal with the MailboxType, not the Mailbox itself, so you can rename the Mailboxes and keep the MailboxTypes intact.
ouch, good one!
can the first condition actually ever be true? might also do      testConductor.getNodes.await.toSet == Set(myself)  (or use subsetOf instead of == if necessary)
This change basically means that it is okay to disconnect clients (without warning) while nobody is waiting for a barrier. Is that what you intended?
    testConduction.getNodes.await.filterNot(_ == myself).isEmpty ?
what is this used for?
Yes, that's the intention as stated in the pull request comment.
The first condition is a just in case things go very wrong. Sure it should never happen.  Why would toSet == Set(myself) be better? Creating a set in a tight loop just to check for a condition?
Yes, much nicer.
`controller ! ClientDisconnected(roleName)` since the controller needs a roleName to do something useful
Creating the set is probably cheaper compared to the getNodes + await though
ouch, indeed. Youre right of course.
Sorry for not reading the front matter (must have still slept this morning). Even so it is not immediately clear to me why this is a good idea. The rationale behind the previous implementation was that the conductor should better know when players are on their way out; if they leave without permission better notify everyone ASAP that the test is failing/failed.
Performance is of no concern here, I think. But Viktors suggestion also makes the intention clear. I just proposed `subsetOf` because we dont use it yet ;-)
Yes, I know and fully support that idea, and come to think of it. I should probably revert some of this, and make the other nodes remove themselves before exiting.  Why didn't I do that before?
I didn't remove the nodes since I can't know in `beforeShutdown` if the test has failed, and removing the nodes just makes the barriers succeed.  The compromise right now is that if any node is in a barrier when a node is disconnected then that barrier and all subsequent barriers will fail.  If I leave the code as it was, then every MultiNodeSpec will print a number of ERROR messages when the test is shutting down normally.
I can't see that this hook is used any more. Good to have anyway?
Sorry, my search capabilities are in for repair, it is used.
no need for volatile AFAICS
I can remove it, if you guarantee that it works :)
hmm. currently it shouldnt be a problem, but if someone were to pass these setups around different threads  good point. leave it in, then.
Am I right to assume that this change is only done to make the `.withValue` sugar available?
not just sugar, but the "overloading" (taking out the current value, replacing it with the new and then restoring the old on exit)
I knew it! There was something which successfully hid amidst the shadows.
The Klang works in mysterious ways.
stopMessage (we always call them messages)
Yes, that sounds better.
please avoid mentioning `Await.result` in code samples, better use `onComplete`  Also: while touching this, please add to the warning above that using gracefulStop to make sure that system.actorOf will work with the same name does not work. I dont think we want to actually fix this, and the problem will go away in a few months. 
There is a warning already: <b>IMPORTANT NOTICE:</b> the actor being terminated and its supervisor    * being informed of the availability of the deceased actors name are two    * distinct operations, which do not obey any reliable ordering. Especially    * the following will NOT work:  Await.result(gracefulStop(someChild, timeout), timeout)    *     context.actorOf(Props(...), "someChild")  assuming that that was someChilds name, this will NOT work
Change 3. to 2. 
Debug residue, removing
Debug residue, removing
Hmm, I find the previous style much easier to parse. One thing per line, lines ordered.
have you tested that this actually works? wasnt there a problem when mixing different netty stacks in the same thread pool?
yes, works :-)
Just to give the reasoning about why this is called '*AES128CounterRNGFast'.  The DefaultSeedGenerator requires Internet access (it goes through the 3 random sources). If it isn't available, it slows down the initialisation. The SecureRandomSeedGenerator uses the SecureRandom seed generator implemented by Java and shouldn't have a start up delay (unless you have evidence to the contrary?).  Perhaps the classes need renaming because now the AES256CounterRNGSecure is inconsistent with AES128CounterRNGSecure
I can revert this change if you want, to me it was clearly dubious to call something "secure" and not use the SecureRandomSeedGenerator. I'll revert my change if you are sure that the original code was right. I'll also add some comments describing why it looks like it looks.
Also, why do you use (new SecureRandom).generateSeed(numBytes) instead of SecureRandomSeedGenerator? It's a bit inconsistent and undercommented to be able to figure these things out if you're not the author of it ;-)
Ok, reverted to the original code. I'd still like to have some comments to those Providers though since these design desicions are undocumented.
No, I think you are on to something. I think the class names should be renamed to be a bit more self-explanatory.
Excellent, thanks. Please create a branch off of master and open a pull request with the renames + comments!
How about AES128CounterRNGRandomOrg, AES256CounterRNGRandomOrg and AES128CounterRNGSecureRandom?
AES128CounterInetRNG + AES256CounterInetRNG + AES128CounterSecureRNG
Alright, looking forward to your pull request :-)
"It is nothing wrong with that." --> "This is expected behavior" or "This is expected behavior since ..."
Maybe clarify what is an "empty" CurrentClusterState?
yes, better, thanks
This is really nice. Talk about disposable actor.
We decided to not have auto-join right?
Very nice indeed. 
we decided to not have auto re-join  I think the `auto-join` still has a purpose, to be able to define seed nodes (== deputy nodes) but not join them automatically at startup
thx - dogfooding
Perhaps make it public. I don't see any problem in that this is queryable in the cluster user API. 
Right. As we discussed. 
this sets it to the default value of `maximum-frame-size`, not to the real configured one; this kind of substitution does unfortunately not work in `reference.conf`, which is always resolved on its own before being merged
lazy vals make my spidey senses tingle; can we not just not create the extension if it is configured off?
this should also mention that for all messages larger than this limit there will be extra performance and scalability cost
um, the idea was that normally this will never be instantiated, because all normal messages are less than the size, but I can make the limit into a "magic" off value as well, and create different implementations of extension based on that. Checking the property on call side is an alternative, with the trade off that the caller needs to know.
different impl based on an on/off switch sounds good to me; keeping one CHM per ActorSystem around does not sound like such a big deal otherwise 
good points, I will adjust
This is static, i.e. does not adapt with changing `mean`; I have a hunch that we might want to make this dynamic.
I was thinking about what way to go, but decided to not base it on mean because it is for preventing sudden pauses that has nothing to do with the normal intervals. I think one know (from experience of monitoring your system) that you have max gc pause of for example 4 seconds.  It's super simple to change to use the calucated mean instead, if we think that is better, but I'm not convinced.
Then Id propose to decouple it completely, i.e. rename it to `acceptable-heartbeat-pause` [in seconds] in the config.
Is this really a time-machine and not a clock?
What happens if this is negative?
What happens if this is negative?
What happens if this is negative?
:-) I'd be happy to rename it to clock. I never liked timeMachine.
I'd love that. Kept it as factor because that was what we talked about, but I also prefer it to be a plain duration.
Let's call it clock so as to not confuse the readers ;-)
Symbolic names ftw
as you see the constructor is private, and it can't be negative, except if you have negative durations, which I could add check for, if we like the defensive style
I can add check (stupid user exception)
yeah, but some day we should provide a real time machine, would be useful
Might also help in cases of overflow etc
Absolutely, put in a TODO Rename to timemachine when invented
The idea with a factor of something else is that there is less knobs to learn how to turn. But I'm fine with either. 
Good docs. But I think this ScalaDoc should be part of the class' ScalaDoc. It really describes what the whole FD is all about. 
Ah, that is what I call geeky. Love it. 
Hakat upp dig? 
:-) no, it's sum of the squares
so it's a "squaredIntervalSum"? ;-)
yes, I'll change it otherwise intervalSum is pretty badass ;-)
Ahh, don't turn me on like that...
soo nice, hard to resist
sorry for teasing: its not legal Scala :-( The closest you can get is `interval2` (since the raised 2 is not a legal character for an identifier)
ah, then it's no fun, I'll stick with the boring `squaredIntervalSum`
You are checking if this node is a singleton AND is also part of the other node's membership table? I don't get it. 
yes, the scenario is that the singleton cluster node sends join and then later receives a gossip, which includes him in the member ring
Ok. Now I understand.
Do we need to check this every time?
what is your concern? How would it be detected if not checked for? It could be moved into the next else branch, but I'm not a big fan of nested if:s
Nevermind, I wasn't thinking of the growing and shrinking
perhaps better name: `isOlderThan` WDYT?
I think that `isOlderThan` is a better name.
So can't we just set uninitialized to 0 instead of having this `Int.MaxValue` dance? Yes, the comparison would have to compensate for it instead. Don't know which is worse?
the reason was to keep `isOlderThan` clean, but I agree that this is a bit ugly I don't know what is best?
so instead of keeping a maximum in the gossip we deduce it from the current members, which means that the new member may well get a number which was used before; this might be what we want, but it should be documented as such (I might have missed it in the RST, but it is not in the ScalaDoc)
the upNumber is not public api, it is only `isOlderThan` that is exposed and as far as I can see there is no problem with re-using the up number for that purpose
yes, as I said it may well be good enough, but its semantics should be documented in the Member ScalaDoc
ok, I'll add comment
%% + cross Cossversion.binary ?
yeah, there are lots of ways to achieve this; you can also leave off the second %; or we just go for the %%; 
there is a CrossVersion thingy in this file also, is that correct?
this is a very long line 
So we're replacing pastExec with proper rounding, is that what you're saying here?
closing over non-threadsafe vars and update them on other threads is a no-go for me
Same comment here as above, any solution needs to be based on non-mutable state.
There is an implicit conversion involved for `max` here, right? Use max in Duration instead, and define a `1.nanos` constant. 
What if this throws an exception?
If what throws an exception?  If runnable.run throws it, then scheduleNext is not called. The PR doesn't change behavior here.
Then we're on the same page :-)  Alright, could you do me the favor of squashing the commits into 1 so it's easy to cherry-pick it into 2.0.4?  Great work!  
Ok. I created a branch wip-2579-squashed-scheduler-drift-akshaal and squashed wip-2579-scheduler-drift-akshaal into it. I don't know whether it's possible to update this pull request or should I create a new one...  Here is the commit: https:github.com/akshaal/akka/commit/3783be2f58d4380e900891d7b8331f4333dc7d86
It's probably easiest to close this PR and open a new for the squash (not a huge fan of push -f :-) )
Here we go.. https:github.com/akka/akka/pull/786
Don't we normally use ConfigExceptions for this?
yes, or IllegalArgumentException (possibly via requires) ensuring throws assertion error, which was the reason for https:www.assembla.com/spaces/akka/simple_planner#/ticket:2798
ah, yes, of course, my bad; will fix
you know this is done for each `@Test`? we have used BeforeAll and a static system variable at some other places
use context dispatcher instead
you might want to extract this to a method used here and from preStart was it intentional to use different delays?
I saw that the differences in delays was by design, good.
Yes, I know. Just modeled after some other test in the doc. I can change it.
This dance is not necessary any longer: I added the junit-interface plugin so that JUnit-tests are picked up automatically.
I mean, only within the akka-docs and akka-contrib projects.
Good to know. Why not everywhere?  /Patrik  5 okt 2012 kl. 20:24 skrev Roland Kuhn <notifications@github.com>:  > In akka-docs/rst/java/code/docs/pattern/SchedulerPatternTest.scala: >  > > @@ -0,0 +1,9 @@ > > +/** > > + * Copyright (C) 2009-2012 Typesafe Inc. <http:www.typesafe.com> > > + */ > > + > > +package docs.pattern > > + > > +import org.scalatest.junit.JUnitSuite > > + > > +class SchedulerPatternTest extends SchedulerPatternTestBase with JUnitSuite > I mean, only within the akka-docs and akka-contrib projects. >  >  > Reply to this email directly or view it on GitHub. > 
I snuck it in when I needed it, planned on discussing it and then forgot about it. Problem right now is that it runs all tests twice, so we should decide on how we want it to be and then do it.
in a finally block?
I had the same thought recently and then decided against it: this is not a resource leak, and it only is problematic if the test blows up.
Which is when I don't want the log file to be bloated :-)
what scheduler? the system scheduler?
please comment also that the interval may be even more off than usual across a restart, up to twice as long
Yes, the word scheduler is bad. Will change to "the periodic message scheduling" or something of the sort.  Will add comment about that the interval might drift up to twice as long (you have to be really unlucky though, but there is a possibility) or shorter. 
I was thinking about going the other way and adding some annotations for the JUnit tests and enhancing the JUnitSuite so it produces the same tags `"timing"` et.c. so we can classify JUnit tests as well.  We definitely need to discuss where we want to go.
class ScheduleInReceive(var target: ActorRef) {   def this() = this(null)    }
Please try to read which code I show in the doc, and which is for the test.
I can write `context.system.scheduler.schedule(500 millis, 1000 millis, self, "tick")` on the next line if you think it's better.
Say what? I'm challenging the choice of having "var target" in the class body, this is completely unrelated to whether the code is used in docs or not.
It's definitely better as the wildcard import pollutes the local namespace and adds 2 extra lines and is more letters than it saves.
Sorry. What I was trying to say was that the `var target: ActorRef` is only there to test the code. It is not visible in the doc.  I couldn't come up with a better solution to not pollute the code that is in the doc.
Viktor, he doesnt want to show the `target: ActorRef` in the docs, which your suggestion would ruin.
Ah, sorry about that, noooow I get it. :(  Good call. But please add a comment about things like that so happy cowboys don't Boy Scout too much in the future.
Absolutely. Will add comments.
I think this gets kind of confusing calling the default buffer `buffer`, and the test allocated buffers `buffer`.
What happens if we loop around with the default buffer? Will we not clear the previously read data?
Yes. That is what "clear()" does.
Sure, what do you propose?
Yes, and we should clear the data after every partial read?
Shouldn't we pass in `into` here?
What about `defaultBuffer`?
Yes, see the deleted buffer.clear() further down in this PR
Missed that nobody ever looked at the things read into the default buffer, clear away.
should this not only happen at the first iteration?
Me and Viktor discussed this in one of the outdated diffs. It should always be cleared since the `defaultBuffer` really is a `throwAwayBuffer` and nobody cares about the data being read.
in that case it might want to have a comment
how about a factor/percentage of nodes instead of a fixed number?
Did you need FixedRateTask after the scheduler fix?
why would that be better? factor 5 % is reasonable for a 100 node cluster, but not for 1000 node cluster
I will remove that, but that will be done as a separate ticket (today)
Maybe log that fewer peers were selected because attemptLimit was reached.
shouldn't be update() called here?
This deserves a comment :)
it certainly should, thanks
What if heartbeatInterval is -1?
What if you do not know the grand total of nodes when you boot the app?
I don't understand what you mean, or what you think is the problem. Please elaborate.  When would it not be good that each node is monitored by 5 other nodes? It will of course not be more than number of members - 1. Why should it be proportional to number of nodes in cluster? That doesn't scale.   /Patrik  8 okt 2012 kl. 18:17 skrev Viktor Klang () <notifications@github.com>:  > In akka-cluster/src/main/resources/reference.conf: >  > > @@ -78,6 +78,10 @@ akka { > >        # how often should the node send out heartbeats? > >        heartbeat-interval = 1s > >   > > +      # Number of member nodes that each member will send heartbeat messages to, > > +      # i.e. each node will be monitored by this number of other nodes. > > +      monitored-by-nr-of-members = 5 > What if you do not know the grand total of nodes when you boot the app? >  >  > Reply to this email directly or view it on GitHub. > 
Nevermind, I might just be overexaggerating.
ok, added yet another comment why this workaround exists - can't wait for the new remoting :-)
alright, added some boundary checks
Instead of having a lot of vars, aren't most of the vars interconnected? Create a case class and use copy?
I have moved the state and related logic to a separate immutable case class, `ClusterHeartbeatSenderState`. Pretty slick. Easier to unit test.
my reading of SI-7203 does not suggest that this code would be affected; either it is (and then the SI must be amended) or the comment and the `val` should go
Yes, I can't see that SI-7203 has any bearing on that piece of code. Removing.
this should have docs (assuming that it is public and for people who want to use their own deque)
Should it be public. Is there any real value in allowing people to have their own deque?
well, people might want to use this for the very same reason why you introduced it (unbounded deque)
missing space between words, also `[${other.getClass.getName}]`
you should also `context.watch(targetActor)` and stop this supervisor when the child terminates (possibly informing the `caller`.
Creating top-level actors should be a rare operation, using it on a per-message basis is too costly (the guardian supervisor would become a bottleneck for creation/termination). Therefore it would be better to create a top-level supervisor, which the query is sent to in a message, who will then create the actor you currently call `JavaAsk`.  I would recommend offering a static factory method which takes an `ActorSystem` and makes such a top-level supervisor, and then an `ask` method which takes a supervisor, Props and message.
it would be more flexible to leave this open, i.e. return the Future to the caller
ah, instead of an `ActorSystem`, the factory method should take an `ActorRefFactory` so that you can also create the supervisor below another actor
please replace this (i.e. the whole listing) with      .. includecode:: code/docs/pattern/JavaAsk.java
in case you didnt know (it sure is not documented yet): youll need to run `akka-docs/sphinx:generate-html` in SBT to get the right HTML output
 he must make sure that the asked actor replies with Status.Failure
the promise returned by Patterns.ask() is fulfilled as a failure, including the exception
I think a better name would be Single-Use Actor Trees with High-Level Error Reporting (or some shorter variant which mentions the single-shot nature).
Given the change to return the `Future` I proposed above, this should be changed along the lines of asynchronous API, but you can make synchronous calls by using `Await.result` on the returned handle.
when changing to returning a `Future`, the timeout parameter could still be passed into the created supervisor, so that that actor uses it as a receive timeout and shuts down the whole operation when that fires
I think `JavaAsk` is an inappropriate name. How about `SupervisedAsk`?
I totally agree, but a problem with that would be that the `javaAsk` is immediately stopped in the below `finally`, and the whole point is ruined. Is it possible to demand that the targetActor is stopping itself after processing, then the stop of the `javaAsk` can be done inside `JavaAsk` by using `watch` of the target and this finally stop is not needed. Perhaps demanding too much of the target actor? 
The supervisor can also watch the caller and then shut itself down upon receiving the Terminated message. This means that the target cannot expect to do anything after having replied, but that is a reasonable limitation in this context.
yes, that was what I *tried* to say, thanks for clarifying ;-)
But, isn't the top-level actor the next bottleneck? Apart from that this pattern is not usable for bulk-operations. In that case a parent actor should create/call/initiate the actions. The issue of a bottleneck might not be the question. Anyway I will present a solution as you mentioned.
Yes. SupervisedAsk is a nice one. And the method could be askOf in order to underline the Props similar to actorOf?
yes, `askOf` sounds good!
the issue is that the guardian actor `/user` is a single shared resource in an `ActorSystem` while your own top-level supervisors are  - private to this pattern - and you can create as many as are needed to handle the load
please add package declaration (docs.pattern)
please move the example usage into an actual SupervisedAskSpec so that it is executed (also, this does not compile as is)
emphasize `*not*` here
include the code from SupervisorAskSpec here (once moved out)
please fix indentation
In doc code we have switched to 2 space indentation for java code also, and line breaks after some 70 chars (unsure about the exact number). This is due to limited space in pdf output.
It's fine to use the full 80 chars, and it will render ok in the pdf.
Id rather invert the lines above and below and remove the sleep
How much slower is the test now on average?
Yes, good idea.
Slow? On my machine it takes about 630ms  The test before it is slow > 3 seconds
Sorry, about that. I didn't mean to say that I speeded up this test step. The preceding test step is slow.
So then my question still stands
This: 643ms Master: 937ms 
we should use more unique names, to not interfere with user name space
Creating such a highly customized Serializer seems like a huge waste of namespace. Can't this be generalized, I'd bet a Object-to-Proto Serializer would be much more interesting
why not string?
WDYM? Unfortunately this has to be specialized, see #3271. But I intend to use this serializer for other possible envelopes, like Broadcast and friends. 
My point was that instead of hard-coding the conversions, it would be nice to have something more generalized, so it could even be used by end-users (I can imagine having richer classes that you want to be Protobuf going over the wire but be reassembled into Java classes). Does that make sense?
Yes, of course! The point here though is that a lot of tells go through   ActorSelection especially with clustering, so it makes sense to handle it   in a hand-crafted way. An auto-generated format for SelectionPath would   probably end up nesting them, since it is a linked structure. I encode   them as sequences here for faster encoding/decoding. For other messages   this is probably not needed.  But this is a proposal PR, so we should think about alternatives.
I don't know, SerializedMessage in WireFormats.proto uses bytes, too. I just kept it the same. 
object-to-proto sounds so general that my instincts say useless, what am I missing? I agree with Endre on this specific feature, maybe we should talk in front of a white-board about what you mean?
+1 add `akka-` to it at least.
I've used `obj.getClass` in some of places. It will be printed as `class foo.Bar` or `interface foo.Bar`. What do we want it to be?
I think it is redundant, name should be enough. When will it print `interface`?
Calling `getClass` on an instance will of course never print `interface`, what I meant was that I used it in  places where a have some kind of `class` where that might be an interface, and an instance and then print them both. Settling on `getName` is fine by me.
Removing the redundant closing `}` would be nice as well. IDEA has some quirks :wink:
Ah, the good old `if (...) always (...)`
success Fail :-)
This, or similar, code seems to be duplicated, I think we ought to switch to something similar to: https:gist.github.com/4338250
Nice! Add a ticket please.
I don't think it's a new ticket, it's Boy Scout Rule & DRY
you can still create a ticket if you fear that you would otherwise forget it (I know I would)
No, I fix it right away. 
If it should be fixed as a part of the PR there's no way to forget it?
This is slightly more complicated than it seemed first. Can we go the ticket way? I have several failures to debug.
Yes, but only because we are under high pressure to stabilize master; DRY and Boy Scout Rule is not something that deserve their own tickets normally  it's the way we write code.
Thank you for the relief, I promise to be a good Boy Scout!
This seems weird, what's the difference between success(Fail(e)) and failure(e)?
 > This seems weird, what's the difference between success(Fail(e)) and   > failure(e)?  I shouldn't have used failure(e). The status itself can be Fail(e) and   Invalid(e) in case of errors.
What about: statusPromise.failure(InvalidException(e)) and statusPromise.failure(e) ?
That could make sense. But this means that I have to change the Transport   API, so this will be a larger change.
Can I use pipeTo with failed futures?
From a quick glance, checking the uses of "Status" trait it should be fairly quick, just remove Status, Fail and Invalid, see what breaks, introduce InvalidAssociationException
    final class PipeableFuture[T](val future: Future[T])(implicit executionContext: ExecutionContext) {         def pipeTo(recipient: ActorRef)(implicit sender: ActorRef = Actor.noSender): Future[T] = {           future onComplete {             case Success(r)  recipient ! r             case Failure(f)  recipient ! Status.Failure(f)           }           future         }         def to(recipient: ActorRef): PipeableFuture[T] = to(recipient, null)         def to(recipient: ActorRef, sender: ActorRef): PipeableFuture[T] = {           pipeTo(recipient)(sender)           this         }       }
Yes, and this might be related to some of the failures. Doing this one now.
a @_ ?
This is not the way to do it, use intercept
You can replace all uses of Promise.successful(...).future with Future.successful(...) (and vice versa with Promise.failed)
Is this line safe?
use auxiliary constructor instead of null
    init(channel, remoteSocketAddress, msg)(statusPromise.success)
use auxiliary constructor instead of explicit null
(banging head against wall)  This is just plain wrong, it should be pipeTo instead. I think you might   found the bug in the throttler.
s @ _ ?
You're welcome :-)
unless this is known to be completed before this actor shuts down you might want to protect against `self` being null here
same problem in case of an intervening restart
I think we should switch to putting a Uuid in SharedMailbox and sort the buddies on that. And then add to buddies after the mailbox has been set, and remove from buddies before the dead letter mailbox has been set. Would that work?
Add comment on the order is extremely important here
This will be extremely expensive since it will require a traversal of the entire mailbox to find the size, also the size doesn't really make sense since the messages will be processed in parallel so when you get the number back out it might be larger or smaller.
And since we aren't adding and removing to the queue frequently, it might be more performant to just use either a CLQ or a Vector with a CCAS?
why all the trouble? the line above is from the ActorModelSpec, and I just sorted the buddies there to better find which ones were missing etc. I think were good wrt. buddie-sorting.
Well, that all depends on the use-case, which is why I made it tunable: if you expect high message rate, then you should dispatch to the actors e.g. RoundRobin and rely on that only (i.e. set BWT=-1), if on the other hand you expect only low rate but must absolutely wake up somebody if possible, then set it to 1, but then job submission is not so frequently done anyway and an appropriate router can reduce the number of traversals by not failing registerForExecution() in the first place. And if normally the queue contains like 10 elements, then set it to 10 and let that equalize the number of active mailboxes (e.g. routing only ever to the first one). I think this is flexible enough, there is no silver bullet which does not require tuning.
And the new code passes the tests even if you stress it real hard?
What do you mean besides running the ModelSpec? Is there anything meaner?
It's never failed reproducibly, only but tuning the params right?
We _could_ add an AtomicLong into the mailbox to keep track of current "size", or perhaps use iterator and iterate over entries to see if we have "enough".
I'll try the Iterator approach, because it feels more cache-striped
 > s/postStop/preStart  True.
I think this deserves more clarification. First mention that default preRestart stops the children, and they will not be recreated automatically. Then, what I think is a little known fact is that if you override preRestart to not stop the children the children will be **recreated** automatically, i.e. new instance of each child.
Oh! I just followed the mailing-list discussion that Roland included in   the ticket, and this was a pattern that Derek mentioned. I kind of tried   to reconstruct what Roland had in mind with this ticket, so it might be better to discuss this with him tomorrow.
it's described in point 6 here: http:doc.akka.io/docs/akka/snapshot/general/supervision.html but that doesn't match what is written here
the full link: http:doc.akka.io/docs/akka/snapshot/general/supervision.html#What_Restarting_Means
 > it's described in point 6 here:   > http:doc.akka.io/docs/akka/snapshot/general/supervision.html > but that doesn't match what is written here  I believe you! The comment I followed was:  "I prefer the constructor for non-childish things, but for creating and   starting children, I prefer preStart().  The reason is because I can   opt-out of child-stoppage in preRestart(), and if I've created children in   the constructor, then I have no recourse to ensure that duplicates don't   get started.  This just provides me with a pattern that "generally works"   and is resilient to refactorings a bit more than the other method."  I think I will close this PR for now, and wait until I can discuss it with   Roland.
why close it? it's a lot of great stuff here
I meant to reopen it after reconsidering some of the stuff :) But you are   right, it may remain open.
yes, we will probably sort it out tomorrow, and I think there is only a need for some minor adjustments of this section 
`to use of`  `to use`
`not like to reinitialize some internals upon every restart, most particularly child actors.`  `like to avoid reinitializing internals on restart. For example, it is often useful to preserve child actors across restarts.`
Unclear what `during the initialization of the first instance` means
It might be useful to link to other parts of the docs that include related discussion, e.g. `For more information see :ref:`what-restarting-means-scala`.
Patrik and Endre are both right: the point was to discuss what it means to override `preRestart`, both concerning not calling `postStop` and not killing all children. At that point it is important to mention that children will recursively be restarted, so you have to make the choice between killing and restarting at each level of the hierarchy.
this can be avoided if the parent does not publish the ref before enqueueing the initialization message; which makes me think: that is only true in the local case, for remote-deployed children failure of delivery must be considered as well
yes, that is a good point, that was the reason why I used constructor instead of "initialization message pattern" for the cluster singleton pattern
Can you explain this in more detail?
Ah, I see.
This is the most dangerous initialization pattern, but as I wrote, in the presence of circular dependencies it is the only possible one.
Also, not publishing is not always possible, see the IO.TCP for example where various registration acts might be needed from the client before using the actor.
I'm not so sure about this warning. It might be too strong. Especially the last sentence can be interpreted as something similar to the problem of publication of `this` reference from java constructor. The important thing is that you must be aware of that the init message might not be the the first received message.
    case "Danger! Danger!" => true
is this needed?
otherwise the test would fail in such a case
I think the author, publisher and the publication date (even ISBN) should be included here. I personally would prefer thumbnails of the covers, but others might disagree.
yes, I'll add some facts, but are we allowed to put up thumbnails? images and copyright is always scary
I don't know. It is not that important, but the other metadata is.
we are using `requiring` in other places
what about TestTransport?
document the > 32k limit also
that takes it from config: maximum-payload-bytes
true, I just taken code from neighbors
why get or else instead of a simple pattern match?
Right. Will fix. 
Remove or use, commented out code is just liability
why not optimize this as the one below?
why copy if its already seen?
This is so badass I don't even know where to begin
is node.receive threadsafe?
Override unhandled instead
Why is this the string representation of the address?
this is very suboptimal
Why top level?
Why top level?
why not case class instead of all the boilerplate?
why the import here?
Always use getBytes with a specific charset
Using what encoding?
Open ticket and remove fixme
Map.empty[Node, Timestamp] is not a singleton
What's the Java API?
I suggest removing this as this goes against how Extensions are normally used within the system. Make your Extension named Node instead so you can do: Node(system)...
Do we relly want to prolong the tests 30 seconds? That's adding 50% to my test time. no like
What's up with all this wasting developer time?
Why this catch block?
why this catch block?
Why this waste of developer time?
Why this waste of developer time?
Why this waste of developer time
Now I've seen the same string quite a few times, please use the Config library's "fallback" mechanics.
We should rewrite the tests so that they only take as long as they need. use awaitCond instead of arbitrary sleeps
Why did this return from the dead?
Why is this back again?
I'll remove and create a ticket
I don't understand what you mean. 
I don't understand what you mean. And the line you are commenting on does not make sense.  Which line do you mean? 
 +  def seen(address: Address): Gossip =  	 163	 +    this copy (overview = overview copy (seen = overview.seen + (address -> version)))  Why is this not optimized as the one above? (only copy if you must)
WDYM? Are you ironic? What is wrong? Please provide understandable feedback. 
Seems to have been some issue with GitHub (or you'e pushed new code today), I've explained on the comment above this one. I'll go through and verify that not all my review has been broken by github
Holy shit, github has completely fucked my review :(
Sure. But why are you constantly commenting on the wrong line? Makes it hard to understand what you mean. 
No, as I said, github has completely fucked up my comments, putting them on the wrong lines. sigh
What is? Commenting on wrong line again. 
read my comments about this, I think I've responded that github has fucked my comments, like 5 times now...
You mean call it 'self'? 
8 lines up. Still don't get it: 'val node = system.node  implicit conversion adds 'node' method' ??
Am I using Extensions wrong? What is suboptimal? 
Ah, you mean passing in *Impl.? 
I need access to the 'systemActorOf' that is onyl in Impl class. What do you suggest? 
This was just completely wrong, I'm talking about the use of address.toString both to set as Node value AND to do comparisons between nodes, don't understand why toString should be used
8 lines up lands on 'memberMembershipChangeListeners'. WDYM? 
WDYM? Where should I create the 'clusterSettings' in a companion object? 
Why do you need that? Also, why are you creating multiple top-level actors instead of just one, that is responsible for the clustering, and that one can have children?
Got it. Fixed. 
The top-levle remarks was around these:   private val clusterCommandDaemon = system.systemActorOf(  	 376	 +    Props(new ClusterCommandDaemon(system, this)), "clusterCommand")  	 377	 +  	 378	 +  private val clusterGossipDaemon = system.systemActorOf(  	 379	 +    Props(new ClusterGossipDaemon(system, this)).withRouter(RoundRobinRouter(nrOfGossipDaemons)), "clusterGossip")
this is asynchronous
this is asynchronous
Should this be called for every recur?
These state transition do not do anything, nor are they documented as what who is supposed to do with them.
Should this be called for every recur?
This can become annoying if there's many retries
This can become annoying if there's many retries
put merge on Gossip instead
Why is this needed? newMemberSet is immutable right?
should we log every time it recurs?
why not toSet?
or why not use ".distinct" since you're not going to use the set anyway
It is two different kind of Node.  * The Node value you are talking about is for VectorClock.Node with is only a glorified typed identifier. Should be the Address string. * The comparison is for Node.Member e.g. the different nodes in the cluster. The comparison is for maintaining a SortedSet for the ordering of the nodes in the node ring.
Because I want the cluster management actors to live under the 'systemGuardian'.  I could create a dummy parent supervisor for the cluster management actors, might be a good idea. Is that what you mean?  So /system/clusterManagementSupervisor/* ?
Still don't know where this line is. 
Got it. See my previous comment. 
Damn. Thanks. New funky 2.0 world. Are they guaranteed to run (being stopped) in order of how stop is called though? 
No. Good catch. Thank you. 
Because I'm stupid. Fixed. 
I know. I should not have added them yet. Will impl them very soon.  They are all documented in the specification. 
Right. Fixing it. 
Fixed. Thank you.  
Fixed as well. 
Fixed all of these to foreach. 
Already done. Committed to my working branch yesterday :-)
Because 'newMemberSet' is a Set but I need it to be a SortedSet. See the comment about bug in Scala. There is no CanBuildFrom for SortedSet. 
I like that. Fixed. 
Because I needed a specific apply method to create the Node instance. Same signature as the generated one but different impl:     def apply(name: String): Node = new Node(hash(name))
Can't use 'distinct' directly since does not exist on Iterable.  Would have to do: values.toSeq.distinct So instead I do: values.toSet
Ticket already exists. Removing comment. 
What do you mean? 
    sealed trait Node     object Node {       private case class NodeImpl(name: String) extends Node { override def toString(): String = "Node(" + name + ")" }       def apply(name: String): Node = NodeImpl(hash(name))     }
It is needed. To let the system gossip enough. Could perhaps be cut with 10 sec, I don't know. But I have tagged all these long running tests with a tag 'long running' so they are excluded by default you have to run:       sbt -Dakka.test.tags.include=long-running  to run them
Welcome to the distributed eventually consistent world.
Since errors were swallowed. Now they are reported as failure. 
I've answered this. 
Error in merge I guess. Fixed. 
Sorry. Must have screwed up merge. 
seen.values.toSet is probably the best option here
Yes. What I did. 
Indeed shorter. Taken. Nice. 
I think this should be accompanied by a factory which accepts `ActorContext` and has the appropriate WARNING signs (and uses context.system, obviously). Then I think this would make sense :-)
Yeah, the reason why I didn't do that is because It hink that map, flatMap etc are still unsolved. I'd like to have another abstraction, like an EventLoop that could be used instead of Actors for this.
okay, color me clueless (I guess I dont wanna know)
why not include the other good changes in scala/mailboxes.rst? * Configuration name * default mailbox
That's not a part of 3437 so please open a ticket on that!
I guess that was a comment on "how to change the default mailbox"
my job is to be picky when reviewing; here is another mismatch ;-)
I'd prefer `RuntimeException`
I kept the original approach here (copied from ActiveClientHandler). We can change it, but it will be not thrown anyway.
ok, if it's not thrown then no problem
make message more unique and use NoStackTrace
WDYM? This is not thrown everywhere and it is exactly the same code that the Server has. Do we want to change it now?
it does not matter whether it is thrown: the stack trace will be filled in during construction unless you use `NoStackTrace`, and getting that stack trace takes *ages*
yes, this is what makes this interesting
not used, remove param?
MessageDispatcher requires it
that is a nice test!
This will not work when messages are serialized. I don't think that type of messages is good in a hello world sample.
crap?  Seriously: thanks! Will make it  wait for it  `new Serializable() { private static final long serialVersionUID = 1L; }`  The thing is that we have this mistake in ALL our Java samples (i.e. we declare none of our beans `Serializable`).
yes, very nice. I tried `System.setOut` which did not work :-(
I think Viktor used the trick before to shadow the implementation of println with a custom one, and since the custom code was not included in the docs, it maintained the illusion of a real println.
not ALL ;-)
I think you should do a proper immutable message class (similar to how Jonas did it)
how about an Enum? If it does not need fields then that should cover everything
no, this should be used as a getting started template, no tricks. it just reflects the fact that println is not observable from the outside :-) 
yes, Enum is alright, but rather weird to define an enum for just one message, ...wait you have two... ok
Enum is the way here. Incredible how much Java I forgot already...
lets call it carefully crafted example
Yeah, wasn't sure about that one, compose by itself doesn't sound right tough. I haven't actually used dataflow yet, so I'm not sure how to improve that.. may be "composition"?  But the other changes are good.
"flow expressions compose" is a statement I think. Reworded: It is possible to compose flow expressions. But anyway, it is confusing.
yes, the original was correct here, and the other change is not correct either.
Are you also saying "makes the *look like*" is ok?
The flow method acts as the delimiter of flow expression composition  vs  The flow method acts as the delimiter of flow expression compose
It's currently:  "The ``flow`` method acts as the delimiter of dataflow expressions ([...]) and flow-expressions compose." <-- which is completely true and valid imho.
no, that was overlooked, there is a "code" missing.
And correct me if I'm wrong, but I thought does is the singular form, and here you are referencing a singular - for-comprehensions, so it should be doesn't, not don't.  As I don't understand what data-flow composition in akka really is, I won't argue the point anymore, but as a native english speaker, "flow-expressions compose" doesn't sound right at all. (See the examples here http:dictionary.reference.com/browse/compose). If you wanted to use the form "compose" instead of composition, I would have thought it would be "The flow method acts as the delimiter of dataflow expressions ([...]) and to compose flow-expressions."
The point is not that "flow method compose flow-expressions", but "flow-expressions can be composed", which is in CS jargon simply used as "flow-expressions compose". See analogue: "applicatives compose".
wosh. thanks drewhk. Haven't seen that jargon before. Don't suppose you have any links handy that elaborate? Tried googling "applicatives compose". Found this: http:stackoverflow.com/questions/7040844/applicatives-compose-monads-dont . Oh-no. Monads. *sigh*
Try Google Scholar. (you won't get noise results from music composition, etc.). Btw, for-comprehensions is in plural form. A for-comprehension is a kind of expression, so for-comprehensions mean the set of those expressions.
Sigh. Ah well, 1/3 isn't a total waste of effort, and my heard was in the right place :)
Isn't it better to assign node.value.ticks once? node.value.ticks = if (pastExec && isCurrent) offset - WheelSize else offset
Could you perhaps move this buffer into apply() or better yet make it part of the tailrec method?
"delimiter must not be empty"
No, the parts in the buffer have to survive for multiple events
Ah, I see. I just didn't see it used anywhere else in the class but didn't realize that it was needed for multiple events. Cool. 
Thankful, I am ;)
But you are right about the apply() one! Thanks!
this needs to be clearer: when sending you need to include the terminator if includeTerminator is true
Id rather call this `minSize`
this should discard frames which are too large
~~~ scala many.reverseIterator.map(Left(_)).toVector ~~~
Well, you don't need to. But you have to know what you are doing then :)
same for LengthFieldFrame then
Then I have to search for delimiters in the passed data.
no, the contract is that above this stage ByteStrings are single messages
if !includeTerminator then it is not necessarily true
WDYM? Especially in that case we have to assume that everything coming in is a separate message, no? And I would find it quite inconsistent to support sending multiple frames in one ByteString when you WILL get them back one by one
Strictly speaking there could be no encoding step at all -- it is so easy to construct it yourself and possibly batch messages. Decoding adds more value. I don't find it inconsistent.
Looking at the eventPipeline I see the following semantics:  * below this stage you have a stream of bytes, represented by a stream of ByteStrings * above this stage you have a stream of frame, represented by a stream of ByteStrings where each ByteString contains exactly one frame  Consistency means that the commandPipeline honors these very same semantics.
I don't really like this consistency criterion. I definitely see value in the ability to batch outgoing frames, while not having to decode them manually. Netty has only http:netty.io/4.0/api/io/netty/handler/codec/LineBasedFrameDecoder.html but no encoder -- since it is trivial.
okay, finally I got what youre saying (sometimes I am a bit slow); this should be part of the documentation (ScalaDoc and rst) then
OTOH you made me thinking about if that is really a good idea to add the terminator in the encode step. It should be either a separate option, or just up to the user completely.
yes, that is a possibility as well; you just need to document it
So shouldn't this be called `includeDelimiter`? And the included delimiter doesn't need to be a byte.
I'd suggest to only have one method that takes an immutable.Seq[Address] and convert it to an immutable.IndexedSeq[Address] inside the method and instruct Java users to use ``akka.japi.Util.immutableSeq(...)`` as we are talking about an API for "power users".
We need to say what it does if it's already joined.
alright, I will copy the docs from the manual
Why is there a StatsMessages and Messages interface that looks like the same? The same question for TransformationMessages. Is this something specific to Java usage? I do not see Messages imported, or linked from docs. 
Oops, residue from refactoring, thanks.  /Patrik  4 okt 2012 kl. 17:07 skrev drewhk <notifications@github.com>:  > In akka-samples/akka-sample-cluster/src/main/java/sample/cluster/stats/japi/StatsMessages.java: >  > > @@ -0,0 +1,55 @@ > > +package sample.cluster.stats.japi; > > + > > +import java.io.Serializable; > > + > > +#messages > > +public interface StatsMessages { > Why is there a StatsMessages and Messages interface that looks like the same? The same question for TransformationMessages. Is this something specific to Java usage? I do not see Messages imported, or linked from docs. >  >  > Reply to this email directly or view it on GitHub. > 
Doesn't say whether it's safe to mutate the resulting collection or not.
Doesn't say whether it's safe to mutate the resulting collection or not.
Can we please wipe all @author tags, we have collective code ownership.
... and also because authorship is maintained by the git log and all the current code is a putpourri of contributions.
missing end paren
it's an sample app, main program that prints results to console.  running this at hight speed wouldn't be very nice this is the main thread
Of course, I meant that it might warrant a comment stating why it's done?
ok, changed to ignore ClusterDomainEvents (that was the intention) and rest are unhandled
I was struggling with java interop, but it was the sealed trait that was the problem, not the return type, so I have changed to `MemberStatus` here, thx
added doc comment :-)
`failure foreach throw`
just curious, is there a signature difference between this and  `lazy val actorClass(): Class[_ <: Actor] = {`
should this be public?
That is a good question. I think it can be useful. An example would be to create actors in different hierarchies based on the class.
You can't have `()` on val definitions right? I think we want to keep the `()` since this definitely has side effects.
ok, good point
yes, this should be public; exposing the class was the whole point of the refactoring
please do not use default values here: they are mainly useful for providing nice and shiny user APIs and make refactoring less reliable otherwise
ok, good point
yes, but not in ActorCell, where we eschew HOFs
rename exitValueOrKill -> exitValue
What is the reason for the removal of this check?
So the code that is removed (or rather the comments) are related to the removed `kill` method (which was commented out) in the `Conductor`. It fills no purpose since we right now can't `kill` a remote node from within the conductor but only tell that JVM to exit nicely.  Maybe it will be added in the future, but right now exit seems to be enough.  What happened was that if you sent a negative exit code it was ignored. Not what you would have expected.
"has used with great success in to" ?
"Actors also provides" ? Shouldn't it be "Actors also provide"
Overlooked this error.  It existed in the previous version.  Git marked that line as added because I had reformatted the paragraph.
Fixed in the commit https:github.com/derekmahar/akka/commit/0e0c55010af9476e9b2981e4b3c2030641b03f10.
Nice catch Roland
Doesn't the RemoteSystemDaemon have the remoteSettings?
No need to deserialize it here (lazily), not interesting what it was.
Nice catch, what if payload is "null" tho?
WDYM? The protocol identifier "akka" supposed to be a constant.
That shared-global-state AtomicInteger is not pretty
There is no need. The actor handles its lifecycle fully, there isn't any bookkeeping for the manager.
Looks better :)
By inlining these throws, you just reverted an earlier review comment :) I'm fine with your changes though.
Yeah, a man can change, right? :-)
I like this way better, so you are welcome ;)
NettyFutureBridge is sexy!
Ah, I wanted to do the same. Removal of Status was a very good idea, I think a lot ot refactorings will be possible now.
Yep, great work!
this duplicates the `dependencies = Seq(actor)` above, should not be necessary
Does that mean that akka-actor is not an OSGi-bundle anymore? You said something about including it in akka-osgi, but I cannot see that; there should be a comment somewhere explaining this.
yes, indeed: comment added
I just tried out your branch, and while the akka-osgi bundle contains all of akka-actor (save for reference.conf), it does not contain any reference.conf; the problem might well be that the compile task copies resources from src to target, so your dependency here is probably just too late.
well, I thought running ``sbt package`` before ``sbt osgi-bundle`` was necessary and would do this trick. I move it to the compile task if you prefer.   
I just did sbt publish-local under the assumption that that should work (because it is very close to what our release script does)
thanks, seams to work by adding the task to compile
string interpolation instead of `format`?
`bunle` => `bundle`
Is there a reason to only support `.conf` files and not `.properties` and `.json`? Perhaps use `ConfigFactory.parseFileAnySyntax` instead?
I'm not sure if `extendedActorSystemConfig` is a good name for this. Could it be confused with unrelated `ExtendedActorSystem`?
this would be nicer as      for {       x <- acceptedFileName       y <- acceptedFileExtension     } yield s"etc/$x.$y"
using `parseFileAnySyntax` makes the extensions dance unnecessary; if you give it only the base name, it will do exactly what you do manually here
I'd probably do this:  if ( x == 1 || unique(possible map (_._1), possible.map(_._2)(scala.collection.breakOut)) )  log.warning("Multiple serializers found for " + clazz + ", choosing first: " + possible)  possible(0)._2
don't like magic names, perhaps turn into a constant somewhere?
Then youd be doing it wrong ;-) But I got your point.
        val ser = bindings filter { _._1 isAssignableFrom clazz } match {           case Seq()  throw new NotSerializableException("No configured serialization-bindings for class [%s]" format clazz.getName)           case possible              if(possible.size == 1 || unique(possible map (_._1), possible.map(_._2)(scala.collection.breakOut))                log.warning("Multiple serializers found for " + clazz + ", choosing first: " + possible)             possible(0)._2         }
is this thread safe?
Is it possible to test that it _only_ has the expected output?
the OutputStreamAppender is synchronized (some locks), and ByteArrayOutputStream is synchronized so I think it is fine
if ByteArrayOutputStream [is synchronized] then you should be just fine
we should not test slf4j/logback - that has been done by others I'm only interested that the output includes correct values for the specific fields that we log  On Tue, Feb 7, 2012 at 4:36 PM, viktorklang < reply@reply.github.com > wrote:  > > +  } > > + > > +  "Slf4jEventHandler" must { > > + > > +    "log error with stackTrace" in { > > +      producer ! new RuntimeException("Simulated error") > > + > > +      awaitCond(outputString.contains("----"), 5 seconds) > > +      val s = outputString > > +      s must > include("akkaSource=[akka:Slf4jEventHandlerSpec/user/logProducer]") > > +      s must include("level=[ERROR]") > > +      s must > include("logger=[akka.event.slf4j.Slf4jEventHandlerSpec$LogProducer]") > > +      s must include regex > ("sourceThread=\\[ForkJoinPool-[1-9][0-9]*-worker-[1-9][0-9]*\\]") > > +      s must include("msg=[Simulated error]") > > +      s must include("java.lang.RuntimeException: Simulated error") > > +      s must include("at akka.event.slf4j.Slf4jEventHandlerSpec") > > Is it possible to test that it _only_ has the expected output? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/314/files#r423695 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
My point was just to verify that we're not printing things we shouldn't print.
But I guess that would only happen if someone changes the test layout. Which is rather unlikely, so nevermind. push!
I can add another regexp check for the whole line  /Patrik  7 feb 2012 kl. 16:50 skrev viktorklang<reply@reply.github.com>:  >> +  } >> + >> +  "Slf4jEventHandler" must { >> + >> +    "log error with stackTrace" in { >> +      producer ! new RuntimeException("Simulated error") >> + >> +      awaitCond(outputString.contains("----"), 5 seconds) >> +      val s = outputString >> +      s must include("akkaSource=[akka:Slf4jEventHandlerSpec/user/logProducer]") >> +      s must include("level=[ERROR]") >> +      s must include("logger=[akka.event.slf4j.Slf4jEventHandlerSpec$LogProducer]") >> +      s must include regex ("sourceThread=\\[ForkJoinPool-[1-9][0-9]*-worker-[1-9][0-9]*\\]") >> +      s must include("msg=[Simulated error]") >> +      s must include("java.lang.RuntimeException: Simulated error") >> +      s must include("at akka.event.slf4j.Slf4jEventHandlerSpec") >  > My point was just to verify that we're not printing things we shouldn't print. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/314/files#r423748
not possible to use self.type here?
to be clear, is the supervisor strategy used by the head actor to supervise the routees? "used for" might sound like it is the supervisor strategy that is used for supervising the head actor. Have I misunderstood the whole thing?
Yes, it's the strategy for the router itself.
So I couldn't get my head around using `this.type` when I implemented this. If there is a more elegant way to get the methods return type parameterized then I would like to know.
what you are asking for is the MyType pattern which is not supported in Scala, see scala mailing list archives (look for Martins posts)
Why is this lazy, it's going to get forced by the line below it?
Find the error
I don't think we can put the Typesafe Copyright on it if it's based on the code that someone else wrote. It needs to feature the original copyright header and it needs to have an ApacheV2 compliant license.
I don't understand, this will be executed all the time?
I think this is not needed, either take size in bytes in the method or the size in bits, in either case it's just  *8 or /8
This prevents using invalid values which will cause exceptions
This is called upon creation of the AES*CounterInetRNG classes only
No, I'm saying that it will _always_ throw the IllegalStateException?
There is no more safety in these than in a manual number, if you specify Seed128 when you really mean Seed192. I don't see the added value compared against the added cost. They aren't to be used by user code anyway.
why INSTANCE.generateSeed(Seed256.size) instead of engineGenerateSeed(SeedSize(256)) SeedSize could be:      object SeedSize {       def apply(bits: Int): Int = if (bits % 8 != 0) throw new IllegalArgumentException("bits need to be a multiple of 8") else bits / 8     }
I think we should call engineGenerateSeed(SeedSize(128)) to dogfood our own code, it also makes it easier to hoist INSTANCE in the future.
I think that the order should be `RandomDotOrgSeedGenerator`, `SecureRandomSeedGenerator` and then `DevRandomSeedGenerator`. The `SecureRandomSeedGenerator` is configurable from the outside, but the `DevRandomSeedGenerator` is hardcoded to `/dev/random`. This leaves more possibilities to configure your way around bad `/dev/random` implementations.
Thanks for clarifying. That is supposed to have the following comment from Daniel Dyers code:   This shouldn't happen as at least one the generators should be  able to generate a seed.  Will fix to make sure it fits  On 2012/07/02 04:21 PM, Viktor Klang () wrote: >> +    throw new IllegalStateException("All available seed generation strategies failed.") > No, I'm saying that it will _always_ throw the IllegalStateException? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/571/files#r1084499
So what you are saying is that I should remove DevRandomSeedGenerator because according to Paul Dyers SecureRandomSeedGenerator code:   * <p>This is the only seeding strategy that is guaranteed to work on all  * platforms and therefore is provided as a fall-back option should  * none of the other provided {@link SeedGenerator} implementations be  * useable.</p>  On 2012/07/03 08:04 AM, Bjrn Antonsson wrote: >> +    new RandomDotOrgSeedGenerator ::  first try the Internet seed generator >> +      new DevRandomSeedGenerator ::  try the local /dev/random >> +      new SecureRandomSeedGenerator ::  this is last because it always works >> +      Nil > I think that the order should be `RandomDotOrgSeedGenerator`, `SecureRandomSeedGenerator` and then `DevRandomSeedGenerator`. The `SecureRandomSeedGenerator` is configurable from the outside, but the `DevRandomSeedGenerator` is hardcoded to `/dev/random`. This leaves more possibilities to configure your way around bad `/dev/random` implementations. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/571/files#r1090104
Yes, if that's guaranteed to work, then remove the `DevRandomSeedGenerator`. 
Is this threadsafe?
Are these threadsafe?
    def generateSeed(length: Int): Array[Byte] = InternetSeedGenerator.GENERATORS.view.flatMap(        g => try Option(g.generateSeed(length)) catch { case _: SeedException => None }     ).headOption.getOrElse(throw new IllegalStateException("All available seed generation strategies failed."))
maybe mention which thing is changed, `m.copy(name = )`
Why not unconditionally do this? This method is for FJP only, right?
very good to test this.
because if the hardcoded "ForkJoinPool-" prefix is changed somehow I don't wan't to mess up we could change it in exactly the same way as we do for thread-pool-executor, using our own counter, yes, that is probably better
Why not just Address.toString? (would allow for more descriptive logging?)
I was afraid of non-supported characters, such as `/`
Good point. Leave it as is and put a FIXME to change it to URLEncode when http:www.assembla.com/spaces/akka/tickets/2123 is fixed
will do, thanks for pointing it out
Might be okay here, but I generally avoid optional booleans. They are only the right thing if you don't really care, if you get my drift 
That does not really do anything, or? Its enough to set it once.
this should be `system.systemActorOf(props, name)`
ah, is it, that's good, I thought since I received a message I must reset that timeout, don't know where I got that from
Should this really be ActorSystemImpl and not ExtendedActorSystem?
I'm skeptic to the PropertyMaster name, does it have to do with "Props" or Java Properties or what? (from the eyes of the users)
Do we need this as a method, isn't it just to override createPropertyMaster?
wow, a new level of hackiness ;-) This method should, if its existence is warranted, be placed inside findClassLoader.
getInstanceFor - says nothing about the semantics, can I call it twice and get the same instance back?
The reflection mention should perhaps be on the DefaultPropertyMaster?
If we remove JavaSerializer I don't want to have this comment here. Document what this field is good for. Can I use it to load any classes I need?
We might want to call this ReflectivePropertyMaster? So if the Spring guys want to add a SpringPropertyMaster it's up to them.
Why is the currentSystem on JavaSerializer?
I think you ought to remove it, seems like residue
Perhaps elaborate here
No explanation why
Don't like putting it here since it's easily overlooked that you need to set it when you deserialize _anything_ since you do not know the Serializer that will be used. Move back to Serialization imho 
Or remove if it is not needed.
Document, or make private[akka] or both
Classy, what does the effects have on Android?
what a beauty...
Its just to facilitate the override.
Yeah, I had lots of fun writing this :-) Will make it even more private.
might, might not,  Will add docs.
no, its on the object and might be useful
because nobody else needs it, only when doing Java serialization.
okay, consider it done.
will add a section in the docs which explains that when using Java deserialization this must be set (which is taken care of automatically when using JavaSerializer and hence only interesting for people who are writing their own serializers).
None. Optionally. ;-)
can't we have a better parameter name than i? like x :-)
why full class name here instead of import?
YES WE CAN!
me too, one suggestion is ReflectiveAccess, we don't really hide the fact that it is about loading classes from strings and creating instances from strings/classes.
yes, but isn't it create semantics, i.e. new instance each time? In that case it confusing to name it get.
didn't I read somewhere that it was possible to replace it, in that case it can't be private[akka]
I created it with Josh in mind, so ReflectivePropertyMaster is just our default, others could implement it without using reflection if they so chose.
as I said above, it does not _necessarily_ create, but our default impl does.
True, will make public again. On the other hand we have other places which require external implementors to put their code in the `akka` package, perhaps we should have a look and harmonize.
hmm, for me getX always returns the same thing, without side effects -- get is over used in java land and I hate when I see getters with side effects  createX is more loose for me, it normally creates a new instance, but can return a cached instance if that makes sense for the specific implementation
Isn't essentially all code relying on the create semantics of it?
I understand that, but what about DynamicAccess or DynamicClassMaster. Looking at the signatures it is all about fqcn strings.  PropertyMaster can be a manager that handles just about anything.
I like DynamicAccess. I chose PropertyMaster for want of a better idea.
DynamicAccess is a good one. go with that
@patriknw Okay, will change to createInstanceFor  @viktorklang I thought the same thing, but then realized that we use it for I need this kind of thing, and thats what the user told me about where to find it. If you reuse something, that needs to handle it of course, but that should be obvious, should it not? Am I on the wrong track?
I mean: loading a RemoteTransport could potentially return one which is not created on the spot, it just needs to deal with the fact that a certain ActorSystem will be calling into it. Or a MailboxType could well be reused, if it does not need any further configuration. Or do we really depend on different instances being returned? I couldnt really find an example (but if Im wrong then this requirement needs documentation, for sure).
Technically, shouldn't there be in the ReflectiveDynamicAccess object?
technically, it should be on the DynamicAccess trait, because otherwise its not overridable. good point.
    def serializerOf(serializerFQN: String): Either[Throwable, Serializer] =      system.dynamicAccess.createInstanceFor[Serializer](serializerFQN, Seq(classOf[ExtendedActorSystem] -> system)).fold(_      system.dynamicAccess.createInstanceFor[Serializer](serializerFQN, Seq()), Right(_))
this was what you mentioned on IRC I guess, I agree that it should be getClassFor here
How is this visible to other threads?
I'd probably looking to use a synchronized block
You could also attempt to be a badass by using Unsafe.monitorExit :-)
Is this to try to mask the fact that it's essentially: _routees = _routees?
Even if the JVM optimizes that away, it's still not allowed to elicit the fence, right?
impossible: cannot create non-nesting synchronized without writing the byte-code by hand. I would have preferred that, too, but thought that hand-written byte-code is not our style.
okay, got your point with using Unsafe, will change.
by virtue of the safe publication through the execution contexts queue; but its on its way out, now, anyway.
Should probably check that resizeProgress is true before commencing
Is this public API?
now that `Router.Resize` is private, the only source which sends this message has set it to true, and only the Router sets it to false, so either I put in an assert or nothing
so why not just do: _routees = _routees? (or use Unsafe to write the field? which cannot be elicited)
not any more
or you do getAndSet(false) in the finally and assert that it was true
Ill switch to Unsafe, which also serves as documentation in this case.
Yeah, Im feeling so BAD today.
come to think about it: I tried very hard to come up with a reason justifying this barrier, and I failed, so I removed it. Made the _routeeProvider `@volatile` for good measure. No non-volatile non-final fields, no cry.
okay, settled for that.
I think the purpose of the test is valid, but it is complicated to test this kind of thing.
what is the purpose of wrapping a AskTimeoutException in a AskTimeoutException?
What exactly (and I mean really the details) is the test meant to ensure? The current implementation of the test fails if any resize() call does not lead to an actual Resize event, which is definitely flawed. And hacking it so that the test works (by doing some blocking) misses the point, IMHO, because that is not what user code would do.
To get a meaningful stack trace (I tried to find which one of the Await.result()s was failing a test, and the original stack trace is of course from the timer task); all other exceptions should originate somewhere in user code, so there you have a hint whats going on. It would have been nice to clone() the original exception and inject it into the clone as the cause, but I have some dim memories of Viktors failed attempt to do so.
A comment detailing this would seal the deal
ok, it just looked strange, a comment would be great
damn, I thought this would be obvious ;-) (follow the types and stuff) [will fix, obviously]
It starts with 1 routee, and the purpose of the router ! (latch1, busy) messages are so simulate that the routee is busy (e.g. computing something or waiting or blocking on IO). This means that when new messages are sent all current routees are busy processing a message and a resize should occur to create a new routee that can handle the incoming message. That is a rather valid use case I think.  Where will message 2 be routed now? To the first routee, that is busy, and the resize happens afterwards? That would mean that routee1 is busy working and has another message in mailbox, while the newly created routee2 is idle. 
Yes, you have correctly diagnosed the problem. The thing is, I had to make the resize() go through the actor in order to ensure proper function in the face of terminating children, which would otherwise lead to conflicting updates of the childrenRefs from within and without the actor. So either we block ALL senders during a Resize event (this would be necessary to guarantee what you describe), or we document that resizing happens asynchronously and is not guaranteed to complete during any specific $bang invocation.   Needless to say that the first variant would be deadlock prone and potentially disastrous to remotely deployed (or looked up) routers, since remoting invokes $bang synchronously, assuming that it will never block.
also: if you want your scenario to work nevertheless, you could use a BalancingDispatcher. I think if we document this work-around, we should be fine.
Just wondering if the RoutedActorRef couldn't have a ReentrantLock just for the initialization. It really isn't that obvious to me that cell and actorContext is one and the same. And I really don't like fiddling with Unsafe, even if the code looks correct. The JVM hates unbalanced monitorenter/monitorexit calls.
Thats what I had, but if this is guaranteed to work correctly, then Id prefer that over the slightly dubious set field before constructor runs deal which would be necessary when introducing another object in the mix.  Wrt. `cell` vs. `actorContext`: if someone breaks it, the RouterSpec will fail (I verified this).
ok, I think we should go for the "simple" asynchronous approach as you suggest and add some note about it wipe the test, or rewrite it to the new semantics, if possible
I find this confusing, if identifyRequest is Some, you're both going to reply AND DeadLetter???
Not very DRY
true, I agree, it should not be done for identifyReqest what I tried to add was that ordinary messages sent with actorSelection was not published to eventStream
ok, perhaps better with varargs
I definitely think we should avoid doing this often. Just keep an internal map of these.
Isn't that last one going to be very expensive?
Change to:  val hasUnavailableMemberStatus = members exists { m  m.status.isUnavailable && m.address == address }
Could also be expressed as      members.find(_.address == address).orElse(overview.unreachable.find(_.address == address)).getOrElse(Member(address, Removed))
Den e najs
you think about that the values are VectorClock and that is expensive? or is it the size operator you think about?  I have an idea that the seen table doesn't have to contain the full VectorClock values, but only a hash of them (less to transfer, faster to compare for equality, and that is the only thing that the seen table is used for (I think))
No, I mean building the set and then doing the size. I think it's probably faster to just bail out early by:  val seenHead = seen.values.headOption val consistency = seenHead.isDefined && seen.values.forall(_ == seenHead.get)  So the logic is that if something seen, then all that is seen has to be the same, this fails fast since it doesn't even begin traversal if seen is empty, and bails out as soon as it knows that there is no full consistency.
nice, and with and additional check on seen.nonEmpty first there is no need for the Option
Just cache the seen.values.head first so you don't do it for each member in the Set since that can be expensive.
    def seenSame: Boolean =       if (seen.isEmpty) false       else {         val seenHead = seen.head         seen.values.forall(_ == seenHead)       }      !hasUnreachable && allMembersInSeen && seenSame
and for the record, it is:      val (_, seenHead) = seen.head
You can't do values.head?
ok, makes sense          val values = seen.values         val seenHead = values.head         values.forall(_ == seenHead)
badass so worstcase O(N+1) bestcase O(1) ;-)
You might want to consider putting the most frequent message tpes at the top
cluster.seedNodes.collect { case a if a != cluster.selfAddress => self.path.toStringWithAddress(a) }
Why? isn't this exactly equivalent as just having yourself as the sender of the message?
Map(address -> (Deadline.now + JoinTimeout))
I'm not sure I understand, ScatterGatherFirstCompletedRouter only supports ask
good point, I'll shut up now
didn't we say that .empty is badass? :-)  that one was funny, thx
No need to store away peers into a local field.  
No need for braces
don't know, not used, removed now
that was an unusual syntax for collect
not immutable everywhere?
Not needed here as we're immediately transforming the Iterable to an immutable.IndexedSeq
and if someone changes it while we are doing that they have themselves to blame, I agree that it much cleaner for the array case; +1
no, just an observation, I guess it is because of the break out param
Then they could just as well have modified it concurrently while they were generating their immutable.Iterable.
Why doesn't this delegate to the toCamelUri that takes the ConsumerConfig and passes in "ActorEndpointPath.consumerConfig". Also, why doesn't the docs mention that ActorEndpointPath.consumerConfig is used? (and why are they used?)
Camel with big C. Capital letter after dot.
Why is this version needed? (Also, it doesn't escape the given `actorPath` potentially creating invalid URIs.
Why is this not a case object?
I don't think this needs to be mentioned. Just clairfy "allows actor to" to "allows Untyped Actors to"
"This section gives a brief overview of the general ideas behind the akka-camel module, the  	 31	 +remaining sections go into the details. " <-- pretty superfluous, don't you think?
Please omit the port number as if someone changes the example, the docs is silently borken.
What is this doing here?
Should probably use FQCNs here for Ack and Failure
Might want to link to Ask here
    `UntypedProducerActor`
should not or must not or can not?
    `Message.MessageExchangeId`
Weird line breaks in this paragraph
You can drop "Since Akka 0.10, " that's ancient history :-)
Does this imply that the last step in the chain is the sender of the response?
Are all these callback-methodnames verified to compile and be current btw?
What does this sentence imply? I.e. what is it intended to convey to the reader?
weird line break.
Isn't it sort of weird not to camel case this method?
So it even works with remote actors?
weird line break
What happens if a to-clause points to an Actor that doesn't exist?
Who owns the copyright of that image?
I think the imports should be visible in the code segments so the user knows what's what.
If you do a static import of TimeUnit.SECONDS then you can make this a bit more concise
Very nice and clean!
I really think this should be renamed to autoAck
Why not cache the toCamelUri instead of the ActorRef?
Hmmm, aren't you required to implement onReceive?
missing newline before
    private final String uri;
    this.uri = "mina:tcp:localhost:6200?textline=true";
missing space before brace
missing space before brace
good one :-)
Yeah I'm not happy with this consumer config in general. It just doesn't feel right. Should delegate of course, hacked together too quickly. It is used for default settings. But it is not correct. the consumerconfig is now also used when you have a producer (which is weird), which doesnt even need it. Would not like to expose this to user. will find a better solution.
I'll just fix this within the wip for docs here, and also do the reference.conf config FIXMEs.
I've added a commit with the fixes, please check that first before I do the doc fixes. (probably tomorrow morning)
done. (commenting to keep track)
changed to cannot. (made onReceive final)
done.  Producers send back to the original sender (if you don't change onRouteResponse), with Consumers the ActorComponent does a tell or ask  with the Consumer Actor and then talks directly back to the exchange.
Changed to: "This option gives you the ability to change routes that have already been added to Camel.   Consumer actors have a hook into the route definition process which can be used to change the route."
yes, it's just an actor path, cool right? (changed it to akka:... path as discussed before)
you get an exception (ActorNotRegisteredException)
I think Martin Krasser made it. not sure, but it was originally in the docs. 
done. (still learning reStructuredText ;-)
no not with a producer, the producer has an implementation of onReceive itself (and its final). if you want to change you behavior you have to use the onTransformOutgoingMessage, onTransformResponse, onRouteResponse methods
does this mean that `super.get` returns the right type only if the extension is implemented in Java?
As far as I can tell it works fine without overriding get when it is written in Java. We have samples for that.  /Patrik  25 jun 2013 kl. 10:35 skrev Roland Kuhn <notifications@github.com>:  In akka-actor/src/main/scala/akka/actor/Extension.scala:  > + * > + * Java API: > + * > + * {{{ > + * public class MyExt extends AbstractExtensionId<MyExtImpl> > + *   implements ExtensionIdProvider { > + *   public final static MyExt MyExtProvider = new MyExt(); > + * > + *   private MyExt() {} > + * > + *   public MyExt lookup() { > + *     return MyExt.MyExtProvider; > + *   } > + * > + *   public MyExtImpl createExtension(ExtendedActorSystem system) { > + *     return new MyExtImpl();  does this mean that super.get returns the right type only if the extension is implemented in Java?   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1563/files#r4861390> .
this comment sounds like it should be on a deprecated class; a quick survey has turned up a small number of mailing list posts and stackoverflow answers which use it, Im on the fence on this one. 
It is just fine if you do an ext for your own proj, or only use scala, or only use java. We might find a solution. I think it is to early to deprecate
Yea, all this is extremely weird.
I think youre right, Im climbing down from the fence to where you are, then well see how the grass looks like on the other side ;-)
I think the need of this kind of documentation is an indication that something is wrong. It would be more uniform if the ActorRef always could be thought of as a path that is resolved for each message sent to it (in practice this process will be optimized using caches).
Yes, I totally agree with you, as we discussed yesterday. I think we should change the behavior. Created ticket 3060.
ok, that will not happen for 2.1 series I would guess, so this PR is still valid and LGTM
Doesn't seem right to me. is the contract in the Serialization API that failure always yields a NotSerializableException no matter what serialization algo used? Also, IMO this shouldn't be special cased here.
This message is completely void of information. Message from who, to who, and what kind of message?
Unfortunately I cannot see the information from here. But the message   should be delivered to deadLetters.
where the message is serialized or deserialized, this must be handled
 > Doesn't seem right to me. is the contract in the Serialization API that   > failure always yields a NotSerializableException no matter what   > serialization algo used?  Then what should I look for?  > Also, IMO this shouldn't be special cased here.  I thought that's that the point of the ticket. Do not close the channel,   just log the error and go on.
Anything NonFatal thrown when serializing or deserializing a message.  Serialization problems shouldn't be handled here, it should be handled where it occurs, i.e. where we have the most information.
It's kind of tricky, as this stuff runs inside the remote pipeline. I'll   try to update the PR.
Just to reflect on my code: payload is a lazy val, so will it throw exceptions? Should it be a def instead? It is used in at most one codepath.
Can this line throw an exception?
Perhaps payload should be the Try returned from the Serializer?
Yes, that would do the trick.
No, I don't thinks so. I think all try blocks were removed around tells in   master, but not here. I can remove them though.
The convention is to use `[{}]`
is the toString representation of DaemonMsg small enough to log?
Yes, it does the right thing (logs message class, sender and receiver   paths)
please change that in the whole file
could it be useful to include `e.getMessage` in the log output?
What do you mean?
What kind of exceptions might be possible here? Are all of critical nature, nothing that should not happen frequent in production?
I checked RemoteMessage, and that is fine, but I asked about DaemonMsg
Ah, Ok. I will check then :)
but you are right that this catch is removed in master, and placed inside `RemoteDaemon` `def !` I don't think it can be too bad to `toString` `DaemonMsg` even though it contains `Props` and stuff. Leave it as is, or move the try to `RemoteDaemon` to make it the same as in master.
Normally, nothing should be thrown during encoding, or else you have designed your application to fail to send some of your messages. I think that is an error.
yes, I agree, I just wanted to be sure
Will we start logging the payload of large messages when they hit dead letters?
deadLetters logging uses the toString representation, so if it prints the   whole message, then unfortunately it will log the content of very large   messages.
That might become a problem, but I think that this is better than before.  LGTM
DeadLetters should only output from, to and type IMO.
Should I open a ticket for that?
Yes, please do. I think that's how DeadLetters should work.
yes, such things are scary, can bring down a system in production
it feels a bit strange to specify the handled messages outside of the actor, normally an actor logs unhandled messages itself
fill in the parameters with string interpolation
I think giving a more specific log message than "unhandled" does not hurt here. 
LOL! Thanks :)
I still think the `e.getMessage` should be in the error message      log.error(e, "Remote message [{}] could not be delivered from [{}] to [{}], due to [{}]",              message, sndr.getOrElse(system.deadLetters), recipient, e.getMessage)
But the exception is included in the log statement. Isn't that enough?
that will only be the stack trace, not the actual message of the exception (AFAIK)
No, I just double-checked, the message is in there: java.lang.IllegalArgumentException: Message was too large, maximum is [1048576] bytes, but message was [1048813] bytes long.
great, my bad   On Wed, Feb 20, 2013 at 10:51 AM, drewhk <notifications@github.com> wrote:  > In akka-remote/src/main/scala/akka/remote/netty/NettyRemoteSupport.scala: > > > -              recipient, > > -              message, > > -              sender.asInstanceOf[Option[ActorRef]]).build)) > > +        val sndr = sender.asInstanceOf[Option[ActorRef]] > > +        try { > > +          super.encode(ctx, channel, > > +            remoteSupport.createMessageSendEnvelope( > > +              remoteSupport.createRemoteMessageProtocolBuilder( > > +                recipient, > > +                message, > > +                sndr).build)) > > +        } catch { > > +          case NonFatal(e) => > > +            import remoteSupport._ > > +            log.error(e, "Remote message [{}] could not be delivered from [{}] to [{}]", > > +              message, sndr.getOrElse(system.deadLetters), recipient) > > No, I just double-checked, the message is in there: > java.lang.IllegalArgumentException: Message was too large, maximum is > [1048576] bytes, but message was [1048813] bytes long. > > -- > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1143/files#r3077559>. > >
Thanks for the doc changes!
np, but I forgot the rst doc, I'll change there as well
Yay, nice :)
While it fixes the current issue, I dont think it is fully correct: this introduces a second Dequeue token flying around, which will then consume messages too fast. I think the delayed Dequeue sending should be done with an FSM named timer, which is then canceled right here before doing the direct thing (in case of PassThrough only; if you want to be really correct you could also cancel and reschedule for throttling mode to get immediate effect of changed bandwidth).
Yes, you are right. The original timer should be canceled, and the new   timing recalculated.
[sixties advertisement voice]  and only FSM timers are non-racy! FSM timers: when you need non-racy cancellation.
sounds good, I'll take a stab at it
hmm, why not use millisecond resolution? seems a bit strange to me that 999ms should be rounded down
yes, I also thought about that rounding, but in the end I trusted the existing code, do we need rounding at all? double.seconds?
how about doing `self ! Dequeue` in case of zero delay?
I would say < 0.01 or so is better than zero.
Shouldn't this be in a companion object?
You can get rid of the rounding, it would improve the code.
@drewhk what is the reason for using 1.nanos here? I would like to change it to Duration.Zero
How will timers behave if they are given a zero delay?
that should be fine, but I would like to change to    def scheduleDequeue(delay: FiniteDuration): Unit = inboundThrottleMode match {     case Blackhole                     Do nothing     case _ if delay <= Duration.Zero  self ! Dequeue     case _                            setTimer(DequeueTimerName, Dequeue, delay, repeat = false)   }  1.nanos feels like a magic number that we don't really need
Good idea @patriknw 
nanoTime may return negative values, is this accounted for in timeToAvailable?
it's only used for producing a duration, i.e. diff between two nanoTime (btw that is the only thing nanoTime can be used for)
perhaps it is the name of "queue" that is a bit non-descript, what is it queueing?
The signature for timeToAvailable:      def timeToAvailable(currentTime: Long, tokens: Int): Long   Doesn't say in which format the currentTime is, is it currentTimeMillis (similar name) or "currentNanoTime" which would hint at nanoTime
I agree, I will change
regarding the `queue` it is holding the throttled (delayed, not sent yet) messages
Can we rename it so that the name has something to do with the contents?
why is this not part of NettySSLSupport.apply?
named parameter would make it more readable `isClient = false`
I was thinking about that, too. I cannot do that however, until I remove the old remoting, as it handles this by hand.
in that case please add a ticket
Let's agree first on if we want to use the build-in netty stuff (setIssueHandshake), or go with the manual way (calling handshake and waiting on completion). I think the former is cleaner if it works (which I think it will).
If setIssueHandshake works, then we should use it, since that's what it's for.
Sorry, but I changed the tag to `multi-node-testing` to be consistent with `multi-jvm-testing`  Do we have some standard for this?
A little caution here. The `SbtMultiJvm` plugin is hardwired for ScalaTest :(  Has been from the very beginning. 
ok, I simply removed that sentence about scalatest
great, I was missing that compared to old make script
Isn't this `.../akka-cluster-experimental...` ?
yes, good catch (the old URL still works, but does not have the last two builds)
Just add a comment on the purpose of this dense section
Add comment about that this is the place to add new replacements.
Wow, isn't that a bit confusing?
This is a bit unorthodox, why do we send ourself as a message, with ourself as the sender? I vote a more descriptive message.
I don't know. We use Echo at a lot of places. I will remove display of it in this section of the docs. It doesn't add any value to show the actor impl here.
same answer as for the scala Echo
Maybe mention that phi is using logarithmic scale (like dB)? I know it is in the article, but I expect that most people will not read that :)
I think `wrong suspicions` sounds funky, `false positives` is better.
since the previous
a standard deviation
I have added a summary
Does this imply that application messages can't be 0 bytes? Is that an issue?
That's not an application message tho? It's the inbound payload (Akka protocol), right?
ok, I see
No, it's even lower layer. This is the payload of UDP packets, or length   encoded TCP frames. No upper layer protocol message can be contained in a   0 byte frame, so this is safe.
I suggest adding the following to NettyFutureBridge:      def apply(nettyFuture: ChannelGroupFuture): Future[ChannelGroup] = {         val p = Promise[ChannelGroup]()         nettyFuture.addListener(new ChannelFutureListener {           def operationComplete(future: ChannelFuture): Unit = p complete Try(             if (future.isSuccess) future.getGroup             else if (future.isCancelled) throw new CancellationException             else throw future.getCause)         })         p.future       }   And then you can write shutdown as something like:      override def shutdown(): Unit = {       import appropriate ExecutionContext here       def always(c: ChannelGroupFuture) = NettyFutureBridge(c) recover { case _ => c.getGroup }       for {        _ <- always(channelGroup.write(ChannelBuffers.buffer(0)))        _ <- always({ channelGroup.unbind(); channelGroup.disconnect() })        _ <- always(channelGroup.close())        } inboundBootstrap.releaseExternalResources()     } 
This section will be extended later?
I suspect "i.e." (*id est*, "that is") would be better than "e.g." (*exempli gratia*, "for example") here. :)
what's the purpose of this?
should probably be `import-global-implicit`
that's a nice trick
no replacement of scalaVersion?
like a bouce
see sbt-site-plugin tickets for substitution
nope, it's all in the code.
How? I mean the question stated is "Should I use Dataflow or for-comprehensions?", and I don't see that answered in the code. 
There is no conclusive answer, as described in the "Conclusions" section below. The question is there for search engine hits.
So when someone searches for it, someone actually tackles that question.
Ok, I just found the treatment a bit "terse". I think it's ok for now.
Need feedback on this line. I only report the first failure in the future group, is this enough?
Yeah, I think so
Nice. This explanation about dead letters was really needed.
and postRestart by default calls preStart.
Very good explanation, but I feel the wording is backwards. I would rephrase it like this.  ``` Every time an actor does not terminate by its own decision, there is a chance that some messages which it sends to itself are lost. There is one which happens quite easily in complex shutdown scenarios that is usually benign: ```
is `for` correct? to references `of` its parent,
yes, very good
good, should we also clarify, if not already done, that `watch` of remote actor will not generate `Terminated` if remote jvm crash or network drops?
yes, good point
shouldn't we add another hook, before shutdown, in AkkaSpec, for this purpose?
why not postfixOps in tests?
I dont much care one way or the other, and I started fixing by adding dots (that was matching todays mood); if youd like to use postfixOps in tests thats also fine with me.
maybe, this is the second occurrence; will think abou it.
ok, that's fine, I just wanted to know if it was a general rule from the tech lead ;-)
Shouldn't we include the port as an Int as well?
Int? Where? Why?
Well, not an Int, but currently the only way to see the failing port from   the code is parsing the message. Although it might be not as important at   all.
or grab it from the ActorSystem.settings, I think the reason for the ticket is only for better log messages
I think Roland asked me in the new IO to report back the address when bind   failed. Although the error reporting there is not via exceptions...
ok, from what I can see in the ticket this should be enough for the old IO
What happens if this throws an exception?
good point, I'll wrap it in a try-finally
Make it a reST hyperlink
Link to the milestone itself perhaps?
I feel that there is a word missing between "testing" and "is", like "library"/"kit" or otherwise.
Should clarify what kind of properties we're talking about here.
We need to explain what we mean by "Multi Node testing"
I also tried to figure out to create an url for a specific milestone, but I couldn't find it. Perhaps by creating a shared filter. I don't think it is that important.
It's rather easy to link to the milestone, but you have to click to see the tickets.  https:www.assembla.com/spaces/akka/milestones/418132-coltrane
kit it is
yes, this is important perhaps also add ", without any special configuration. Then you can run the same tests distributed on multiple machines." 
try if this works    node(node2) / "user" / "ponger"
good, I will adjust the cluster sample to use the same (and adjust the tests to not depend on STMultiNodeSpec)
Works like a charm. Reads much nicer.
Yes of course.  Whatever happened to `|blaha|` substitution in these blocks? The Akka version should be a replacement string. 
It would be nice to have a few words, or even better, an example when this tool is useful.
It was "moved" to sbt site plugin. I still think it is rather urgent, or are we going to hardcode the artifactId for pom.xml instructions (or don't we have any of those? https:www.assembla.com/spaces/akka/tickets/2461 https:www.assembla.com/spaces/akka/tickets/2431  Here you don't need it, and not the parsed-literal either. 2.1-SNAPSHOT is replaced by the release script, so that is another thing.
Im working on this (writing a preprocessor in our SBT build, seemed the easiest of a lot of crappy ways).
Yes, but I want the `cross CrossVersion.full` to go away when we hit Scala final. So the preprocessor would be nice.  Is it going into the `site-plugin`? 
Great catch. Found a couple more of them. Switching between MD and ReST makes me dizzy.
that was a new `"` syntax for me
Yes, you're right. It actually is kind of weird looking. Should I change it?
I think you should change, if there is no good reason for using the quotes (we don't need to learn them jvm internals here).
You should ask Dr.K. ;) I just didn't look close enough. No need for quotes, I'll change.
ah, I thought it was your old self...
Just continuing the tradition
Can we come up with a more efficient solution?
Two loops is more efficient, but uglier as well or we can keep the watchedBy collection sorted or we can have to watchedBy collections. or any more ideas you have
Since the above solution is O(2*N) anyway, there's no need for creating an intermediate collection.      def sendTerminated(ifLocal: Boolean)(watcher: ActorRef): Unit =        watcher match {         case a: ActorRefScope if a.isLocal == ifLocal =>           try watcher.tell(terminated, self) catch {             case NonFatal(t)  publish(Error(t, self.path.toString, clazz(actor), "deathwatch"))           }         case _ =>  N/A       }      watchedBy foreach sendTerminated(ifLocal = false)     watchedBy foreach sendTerminated(ifLocal = true)
perhaps add a      def isLocal: Boolean = host.isEmpty
That's the two loops approach, but much prettier than I thought. I like it.
Do we need this comment anymore if we use ActorRefScope wich Viktor proposed?
Is it safe to ignore ActorRefs without ActorRefScope?
why do we have the try-catch around tell?
@drewhk Yes, it should be: private[akka] abstract class InternalActorRef extends ActorRef with ScalaActorRef { this: ActorRefScope   However, you can add ERROR logging there to make sure.
My intuition would be that not all ActorRef's are actual actors, so we might get exceptions when invoking tell. But I am not a dungeon-dweller :)
I think the try-catch is an artifact from the past where tell was allowed to throw exceptions. However, if it _does_ throw an exception, if we don't catch and continue, the system is broken.
I think that is good do we have any ActorRef without ActorRefScope? should it be silently ignored?
or just remove the case, if it should never happen
How does logging work from here? I am inside DeathWatch (and ActorCell via self type)
Did you see my example code above?
but we rely on that tell should not throw exception in other places, to avoid sprinkling this all over the place
I saw publish, but I was not sure if it applies to my case.
That's true, let's remove and pray :-)
Why wouldn't it?      protected final def publish(e: LogEvent): Unit = try system.eventStream.publish(e) catch { case NonFatal(_)  }
I would let it blow up instead (remove the `case _`)
The irony is that it _will_ blow up if the default case is removed, since the local-check is done in the guard. So if you remove the default, you haveto make the guard into an if-expression in the closure body instead.
I noticed that already. I just added another case statement without the guard but with matching on ActorRefScope. Of course I can move the guard inside the case statement as well. Which one do you prefer?
perhaps just scrap the match all together and just do:      if (watcher.asInstanceOf[ActorRefScope].isLocal == ifLocal)       watcher.tell(terminated, self)   
Could  sendSystemMessage throw an exception? Because there is a similar catch construct just one method down.
Not likely, but lets open a ticket about fixing all such occurences separately.
I am at least half-certain that Addresses may also be interrogated for local-ness, and there ActorRefScope does not help; Id like to keep this comment; and we should also add an `isLocal` method which encodes this convention.
But if the Address is the same as our listen address then isLocal should return true. 
no, Address has nothing to do with the remoting implementation: the address either specifies host/port, in which case it is non-local, or it does not. Address.isLocal only means that this Address cannot be migrated to other systems or nodes without additional information.
Maybe we should use a different name for that function. isParial? migratable? WDYT?
no, its really about the scope of the address, not unlike IPv6 addresses (host-only, link-local, global); it is basically the realm within which this address is supposed to be unique; hence I think there should be both      def hasLocalScope = host.isEmpty     def hasGlobalScope = host.isDefined
Perhaps a small comment indicating the choice of this seemingly magic value?
Is this still a sensible figure with the Thread.sleep inside?
yes, it normally completes this part in 2.5 seconds
um, it's derived from empirical studies :-)
added the comment
why Buffer and not `var` of `Vector` (or something)?
I thought it would be nicer to have a mutable data structure with the `+=` method since that's also what the `ComposablePartialFunction` exposes. But either works, of course. Do you have a preference?
the akka convention is to prefer `var` with immutable over `val` with mutable, exceptions may exist (e.g. performance reasons). You can still use `+=` For vector it's `:+=`
Isn't the name confusing here? PartialFunctions are composable by default. This is more like a MutablePartialFunction.  What about making it a builder, and then you can call become() after you built an immutable final behavior? What do others think?
Good idea. I made it a builder, and added another trait to show class+trait composition of the actor. I used a simple `receive` instead of `become`, hopefully that's just as good.
I'm making this a one-shot builder to alert users to ordering mistakes, e.g. adding a PF after it has already been built.
to be strict this should be done in a finally
Yes, of course. There is no reason to get sloppy. Will fix.
enough with `ActorSystem`?
note that when you fw port this, the property is `akka.loggers`
`loggers += StandardOutLogger`
this is a bit confusing, the log level doesn't matter if you use `NoLogger` there is a minor performance difference, but for this description I think you should just skip loglevel
I want to use the `settings` that's part of the `ActorSystemImpl`, and `startDefaultLoggers` takes the impl.
`settings` is part of `ActorSystem` api
It's the `stdout-loglevel` initial logger before the actor system has started up, and if you set the log level to `DEBUG` you will still get messages during startup.
I can of course skip the setting since the normal configuration is `WARNING`, but I think that it could be worth mentioning.
Yes, you're right. No need then.
yes, you are of course right, but reading the ticket title again "describe how to shut off *all* stdout logging". That means that we don't have a way to shut it off completely. We should have one log level for that. Not that I like j.u.l. but they have it:      /**      * OFF is a special level that can be used to turn off logging.      * This level is initialized to <CODE>Integer.MAX_VALUE</CODE>.      */     public static final Level OFF = new Level("OFF",Integer.MAX_VALUE, defaultBundle);
Yes, I think that I'll add that level, and just remove the `NoLogger` completely. It's of no use if you can set the level to `OFF`.
yes, sorry for not suggesting it earlier
sorry for being picky about this detail, but doesn't this look better (if construction fails you will not get an instance back anyway):      val system = ActorSystem(name, config)     try {       system.log.error("Danger! Danger!")     } finally {       system.shutdown()     }
Yes, you are right. Don't know why I put the try around the system creation.
Why the inconsistency about using quotes for the values? At the top of the PR no quotes are used.
No good reason for not being consistent. At least all the doc should be consistent.
Not sure about the wording. Just a suggestion:  *Concurrency* means that two or more tasks are making progress even though they might not be executing at exactly the same time. This can for example be realized with time slicing where parts of tasks are executed sequentially and mixed with parts of other tasks. *Parallelism* on the other hand arises...
...similar behavior as blocking.
Shouldn't it be "asynchronous by nature:"?
s/at the same instant/simultaneously
I'd drop " A parallel system is always concurrent, but not necessarily vice versa." it doesn't convey the truth, the whole truth and nothing but the truth anyway. Parallelism only ever exists at the hardware level. Cmp: instruction-level parallelism (multiple FPUs, ALUs etc), multicore, fused-multichips, multisocket, multi-machine etc.
s/has been finished/returns a value or throws an exception.
" after a finite number of steps" ?
This is not true. Bounded blocking is still blocking.
there's a "to" missing in between "on" and "the"
I think we generally use "blocking" to mean "park the Thread until X"
This had the following definitions: http:docs.oracle.com/cd/E19120-01/open.solaris/816-5137/mtintro-75924/index.html  Parallelism 	 A condition that arises when at least two threads are executing   simultaneously.  Concurrency 	 A condition that exists when at least two threads are making progress. A   more generalized form of parallelism that can include time-slicing as a   form of virtual parallelism.  For me it seems like Concurrency is a subset of Parallelism by those   definitions, as if two threads are executing simultaneously, then it   follows that two threads are making progress in an overlapping time period.  I know that there are many different definitions, so I am happy with   whatever we take.
Yes, the "gateway" section of an asych call should be wait-free in my   interpretation.
Definition comes from "The Art of Multiprocessor Programming Ed.1."
For me:  Parallelism = Simultaneousness Concurrency = Mutual progress
You can have concurrency without parallelism and parallelism without concurrency (only one thread of execution that only ever runs on one execution unit, i.e. when the thread of execution is not _parallelizable_, be it at instruction level or otherwise)
We should use our definitions. Imagine a bounded blocking operation of 2 billion years.
How is that any different? Simultaneous *things* progress mutually, too.  But part of my goal with taking this ticket was to reach a consensus, as   there are many books, definitions not completely compatible. So I consider   this as a team exercise and I am not promoting any particular definition :)
It's different in the sense that if all other cores are effectively only executing NOPs, there is no _meaningful_ work being done.  Heh, I'm just pushing my own view on things here ;-)
Go ahead :) Others can chime in as well ;)
This should also be added: http:en.wikipedia.org/wiki/Non-blocking_algorithm#Obstruction-freedom
it doesn't have to be signal of completion to the caller
`Please note that some of the terms are defined slightly differently in various contexts. Our goal is not to unify these but to provide definitions of terms that will be consistently used in the scope of the Akka documentation.`  `Please note that, for many of these terms, there is no single agreed definition. We simply seek to give working definitions that can be used within the Akka documentation.`
`asynchronous`  `*asynchronous*`
I don't really get this bit. Do you mean that asynchronous calls can be made synchronous by using blocking?
"as it is necessary that a participant thread be able to delay the progression of other threads indefinitely"  "as each thread is delaying the others"?
Have we talked about what a 'thread' means? :) It's complicated in Akka because, e.g. we could could have concurrently executing actors in a single OS thread. Also, in the case of a single OS thread, can we be said to ever use "blocking" by this definition?
I think this definition is more about the ability to block others more than blocking yourself. And if you have only one OS thread than the only way to have blocking is by interacting with a hardware component that signals an interrupt and the thread can proceed. 
"can be used" is somewhat weak here. We do intend to use these in our docs (eventually ;))
I am a bit confused about this question :) Can you clarify?
No, that was not the point. The blocking definition I used requires the ability to delay another threads indefinitely. If this condition is not present, one thread will make progress, and there is no deadlock -- although livelock is completely possible.
Hmmm, I think we usually use "blocking" in a different way, to mean "holding onto a thread unnecessarily while waiting for something else to happen". By the way, I'm not suggesting this is the correct definition, just that the one you're providing is significantly different from our usual informal usage.
I actually don't think so. What you describe is more like being   synchronous for me -- which is similar, but here blocking means that you   can potentially harm the progression of *other* threads. We are, however   free to introduce another name for this definition, if we stick to it.   Probably we can differentiate between a blocking object, and a thread   being blocked (by itself or others).
[Moving [discussion](https:github.com/akka/akka/pull/1152#discussion_r3098722) to new diff]  I was a bit confused by what the first two sentence meant. Maybe it was just the phrasing. Is the following kind of what you mean? If not, maybe you could try writing those sentences a different way so I can understand them. :)  "A method is synchronous if it blocks internally. A method that performs a CPU intensive task internally is also synchronous. It is considered synchronous because, despite not blocking internally, it still prevents the caller from making progress while it executes."
Do we consider it to be blocking if we have independent workers operating on a thread pool, and each worker blocks, thereby using up a thread? I think "using up a thread unnecessarily" is still an important concept that we want a term for (because we want to warn users not to do it), even if each of those threads is not interfering with other threads.
It would be blocking by the definition used here, because the workers that   acquired all of the threads block all other workers sharing the same pool    from making progress for an indefinite amount of time. The way the   definition is constructed (not by me, but various others) is so that it   fits nicely in the framework of progress conditions (non-blocking   properties) below. So I definitely want to keep it. On the other hand I   never care about names, so we can rename it.
Okay, heres my tune: Concurrency means that related processes make mutual progress (which differs ever so slightly from Viktors definition), whereas Parallelism means that processes make progress at the same time (be they related or not). This means that the two are completely orthogonal and all four combinations exist and are meaningful.  From this follows that Parallelism is related to how processes are executed while Concurrency is an inherent property of how processes are related to each other; if two processes are concurrent by nature, proper synchronization will have to be employed to ensure correctness for all possible modes of execution. Properly written actors are not concurrent by this definition.  The only relation between Concurrency and Parallelism which I see is that processes can be related such that a minimum level of parallelism is required for correctness (or else deadlock). Which brings me to the conclusion that Parallelism is not necessarily instantaneous, it is about overlapping time intervals and can also be achieved by time-slicing (which is the still the dominant implementation on todays hardwarewe typically use many more threads than we have execution units).
 returns (with or without a resulting value).
yes, I agree that this is useful, but you should spell that out explicitly
We need different terms for these two concepts: blocking means parking a thread a.k.a. wasting it, and for this it does not matter whether it is bounded or not (we should qualify it as such where needed). Not making progress for indefinite amounts of time can still be desired, e.g. for an actor which tries to break a Rijndael cipher (which is indefinite for practical purposes). Unfortunately blocking is already taken, what are the alternatives? We need it in order to describe starvation scenarios, which can result in a deadlock. hogging? squatting? The word should have just the right amount of negative connotation to it ;-)
yes, non-blocking is about not wasting the thread; what we need to describe progress of other entities is something like non-hogging or so, which means that processing time is bounded; maybe we can borrow terminology from soft realtime systems?
I think it would be clearer if you added (e.g. by wrapping an asynchronous API and applying blocking to await its result).
So I do not subscribe to this definition of Concurrency. Concurrent processes can be completely unrelated since concurrency is a property of a system where one or more tasks can make progress at _the same time_. Parallelism is about the possibility of simultaneous execution, as in algorithms and hardware.   I think that we shouldn't stray to far from wikipedia http:en.wikipedia.org/wiki/Concurrency_(computer_science) and http:en.wikipedia.org/wiki/Parallel_computing since that will only confuse people.
The important aspect is that the participants in the deadlock do not make externally visible progress for an indefinite time span precisely because their progress is mutually dependent. This is where the summary concept of blocking and hogging is useful (though most cases will use blocking to achieve the effect).
Not just wikipedia, other sources give similar definitions.
My intention with this definition is to clearly demarcate where synchronization bugs must be considered and where not. Since we are doing things in ways which are not (yet) the commonly established conventional wisdom I think that we have some leeway here.  What I dont like about the definition you cite is that it mentions time, which to my mind does not have a place in defining concurrency. You can express concurrent processes with synchronization bugs using -calculus, and that calculus does not know about the concept of time.
You don't have to say anything about _time_ if that is an issue, that can be rewritten, but I still think that your definitions are going off in a new direction, and that will confuse the intended audience.  Doesn't -calculus have the notion of concurrency as one of its cornerstones? How is it defined there?
Interestingly enough, the large and heavy book by Sangiori and Walker does not list concurrency in its index or table of contents, and looking into Milners book yields just section 4.3, which does not feature a formal definition of the term but merely introduces composition (i.e. concurrent processes) and summation (i.e. alternatives). The distinguishing feature of the composition syntax is to allow reactions to take place; in fact it is the only syntax allowing reactions. My laymans interpretation of this fact is that concurrency is intimately linked to reactions, supporting my earlier argument.
I think InitialGWTimeout is misleading, it's used for things that are not initial just below.
You only need to execute the logic below if x ne expired and x ne cancelled.      get match {       case `expired` | `cancelled` => ()       case other => ...     }
ok, I'll rename
These tests are not tagged as `TimingTest` and 10 ms might seem like ages, but I think it needs to be dilated to have some safety margin on the slow Jenkins.
oops, it should have been 10 seconds, the test should verify that cancel is done before the task exec, thanks
Can the ExecutionHandler be backed by a Dispatcher, and if so, should release be called?
I actually checked that, it is always an   OrderedMemoryAwareThreadPoolExecutor
It can't be backed by a `Dispatcher`. The code creates a new `OrderedMemoryAwareThreadPoolExecutor` for the `executionHandler` if the config settings are present.
implmentation => implementation
Maybe a short explanation in the docs about why circuit-breaker is used?
there is already one sentence about that
it looks like you have some auto line break setting that doesn't match our conventions, 120 char lines are ok
OTOH we have a ticket for making sure that code samples fit into the printed docs, which made me press cmd-shift-f on a few files in order to do some boy-scouting; maybe I was a bit too trigger-happy with that one, since this file is not in the docs.  But let me ask: do we really care either way for medium-long lines? wrapping around 80 has the advantage of keeping the github displays more readable at times, so I guess Im only saying that requiring long lines might be over the top (in Java sources, where there are no one-liners by definition).
Viktor is often nagging on me for breaking up lines, but I insist on keeping them below 120, which I think is a nice max. 80 is too short.  I see no difference for java and scala for these formatting conventions.  Doc samples are different.
of course you don't need to undo these formatting changes
Ideally line breaks should be done virtually by "formatting" in your editor. Since that is not the case, I don't think we should sacrifice readability for people using unix terminals from WWII. 120 is a good modern limit.
    case null =>  ?
or rather:      if (message == null) throw new InvalidMessageException("Message is null")
yes, I guess I should look at the bytecode here, the whole dance was because I was afraid of boxing here (critical path)
I checked, it's fine with `if (message == null)`, a primitive is already boxed when entering here thanks
I have no idea if this is important or not. Is the test trying to see if `rInt` can be converted implicitly into a `Nothing`?
please put a FIXME in, this will work again as soon as we get a more recent scala version
Added a test that will start failing when this bug is fixed in the Scala Library
Pattern matching ftw
wouldn't it make sense to have two methods?      setReceiveTimeout(timeout: FiniteDuration)     clearReceiveTimeout()
I don't like the wrong usage of the name `frequency`. Should we change while we are touching this? It should be `interval`.
that is sad
This is Java, so null is what they use, right?
ok, then it should be `setReceiveTimeout(timeout: FiniteDuration)`, which is actually rather convenient from scala also and it should have some docs about null == clear
yes, I think so, now that you mention it
I don't see how this improves over try-catch itself. (In fact I had to read it a couple of times to follow the logic)
OK I've changed to try/catch. There are a lot of possible ways to do the error handling so feel free to spell out your preference. I've used a `var` and `null` in this latest version because I figure we're going for optimal speed here, eschewing `Option`s and `Either`s and higher-orderness.
Can't this code live inside the `try`, so `channel` can be a `val` and we don't have to do the `null` set/check thing?
Same thing here.
I was thinking about that, but I wasn't sure about sending about closing-the-channel/sending-a-Failure-message once the channel has been registered. But I think you're right. :)
Nice. So much cleaner with publishing in `postStop`.
import InternalClusterActor._ at the top in this method?
This looks brittle to me
Why not PoisonPill & shut down cluster on postStop?
This means that we need to be very careful about creating children in this guy
Yes, i struggled a bit with the dependencies here. Do u think it would be better to send the publisher ref to the parent from core, and start the metrics triggered on that?
Good suggestion, thanks
Sounds better. And also, I guess one wants some DeathWAtch sprinkled to make sure that i fsomeone dies what shouldn't, things are "handled"
Awesome catch. So adding this is what provokes a vector clock conflict that leads to a merge instead of a win.  One thing, though, won't the vector clock just grow with new nodes? We need to prune it at some point right?
yes, and that's the case for dynamic port (0) as well the ticket is: https:www.assembla.com/spaces/akka/tickets/1289#/activity/ticket:
this is racy: child creation could check flag before this and create child after line 90 is finished; I think this is one of the few places where Id use a conventional Lock
Yes, very true. May i ask why this is not an ordinary actor?  25 sep 2012 kl. 16:28 skrev Roland Kuhn <notifications@github.com>:  > In akka-remote/src/main/scala/akka/remote/RemoteDaemon.scala: >  > >   > >      case t: Terminated  > >   > > -    case unknown        log.warning("Unknown message {} received by {}", unknown, this) > > +    case TerminationHook  > > +      terminating = true > this is racy: child creation could check flag before this and create child after line 90 is finished; I think this is one of the few places where Id use a conventional Lock >  >  > Reply to this email directly or view it on GitHub. > 
Yes, you may ;-) The reasonin short formis that look-ups must work for children which are several path levels distant, i.e. getChild needs to be a custom implementation. And (at least at the time it was written, could be re-checked) it needs to be synchronous for creation because otherwise sending to a newly created remote-deployed child would fail.
I have introduced a Switch lock to avoid the race
Doesn't that do a copy of the original collection? Would it be preferable to have a "foreachChild"-method instead?
Really? What if it's DeadLetters?
I wanted if-else semantics, and that is what Switch.fold does, but I'll change it to the following to avoid confusion:      val isTerminating = !terminating.whileOff {...}     if (isTerminating) log.debug...
um, shouldn't happen (internal api), but I have added some safety net, including watch of the termination hooks, please review
ok, changed to foreachChild
ouch. backport to 2.0?
wow. yes please. Some tests for this sucker could definitely be added as well.
This is still racy: if TerminationHook message is being processed and finds a child, but before finishing `switchOn` that child terminates (i.e. removeChild is called), then there will never be a confirmation of the shutdown. Terminated processing needs to be done under the switch lock.
ok, I will do that
parameter "remove" is not used?
This is possibly due to my limited understanding, but why "terminationHooks foreach { _ ! TerminationHook }" is missing here? We do not notify hook registrants when one of them is the terminated one?
ooo, thanks so much, for one of the cases it happens to be the sender. will of course fix
yes, the original case was only `case Terminated(_)` which as I understand it is received when it is time for the system guardian to stop. Now I have also added watch of the hooks, which is the `Terminated(a) if terminationHooks.contains(a)`, i.e. the hook is terminated before it's time to start the stop procedure of the guardian. I can add a comment about that.
I think we should check who's Terminated here
yes, that would feel better, they are triggered by these watches, right?      systemGuardian.sendSystemMessage(Watch(guardian, systemGuardian))     rootGuardian.sendSystemMessage(Watch(systemGuardian, rootGuardian))  there is some initialization order to think about here  this termination hook business is only needed for systemGuardian, perhaps I should make two different Guardian classes, so that userGuardian can be kept simple
added the check in SystemGuardian that it is the user guardian that terminates created a separate class for SystemGuardian, more clean
Add a comment that they are ignored for a reason
or instead do:    case Terminated(`guardian`) => ...   case Terminated(other) => terminationHooks -= other  no need to check presence before removal.
good suggestion, thx
This is not needed, right?
Is it intentional that if this throws an exception, the function may not have been applied to all children.
true, will remove
yes, same behavior as foreach of ordinary collections
    allChildren foreach { case a => if(a.asInstanceOf[InternalActorRef].getParent.path.address == address) system.stop(a) }
    allChildren foreach { case a: InternalActorRef if a.getParent.path.address == address  system.stop(a) }
@patriknw: I don't think we want it to blow up if the PF doesn't match
So how about:      allChildren foreach {       case a: InternalActorRef if a.getParent.path.address == address => system stop a       case _ =>     }
since I feel a lack of comments from my side on this PR is not acceptable, I have no choice but to say :+1:
but this class already adds children using InternalActorRef assumption, so it is very good to blow up if this fails
Change the filter + foreach to simple foreach, then merge.
Why the duplication? create a method that returns this string instead?
I don't think the one source file requirement is important. A larger contribution might benefit from being split in several files according to normal conventions. It should include tests, which should be placed in different files. It might be a java contribution, which might require multiple files. It doesn't make sense.  The unit could be the package name.
Comment is still valid :-)
well, I have not pushed the fix yet, but even so, I wouldnt fix this precise instance: if you look closely, the above text is not a repetition 
Remoting is experimental?
No, you're looking at akka-remote-tests
:) Ah, I was already so proud to work on something experimental! AkkaX project or something
still smells like a bug is hiding in there somewhere
Captain Paste :S
I saw in the versioning document that the term Experimental Features is used for Scala. Should we also say Features instead of Modules?
could you please correct this with "major releases", I ment minor, according to the versioning scheme <major.minor.micro> also "binary compatible between minor " should be "binary compatible between micro " thanks 
that's a looong line
That is true, no doubt about it. I wonder how that is part of my pull request, though  (will fix)
I'm not sure I understand the separation of the contrib docs. Will it be a separate document, not included in ordinary docs? In that case, is this the right name and title?
not `Long` ?
why pass sender in Message and not use forward?
Why the tick? Rate limiting?
I assumed that the contrib docs are included in the normal docs. That's what the build produces now anyway.
yes, they are included; I added this file so I could more quickly build the contrib docs by themselves, dont know whether we want to keep this or not (contra: links to the official docs wont work); if we want to keep it, then I agree that this file needs some cleanup
I vote for not keeping this then
okay, removing it
because you need to store the sender of each message in any case (for resending); another benefit is to enable normal communication between the two end-points, which otherwise would have to be a bit non-obvious
Maybe a short explanation about the tradeoffs with different values?
Aren't a definitive article missing in the beginning of the sentence? Also "the thread pool core size"? WDYT?
I don't think that should go here. It depends on OS, workload etc.
In case you're wondering, this was moved within the source file to match the ordering in the documentation.
why don't we have `getTestActor` in JavaTestKit? I think `getRef` is too general.
perhaps add an assert on `two` as well
I'm not sure I like this, it seems a bit leaky.
We don't need to create a new message for each send
What if "a" is already in the map?
ok if I make it private[akka]? isn't existenceConfirmed also an implementation detail that shouldn't be exposed to users?
true, will fix
so, one devilish scenario would be  1. create "remote/foo" 2. stop it 3. create new "remote/foo",  then it might receive the second registration here before the first Terminated msg arrives, and when that Terminated arrives it will remove the second from the supervisors Map, and it will not be handled  right?  yes, this is very tricky to cover all corner cases here, perhaps we don't have to cover everything, but cleanup on best effort basis?  I'm open for suggestions
private[akka] won't help Java users. And if addressTerminated is there it will need to be explained in the ScalaDoc
Should it overwrite the old silently? Should the new one replace the old?
new replace old is what I think is best  is there a way to lookup the supervisor from the actor ref path in the Terminated msg? supervisor is always it's parent, but it's a bit more tricky than that for those remote deployed  if we can derive and lookup the supervisor then this map wouldn't be needed  however, it doesn't solve the devilish scenario
I think both auxiliary flags are of the same quality here, meaning that knowing the basis upon which the actors death certificate was built can be interesting to an actor. In this case it might well mean that re-deploying on the same remote node will not make sense. So Id say keep it private and document what it means.
But can it be "existenceConfirmed" AND "addressTerminated"? If not, I think it should be one field with different possible values.
That said, I do find default arguments hard to refactor, especially of such a meaningless type as `Boolean`, so please pass it explicitly where this message is created, and use the arguments name to be perfectly clear.
this comment should also include an explanation to the relationship with the other ChildTerminated which might be generated by the remote daemon
yeah, unless boolean milliseconds is a new invention ;-)
didnt you see my scala/scala pull req? ;-)
it's the best duration in the universe so it wouldn't surprise me if it handled that also ;-)
Perhaps use a Deadline instead?
@rkuhn I was hoping for a suggestion from you here. How do I derive the path of the supervisor from an ActorRef of a remote deployed actor? Any existing utility?
not handling a `Terminated` still results in a `DeathPactException`
You dont, would be my suggestion. The name mangling code is currently confined to one single method and it should stay that way.  Wrt. replacing or not I dont see the issue: the old and new entries will have to be identical anyway.  For your devilish scenario the only solution I see would be manual ref-counting (i.e. keep an Int together with the value and remove the mapping once the last user goes away).
ok, keeping it simple then, i.e. as is
actually we have to do that, because the actor in the message is different
removed the default value added some docs
Changed to Deadline
Isn't this tested by the line above?
actually not, second parameter list not used in equals      scala> case class Foo(s: String)(i: Int)     defined class Foo      scala> Foo("aa")(3) == Foo("aa")(2)     res11: Boolean = true   I have added a check on addressTerminated also.
Nice catch, but you missed the same one in `akka-docs/rst/java/remoting.rst`. I guess it makes us even :wink: 
import SupervisorStrategy.{ Stop, Resume } above and use Stop and Resume individually, since that is how we want people to write it
Perhaps a link to the docs on lifecycles?
REPL-can? I'm about to open a can of REPL on you! ;)
I have instead put forwarders into `Act` for these things; this is supposed a DSL and one import should give you all of it.
yes, good one
Either we explain that you have to pass `-Dakka.test.tags.include=long-running` to sbt when you start it up, or we change the sbt run command to `akka-sample-multi-node-experimental/multi-jvm:test-only sample.multinode.MultiNodeSampleSpec`.
ah, yes, that is a better way to do it (it is a bit magic, though)
This could be confusing, because this returns the message, but does not connect. connectCommand maybe?
`_` is a no go for me
so youre saying it should be `soReceiveBufferSize`?
not relevant for those, but if we use the same for the Ack type of messages, how does the instanceof match look like in onReceive?
I could not parse this :)
`if (msg == closed)`
I don't think this add that much value over `new Register(handler)`
perhaps place all messages in a separate object, `TcpMessages`
yes, but it is more uniform to have factories for all messages
this was a test balloon for making the whole Java API of the Tcp object consistent (i.e. everything is factory methodsscary quotes for the case objects)
That is an additional stuff to import. I prefer having a minimal set of   imports.
yes, that's fine, but why not `def receiveBufferSize`?
Well, its kind of a Chimera. SO_RCVBUF would mirror the lower level stuff.   receiveBufferSize is closer to Java/Scala. soReceiveBufferSize is between   them. I personally like it.
the conventional name of these is SO_RECVBUFSIZE and so on, and it might make sense to somehow retain this prefix; that is the reason for the SO object for Scala, but unfortunately it does not work for Java, so if anybody has other ideas how to present this then dont hold back
There is no import minimization for Java, so I think `TcpMessages.connect()` is a good proposal.
if it were `TcpMessages.register(handler)`, would that work better?
Ah, yes, if its only for Java it makes perfect sense.
it could be a separate object for the java api only, containing the factory methods only
yes, that is what Im leaning towards right now
Yes, that would do the trick. I am sold on a TcpMessages class, as it will be clear that calling connect gives a message and does not do the connect itself.
yes, that one is clear, but I was thinking about the case classes, then you need to know the type anyway
the java api object that I mentioned can contain factory methods and helpers for checking instance type and casting
`soReceiveBufferSize` or `TcpSo.receiveBufferSize`
all NoAck(null) should return NoAck, right?
which kind of return do you mean? I changed `wantsAck` below as well
(NoAck(null) eq NoAck) must be === true
shouldn't we use explicit return type?
Should it be `Command` or the dollar-thing? That decision is one I have not settled on yet.
Because we are not lazy. Nil.iterator eq Nil.iterator
where is it shutdown?
seems like an irrelevant micro-optimization, but we can of course do it (I dont see anybody writing `NoAck(null)` in their code)
good catch: it has been forgotten
It's about discipline. We are awesome, and so is our code. We don't let low hanging fruit hang. ;-)
this is not java
ah, sorry, part of the udp changes
okay, the deciding factor is that your proposal does not really work ;-) (case class quirk)
Are we using the Queens English now? Or is this a stylish aberration? ;-)
I think there should be a one-to-three-word description of the purpose of the branch between X and Y (BTW: I miss that in your branch names lately )
Pretend Gandalf says it
Yeah, I just realized that I had a hard time shortening those names while still maintaining context. A GreaseMonkey-script that adds a tooltip to the branches pulled from Assembla? ;-)
no, I just find it nice to have some kind of hint what might be the topic (without having to go to some websites); even if it is very imprecise, it usually reminds me of the tickets text.
should this be removed?
No, I think it's fine. You're seeing me moving the #connect *up*. I'm not trying to remove it completely.
Same as above.
I was waiting for someone to solve it already as I had no idea if this is   the right annotation here -- but no one fixed it, so I relied on your   review :)
please add explicit return type `Command` to all of the methods above
> please add explicit return type `Command` to all of the methods above  Ok
yes, that is correct annotation It's even in the bible: http:doc.akka.io/docs/akka/2.1.0/java/testing.html#Asynchronous_Integration_Testing_with_JavaTestKit
Why does it throw if it returns an Option? In that case just return a Try instead of an Option?
Needs docs regarding what it throws and when
.... but with the same headers.
Might be better to put this in the companion object of CamelMessage, so Java users are less likely to see it, and Scala users cant.
no relative imports please
Add docs about not closing over "this"?
No need to repeat type annotations
    messages = {       for((sender, msg) <- messages) aref.tell(msg, sender)       Map()     }
    +=
    case msg @(_: FailureResult | _: MessageResult) => context.parent forward res
Any reason why this is done prior to creating the CEA?
Document as internal
Document as internal.
sealed case class? what is wrong with "final"?
So you have made sure that in all places where exceptions can be thrown, the actor instance is not corrupt?
ie. very nice!
The UID should go on the individual classes
Why not private[camel]?
nm, parent type is private[camel]
Why is this an object within a class?
replace with "import system.dispatcher"
I don't get this. why call getRouteDefinitionHandler twice?
Import the methods you need
damn that intellij...
its wrong, i'll change it
yes makes sense
yes thats better.
yep, I blame the beers at strangeloop
no i cant think of any, will reorder
will put it outside of the class
That doesnt seem to work : stable identifier required, but ConcurrentActivationTest.this.system found.
changed to final
Yes quite confident, the only thing that can go wrong is registration, in which case the actor is stopped and never goes into service
wouldn't it be useful to include the `name` in the exception msg?
    /**      * INTERNAL API
but preRestart is performed on the old instance wouldn't it be more natural to do it in postRestart, which calls preStart, i.e. only in preStart?
same comment here regarding preRestart and postRestart
no need for `val aref`
remove parens around msg
everything that should support Serializable should have `@SerialVersionUID(1L)` that also includes case classes that are sent over the wire or need serialization for other reasons
you mean that I should take it out and get people to use copy? I like that, I've actually used copy a couple of times in tests instead of these methods, cause the semantics are clear already instead of some weird named 'withBody' etc. let me know.  Maybe all this with... stuff should be removed, ar least for Scala and leave things there for Java, and also leave in the methods for Scala that do something different than case class copy style
I'll add it
Ah right, thats correct. I'll remove it.
No, i was only thinking that you should use copy internally, since it's a case class. You need this for java.  25 sep 2012 kl. 21:25 skrev Raymond Roestenburg <notifications@github.com>:  > In akka-camel/src/main/scala/akka/camel/CamelMessage.scala: >  > >     */ > > -  def withoutHeader(headerName: String): CamelMessage = copy(this.body, this.headers - headerName) > > - > > -  def copyContentTo(to: JCamelMessage): Unit = { > > -    to.setBody(this.body) > > -    for ((name, value)  this.headers) to.getHeaders.put(name, value.asInstanceOf[AnyRef]) > > -  } > > +  def withBody(body: Any): CamelMessage = CamelMessage(body, this.headers) > you mean that I should take it out and get people to use copy? > I like that, I've actually used copy a couple of times in tests instead of these methods, cause the semantics are clear already instead > of some weird named 'withBody' etc. let me know. >  > Maybe all this with... stuff should be removed, ar least for Scala and leave things there for Java, and also leave in the methods for Scala that do something different than case class copy style >  >  > Reply to this email directly or view it on GitHub. > 
Could be more cheaply expressed as:      import context.{ watch, actorFor, parent }     seedNodes foreach {        seed => if (seed != selfAddress) watch(actorFor(parent.path.toStringWithAddress(a))) ! InitJoin     }
What if it was empty before the attempt to remove address?
    if (seedNodes.isEmpty || seedNodes == immutable.IndexedSeq(selfAddress)) ?
I'd put the:      seedNodeProccess = if (...) else if (...) etc
I'd move this up one line
I'd move this up one line
hmm, I rather like this style, better matches the intent with the indent
Is info level ok here? Should it be a warning? Somebody actually tried to do something wrong.  Same thing above. 
...in _the_ existing cluster.
yes, I'll make it a warning could be triggered by jmx user command, but still it's wrong
yes, and I think scalariform indentation will be wrong if placing it above the case
you mean initially empty, that is not possible and is ensured by https:github.com/akka/akka/pull/1136/files#L0R899
parens are not needed here
why not `import LsKeys.lsync`?
Isn't this on by default for SBT 0.12.x? I remember removing it recently since it wasn't needed anymore for sbt-multi-jvm and the sbt-assembly plugins.
May be, but it isn't what the docs for it says:  "There is also a release available if you are using sbt 0.12.0-RC1  addSbtPlugin("me.lessis" % "ls-sbt" % "0.1.2")  resolvers ++= Seq(   Classpaths.sbtPluginReleases,   Opts.resolver.sonatypeReleases ) " - https:github.com/softprops/ls
I think that is only needed for 0.12.0-RC1. Just like the doc says.
Everything else looks to me like it just avoids the reverse DNS lookup for incoming connections, without changing the semantics (well, the guess of the peers address changes from possibly resolved to numeric IP, but we should not depend on that); so either this change fixes a bug exposed by the aforementioned should-be-irrelevant change, or I dont understand it.
`val naked = nakedAddress(handle.remoteAddress)`  so this was only a cleanup to skip double-naked, should be same result
Do we lose the sender here, is this intentional?
Do we lose the sender here, is this intentional?
it should be the same as before bad things happened, https:github.com/akka/akka/commit/514f68aec0a0ce2b7d72d9440531c0c1411e83d1#L4L244
Yeah, but is it correct not to forward?
could you create a separate ticket for that then - I don't know if it has any implications, but I hate branches that fail and would like this to be merged
Nevermind, it's a test :p
shouldn't we avoid relative imports like this?
Absolutely, sorry about that. I blame IDEA :wink:. Will fix.
why not just return `0L`?
So, I thought that when we stopped the driver, the time would progress as normal for the timer. Of course we can let it stand still. We should still do some kind of sleep so the timer thread doesn't busy spin.
well, at this point its stopped, so it should really not spin anymore, Id think
The driver is "stopped" but the timer thread is still alive, at least until the close method is called on the timer. Yes, that happens right after each other, but the timer thread might start to "spin" until that happens.  Just trying to avoid weird behavior on really slow boxes. (Yes, we have one of those you know ;)
if this fails then you've already scheduled it...
Is this really going to shut the window completely?
I'd expect to see some handling here if the compareAndSet fails.
Right, `current == currentBucket` won't necessarily hold when enqueuing later.
I wonder if the `tail` `TaskHolder` could be annotated with its bucket so you could do an atomic check when the new `TaskHolder` is enqueued.
to be precise: it only fails if the job has already run, which is precisely what it shall do in that case
I think so, but that was the question I'd like a second opinion on
currently TaskHolder is 24 bytes large, that would make it 32; also I dont think it is necessary since during the switch the value makes a transition X -> Pause -> X', where X' is guaranteed to be different from X, so we can detect the change without internal round marker
Ignore previous comment. :)
That can't be right
I don't get you. The bundle provides correct Manifest. Is it a question of Scala versions? The problem we have is to use several versions of Scala (2.9 and 2.10) in the same container. By default, the bundle is resolved with the higher version and the rest of the project is using 2.9.
Akka 1.3.1 will not work with Scala 2.10, and Akka 1.3 is EOL (i.e. there is no 1.3.2 planned)
scala 2.10 is not contained in this range , those are every scala 2.9.x only. Ok, thanks for the information about EOL.
Man, that exclusive-paren always gets me. too similar to ] :-)
Instead of putting isFuture checks in each branch of the match clause, I'd have a special match clause for when isFuture is true and another when false, separates the concerns
hmm, this would not lead to less lines or less noise, there will always need to be these six cases, so I dont see a compelling reason; OTOH Id need to repeat the implicits (and I dont want to put them outside because calling `universe..lub` is a costly operation).
Nah, you could just make them lazy vals. But in any case, it's just my experience that repeated checks are costlier to maintain than single ones.
are these casts really necessary? i.e. are the proper types not available for the compiler without our help?
this for example smells like it might use a type parameter to match the input with the output
I think you mean   by running `bin/karaf`  
please add copyright header
please remove commented portions unless used in certain circumstances (in which case please document)
this is the old way of doing things, the new remoting is configured differently; but you are just using the default values, so probably better remove this section (possibly apart from the next line)
running on a fixed port is always problematic because it keeps us from running e.g. different PR-validator jobs in parallel; for now just change to an otherwise unused port here and then well think about how to do it better afterwards
please add comment for OSGi-dummies what exactly this does (I think it just publishes two services within this bundle)
ouch, yes, this needs to be understood and fixed
okay, I see that there are several things for me to learn 
please mention that that is tracked in ticket #2990 
```java 	public ServiceReference< ? > getServiceReference() { 		return reference; 	} ``` due to the wild card, I don't see yet a way to enforce the typing. I may change the instanceOf to pattern matching if you prefer.
I suspect Raman added it here to let people add bundles to the container, no more useful
for me also ^^
By the way, it may be more useful to use directly the provided ``registerService`` in akka-osgi ActorSystemActivator
should be corrected now
  I tried it again (this code was developed by Raman) and got this error first : ``` Exception in thread "RMI TCP Connection(idle)" java.lang.NullPointerException     at com.typesafe.config.impl.SerializedConfigValue.writeOrigin(SerializedConfigValue.java:202)     at com.typesafe.config.impl.ConfigImplUtil.writeOrigin(ConfigImplUtil.java:224)     at com.typesafe.config.ConfigException.writeObject(ConfigException.java:58)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)     at java.lang.reflect.Method.invoke(Method.java:597)     at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:940) ``` before the related exception. the NPE is reliated to ```java   not private because we use it to serialize ConfigException     static void writeOrigin(DataOutput out, SimpleConfigOrigin origin,             SimpleConfigOrigin baseOrigin) throws IOException {         Map<SerializedField, Object> m = origin.toFieldsDelta(baseOrigin);         for (Map.Entry<SerializedField, Object> e : m.entrySet()) {             FieldOut field = new FieldOut(e.getKey());             Object v = e.getValue();             writeOriginField(field.data, field.code, v);             writeField(out, field);         }         writeEndMarker(out);     } ``` 
s/It finally provide/Finally, it provides
I assume not including the extensions section is because, in an OSGi container, you want the extension to be initialized in an on-demand fashion, not at startup?
@jamie-allen, we have changed that in master, the cluster extension is always loaded when using the ClusterActorRefProvider
Oh, okay.  Thanks, Patrik!
Feel free to ask questions here if any... I'll do my best to answer them!
Ok to be negative?
Why the Option-suffix?
this is an internal message sent to itself so I see no reason to do any boundary checking, if that is what you are suggesting?
it's nothing I like, but it's a consistent naming I have used for the optional addresses in the FSM data, and the reason is that in pattern matching I have `Some(newLeader)` also, I think `newLeaderOption foreach` reads better than `newLeader foreach` if you have suggestion of better naming I don't mind changing
I actually find the `case m @ (MemberDowned(_) | MemberRemoved(_))` more readable.  Is there a performance benefit?
no, it didn't compile, `m` must be of type `MemberEvent` so that I can use `m.member` 
Ah, sorry. You changed the collection to `ClusterDomainEvent`. Missed that.
this is some kind of pattern matcher deficiency, since it should clearly be able to figure out the LUB of MemberDowned and MemberRemoved (but it doesnt)
And this isn't allocating anything on each invocation of this method?
Sloppy of me to assume it wouldn't. Just checked and javap says "No". The allocation is in the catch any block of the method. It's only when there is an exception.
Cool, then it's fine, well, unless the exception is an OOME ;-)
Is the code reuse between the two branches more important than readability?      case e: InterruptedException)        unreserveChild(name)       Thread.interrupted()  clear interrupted flag before throwing according to java convention       throw e     case NonFatal(d))        unreserveChild(name)       throw e
Probably not. I can expand it into two cases if that's more readable.
I think we can safely throw here: the system messages will then processed when the mailbox runs again. And then turn line 236 into `if (Thread.interrupted())` (because `processAllSystemMessages` does not actually check this, it only catches IE and rethrows).
Sure we could throw directly after the invoke.  Thanks for making me reread the code and comments in `processAllSystemMessages`. It relies, as you pointed out, on `systemInvoke` throwing InterruptedException, which it no longer does. Will fix. 
does it need to be public?
alright, I have added public to all similar descriptions in reference.conf files
Shouldn't there bee a barrier here for everybody so fourth don't start running the next test case?
Ok, I see the `after {}` statement now, but won't that just work for the first entry?  What will the test conductor think about you reusing barrier names?
There is, in `after`, L42
Why will it only work for the first? I assume that test conductor doesn't care about reuse. When all nodes have entered the barrier it is released and same name can be used again. @rkuhn is that correct?
Just being pessimistic ;). Read the barrier code in the TestConductor and as far as I can see it should be fine to reuse the names.
ok, but I tink there is a race when combining barriers with shutdown/remove. Got that failure on jenkins (akka-pull-request) now. We have a ticket for that, #2137.
correct in principle, but might show errors in the wrong location: what if the first version is not entered by a node but the second one is? (hard to imagine for a cleanly written test, but nevertheless) So I would recommend as a style that barrier names should not be reused unless good reasons force it.
Is this the correct order? Should we introduce a composite operation to avoid accidental fcukups?
Yeah, we have already talked about that. If there is not a ticket already, add one. 
Already opened: http:www.assembla.com/spaces/akka/tickets/2137
it was the correct order, or at least mostly working order, but when 2137 is merged it will be the reverse order, and I have already fixed that in my local branch of this pull req.  Is there any case where one wan't to do shutdown without a removeNode? Easiest would be to always do removeNode under the hood when doing shutdown.
If it isn't done automatically I'd like to have a comment explaining why the order is as it is.
I tried to say that the order *will* be:          testConductor.removeNode(third)         testConductor.shutdown(third, 0) 
This config property is a duration. should be `30 s`
Sure I know. Typo. 
Pushed fix. Thanks. 
so the reason that you increase this is to make sure that the awaitCond Leaving is ok before Exiting happens? 
might be good to add a comment here that you have increased the leader-actions-frequency, so that Leaving is detected before Exiting
It is so I have a chance of testing 1. before 2. happens. I could also split up in two tests. Setting the duration to 30 seconds (to turn it off) in one test and normal in the other. WDYT? 
I think it is fine, but a small comment about it might be good
Ok. Will do. Thanks. 
Might want to mention that we will not guarantee binary compatibility if the trait is used.
Sounds  strange (given that we do support other traits). How would you word it?
Use of the trait is discouraged because of potential issues with binary backwards compatibility in the future, use at own risk.
Please use infix operator notation for higher order functions (Scala Style Guide):  ``` system foreach (_.shutdown()) ```
Get rid of the curly braces and the unnecessary type annotation:  ``` this(context => None) ``` 
Get rid of the curly braces and the unnecessary type annotation:  ``` this(context => None) ``` 
See next line ...
Don't use type annotations for local variables.
Don't use type annotations for local variables.
Add type annotation to not-private members.
All semicolons like these need to be removed
Put explicit method return types on all methods
No braces on case clauses
Remove this, it should be obvious when something is built
All tests should be WordSpecs with Mustmatchers
How is this to be used from Java?
The two other constructors (no-args and plain String) have been added to allow Java users to use the ActorSystemActivator, but if you want this third option to be available for Java users as well I'll refactor this class to have a protected strategy method that people can override to achieve the same goal.
This implicit conversion is meant to make it easier for people to build their test bundles using the Fluent API in our BundleDescriptorBuilder - with this thing, people can just use the BundleDescriptorBuilder or plain PojoSR BundleDescriptor (whatever they prefer) and mix them up transparently.  I can remove this implicit def obviously, but all it would do is require an extra 'build()' call on a method like this     val testBundles: Seq[BundleDescriptor] = Seq(     bundle(TEST_BUNDLE_NAME).withActivator(classOf[TestActorSystemActivator])) 
I vote for the .build to be there since in 2.10 and forward you'll have to import language.implicitConversions to use that w/o warnings.
Alright, then document this constructor as Scala API and the others as Java API
is there a specific reason why youre calling this twice?
might just say `this(_ => null)` 
on the other hand, isnt `null` bad, and `None` is much nicer?
If I understand correctly (which may well not be the case), then it should be possible to have actors created automatically by referencing them in the XML file. If so, please add a test case which demonstrates this.
Yes, this is a good way to do it. We dont even need to obey any order on the incoming requests, so it is fine to just send them via      endpointFuture onSuccess { case ref => ref ! assoc }
What happens if a write is attempted when buffers are full?
Why is this a Future and not a strict value? Making it a Future basically means that someone either needs to block on it, or that there is a time window between signaling full and the writer actually throttling; see my question above.
Yes, there is truth in that... 
    result._1 must be(addressA)     result._2 must not be null
Thanks, I have to familiarize myself with more idiomatic test assertions.
dummy arguments are best named `_`, because then it is completely clear that they are not used
this `@volatile` does not serve any purpose I can see (no threads involved)
you could also just return the handle from here and store it in a `val handleB`
leave this off: it is implied
This looks really useful, would be interesting to see some test cases using it (I mean in TestTransportSpec for simulating complex series of events).
that obviously needs documenting ;-)
this should go into the `akka.remote` section
That's the plan. I just wanted to keep it separate from the rest until   everything settles.
you might want to put in a FIXME so that you dont forget to include the changes to that code in master (at the point when this goes live); no need to rush right now
this is a bit of a strange name (dont know what it is used for, yet)
this could potentially benefit from being a FSM, e.g. with Initializing/Buffering/Writing states  Id also remove the `active` vs. `handleOption` duplication: just have handleOrActive, where `None` means active
uncorking the write side is not logically connected with starting the read side, so I wouldnt put these within one method
OTOH it looks like this is a left-over
no need to pass the handle here, just do this line in the writer instead
`decodePdu` should be lazy wrt. decoding the actual payload, since we might throw it away in untrustedMode (it might be a mem bomb )
this will need to be fixed to use the actual appropriate address, right?
we conventionally call those Settings
I guess you know what Donald would say ;-) (and for such a small match at that)
Why is this optional?
Add #FIXME document
On all o' 'em
Why is this public API?
    table.get.get(resource) match {       case Some(r) => r.isAvailable       case _ => false     }  Creates fewer classfiles and less allocations
This will create one new detector for each colliding thread, is this by design, and if so, why?
Why do we want/need all these debugs?
No need to create a StringContext for a single param
Why Terminated(_) ?
There's no check here whether reader is already non-null or not.
remove semicolon and add newline
what if buffering is already false?
what if buffering is already true?
I think this could benefit from being an FSM Actor or just use become.
Why this catchall?
What does that mean?
I think this is erronous usage of @implicitNotFound  
Right, an FSM makes completely sense here.  > Id also remove the `active` vs. `handleOption` duplication: just have   > handleOrActive, where `None` means active  Oh yes, this is a remaining of a huge refactoring! Thank you, I will   change.
Ok, good idea.
Yes, I used the assumption that it is called on startup. Not nice, will   refactor.
Thanks, good idea!
Yes, definitely! This is mentioned in a TODO comment at another place, and   this is something I will fix. I have to change the protobuf definition   though, and I wanted to postpone it after everyone says OK to the current   architecture.
No, this code is not called from my Remoting class, this is just there to   make the old NettyTransport happy (he inherits these methods). In my code   this is handled in EndpointWriter and it uses the address in the handle.
ok, so be it :)
Backwards message compatibility. At decoding I assume the old "akka"   protocol if this is not set. But I guess, as it seems now, this could be   required.
This is to be used with the failure detectors. Do you think we should hide   it? Optionally we can move them to something like remote.util and leave   them public if people have the irresistible urge to use failure detectors.
Ok, will change.
Patrik needs them. I am happy to remove them if he agrees.
Yes, but only one will win. How can you avoid that the colliding threads   do not create competing versions?
There is only one child.
What is the coding convention for strings? When to use interpolation,   formatters or plain concatenation?
Oh Thanks! While it is unlikely that this method gets called twice, it is   better to play safe, indeed.
Then the stash is empty already.
That is exactly what Roland proposed, too, and I agree with both of you on   this.
No other thing can arrive. This looks stupid now, but payload and control   message deserialization will be split in two -- and then this match could   go away. I need to change the protobuf though, and I do not want to touch   it before you say OK to the overall architecture.
Oh, well. Sloppy docs are sloppy. I will improve the quality. Thanks!
 > s"..."?  You commented earlier: "No need to create a StringContext for a single   param". Could we agree on a convention when to use which?
The logger is only created once, right?
Shouldn't these be FiniteDurations?
Aren't you running SBT? (Scalariform ought to have changed this into the right-arrow)
you could rewrite the 2 lines above as:  case a if (a eq ra) || transport.addresses(a) => Some(rootPath.address)
So this field is here to guarantee that two remote actor refs using different transports but points to the same actor on the other end will compare equals? What happens when actorFor is used with different schemes?
So what is the semantics? Can this method throw something?
And users generally shouldn't create instances of them :-)
How often is this called?
I think we should enforce this (the responsibility-uniqueness) when things are put into the transportMapping and not per inbound message.
Shouldn't those 5 seconds be configurable?
mapping uniqueness should be enforced here.
who is the sender of this tell?
Or will it cross over the wire?
Should be documented as non-threadsafe since it doesn't use CAS/locks to ensure that there are no lost updates
I think HeadActor needs a new and more badass name. "Robert"?
no need for all those braces ;-)
This is not a strategy, it's a Directive
It's also only used in one place, so doesn't need to be a def, just move it to where it is used.
transportsAndAddresses is just used in "success", move the expression in there
Is it OK to get N Listen messages? (become after first or not?)
Is it OK to get Send before Listen?
No need for the braces :-)
No need for braces
No need for braces... Just adds vertical space and has an annoyance factor cause I need to scroll more. remove it please.
Don't add an onFailure and an onSuccess (2 callbacks) when only a single onComplete is needed.
Should this really have a random name?
Also, this method is only used in one place, so there's no reuse to be had, so just inline the logic here.
I'd get rid of the "val wrappedTransport" and inline that whole expression where wrappedTransport is used.
no need for the braces in "try" here
Create a new Set for each call? Why?
Is this user API?
so we throw a match error if other party sends Connect without origin?
Why yield serialized?      for (originAddress <- origin; serialized <- serializeAddress(originAddress))         controlMessageBuilder.setOrigin(serialized)
Do you need braces here?
Only used in one place, inline expression there.
What is the purpose of this object?
Is this public API? Is it intended to be extended or something?
associationHandlerPromise.future.map(HandlerRegistered(_)) pipeTo self
register a single onComplete instead of onSuccess and onFailure separately
This is not a TODO, this is a FIXME.  FIXME == must be fixed before release TODO == nice to do in the future
Why is the queue a List (append is extremely costly)
put extends on the same line please
use trySuccess instead of the check-then-act?
Is this public API?
Don't think this line is needed..
I'd do:      def fakeTimeGenerator(timeIntervals: Seq[Long]): Clock =       new AtomicReference[List[Long]](         timeIntervals.tail.foldLeft(List[Long](timeIntervals.head))((acc, c)  acc ::: List[Long](acc.last + c))         ) extends Clock {           @tailrec override def apply(): Long = {             val current = get             if (compareAndSet(current.head, current.tail)) current.head else apply()           }         } 
This is a FIXME, please revisit all TODOs here and make sure that the ones that have to be done are FIXMEs
Ok, then I will review the uses of interpolation in my code and remove the   ones that might impact performance.
Most probably yes. Will fix.
This should be removed from RemoteTransport. I don't think that the RARP   should have the right to control clients.
Ah, right, toSeq will be the right thing.
When a remoteActorRef is created. The result is cached in the   RemoteActorRef.
 > I think we should enforce this (the responsibility-uniqueness) when   > things are put into the transportMapping and not per inbound message.  It cannot be done easily. The problem is that the address part after the   scheme string in the URL may have transport dependent meaning, so only the   specific transports know if their responsibilites overlap. For IP based   things subnet masks are the most obvious filters, but who knows what kind   of filters people want.
Oh yes! This is inside a config parameter, forgot to use it here. Thanks!
No, it is strictly local.
This is also local.
And this is local, too.
Yes, it mustn't be used outside HeadActor. I document it and hide it.
I think it should be named after me. EndreActor. That would be badass.   Joke aside, if you have any good name ideas (not just here) then don't   hesitate to share -- I am trigger-happy to rename things.
No, and it is not Ok to receive multiple Listens. "become" powers needs to   be activated. Will fix.
No. Will fix.
The idea was to improve readability somewhat. So in general, I should   inline all local variables, functions, that are used at exactly one place?
Ugly hack is ugly. I will fix it.
Nice catch! We should throw an exception.
 > What is the purpose of this object?  Contains message classes, constants and a handful of functions that are   used at several places.
It will exceed the line length limit.
Who has this method?
Umm, maybe this should be backported to cluster as well?
We use it for our testing of the failure detector. I don't think it's necessary to expose that to users. The less api the marier. I think it should be scoped to the failure detector (define inside it's companion obj).
You can remove them.
style, in this case the if is not a guard of the case, and I would write it as      case Send(msg, senderOption, recipient) =>       if (!buffering) sendMessage(msg, recipient, senderOption) else stash()
I think it would improve understandability anyway to use Terminated(`reader`)
and that was of course with      `reader`
    try if (!handle.write(pdu)) {
addresses is a Set, right? then this should probably use mkString to construct a nice logger name
is it intended to include the `Set` in the string?
List? Seq or IndexedSeq
inside actors we normally don't use `private`
uuh, inlining that would be pretty ugly
we have those to be able to run tests directly inside Eclipse, when the scalatests runner is working we will remove all of them
I'd add some indentation of the whole config, and fix formatting of next line
wow, more than 120 chars?
Yep, it will be 130 :(
Im with Endre and Patrik here: theres nothing wrong with naming single-use values.
this means that there is no dedicated supervision for this manager and nobody watches it either; also, it would be nicer to not create top-level names (with some semi-complicated scheme); there should probably be a TransportSupervisor which reports to the HeadActor (or similar)
ouch: this will break with ClusterActorRefProvider, but well want to use one remoting for all purposes; this needs some thought
Wow, thanks, I was not aware of that. I have to look at CARP.
so far ClusterActorRefProvider extends RemoteActorRefProvider
I think these would benefit from something like      def ape[T](msg: String)(thunk: => T): T  which is used like      ape("Error while decoding incoming Akka PDU") (codec.decodePdu(pdu, provider))
and then remove these helpers, because the result would be one-liners
Id remove a lot of " on this line: `"akka.loglevel=INFO"`
well, I guess `ape` could be implemented using `Try`, but the point is really to transform the exception which is thrown
Good ideas, thanks!
I'd probably check isEmpty _before_ size
This seems very, complicated, simplify?
This comment seems to be repeated througout the docs, is this intentional
yes, since consistent-hashing isn't in master yet, so I will change once it is
This one can be folded into the pattern match. 
You mean that this event is sent out even if there is no convergence? Is that a good idea? 
yes, but then I'd have to add another one to ignore the case when it's not convergence
Probably more clear if you name the actor and move the message send to the line below.
yes, it contains the convergence flag, perhaps that is confusing
Do you prefer this?         find the node with least routees       val numberOfRouteesPerNode: Map[Address, Int] =         currentRoutees.foldLeft(currentNodes.map(_ -> 0).toMap) { (acc, x)            val address = fullAddress(x)           acc.get(address) match {             case Some(count)  acc + (address -> (count + 1))             case None         acc + (address -> 1)           }         }        val (address, count) = numberOfRouteesPerNode.minBy(_._2)       if (count < settings.maxInstancesPerNode) Some(address) else None
I will do that change to LeaderChanged. It will be harder to keep track of, but more useful. https:www.assembla.com/spaces/akka/tickets/2518
Good. Does not make sense to observe not yet committed state. 
Could be encoded with Option[Either[LeaderChanged, LeaderChanged]] and remove a var (since I assume they can't be both Some at the same time?)
Does this do what you think it does?
Is this really the desired behavior?
I hope so ;-) and as far as I have tested it does. It's supposed to publish the stashed event, and store it in publishedLeaderChanged. Do you see that it does anything else?
yes, all other events should just be published to the eventStream
The {} is not a function, it's a block of code that evaluates to a function.
I think that piece of code is deceptive. Can we make it more readable?
But it doesn't check if it's an event, it could be some stray message, and then it won't get sent to unhandled, and it's hard to debug.
it starts here https:github.com/akka/akka/pull/703/files#L1L203 and diff only returns events, so I don't see the problem, I can move all this to a separate function to not mix up with the actor receive, which I guess is what you did
that was a strange link conversion, it starts on line 203, `diff(oldGossip, newGossip) foreach { event `
empty path for deploy?
acc + (address -> (1 + acc.getOrElse(address, 0)))
Roland implicitly suggested withDefault instead of map(_ -> 0).toMap
so something like:       val numberOfRouteesPerNode: Map[Address, Int] = currentRoutees.foldLeft(currentNodes.withDefault(_ => 0)) { (acc, x) =>       val a = fullAddress(x)       acc + (a -> (acc(a) + 1))     }
Shouldn't this be the FQCN?
I don't know what that path is used for. That was how it was done in corresponding RemoteRouterConfig. I have changed to use named parameters instead, so that the default value for the path is used, i.e. ""
ok, thanks, that is nice, I have changed to something like that  withDefault doesn't replace the `map(_ -> 0).toMap`, because that is the conversion from Set to the initial Map, but anyway it's a nice improvement to use withDefault
I don't like this.
Does this actually work?
not-a-fan here also
I only noticed because tons of tests failed otherwise ;-)
ouch, I blame my scrapped Eclipse workspace for losing this piece of configuration
Is this correct? 1, "second"
it will be, once we update to scala 2.10s next milestone
Honestly I'm not a fan of it anymore. But nothing to do about it.  Ideally I'd like to see:      class A extends Actor {        context become {         ...       }     }  One thing to learn and no special treatment of the first behavior. 
That is already possible with trait Act (the DSL).
Yes, but that is a very different thing. 
yup, just sayin
isn't this breaking the scaladoc again? we have a script for doing this, or isn't that needed any more? ./scripts/fix-protobuf.sh
I have not tried scaladoccing this, but the failure for that one had a different cause (something with a builder)
I've done a publish-local to be able to verify and try the artifacts without any problems. That should have created the docs.  This file is also in test, if that makes a difference.
ah, I didn't notice it was in test, thought that it was the generated one. all good!
Why are these removed?
They are only needed for sbtosgi < 0.3.0. It has been upgraded to 0.3.0 which is the latest stable. 
Have you verified the generated jars? (we need to make sure we don't botch the RC)
Yes, to the best of my knowledge. To quote my own pull request "Locally published artifacts have been verified to run a MultiNodeSpec based test, which should cover remoting"  Is there any smoke test that we have, or should I add a ticket to create one?
I don't understand what the MultiNodeSpec has to do with OSGi bundles. Have you've verified the bundle metadata and the generated poms?
I have no idea what "the bundle metadata and the generated poms" should look like. Please enlighten me.  As for the MultiNodeSpec, I just made sure that the bundle jars, which are the only jars we publish, actually contain classes and are usable in the best way I could think of. 
Generate and publish-local the jars, apply the osgi-changes, publish-local new jars, compare the old with the new. There should be no difference (if sbtosgi doesn't do things differently, in which case we need to investigate further)
Ah, sorry. Now I get it.  You were referring to the sbtosgi changes specifically. I was talking about our jars in general.  Will double check. 
Great thanks. Also cmp the poms.
Since I did two builds I had to zero out the publishing dates in all ivy.xml and MANIFEST.MF files with some `sed s/`, but apart from that all is OK.  Every byte matches in all produced files. 
not much use of a marker when there is only one concrete implementation, we can add this later, if needed
I'm a bit sceptic about this symbolic operator `-` It's not in symmetri with `:+` I'd simply name this `remove`
could be an import in the beginning instead, similar to the other things
is the `toSet` needed? isn't it already a Set?
why not use `toSet.max` instead of `foldLeft` ?
In latest and greatest we use `: FiniteDuration`
use single `"`
60 seconds is very long, why? I prefer that you use `within` It can be used at the top `taggedAs LongRunningTest in within(20 seconds) {` or further down      within(20 seconds) {       awaitCond(clusterView.members.filter(_.status == MemberStatus.Up).size == roles.size)       awaitCond(clusterView.clusterMetrics.size == roles.size)     }
remove ` == true`
this is racy, it's enough with the next awaitCond assert
why move this here? it's not something that should be done in general
perhaps write the above 3 asserts something like this:      mergedGossip.nodes.map(_.address) must be (Set(m1, m2, m3).map(_.address))
Ok. I thought it was best to use it for >=1. Shall I remove it?
Yes, good catch.
That is better. I tested .max before and it wouldn't compile, requiring Ordering but now this does not. thx.
I like to allow enough time to insure that all the data between the nodes is not going to do the wrong thing but I will reduce the time.
that was a stupid mistake. thx.
I was using it then simply forgot to revert that which I will do now.
Yes, please The purpose of the marker traits is to be able to subscribe to more than one subclass, e.g. MemberEvent  17 sep 2012 kl. 16:53 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/ClusterEvent.scala: >  > > @@ -89,6 +89,16 @@ object ClusterEvent { > >    } > >   > >    /** > > +   * Marker interface for cluster metric related events. > > +   */ > > +  sealed trait MetricsEvent extends ClusterDomainEvent > Ok. I thought it was best to use it for >=1. Shall I remove it? >  >  > Reply to this email directly or view it on GitHub. > 
If removed, all of the metrics multi-jvm tests fail because clusterView.clusterMetrics.size is always 0. So if we don't want it enabled yet then another test strategy is needed :( 
Subscribe to ClusterMetricsChanged instead should give the same result, or am I missing something?  /Patrik  17 sep 2012 kl. 22:13 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/ClusterEvent.scala: >  > > @@ -89,6 +89,16 @@ object ClusterEvent { > >    } > >   > >    /** > > +   * Marker interface for cluster metric related events. > > +   */ > > +  sealed trait MetricsEvent extends ClusterDomainEvent > If removed, all of the metrics multi-jvm tests fail because clusterView.clusterMetrics.size is always 0. So if we don't want it enabled yet then another test strategy is needed :( >  >  > Reply to this email directly or view it on GitHub. > 
In the multi jvm tests I'm not subscribing, but doing things like: awaitCond(clusterView.clusterMetrics.size == roles.size). You are suggesting re-writing all metrics multi tests?  
No, that will be just fine. ClusterView subscribes to all events (`ClusterDomainEvent`), so it will get ClusterMetricsChanged anyway. All I want is that you remove the trait `MetricsEvent` and change `ClusterMetricsChanged` to:      case class ClusterMetricsChanged(nodes: Set[NodeMetrics]) extends ClusterDomainEvent
What is the purpose of this trait? I think it should go, and move the awesome documentation to ClusterNodeMetricsCollector
`context.actorFor` should be the same thing, can you please change in ClusterDaemon.scala and ClusterHeartbeat.scala also
this documentation is obsolete
shouldn't this be `apply` in `Metric` companion instead?      object Metric extends MetricNumericConverter {        def apply(name: String, value: Option[ScalaNumber]): Metric = ...
here it would be easier to understand if define was Metric.apply, i.e.       def systemLoadAverage: Metric = Metric("system-load-average"
what do you think about using plain Try instead of this `wrap` E.g.      Try { 17 / 0 } getOrElse 5 
Works for me. It made more sense in an early-on iteration but I've been wondering the same. If it is not reflecting a best practice then 86 it? What does Viktor think?
He loves deleting things, so don't give him the joy to do it. Do it yourself.  Yeah, I agree with the background, but not any longer.  /Patrik  20 sep 2012 kl. 21:38 skrev Helena Edelson <notifications@github.com>:  > In akka-cluster/src/main/scala/akka/cluster/ClusterMetricsCollector.scala: >  > > + * > > + * This delegates metrics sampling to the [[akka.cluster.MetricsCollector]]. > > + * Calculation of statistical data for each monitored process is delegated to > > + * the [[akka.cluster.DataStream]] for exponential smoothing, > > + * with additional decay factor. > > + * > > + * This strategy samples and prepares highly variable data for further analysis by > > + * other entities such as load balancing routers. > > + * > > + * INTERNAL API. > > + * > > + * @author Helena Edelson > > + * > > + * @see [[akka.cluster.DataStream]] > > + */ > > +private[cluster] trait ClusterMetricsCollector { > Works for me. It made more sense in an early-on iteration but I've been wondering the same. If it is not reflecting a best practice then 86 it? What does Viktor think? >  >  > Reply to this email directly or view it on GitHub. > 
What trait ? It's gone :-)
yes :) again can't believe I didn't see that. thanks
this is never shutdown
this is never shutdown
thanks, will fix
could use      node().isLeader must be(ifNode(a1)(true)(false))  if you are so inclined. Otherwise I might as well remove that method, so please comment.
yes, that is much better, I will change to that (and look more carefully what other goodies we have in there)
Just discussed with Viktor: the number of parentheses is too high, so I am thinking about making the syntax a bit nicer. Dont let that get into your way, I can do that after you merged this and then Ill experiment with how different options look like.
ok, pushed change to this branch
uid here also?
I think I mislead you regarding the transitions to down (when reachable) by referring to https:www.assembla.com/spaces/akka/tickets/1907 In the code I can't see that we disallow this transition, but we don't have any tests for it so it is not a recommended operation. I will adjust the ticket.  Anyway, the diagram is currently not consistent. The transition from joining to down should be removed, or transitions should be added from all other states. I suggest that we remove joining->down until we have tested it.
do we need to add anything of this in the Cluster Usage page also?
I'm not sure, since the identifier while looking at it from they view of the user is hostname:port. The uid part is more internal and is described below.
sure, but you added it at another place
Yes, you're right. I'll add it here for consistency.
From what I could see in the code we only ever move nodes that are in the unreachable set to down. Yes, that arrow from joining should go.
Rephrase since "When an X is seeing that the Y it is seeing has been seen by Z" has just too many "see" in it :)  Maybe "When X can prove that the Y he last observed has been observed by all Z"
default maximum 5 (since there can be less than 5 nodes)
In other words, only one node is enough to mark another node unreachable to have it marked unreachable by the cluster itself.
Which one is next? Shouldn't this be different in 2.2 and master?
(I meant line 127 "next release")
Pitfalls? Why wouldn't I as a user having it on according to this description? Or is that covered elsewhere? In that case please link to it.
Yes, there are too many seen in it.
Very nice addition.
Hopefully we will revise this document before we make a real release :wink:
Described in the usage document. Can definitely be repeated.
perhaps   you have to stop the actor system and start a new one which will then receive a different UID.
vector clocks are foremost a data type and not an algorithm
here I believe the footnote is not sufficient: becoming reachable again should not be mentioned here
this is not true: leaving is needed in order to reliably remove the entry from the membership list, and I think we should persist that hard-earned knowledge here so that we dont forget it.
we should point out that this leads to lots of single-node clusters unless appropriate measures are taken to have them shut down
Is this true for all nodes leaving? I thought that it is only needed for reliable removal of the leader?
yes, my memory agrees, but this would still mean that LEAVING cannot be used to implement the partition handoff process
I don't think we should mention partition handoff at all now. Should be moved to "later section"
It's in the _later section_ as well. I'll remove it here.
keeping this at class-level is a bit irritating, why not just have local vals in the test cases?
yes, it is, but how can I change it to a val when it is used later, see L89
Its an extension, just call `Cluster(system)` again. Given that you just always use it twice, you could also do it without the `node` name ;-)
ah, stupid me, thanks
they are complement to each other they complement each other 
what combination was it that @jboner added runtime check on? should that be mentioned here?
That you can not use a BalancingDispatcher with *any* kind of Router.  Now that yields a ConfigurationException.  So that violates this whole section of the docs. 
Good catch Patrik. 
Your right, he made sure that there was no Router configured if you used the balancing dispatcher. Kind of defeats my whole doc. Redo.
thx, rewrite error...
After discussing with Roland, this will be changed to just disallow the Router to run on the BalancingDispatcher (ticket #2136), so the doc is valid again.
It should still mention that routerDispatcher cannot be a BalancingDispatcher.
Absolutely will fix...
Re-reading the last sentence of the above paragraph: this is wrong, the dispatchers for head and routees are completely distinctly configured. This should be made more clear in the docs.
ment -> meant
insidea -> inside
Honestly, I think I would delete this paragraph, as it does not really help and could potentially confuse people. The one above is the important one.
Ok, fine, I think you're right, but what about the config information below it?
theyre fine, I think
damn I need to configure spelling in emacs, this is embarrassing...
I think we should have a ssl section      ssl {       enable = off       key-store = "keystore"       ...     }
is this really an error that can be ignored? if ssl is configured you can probably not accept to run without it because of wrong configuration
since you use .isDefined combinded with .get      sslHandler match {       case Some(h) =>   h       case None =>
I wouldn't check for nulls here
I'd probably escalate this as this is a serious security issue
never use Option.get. Does it really make sense to keep them as individual options, they are both required, right? So make it Option[(String, String)]
Avoid this branching here by concating the empty list in case it's not defined and a List with the one item if defined.
Don't use this, use NettyRemoteTransport.notifyListeners() instead
Drop this check
I don't really like methods with innocent sounding names like "getX" that do a whole lot more than just getting something. Any suggestions as how to improve on this?
On 2012/05/25 06:22 PM, viktorklang wrote: >> + >> +  def getSSLHandler_? : Option[SslHandler] = { > I don't really like methods with innocent sounding names like "getX" that do a whole lot more than just getting something. > Any suggestions as how to improve on this? I can rename it to 'initialiseAndGetSSLHandler_?' or more verbose 'initialiseSSLContextAndGetSSLHandler_?'
On 2012/05/25 07:49 AM, patriknw wrote: >> @@ -151,6 +151,33 @@ akka { >>   >>        # (O) Maximum time window that a client should try to reconnect for >>        reconnection-time-window = 600s >> + >> +      # (I&O) Enable SSL/TLS encryption. >> +      # This must be enabled on both the client and server to work. >> +      enable-ssl = off > I think we should have a ssl section > >     ssl { >       enable = off >       key-store = "keystore" >       ... >     } > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476/files#r879933 perfect, will do
I'd prefer something like: tryInitializeSSL (dropping the _?)
Can we improve on the helpfulness of this error message?
I suggest to just wrap in a RemoteTransportException and rethrow and let the layer above decide if it should be logged or not
We definitely want to help the user identify which setting was missing
Just wrap in a RemoteTransportException and let the caller decide if it should be logged or not
Doing the changes below means that you don't need to provide the NettyRemoteTransport into this method
This should be private[akka] and be documented as internal api.
Why these changes?
it's a MultiNodeSpec and should therefore be tagged as long-running. It's that in master also.
On 2012/05/30 04:30 PM, viktorklang wrote: >> @@ -52,7 +52,7 @@ class RandomRoutedRemoteActorSpec extends MultiNodeSpec(RandomRoutedRemoteActorM >>    def initialParticipants = 4 >>   >>    "A new remote actor configured with a Random router" must { >> -    "be locally instantiated on a remote node and be able to communicate through its RemoteActorRef" in { >> +    "be locally instantiated on a remote node and be able to communicate through its RemoteActorRef" taggedAs LongRunningTest in { > Why these changes? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476/files#r900378 not sure, will try to revert
On 2012/05/31 07:46 AM, patriknw wrote: >> @@ -52,7 +52,7 @@ class RandomRoutedRemoteActorSpec extends MultiNodeSpec(RandomRoutedRemoteActorM >>    def initialParticipants = 4 >>   >>    "A new remote actor configured with a Random router" must { >> -    "be locally instantiated on a remote node and be able to communicate through its RemoteActorRef" in { >> +    "be locally instantiated on a remote node and be able to communicate through its RemoteActorRef" taggedAs LongRunningTest in { > it's a MultiNodeSpec and should therefore be tagged as long-running. It's that in master also. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476/files#r905537 okay, thanks for that
I'm confused about what this diff shows, master already has  `"lookup remote actor" taggedAs LongRunningTest in {`  so why does it show up here? probably no problem
how often is this invoked? has it been tested on linux (the blocking issue we discussed previously)
jsessionid should probably be removed from this url?
great that you have fixed ways around the SHA1PRNG issue, just to make sure I understand, what is the default if user doesn't configure anything? if default will still use the broken SHA1PRNG I think we should log warning (on Linux)
On 2012/06/07 10:59 AM, patriknw wrote: >> +   * Construct a SSLHandler which can be inserted into a Netty server/client pipeline >> +   */ >> +  def apply(settings: NettySettings, log: LoggingAdapter, isClient: Boolean): SslHandler = { >> +    if (isClient) initialiseClientSSL(settings, log) >> +    else initialiseServerSSL(settings, log) >> +  } >> + >> +  private def initialiseCustomSecureRandom(settings: NettySettings, log: LoggingAdapter): SecureRandom = { >> +    /** >> +     * According to this bug report: http:bugs.sun.com/view_bug.do;jsessionid=ff625daf459fdffffffffcd54f1c775299e0?bug_id=6202721 >> +     * Using /dev/./urandom is only necessary when using SHA1PRNG on Linux >> +     * <quote>Use 'new SecureRandom()' instead of 'SecureRandom.getInstance("SHA1PRNG")'</quote> to avoid having problems >> +     */ >> +    settings.SSLRandomSource match { >> +      case Some(path)  System.setProperty("java.security.egd", path) >> +      case None        > great that you have fixed ways around the SHA1PRNG issue, just to make sure I understand, what is the default if user doesn't configure anything? if default will still use the broken SHA1PRNG I think we should log warning (on Linux) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476/files#r942223 No, the default is what I had it as previously. It uses the internal Java SecureRandom algorithm, not SHA1PRNG as the quote states.
On 2012/06/07 10:55 AM, patriknw wrote: >> + >> +        # (I&O) Protocol to use for SSL encryption, choose from: >> +        # Java 6 & 7: >> +        #   'SSLv3', 'TLSv1' >> +        # Java 7: >> +        #   'TLSv1.1', 'TLSv1.2' >> +        protocol = "TLSv1" >> + >> +        # You need to install the JCE Unlimited Strength Jurisdiction Policy Files to use AES 256 >> +        # More info here: http:docs.oracle.com/javase/7/docs/technotes/guides/security/SunProviders.html#SunJCEProvider >> +        supported-algorithms = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"] >> + >> +        # Using /dev/./urandom is only necessary when using SHA1PRNG on Linux to prevent blocking >> +        # It is NOT as secure because it reuses the seed >> +        # '' => defaults to /dev/random or whatever is set in java.security for example: securerandom.source=file:/dev/random >> +        # '/dev/./urandom' => NOT '/dev/urandom' as that doesn't work according to: http:bugs.sun.com/view_bug.do;jsessionid=ff625daf459fdffffffffcd54f1c775299e0?bug_id=6202721 > jsessionid should probably be removed from this url? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476/files#r942202 will do
Why not throw an exception here instead of having to check for `None` in the code which uses it? That keeps config stuff local and simplifies the code which consumes these settings.
And with my above suggestion, these validations would not be necessary: simply have defaults which do not trigger them (as you did already).
good idea, but then these must be lazy, not used when enabled = false
why not test actual communication with SSL? could include non-expiring certificates with the source.
no: if the user changes this to `""` without enabling SSL, why should he reasonably expect it to work?
ok, but it is `""` as default in reference.conf then it must be removed (commented out) from reference.conf
No, actually it is `"keystore"` in reference.con. But an even cleaner solution would be to change that to the empty string, change the settings to lazy val and to not being of type Option, then force the lazy vals in `EnableSSL` to validate them. This gives exceptions only if SSL is on, and it gives them immediately, and the rest of the code does not need to be bothered.
ok, yes that was what I was thinking when I proposed lazy. good
This should be written in Scala
This should be written in Scala
This should be written in Scala
This should be written in Scala
On 2012/06/07 10:48 AM, patriknw wrote: >> +     */ >> +    @Override >> +    protected void engineNextBytes(byte[] bytes) { >> +        rng.nextBytes(bytes); >> +    } >> + >> +    /** >> +     * Returns the given number of seed bytes.  This call may be used to >> +     * seed other random number generators. >> +     * >> +     * @param numBytes the number of seed bytes to generate. >> +     * @return the seed bytes. >> +     */ >> +    @Override >> +    protected byte[] engineGenerateSeed(int numBytes) { >> +        return (new SecureRandom()).generateSeed(numBytes); > how often is this invoked? > has it been tested on linux (the blocking issue we discussed previously) > > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/476/files#r942174 It is never called
why use visibility modifiers in java actors, we never use it in Scala actors
sorry, it wan't a real actor, it is a abstract base class
maybe comment that this is the main methods meaty part
here it is known that storage is defined, so use `storage.get ! Get(key)` for clarity
missing empty () here
ah, wait: is this based on current master? if not, disregard
Perhaps use options instead of null
style thing: I would prefer a /* */ comment here, and only one for the whole method. This would probably make it more readable, especially since the syntax highlighting is not perfect.
is it possible to omit the dot before pipeTo and the parens around it?
thx, it's wrong it's based on master and should have ()
yeah, I used it in other places so it should be that here also
of course! you can even say      conterService ask GetCurrentCount pipeTo progressListener
yes, they can actually be omitted -- nice
and what happens in the other case?
 to hammer the point home ;-)
this (same Thread) is probably not correct anymore.
maybe mention that exceptions raised during callbacks are simply logged as Errors.
hmm ``ctor`` is not correct, might be associated with Actor trait. You could write Using and actor's... 
This particular section should probably be updated as a part of the patterns docs.
remove superfluous braces
remove superfluous braces
remove superfluous braces
remove superfluous braces
For actors you can use extends Actor with ActorLogging instead
Our conventions is to use brackets around log parameters.      "Message [{}] not exp....
Because it reads more naturally. In this line I want to "apply" conversions. 
It would have been nice if we could still verify the credentials before building. There doesn't seem to be a target for that in the plugin though. Maybe pushing a small dummy file?  (Now in the right place in the script)
I was thinking about that, but since the upload is the very last thing of the release script it can be done again, separately if it fails.
Sure. You're right.
Doesn't this also sync the docs and api directories to the server? Should they also go to S3?
ouch, completely missed that. I will add this back (excluding downloads) and all ssh checks.  Is it correct to use `--exclude ${release_dir}/downloads` ?
The exclude should do it.
why this FIXME?
leftover from the revert, will remove, thanks!
I have extracted some common things to `MultiNodeClusterSpec` so this can be replaced with       commonConfig(debugConfig(on = false).         withFallback(ConfigFactory.parseString("akka.cluster.auto-down = off")).         withFallback(MultiNodeClusterSpec.clusterConfig))  Should we use auto-down = off as default for all our cluster tests?
with `MultiNodeClusterSpec`, if you like
In `MultiNodeClusterSpec` this is named `cluster`
those 3 lines are identical on second, third and fourth, you can extract them to a      runOn(second, third, fourth) {
Great. I'll use that. Thanks.   'auto-down = on' should be default in user applications, but we can disable it in tests to have one less moving target. Just have to make sure we have enough tests that cover it. 
Saw that. Nice. 
Ah, didn't know. Nice. Thanks. 
concatenate these two events
I think we can move this to the base config. And turn it on for the tests we need it in. 
No biggie but mySelf is a single word and should be 'myself' - looks ugly now.  Where is controller defined? 
mySelf comes from MultiNodeSpec, I can change that to myself controller is a role, defined in this test, in the object
RE mySelf - thanks, do that RE controller - got it, thanks 
Open a new Socket on port 0 instead, obtain its port, shut it down and use that one.
Why an Array?
    case `originAddressByteString`  super.receiveMessage(remoteMessage)
Use DynamicVariable instead and do "withValue" to scope the changed value
Should be in configuration file
Is this threadsafe?
Never use return in Scala      bytecodeCache.remove(fqn) match {       case null => load via rcl       case bytecode => defineClass(fqn, bytecode, 0, bytecode.length)     }
This will throw a TimeoutException instead of ClassNotFoundException if it times out, is that intentional?
    def getOrigin(rm: RemoteMessage): ByteString = {       import scala.collection.JavaConversions._       INPUT_FIELD.in(rm).get.getMetadataList.collectFirst( { case x if x.getKey == "origin" => x.getValue }).orNull     }
This should not be in code
Why is this needed? (it's very expensive)
never do this, use akka.util.NonFatal
How can fqn be null here?
This shouldn't be added into akka-remote, it should be a sub-module to akka-remote so people don't need the extra dependencies if they don't use rcl
You're right. I was overthinking. I did a quick benchmark for contains (10 entries)  Array : 776 Bytes : 1.3 ms s.c.i.HashSet: 1088 Bytes:  0.28ms s.c.m.HashSet: 896 Bytes: 0.30 ms  j.u.HashSet: 1200 Bytes :  0.35 ms  Silly me, there is no reason not to use s.c.i.HashSet. 
Thanks for the tip. I first used 0 as port in the system settings but then didn't found a non-reflection way of findig the exact port opened. This will work nice.
We are using a blocking implementation of RCL. Also when/if we are going to support "reloading" I'd prefer to use a separate channel to do this.  The problem is that the RCL messages may be left waiting by the OrderedMemoryAwareThreadPoolExecutor for the users stuff to complete. And in the case of blocking RCL we "deadlock".  I like to be able to use idiomatic akka way for RCL communication therefore we have the separate system.  Alternatives? WDYT? Is this to much? Leave it be for the moment?
Yes, because it is called from within loadClass method which is synchronized. We have a full blown lock here.
You're right. Right now I am using a debugger to see what is going on (non-multi-jvm setup). Will do proper debug logs next.
Knee-jerk reaction from Java. ;)
You're right. It is unnecessary check.
I'd need to jarjar them in any case.  If no extra dependencies are needed can this stay here or do you think extra module is right in any case?  I'll get rid of them. From Guava I am using LoadingCache but that can be done with locks and HashMap (same as remoting is doing) and for ASM stuff I can always manually parse the constant pool table from bytecode.
Also, Arrays are horrible.
But don't create an entire ActorSystem for that, it's extremely expensive.
It should be in it's own module, so people who do not use it don't need the added weight.
Yeah. It's own module. Since it will probably only be used in dev mode. 
would be nice with `system.shutdown` in `afterAll`instead, to avoid mem leaks in case of failure
you already create an ActorSystem in constructor, and this shadows `system` in TestKit
I don't think AtomicReference should be visible in the api, isn't that an implementation detail? Now the value is also changeable from the outside, without going through `refresh`, is that by design?
I tried to make the #cache-actor-setup self contained, but I guess it's not worth it as anyone should know how to create an actor system anyway. Good catch.
Hi!  It is perhaps a bit more succinct to do this:      val stopRefresh = system.scheduler.schedule(0 millis, retryInterval) { cache.refresh() }  Cheers, 
Hi & thanks for the comment Victor, Roland pointed out in another comment that this way we avoid problems if the refresh would take longer than the `retryInterval` (schedule the refresh when the previous is done, instead of having a hard refresh timebox): https:github.com/ktoso/akka/commit/150a6a83fbce2f762b6d30f4987b31a01483056b#commitcomment-1381816 I think I'll agree on that - maybe the name of `refreshEvery` should be changed to better reflect how (where) the delay is instead?
The variant Viktor proposed will actually run the `refresh()`, while I was referring to making the sending of `RefreshCache` messages recurring. While those messages could potentially pile up, running the refresh job should be fine (if it is done synchronously). I still think that the `scheduleOnce` approach is the easiest to grasp (and also it does not leave state lingering around which needs to be cleaned up).
I feel that this wording is a bit confusing. The assumption which isn't true on Android is that` OP_CONNECT` implies that `finnishConnect` will succeed, but you could also interpret it as being that reties are needed isn't true on Android?  How about `... will succeed, which is the case on Android.` 
I don't like the word "currently" here. I'd rephrase into something like: "The aggregator pattern supports writing and actor that..." or "With ordinary actors it is not not straightforward to aggregate..." 
s/close-overs/close-over problems.  It can still close over conversational or actor state, which is a nice feature.  "never" is perhaps a too strong word, you can still do mistakes, such as accessing sender and thinking it is the sender of the original message
should it be `final`?
perhaps mention that `context.become` is not supported
this header must be checked by someone who knows more about the legal implications of having a different license and copyright
define explicit return types on all public api, i.e. `def expectOnce(fn: Actor.Receive): Actor.Receive =`
the name `handleResponse` is maybe confusing, `handleMessage`? Does it have to be public?
Is the order important, or `Map[Long, BigDecimal]`?
increase to 1 second, to avoid false test failures on build servers
better to use: `context.system.scheduler.scheduleOnce(50 milliseconds, self, TimedOut)`
Can this class be simplified by using some existing data structure that is good enough? Wouldn't `java.util.LinkedList` and its `listIterator` do the job?
Absolutely! I'll make that change.
Sure. Will make the change accordingly.
I have been weighing my options here. There was just a comment on the forum wanting to override receive. Did not make much sense to me but that's where I want to be cautious. If we all feel it should be final, I'll make the change.
I have no problem taking it out or use a Typesafe header. The MIT license is more liberal than the Apache license used by Akka, yet it is compatible. I had to put this in as I point blog posts and discussions to my code. Just let me know how you guys want it and I'm ok.
Sure. Will do.
NP. Fine by me.
These came from Jamie's post. I prefer not to change them unless you feel strongly about it. This is to retain as much code from the old example as possible so when people look at the diff, they do not have to be concerned about this change. 
I really like this much better replacement. Thank you very much!
I'm afraid not. The WorkList is reentrant as any partial function executed here can modify the list itself. This includes adding, removing, and removing itself from the partial function. listIterator only handles removing the current entry or we'll run into a ConcurrentModificationException. I've looked at my options and resorted back to implementing my own (which turns out not such a big deal). Please let me know if you find something better.
Changed 'never' to 'not'. Added a section on sender references to the pitfalls section.
yes, I'm not sure, overriding receive is almost like using become, not really supported by the Aggregator unless you really know what you are doing, might be fine too keep it open for power users
We prefer to use Apache 2 for all code in akka repo. It would be a lot easier if you stick to the same header as in the other files, otherwise we must ask layers for advice.
The closures registered dynamically will of course close over the actors innards, in this sense the sentence is wrong. What you want to say is that accessing mutable state within these closures is not problematic.
what about restarts?
Hmm, the following works but might look surprising:  ~~~scala other ! Request expectOnce {   case Response(x) if sender == other => ... } ~~~  The surprising part is that `sender` means something different in this scope; this will be natural for newcomers and understandable for gurus but possibly a head-scratcher for intermediates. We can try it out, it is `akka-contrib` after all ;-)
this is the bigger of the two problems
this might want to use the new aroundReceive which is being discussed in the context of `akka-persistence`; we should either await that before going forward with this or create a ticket so we dont forget.
this duplication would go away by always keeping one entry as the parent in the list and starting from there
since you traverse for removal I think using an `immutable.Vector` would be preferable: storing the reference in a `var` solves the concurrent updates as well, I think it is cleaner that way because the processing is done on an immutable snapshot (to avoid endless loops)
this is not a nice API: changing a boolean flag by the presence of an implicit argument
this is targeted for 2.2.x, so we should not wait, but a ticket is good
This goes into the definition of "close over." I could not find the formal definition and it seems our understandings may be slightly different. Perhaps we should stick the definition in the Akka documentation (apologies if it is already there and I just happen to miss out).  My understanding of close over in the context of an actor is as follows: "Access to the actor state from a closure that may be executed from another thread." It seems inaccurate based on what you're saying.  I'll change it as suggested.
Very good point. Yes, these short-lived actors are stateful. A restart will loose its state. The requester will need to retry the message.  This makes me look at FSM which has its state externalized. Do we actually use this to populate the actor after a restart? Do we make a copy of the state and roll back to that copy if the actor restarts. Only then can we guarantee the state is not corrupt.
Yes, this is an interesting one. I'd personally stay away from this pattern as it is incompatible with forward and actorSelection. You actually expect the message to come from the actor you send the request to. But yes, if you can live within these constraints, you could use this sender reference to qualify the message. And surprisingly, it does work.
Agree. You really have to be careful what sender you're using.
Not sure I'm clear about this suggestion. Do you mean keeping the parent as a state in the list? Would we have conflicts on reentrant calls into the list (list has a parent that is not the current parent in this iteration)?
Let me take a stab at that. From the efficiency point of view it may not be as efficient as the current implementation, though.
Could get around this by not using '+=' but a proper name like 'add' with two parameters. What do you think?
I would definitely go for `add` with explicit parameters and a corresponding `remove` just so the "names" are balanced.
I think the idea here is to always have an _empty_ head node so you could just send in `head` and `head.next` to the inner function (which could have a different name than process as not to confuse) and have the null check for `entry` at the beginning of the inner function.
FSM doesn't keep the state after restart. I think the aggregator should loose its state, or manage it on the app level.
I don't find it unintuitive that sender in this case corresponds to the sender of the `Response`. It's not a callback, it's an added receive block. 
While I am not feeling strongly about one or the other and will make the mods as suggested, I could use some education on the "nice API" comment. Since I'm relatively new to Scala and see quite a bit of precedence in using such an API, it becomes confusing to judge a nice from a not nice API. One precedence being the use of an implicit timeout on 'ask' and the other being the use of an implicit execution context on Future methods. What would make one case justifiable while the other not? Please feel free to point me to any previous discussions or literature, if applicable. Thanks.
I would support creating a ticket. Once aroundReceive is in place I'd be happy to make the proper modifications.
An implicit argument is used to capture some context of the call site, providing additional infrastructure to make things work (e.g. an ExecutionContext). Passing these along manually is just a burden and boiler-plate to be avoided. In the case of the implicit Timeout the case used to be more clear than it is now; the Timeout type was created as a wrapper for Duration precisely for the purpose of being used implicitly, since we deemed Duration to be more like Int (i.e. too generic a type to capture implicitly).  In the case under discussion here you use the implicit to change the semantics of the method call rather drastically, and it is implicit only to avoid having to pass a second argument.
A closure is a lambda expression with free variables which are bound to the enclosing lexical context; the core feature is that it transports these bindings out of that context and uses them wherever it may be expanded.
Thanks for all the input. I agree the use of Boolean as an implicit is definitely not right. Although we can argue about timeout being contextual information, this case is definitely not. It is there merely to express the append as +=. So it is all clear to me to change. Lesson learned.  Talking about context, one other programming model issue I'll have to address is to pass contextual info across actor messages. We have implicits in Scala calls through the stack. We use ThreadLocal extensively for such purposes in a threaded environment. We don't have anything like that in the actor model. Contexts have to be explicitly passed as part of the message. Just food for thought for now. It is best to have these discussions publicly so I'll take it up sometime soon. Thanks again.
Got it. Thanks! I'll put it in.
I think "closure" is clear :-) The issue is more on "close over." Based on the definition you gave on closure, are you saying that close over means just the transport of the bindings out of the context? That would be the same as closure. I'm rather thinking of "close over" as expanding and using these bindings in a place or context that can cause consistency issues. In that case the use of "close over" in the text above is correct. But if "close over" and "closure" are supposed to mean the same thing, then yes, the text should rather say  "accessing mutable state within these closures is not problematic." Thanks!
this is wrong, fourth doesn't mean that it can't be leader, hosts/ports are unknown to the test so any node can be leader when run you might want to use the new role sorting facilities that I have added in https:github.com/akka/akka/pull/483 (not in master yet)
   _.address == address  ?
That might work as well. But checking on port should be sufficient. 
I know. It slipped through. I have already pushed a fix. Planned to enforce leadership to 'first', but then decided not to and left this line by mistake. 
same port, different hosts?
Fixed and pushed checking on address now instead. 
override is not really needed, is't?
I need `override` or "The compiler says NO!". I could change the name, but that feels weird.
I would say that changed name is normal practice `(_decider: SupervisorStrategy.Decider)`
and you don't even have to change the name if you remove the `val`
INTERNAL API marker
this looks interesting
In fact, plain wrong. This unwrapping should be done only in the SerializeAllMessages case. Thanks!
stupid IntelliJ. I usually remove these but this slipped through...
IntelliJ again. After a number of imports reached it switches to underscore. I don't know why the actor.Identify import remained then... Sigh.
I'd say 'healthy *nodes*' or 'member nodes' in the cluster not 'systems'
Same here. Not 'system' but 'node'
Can you not use awaitCond(..) for the 'isAvailable' expressions instead of the sleep? 
they are true from beginning (phi = 0.0) so that would not verify much could be used for the `must be (false)` case in the next case, but we might want to check that first and second are not affected anyway  I can reduce the length of the sleep here and use awaitCond in next test - that is a good enough compromise
You sure this won't cause problems? :-)
well, all our code still compiles. No Java code is supposed to call Function.apply, right?  The alternative would be to add ThrowingFunction, but that's also ugly. 
I was just fearing that it would require people to add " throws Throwable" everywhere.
that will not be the case, you can always skip the exception in the method signature that implements interface  why is the @throws(classOf[Throwable]) needed? all methods can throw a Throwable anyway 
nope (Chuck Testa): `throw (Throwable)thr;` does not work unless you add `throws Throwable`, which you cant unless the interface also has it. (been there ). But all of this will probably be moot in a few minutes anyway.
ok, what I was thinking of is that in practice you never use Throwable directly, you use subtypes of RuntimeException and Errror, which can be thrown from any method, but sure, if Exception can be thrown it makes sense to add @throws(classOf[Throwable]) or @throws(classOf[Exception])  On Thu, Jan 26, 2012 at 9:05 AM, Roland Kuhn < reply@reply.github.com > wrote:  > > @@ -10,6 +10,7 @@ import scala.Some > >   * A Function interface. Used to create first-class-functions is Java. > >   */ > >  trait Function[T, R] { > > +  @throws(classOf[Throwable]) > > nope (Chuck Testa): `throw (Throwable)thr;` does not work unless you add > `throws Throwable`, which you cant unless the interface also has it. (been > there ). But all of this will probably be moot in a few minutes anyway. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/266/files#r386976 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
Can't do that since you pass a Throwable in, and need to rethrow it if not handled.
Why? Arrays are mutable.
Use IndexedSeq instead (Vector)
(0 until count) map { i => "\"akka:" + akkaSpec(i) + "\"" } mkString ","
What do you say about taking this one step further and remove the source parameter, and derive it from      context.self.asInstanceOf[LocalActorRef].underlying.actor
I'd probably do:  case _ => if (context.system.settings.AddLoggingReceive) new LoggingReceive(source, r) else r
for the record: the original reason for this test was to support HotSwap messages with LoggingReceive; that is gone 
me too (that is: the current me)
yes, since HotSwap is dead we can do that. Probably easier:      context.asInstanceOf[ActorCell].actor
Is it _only_ -1 or is it any negative number?
is it possible to do: akka.actor.debug.lifecycle = on?
Perhaps clarify why listener is the second argument?
Make these an enum?
No, they are not enums, not related, but it triggered me to rearrange declaration of messages similar to how it is done in scala companion objects.
thx, it should be negative, fixed
Shouldn't it be named "router"?
umm, but it is a router of workers
Name the val "workerRouter" and same with the name?
mathematically speaking it is probably more of an approximation than an estimate
yes, thats a good one to demonstrate!
ok, changed everywhere
Is util.manifest user api? I think this looks very unfamiliar to Java users, because class names always have capital letter.
methods should start with lower case
Looks good, I probaby missed the discussion on taking out the communication style. Is this a similar point to the one in the producer, basically more standard behavior?
return true? can't see in the diff where this is used, but since communicationstyle is gone, it should not be necessary?
I spoken to Martin about it and he is fine with removing the blocking style as there is no benefit in using it after the !! has been removed.
good spot. I'll remove it.
Good stuff, it's really coming together. No other comments.
Use: log.debug("ENQUEUING message in amqp-based mailbox [{}]", envelope)
Why does every mailbox have its own ConnectionFactory?
What if something blows up here?
Don't log N throw, either log or throw.
Add explicit return type
log.debug("DEQUEUING message in amqp-based mailbox [{}]", envelope)
Why a try-catch within "withErrorHandling"?
def numberOfMessages: Int = withErrorHandling { pool.withChannel { _.queueDeclare(name, true, false, false, null).getMessageCount } }
Great to see this!
withErrorHandling { pool.withChannel { _.basicPublish("", name, MessageProperties.PERSISTENT_BASIC, serialize(envelope)) } }
Isn't an empty-check cheaper than a count?
What if this fails?
Isn't PoolableObjectFactory parameterized with the object type?
Return type and parens to denote side-effecting
What's your suggestion on this? Would adding a try/catch in the catch block to wrap the exception in an AMQPBasedMailboxException be okay?
What is consider to be fatal, and what is recoverable?
A connection loss would be recoverable, an unreachable server would be fatal. But how can I differentiate the 2 cases? The library just throws IOException.
Isn't connect() a blocking operation? How long timeout do you use? A connect() throwing an exception would likely mean that server is down, right?
There seems to be no way to do this in rabbit. Rabbit always returns a DeclareOK Object when asked for a queue and that contains the message count. If you have any idea to make it better please tell me ;-)
Fair enough, but should be documented in a comment, using size for isEmpty alerts my spider senses
You're right, connection timeout is 0 by default, which means infinite. Should be configurable I guess.
Okay, I'll add a comment.
I think since 1.6.
I'm still not sold on every actor having it's own connection pool.
Is this mailbox usable with other AMQP brokers than rabbitmq?
Any need to extract selection of PERSISTENT_BASIC to config? Other message properties that might be needed/configurable?
numberOfMessages expensive? @viktorklang is hasMessages invoked often?
I think we should add some kind of circuit breaker mechanism that can be used for all durable mailboxes. I created ticket: http:www.assembla.com/spaces/akka/tickets/1734-circuit-breaker-in-durable-mailboxes
Perhaps you can place the pool in the extension instead.
Is this override needed? I see that we have it in some other extensions also. I'll take a look.
connectionTimeout should be duration, i.e. connectionTimeout = 1s
ConnectionTimeout should be Duration Duration(config.getMilliseconds("akka.actor.mailbox.amqp.connectionTimeout"), TimeUnit.MILLISECONDS)
Use akka's logging mechanism instead. pass in a log: LoggingAdapter in constructor
For the record, it is needed for extensions that are written in scala and to be used from java also. So it is correct here!
I just tested it with Apache Qpid and it worked.   The guide on rabbitmq.com says "The client API is closely modelled on the AMQP protocol specification, with additional abstractions for ease of use.", so it should work for any AMQP broker.
Hm, the other properties would be either of type "text/plain", which is not needed, or? Or BASIC, which is nonpersistent, or MINIMAL_PERSISTENT_BASIC which would be persistent, but without content type set to "application/octet-stream". 
ok, you know this stuff best
perhaps pass down `config` into the constructor so that local overrides are possible in the dispatcher config section
for every reschedule attempt, which I think qualifies as a yes.
now imagine some very creative user attaching a logger which employs an AMQP mailbox (just to be safe, you know )
Sorry, but I don't get it ;-). The message is exactly the same as in any other durable mailbox. Can you please clarify?
I'd really like to do it differently, but I haven't found a way, yet.  edit: Okay, maybe this could be done with a Queueing consumer, but it is not documented when the messages will be ack'ed. If they are ack'ed on dequeueing that would do it. The QueueingConsumer contains a java.util.concurrent.BlockingQueue with the prefetched messages... I'll try to find out how the QueueingConsumer behaves.
I just copied this from the other mailboxes, none of them passes the config to the constructor. I was already wondering, why it is passed in every time. Is this really necessary? The config is only read in the AMQPBasedMailboxSettings class.
well, then every other durable mailbox has the same problem. "everybody else does it" is not a good defense ;-)
The config being passed in here is the specific dispatcher section, while your Settings only look at the defaults. You should extract the Settings from      config.withFallback(defaults)  for a suitable definition of "defaults".
ffnet Sie eine ticket, bitte?
hr har du det: http:www.assembla.com/spaces/akka/tickets/1834-durable-mailboxes-should-not-log-enqueued-messages
Ah, I see. But I need the config in the AMQPBasedMailboxSettings class, because the AMQPChannelPool is created there. Is it possible to pass the config to it? Also, if I understand this correctly, this means, that all the other durable message boxes also just use the default values!
Currently you are creating the Settings object as an extension, which makes it scoped to the whole ActorSystem. I think in this case (with user/passname info etc.) you might want to scope it to the actual mailbox instead, i.e. not make it an extension but create it in the MailboxType constructor; that is why the specific config subtree is passed as a constructor argument.  And yes, it would make sense to look into the other durable mailbox types as well whether this change is applicable to them also. (ticket created)
Okay, thank you!
below you are reading `akka.actor.mailbox.amqp.*` from the resulting config, which would require the user to configure like this:      my-dispatcher {       mailbox-type = "...AMQP"       akka.actor.mailbox.amqp {         hostname = "..."       }     }  Instead I would recommend doing `userConfig.withFallback(system.settings.config.getConfig("akka.actor.mailbox.amqp"))` and then below `getString("hostname")`.
Oh, thanks, will fix this as soon as i get home.
Please add a case to the Java API tests, too (I merged that already, you should see the file in master and 2.2)
What do you mean? What and where should I test java api? resolveOne does not add anything that is complicated from java.
yes, but now we have at least one "centralized" test for all the pattern related Java APIs
I don't mind adding more to that, but what should I add there? I have not changed patterns, except for boy scouting the docs
 > I don't mind adding more to that, but what should I add there? > I have not changed patterns, except for boy scouting the docs  Hm, true. Maybe it is not needed at all.
two thoughts:  * remove the `new` * should we differentiate between no-reply and negative-reply?
it might be a good idea to have test case which explicitly spells out that the FiniteDuration variant is for the Java API (so that nobody mistakenly rationalizes it away)
I think we should keep it simple, i.e. not differentiate. It's just convenience after all.
yes, I thought the same but wanted to have it on the record ;-)
but it is not only for java, nice for scala as well when implicits not wanted. I included both methods in the test I wrote.
yes, I agree; it would be enough to add a comment to the test
Hmmm, this is a bit.... suboptimal  Can we come up with a clean solution to this?
can try... the problem is already illustrated with the behaviorStackPlaceHolder trick, which is that 1) the behavior stack depends on the actor instance (it already depends on instance.receive, and with this patch additionally depends on instance.aroundReceive)... but 2) we want to allow modifying the behavior stack while constructing the instance. Already newActor() contains the fixup to put instance.receive in front of the stack, and with this patch it needs the additional fixup to apply instance.aroundReceive to the stack elements.  Possibly there's a cleanup that eliminates behaviorStackPlaceHolder as well?  The cleanest cleanup would be to prohibit using become() during construction, but obviously that isn't compatible.  A possibility might be to make Actor fill in ActorCell.behaviorStack and ActorCell.actor during Actor construction... that could look like, in ActorContext:  ```scala def private[actor] setInstance(instance: Actor): Unit ```  In ActorCell:  ```scala override def setInstance(instance: Actor): Unit = {   require(actor eq null)  we can only be invoked once   actor = instance   behaviorStack = Stack.empty.push(actor.aroundReceive(actor.receive)) } ```  In Actor, the initializer for `val context` would do a `context.setInstance(this)` and therefore the instance must be set before anyone can invoke `context.become`.  We'd remove `behaviorStack` and `actor` initialization in `newActor`, and kill off behaviorStackPlaceHolder. 
Care to take a stab at it?
There is a problem though, if aroundReceive depends on Actor state, then it will blow up in ctor.
The patch comes out something like this: https:gist.github.com/2822571  It's ugly in its own way.  Tricky bits include:  - creating the instance may not run Actor's constructor (an existing instance can be returned)  - receive can depend on `context` and `self`, e.g. `akka.event.LoggingReceive` does  - handling restart of the actor  As you say, receive and aroundReceive will be called before the actor's constructor completes, which may be unexpected.  Also, while the above patch handles getting the same instance back on restart, it doesn't handle the case where a constructor is _never_ called (not even the first time). I don't know if that case is "allowed" but if it is, this approach can't work.  Side note: I think the original pull request has an issue that on restart the actor is not null inside become() during the second construction of the actor instance. So it probably uses receive/aroundReceive from the first instead of the newly-restarted instance. This is probably fixable by just setting actor to null during restart.  Side note 2: maybe it's too late at night but with either the original pull request approach or this patch, I wonder if behaviorStackPlaceHolder can just be Stack.empty; if we're using behaviorStackPlaceHolder then I think receiveMessage will NPE anyway because it tries to invoke actor.unhandled with a null actor? It seems to already be true that receiveMessage is never called unless there's an actor instance and thus an actual non-placeholder behavior. Don't know.
Remove this comment, or write something real
From user api, its: Companion object providing factory methods.
Java API alias for apply
this asInstanceOf shouldn't be needed, I think that was something added when we had problems with overloaded signatues
why change? clean() is *very* side-effecting
Is there any test which tests massive concurrent usage?
I generally prefer () to {} since () is the Unit value and {} simply infers it.
Add a FIXME here, it's a bit wasteful to create Futures for one-offs, just use EC.execute instead
this is wrong include, should be DangerousJavaActor.java
No there isn't currently.  Should I look to put something into akka.performance.microbench?  Need some pointers here.
  30 maj 2012 kl. 01:20 skrev scullxbones<reply@reply.github.com>:  >> +      breakers.multiFailureCb.withCircuitBreaker(Future(sayHi)) >> +      awaitCond(breakers.multiFailureCb.currentFailureCount == 0, awaitTimeout) >> +    } >> + >> +    "increment failure count on callTimeout" in { >> +      breakers.shortCallTimeoutCb.withCircuitBreaker { >> +        Future { >> +          100.millis.dilated.sleep() >> +          sayHi >> +        } >> +      } >> + >> +      checkLatch(breakers.openLatch) >> +      breakers.shortCallTimeoutCb.currentFailureCount must be(1) >> +    } >> +  } >  > No there isn't currently.  Should I look to put something into akka.performance.microbench?  No, I don't think that is necessary.  >  Need some pointers here.  I think what Viktor is looking for is some test that verifies functionality with concurrent usage. Share same circuit breaker between several threads, eg futures. Simulate failure and verify that it behaves as expected.  >  > --- > Reply to this email directly or view it on GitHub: > https:github.com/akka/akka/pull/493/files#r896796
Yes, exactly, once that's done I'm merging this one into master!
missing  +/**  	 3	 + * Copyright (C) 2009-2012 Typesafe Inc. <http:www.typesafe.com>  	 4	 + */
Why did you remove this section? We still have the concept of a singleton cluster. 
Because there were two identical sections. http:doc.akka.io/docs/akka/snapshot/cluster/cluster.html#singleton-cluster
Ah, ok. Good with good commit messages :-)
shouldn't this be `maintainAddressTerminatedSubscription()` ? passing in self will just execute the block, since self is local
you are right, code will not work in most cases
The code inside `maintainAddressTerminatedSubscription` doesn't guard the block with try-catch. If we throw an exception we will leak the subscription. I think we need to be defensive and add a try catch inside `maintainAddressTerminatedSubscription`.
um, but we are normally not that defensive, sendSystemMessage should not throw exception (it is even documented: "Is only allowed to throw Fatal Throwables.")
Ok, fine by me. It was just a change from the old behavior.
I guess that every test that uses actorFor and the TestConductor needs to use a lazy val to avoid this problem.  Can we document this somewhere?
yes, it's not actorFor that is a problem, but `node(master)` uses test conductor  it's a pitty that it's not possible to tag the whole Spec and have scalatest to ignore it completely instead of always creating an instance of it -- annotations not invented here syndrome
Yes, I got that it was the expansion and lookup. I meant in this way. I guess we just have to be extra careful to not do "global" initializations in the tests.  Tagging the whole spec would have been nice. 
Won't be confusing for Scala compiler if you do: awaitTermination() ?
I suggest the following in ActorSystem:      def awaitTermination(timeout: Duration): Unit     final def awaitTermination(): Unit = awaitTermination(Duration.Inf) Not Long.MaxValue.nanos because it easily overflows. Implementation gets to deal with Inf
damn, I started with default value param, but changed for Java api, but missed to remove here, will fix
I say the same - ?? thx
If there's an Error/non-exception throwable then system hangs forever
This should be in a finally block
tried Inf, but it doesn't work, throws IllegalArgumentException for most things for example IllegalArgumentException("toNanos not allowed on infinite Durations")  I can change to some reasonable big nanos.  Where do you think it will overflow BTW?
umm, catching OOME is not god either, I'll change to case e => and write a note referring to http:www.assembla.com/spaces/akka/tickets/1310-revise-exception-handling
Use a LinkedBlockingDeque instead of CLQ and do:      @tailrec def run(): Unit = dq.pollLast match {       case null  ()       case some  try some.run catch { case e = log(e) } finally run()     }     try run() finally latch.countDown()
I'd still recommend having this final and calling the other with Duration.Inf, then the implementation of that can deal with Duration.Inf as it wants
Patrik, did you see the above comment? Thanks
Might want to make these final
ok, I did that down in Impl, but you are right it can be here instead
ok, making class final and akka private, not user api
Is the reason to be able to register new callbacks during shutdown? I didn't thought that was important.  LBQ doesn't have pollLast.   I guess I could use CopyOnWriteArrayList since I know that I only add, and use remove(l.size -1) to take from right, but it It will not be any hard guarantees of order in case of concurrent add and remove.
I didn't say LBQ, I said LBD (Deque)
ah, sorry, I'll finish this up during the weekend
Np, I can do it right now.
Thx, but if is not urgent I would like to complete it tomorrow. Preparing for Ebbas birthday party now.   /Patrik  20 jan 2012 kl. 16:25 skrev viktorklang<reply@reply.github.com>:  >> @@ -494,4 +527,35 @@ class ActorSystemImpl(val name: String, applicationConfig: Config) extends Actor >>   } >>  >>   override def toString = lookupRoot.path.root.address.toString >> + >> +  class TerminationCallbacks extends Runnable with Awaitable[Unit] { >> +    import scala.collection.JavaConverters._ >> +    private val callbacks = new ConcurrentLinkedQueue[Runnable] >> +    private val latch = new CountDownLatch(1) >> + >> +    def add(callback: Runnable) { >> +      callbacks add callback >> +    } >> + >> +    def run() { >> +      for (c  callbacks.asScala.toSeq.reverse) { >  > Np, I can do it right now. >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/238/files#r371465
Don't worry about it, I've already fixed it. have a great weekend!
sorry, I shouldnt have eaten all that fudge
The crackpipe didn't help either
or the Java samples
This sounds wrong, is it true?
Why not use PoisonPill?
I think it's faster to ask all first, then await all.
I didn't validate the quality of the existing tests, but I agree that that looks very much like a PoisonPill. I can change that
First ask then await?
Would probably be even better with loner names ;-)
Shouldn't it also test that it gets the responses from the right nodes?
Great, Boy Scout!
Nice to have the actor system name in the thread names, how long can a thread name be in practice? Could they be arranged with same thread group? Maybe they are?
Good point, I'm thinking about how we could collect all created threads and do clean shutdowns etc
this would probably be cleaner to read if the Settings instance were cached, either in a field or in the constructor      final Settings settings = Settings.instance.get(getContext().system());     Connection connection = connect(settings.DB_URI, settings.CIRCUIT_BREAKER_TIMEOUT);
can this be written in a more readable way?
yes: use a default case instead of orElse (either with a guard or appoyOrElse)
you mean like:        OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1 minute) {         case _: ArithmeticException  Resume         case t: Throwable if super.supervisorStrategy.decider.isDefinedAt(t)             super.supervisorStrategy.decider(t)       }  I don't see how applyOrElse can be used here.
not tested:  ~~~scala OneForOneStrategy() {   case _: ArithmeticException => Resume   case t => super.supervisorStrategy.decider.applyOrElse(t, Escalate) } ~~~
yes, then the user decides that default-default is Escalate, which is fine by me I will go with that.
It must actually be:      override val supervisorStrategy =       OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1 minute) {         case _: ArithmeticException  Resume         case t            super.supervisorStrategy.decider.applyOrElse(t, (_: Any)  Escalate)       }  What do you think about making `SupervisorStrategy.escalateDefault` public?
this should be an akka.util.Duration, then you can do shutdownWait.dilated.sleep()
I don't think this will work, since we run tests in parallel in same jvm
Actually not: ```parallelExecution in Test := System.getProperty("akka.parallelExecution", "false").toBoolean,```  I've run the tests and the default dispatcher tests pass, but the remote tests don't.  When configured non-daemonic there is still a daemon thread.  That could have been done on purpose for all I know and the test needs modifying.
I use parallel tests run all the time: http:akka.io/docs/akka/snapshot/dev/building-akka.html#parallel-execution
it should be registerConsumer(endpointUri: String, ...)
Misses Copyright header
remove the case that does nothing? Or are you making sure that you will 'never' get a match error?
Misses Copyright header
It might be handy if the user can configure the DefaultCamelContext (like setStreamCaching and others, now it's hardcoded). Might be a feature.
Is this TODO still relevant? Change all TODO to FIXME
findActor looks out of place on ConsumerRegistry. separate in another private camel trait, ActorEndpointRegistry. Or maybe even better, add this as a method to ActorEndPointPath, 'toActorRef'.
It would be nice to have it on ActorEndpointPath. Would the path be dependent on actor system? Ticket please.
Do we want to add them gradually, or in one go at the end? There is a ticket for that.
More than happy to do so. Are there any guidelines for TODOs/FIXMEs anywhere?
Yes, I wanted to make sure that I am not getting match error. My assumption was that unless this function exhaust all the possibilities map will throw MatchError.  Also it is more explicit for the reader of this code what is happening here and what we are expecting to get.
I only saw Viktor comment somewhere to make TODO into FIXME, I don't know if it's documented.
Ah, didn't know about ticket. Check every file In one go is fine in that ticket I guess.
Yes, it is dependent on system. On ActorEndpointPath object the apply(ActorRef) -> you know the actorRef (you could keep it) and you also know system through the ref). on the fromCamelPath on the object, you could pass system (or Camel) in as a parameter. I'll add a ticket. 
I wouldn't be comfortable with AEP containing a reference to the system. Think about it from the responsibility point of view. It's not a responsibility of a AEP to find and actor. It is a responsibility of a system. I would rather have a wrapper of a system doing this (with or without implicit) or AEP singleton could have a method findActor(path:AEP, system:ActorSystem)
I have a better idea:  We could add a method to AEP findActorIn(system:ActorSystem)  This would be most natural. WDYT?
Good idea do that, that leaves the responsibility in the right place, no syste,m in AEP, and so separates the lifetime of AEP and system, since you always need to pass it in. I'll edit the ticket.
might be clearer as `case ResendState('uid', buffer)` (cant write backtick in GFM)
Is there anything that prevents `nacked` and `nonAcked` to contain duplicates which would lead to `newNacked.size` being equal to `ack.nacks.size` even though it doesn't contain all the elements asked for by `ack.nacks`?  Yes the code below should filter out those, I see that, but I haven't read the other side of the communication :wink:.
Of course, true.
No, that should not happen. 
I have fixed this `case ResendState('uid', buffer)`
I think it might be better to recommend loading the extension when the actor system is started by defining it in ``akka.extensions`` configuration property::      akka.extensions = ["akka.contrib.pattern.ClusterReceptionistExtension"] 
@patriknw Alright, how do you get a reference to the receptionist then in order to register the actors?
Looks like the recommendation for loading the extension via the config is already there. I'd just like to understand the answer to my question above.
I don't think it hurts to repeat the recommendation to start the receptions here.  When you register the actors you use `ClusterReceptionistExtension(system).registerService`, which of course also load the extension if not already done, but if you have a node that doesn't have any actors that are registered the extension will not be loaded and things will not work.  I suggest that you add description of the akka.extensions conf property above, and change ClusterClientSpec to explicitly use `ClusterReceptionistExtension(system)` instead of the `def receptionist` (remove the `def receptionist`).
This makes the code examples in the docs more clear even if it's less DRY in the specs.
or we could just set this `closeInfo` unconditionally and then the supervisors Stop command will do the right thing in case of error, no?
Yes, but the code follows this style in the other places.
That may be true, but in the end this is a style which relies on try-catch instead of actor supervision, which I think is wrong. Setting things up to deal with the unexpected is more robust.   Regards,   Dr. Roland Kuhn  Akka Tech Lead  Typesafe  Reactive apps on the JVM  twitter: @rolandkuhn
So I really can't make this look nice. It juts feels so wrong to unconditionally set this field (twice) for every connect, and create the corresponding objects, just in case something sometimes goes wrong (which is not the normal case).  I can make the code a bit more DRY though.
braces are superfluous
braces are superfluous
technically, shouldn't this be Option(in)?
Should this really leak to the user?
Are users supposed to deal with these explicit types?
Very nice clarification
Really love Akka FSM
Very nice sample!
I could argue that I know that FSM will never pass null to code which is supposed to use this. 
The type is visible if the user calls sender.getClass under the right circumstances, so we might as well document its purpose.
no, same as for PromiseActorRef
Should we really document blocking in `preStart()`? That smells bad to me. It would probably be better to spawn another child which does the actual counting, start that from an ask-Future obtained like here and have that send an Initialized message back to the Counter, which from that point on will forward message to its counting child. I know, this is much more complex, but it is what people will encounter in real life, isnt it?
Perhaps not recommended practice
Have to agree with Roland on this
I'd do the reformats in separate commit/pull request next time. Otherwise it's quite hard not to miss something tiny but important.
Sorry about that, the akka build does this automatically, I'm not sure how to disable this. Howdo you build? if we all use the same formatter it should not be a problem, which is what should be the case when you use the akka-camel build.
Maybe from now on it won't be a problem, but initial reformat could have been done in separate commit. This would make the review easier.
Race condition: t1: camelObjects.put(...)  t2: camelObjects.remove(...)  but there is nothing there because ConcurrentHasmap doesn't guarantee that writes happen before reads Effect you've got processor which is not stopped and EndpointDeactivated never gets sent.
You need to wrap camelObjects with an actor and not share it between actors. It is breaking actor principles - no shared state.
This can not be called before registerProducer, correct me if I am wrong. So, why the whole if contains stuff? Also producer could store send processor in a private field so no need to do map lookup for every message - this is slow.
to be consistent we need to either send it to the eventStream like in IdempotentConsumerRegistry or in change it and send it directly to activationTracker
Because the Actor can be restarted. The ActorRef will stay the same, the Actor object will be another one. If you store the processor in a private field, you also need to handle it in preRestart (since it is state of the actor)
what if anything in this method fails. Someone is going to wait forever for activation. We need to send EndpointFailedToActivate message.
this is not idempotent in case actor restarted
this will overwrite in case of restart
I'll have another look at this. You don't want to go to the map every time you send a message.
I don't like this auto-reformat...
I did have that in mind, and left that idea when I wanted to keep the processors and endpoints somewhere else than the Producer (so that the Camel extension just keeps track of it). If I add the endpoint and processor back as vals in the producer, register creation, I can wrap all of this in an actor.
It is now only published on consumerRegistry. Should we not  change it so that the ActivationTracker publishes these events?
I added it in, because I was not sure if it could be possible in an actor restart at failure/supervision.
Question, Terminated is not sent for restart of actors, correct? Is the only way for us to cleanup actor at failure/restart/supervision to use the preRestart and other hooks? 
I am not sure what you mean. AT tracks them and passes them to anyone awaiting for events.
Maybe something is missing in the akka standard settings? you can check the scalariform settings. (unless it's something that happened in my editor by mistake
I mean the eventStream.publish(...) code.
Terminated is only sent when actor is dead - no more restarts. I am not aware of other ways, so what we need to do is probably cleanup inside the registry in case of re-registration. I would avoid pre/postRestart  hooks.  
OK, so when registration is requested that is already registered, we stop the existing, throw it away, and add a new one? or do we leave the existing processor alive across restarts?
Since Endpoint and SendProcessor are mutable I can't return them as responses from the actor I could create for this, which would be necessary if that state would be there. I'm now opting for a guard around register and remove, using standard mutable HashMap and ditching ConcurrentHashMap. remove being called before register would mean the actor gets terminated before it gets created, which is not possible I think, when using a lock and a normal HashMap. 
Ah, you mean AT already subscribes to the events, so I can just send the events using the eventStream. I'll do that.
I think I messed that up. Later on the reformatting didn't do this.
This object can be accessed from multiple threads and it is not thread safe.
It is thread safe, 'lock.withGuard'.
through calling remove on Registry (also happens if re-registered). The ProducerWatcher also uses it in Terminated. in remove the SendProcessor is stopped. In other code I have never seen the endpoint stopped or something like that, so I haven't. 
ok, with akka we don't need locks at all - ask Viktor :) We can do the same if we wrap camelObjects with actor. Check how it's done in IdempotentCamelConsumerRegistry
hehe, yeah I know, although if you look a little deeper there are locks here and there.   The only thing is, that then that actor needs to give the SendProcessor and Endpoint back to the producer actor, in some message. But the rule is, no mutable messages, and these classes are mutable. So which rule do I break?  (BTW, Await.result blocks ;-)
What I meant is this  https:gist.github.com/1659645
Sure, that is what I also meant, but you are sending mutable messages back here: sender ! (endpoint, processor)  Endpoint and processor are mutable. So, should I break that rule?  Also, the register request response call awaits (and blocks), not in the same way as a lock, but still it is blocking, so the benefit is not so much. Also, now you have to handle the timeout, which is not there in the lock version.
I think the mutable message is a lesser of two evils here. I'd rather have nice clean encapsulation of a state in one actor rather than actor sharing state with Camel and locks to guard it. Easier to understand and manage. No accidental lack of locks. Isn't it what the Actors are about? To simplify concurrency?  When it comes to mutability of the objects sent in the response. It is a good rule to avoid it, because in general changes in the message might affect both sender and receiver. In our case changes to processor and endpoint are orthogonal to the CamelProducerIdempotentRegistry, and also we have a contract that CamelProducerIdempotentRegistry only mutates processor if the actor is dead.
Another very simple alternative would be to use preRestart and postStop and override them as final and provide preRestartProducer and postStopProducer hooks which original methods could delegate to. This is under assumption that we dont want to reuse processor and endpoint, but it could be slow.  I am not sure though if I like this alternative because of the non-standard hooks.
doneSync has nothing to do with the way we should respond to the sender of the message. It is only used together with spring transactions. So we should do the same thing in both cases.
We don't need to do this FailureResult/MessageResult business. The decision wether the client of our endpoint is going to be blocking(Sync) or non-blocking(async) is made by the client by deciding whether to use tell(!) or ask(?) style of communication.
Check this out http:camel.apache.org/async.html
this whole method can be rewritten it 2 lines: originalSender ! if (exchange.isFailed) exchange.toFailureMessage(cmsg.headers(headersToCopy)) else onResponse(exchange.toResponseMessage(cmsg.headers(headersToCopy)) 
we don't need this methods. The same can be done with camel routing, i.e. <pre> object CamelRoutes extends App{   val system = ActorSystem("Test")    val camel = CamelExtension(system)   camel.context.addRoutes(     new RouteBuilder(){       def configure() {         from("direct:a").to("direct:out").choice()           .when(header("respondTo").isEqualTo("c")).to("direct:c")           .when(header("respondTo").isEqualTo("d")).to("direct:d")         from("direct:c").process(new Processor { def process(exchange: Exchange) { println("C:" + exchange.getIn.getBody)}})         from("direct:d").process(new Processor { def process(exchange: Exchange) { println("D:" + exchange.getIn.getBody)}})         from("direct:out").process(new Processor {           def process(exchange: Exchange) {             val body = exchange.getIn.getBody.toString             println("out:" + body);              val response = new DefaultMessage             response.setBody("RECEIVED:"+body)             response.setHeader("respondTo", if (body.length() % 2 == 0) "c" else "d")             exchange.setOut(response)           }         })       }     })    val producer = system.actorOf(Props(  new Actor with Producer {     def endpointUri = "direct:a"   }   ))    producer ! "some message 1"   producer ! "some message 12"    Thread.sleep(2000)   system.shutdown()  }  } </pre> Output: <pre> out:some message 1 C:RECEIVED:some message 1 out:some message 12 D:RECEIVED:some message 12 </pre> Check this out: http:camel.apache.org/routes.html
don't need that either
Sometimes I think that what we really need here is: <pre> class Producer(endpointUri: String) extends Actor{    private lazy val camel = CamelExtension(context.system)    final def receive = {     case msg =>   camel.template.asyncCallbackSendBody(endpointUri, msg, new Callback(sender) )   }    final override def preRestart(reason: Throwable, message: Option[Any]) {     sender ! Failure(reason, Map())   } }   class Callback(sender: ActorRef) extends Synchronization {   import akka.camel.CamelExchangeAdapter    def onFailure(exchange: Exchange) { sender ! new CamelExchangeAdapter(exchange).toFailureMessage}   def onComplete(exchange: Exchange) { sender ! new CamelExchangeAdapter(exchange).toResponseMessage} }   </pre>
" Isn't it what the Actors are about? To simplify concurrency?"  Of course, for the user. There used to be many places in the akka code where no actors where used for internals. That flavor might have changed.   "When it comes to mutability of the objects sent in the response. It is a good rule to avoid it, because in general changes in the message might affect both sender and receiver. In our case changes to processor and endpoint are orthogonal to the CamelProducerIdempotentRegistry, and also we have a contract that CamelProducerIdempotentRegistry only mutates processor if the actor is dead." And you seriously find that more clear than two locks?  Anyway, I'll change it to an Actor, it's one way or the other, most important for me is a working version.  On timeout on registerProducer I'll send EndpointFailedToActivate and throw exception to the Actor.
both ? and ! are non-blocking. I have no problem with always making this async, since both are now async in 2.0.
Sure you can use routes, but not inside the actors in a programmatic way. I see that as two distinct features. Why would we drop a feature if it is something people? (We use both receiveAfterProduce and receiveBeforeProduce in existing projects)
I disagree. It's a useful feature. What we normally do is that the Producer has some transformation code to the endpoint and from the endpoint (changing headers dynamically etc). The receiveBeforeProduce and receiveAfterProduce are perfect for this.
Actually, I thought about it again. I'll rather keep the locks. No timeouts, no unexpected failure because of load, simpler. I'll ask Viktor for advice.
You can do exactly the same with routes honestly - we don't need this feature. And also If you don't mind lets just use producer as above. No registration hassle, simple reliable and we delegate the hard stuff to camel. If it's too slow we could consider creating endpoint and using:     camel.template.asyncCallbackSendBody(endpoint, msg, new Callback(sender) 
Look at the code below. This feature is already there in camel. We dont need to duplicate it. from("direct:start").transform(body().append(" World!")).to("mock:result");  More here: http:camel.apache.org/message-translator.html 
All we need is to be a gateway to camel. No need to write more code than this: <pre>  class Producer(endpointUri: String) extends Actor{    private lazy val camel = CamelExtension(context.system)    final def receive = {     case msg =>   camel.template.asyncCallbackSendBody(endpointUri, msg, new Callback(sender) )   }    final override def preRestart(reason: Throwable, message: Option[Any]) {     sender ! Failure(reason, Map())   } }   class Callback(sender: ActorRef) extends Synchronization {   import akka.camel.CamelExchangeAdapter    def onFailure(exchange: Exchange) { sender ! new CamelExchangeAdapter(exchange).toFailureMessage}   def onComplete(exchange: Exchange) { sender ! new CamelExchangeAdapter(exchange).toResponseMessage} } </pre>
I've asked Martin and Viktor to check this and see what they would like. As a user of akka-camel I'm biased to keeping certain stuff in that works really well for me. the route that you show does not allow me to add any kind of built up state (based on previous messages) in my custom producer into the messages.
And here you have the full use-case with transformation of the message:  https:gist.github.com/1664313
All dpeends on the constraints, which are not immediately obvious here. Could you elaborate?
Nice example, but you haven't wont me over yet ;-) That means that I have to do all transformations in camel processors, instead of just doing it right where I produce messages and receive responses, in an actor style. As a user I'm not excited. I'm not sure that we win much with this scope reduction IMHO. 
The registerProducer gets called in the producer initialization, remove is called on Terminate. If you use a registry actor to register the producer, and the system is under a heavier load, you would sooner get timeouts then when it is not. so that's variable. which means sometimes the sendprocessor and endpoint are not received which will fail the producer actor at startup, and sometimes not, depdending on the timeout. In the meantime the registry actor does not know that the producer actor did not get the message, and has created the sendprocessor anyway. The lock makes this mechanism simpler, if the creation of sendprocessor and endpoint succeeds, the actor gets them, and less different behavior under different load/timeout conditions. I also don't like sending back the mutable SendProcessor and Endpoint back as messages, since I don't know for sure if that can cause problems. 
can you register or deregister multiple endpoints at a time? Or do you just guard against registering the same at the same time?
@Viktor Does a Terminate happen when an actor fails in default supervision (restart or stop?)?  I'm not exactly sure on this. I haven't had time to investigate this.
You can register multiple (different) endpoints at the same time, say you have producer to "http:server1" and to "http:server2" and you get two producers for it. You can also register the same uri for more than one producer actor type, but that should result in 1 endpoint for every producer actor (since it is keyed on ActorRef). The Terminated message handling is in essence serial? so the remove would happen one at a time through the ProducerWatcher, threadsafe inside this ProducerWatcher actor, so that would not be many at a time? But without the lock you could have a register and remove of the same endpoint at the 'same time' from different threads, if the producer actor is started and immediately stopped?
Just add a proxy actor to producer and do the complicated stateful stuff in it.
Or use a processor:     from("direct:a").to("direct:out").transform(body().prepend("TRANSFORMED<").append(">"))           .choice()           .when(header("respondTo").isEqualTo("c")).to("direct:c")           .when(header("respondTo").isEqualTo("d")).to("direct:d")           .otherwise().process(new Processor { def process(exchange: Exchange) { other ! exchange.getIn.getBody}}) 
Why would you ever need: "final override def preRestart(reason: Throwable, message: Option[Any]) {     sender ! Failure(reason, Map())   }"  The entire idea is that failures are transient and should not be observed from the outside, it's between parent and child. Only thing interesting for the outside world is Terminated.
You're still being a bit vague. If you can (i.e. Camel supports that) register multiple endpoints at the same time, and that should be possible, just use a CHM and putIfAbsent, only one wins.
Ah, you mean like that.   DefaultCamelContext code synchronizes on endpoints collection for creating endpoint..  A new SendProcessor is created with an endpoint, so one at a time.  I had a CHM before, Piotr commented that there was no guarantee of order of remove and put on CHM. So I went with the maybe too safe lock on the HashMap.
a lock wouldn't give you that guarantee either.
 Hm, that's true. Does actor start and Terminate guarantee that start happens before Terminate? If not, would that be reason enough to put all this state in a separate actor? Keeping in mind that the getEndpoint is synchronized on endpoints inside DefaultCamelContext.
Something you do in preStart should definitely come before Terminated, which is published when the Terminate() system message gets processed.  You can always do a conditional remove in the case of uncertainty.  A rule of thumb in trying to go lockless is always use conditional operations: putIfAbsent(k,v), replace(k, old, new), remove(k, old,new)
receiveAfterProduce is also needed to create custom responses as explained in http:krasserm.blogspot.com/2011/02/akka-producer-actor-new-features-and.html (example: ACK after message was added to a JMS queue - interaction with the producer actor is in-out, interaction with the Camel endpoint is in-only). Furthermore it is needed to create non-blocking 'producer pipelines' as shown in http:www.slideshare.net/krasserm/system-integration-with-akka-and-apache-camel (although since Akka 1.1 this is possible with futures as well) - see slides 42 and 43 
Important point, I forgot that we also use it like that for sending over ActiveMQ, which I think is the same. 
Ok. So CMH has your approval if I use it that way? I'll have to work out the conditional ops, but thats fine. 
CHM is fine, we have a ticket to switch all CHM usage to Ctries or NBCHM in the future, but that s an implementation detail.
re in-only and ACK: --------------------------------------------- you can use camel route:  onCompletion(constant("done"))   to send confirmation back.  re pipelines and custom responses: ----------------- They are also covered by camel see below: from("direct:a").to("direct:out").transform(body().prepend("TRANSFORMED<").append(">")) .choice() .when(header("respondTo").isEqualTo("c")).to("direct:c") .when(header("respondTo").isEqualTo("d")).to("direct:d") .otherwise().process(new Processor { def process(exchange: Exchange) { other ! exchange.getIn.getBody}})   We really do not need to reinvent existing features.
You make the assumption that the producer actor sends the message to a Camel route which need not necessarily be the case. Furthermore, my initial idea with akka-camel was to have actors processing messages between endpoints instead of Camel processors. Although everything could be done in Camel as well, the threading model is completely different, making it trivial to implement stateful processors, for example. I just didn't have time to further work on actor-based EIPs which still would be a nice to have IMO.
I still don't see why would we want to duplicate the features existing in camel. Can you give a specific example? 
What wrong with the example I just gave? This is similar to asking why to have sbt when there is Maven ;)
Are you referring to slide 42? There are at least two ways of solving that problem without adding this feature. Camel way:  <pre> context.addRoutes(new RouteBuilder(){ def configure = {   from("direct:a").to("jms:someuri").onCompletion().to(actor) check ActorRouteDefinition }) val producer = new Producer("direct:a") producer ! "some message" </pre>  Actor way: <pre> val producer = new Producer("jms:someuri") val actorWeWantToForwardTo = ... actorOf(Props(new Actor{    def receive = {        case response : Message => actorWeWantToForwardTo ! response        case request => producer ! request    } })) producer ! "some message"  </pre>
* No I was referring to implemeting stateful message processors based on actors.  * Again, in your last response you are making the assumption that a Producer actor is sending messages to a Camel route. That's only a special case. There are cases where you cannot control the destination endpoint implementation and need a mechanism on the actor side to send custom responses.  * As a (IMO more elegant) solution to your 'actor way' example, use monadic composition of futures as shown on slide 43 of my presentation (http:www.slideshare.net/krasserm/system-integration-with-akka-and-apache-camel). This actually *is* an alternative to receiveAfterProduce. The receiveAfterProduce callback was introduced at a time where non-blocking future composition wasn't possible in Akka yet. More important, there are users out there that make heavy use of receiveAfterProduce. So you should ask them on akka-user whether they are willing to migrate to an alternative solution.  But the most important issue is related to using a producer template in your proposed Producer trait implementation:  * Although the producer template exchanges messages with the endpoint in a separate thread, it uses a blocking send method that blocks a thread of Camel-managed thread pool. This makes usage of endpoint producers that support non-blocking IO (such as camel-jetty or camel-ahc) completely useless. * Whatever the implementation will finally look like, supporting non-blocking interactions with endpoints (on both Consumer and Producer side) is a *must have*. This is a prerequisite to have non-blocking message processing routes based on Akka actors. In these routes it is important that no single thread is blocked waiting for a response as shown in the sequence diagram of http:akka.io/docs/akka-modules/1.2/modules/camel.html#asynchronous-routing-and-transformation-example (below the code section). This is not possible with your proposed producer template based implementation. So please don't sacrifice scalability for a simple implementation.  Furthermore, a discussion around around alternative implementations in Camel should also be made on akka-user. We should let users decide if (or how much) of Camel route programming they want to do. My initial idea was to expose only endpoint URIs to the application developer and to do the plumbing (route construction) and message processing only with actors. This *is* an alternative to the Camel DSL and IMO it is good to have Camel DSL alternatives (for reasons I'm happy to explain elsewhere).  
re template issue: --- The name of the method in a template is quite misleading - it suggests it does things in async way. This means we need to create the endpoint and processor and shut it down afterwards.  I am glad you(Martin) are reviewing this code so we can avoid traps like that. * Would you like to have a look at ActorComponent as well? * Also there is an issue with Message dependency on CamelContext - can you suggest any elegant way of getting rid of this dependency?  re receiveAfterProduce: --- The only reason I was suggesting we drop it, is because it would allow making the Producer implementation MUCH simpler, without the need for deathwatch, CHM and extra actors. Since it could be  done elegantly - as you suggested - with futures, than it should not be an issue for the users. 
There is some good discussion going on here. At the same time I would like to close and merge this pull request. Leftovers can be done in another ticket. The decision to drop/replace receiveAfterProduce should be a ticket on it's own, I would like to continue with the next tickets.  Can the pull request be merged?
Do you want to close this one, and create another pull request based on the same changes, so we can see changes you applied since this discussion started? Ideally without reformats...:)
Ok that is probably a good idea. There are too many comments here now to see what is what :)  I think the best thing is that I take the latest wip-camel with recent merges, rebase my changes on top of that and exclude the reformats, as one commit, and create a new pull request. I'll have to do that a little bit later today though.
@piotrga: doing IO asynchronously in a separate thread != non-blocking IO. That way the template method does what its name says. It's only that it doesn't use the later introduced Camel AsyncProcessor interface that is needed to leverage the non-blocking IO functionality of some producer endpoints.  I'll take a look at the ActorComponent and the Message dependencies when I have time.
why not val (endpoint, producer) = camel...
so, we are not restarting endpoint and processor?
why do we create this stuff upfront even if they already exist? when prev =! null
Do we really have to use remove(key,value) instead of remove(key)? We should be in thread-safe environment here.  If we are doing it just in case, it would be an alarm bell.
shall we stop the endpoint too?
Can you please explain why do we need to treat done sync differently than done async?
Why do we do receiveAfterProduce only in done sync? Is this the convention we are making up?
Why don't we copy all the headers? Is this performance optimisation?
copy/paste? naughty :)
just extract method, man :)
Really? You need to test that?
what about: (processor.isStopping or processor.isStopped) must be (true)
what does it do?
this would suggest that remove might not be so good name for it. Maybe unregister?
unregister is more likely to stop some things than remove
do we need to check it again here? It was already tested in other tests.
What if we awaited deactivation? Would we have to check the isStopping?
i don't think so, The endpoint can be a singleton, so that could mean you stop everyone's endpoint. I could check withe the isSingleton, but would have to test what really happens if we stop it. It wasn't in the original akka camel code.
because you want to use putIfAbsent, and you need to create what you want to put. Nothing much happens in the constructors though.
correct. We only start when it's not there yet, no unnecessary stop/start
Oh forgot about this one. will do
we are doing it in both. it's not so easy to see in this commit. (sending the message to self, and then            case res: MessageResult  receiveAfterProduce(res.message)     case res: FailureResult  receiveAfterProduce(res.failure)
will do. unregisterProducer
I would still like to know, since the context is a little different.
It makes the akka-camel part of the full build, and is executed as part of it
I'll remove it. the actor instance would have to die/stop, replaced by new instance and die again, still attached to the same ref, for anything weird to happen, in which case remove(key,value) would only make sure in the case that this happens exactly at the same time, that remove is only called once.  I don't think that is possible. worst case you get a EndpointFailedToDeActivate
I think these comments were left over from the previous pull request. default restart stops actors. Unless user overrides this behavior
Was in original code. I left it purely because of that, maybe Martin can tell us the idea behind it.
Not really I guess, especially with the SharedCamelSystem.
Sounds like the title of a rap movie.
Use Props.empty instead guys.
isn't it slow? can we do it in lazy way? Otherwise restarts would be slow, wouldn't they?
No wait! I just realised why it's done that way. It's an optimisation. If exchange is done in sync way it is safe to call receiveAfterProduce because we are in the same thread as when we called produce.
see my comment above ("No wait! I just realised why it's...")
re: "Use Props.empty instead guys." Genious. And obviously it's in the docs... I just read it... :)
    logConfigOnStart = on  
here you should create a reference to the section *Logging of message invocations on certain actors* in testing.rst
BTW, it's worth mentioning here that those log at DEBUG level and you must enable debug log level with      akka {       loglevel = DEBUG     }
Rename FaultHandlingStrategy to SupervisorStrategy
Can actor be null here? and if so, what should be done?
if actor is null here?
if actor is null here?
perhpas rename all things "FaultHandl" to SupervisorStrategy
cleaner and cleaner :-)
that was not possible
these finals don't actually do anything, right?
I think our convention is to use [] around variable data in log messages, at least Jonas has pointed that out for me [{}]
Atleast they signal thaty they are constants and are as such inlinable.
nope: they are not constant at compile time, hence no constants. What should be inlined?
just verified that the bytecode generated with and without `final` is identical  (vals are always private final fields)
but isn't there an accessor method that is different?
ah, right. But according to http:stackoverflow.com/questions/3961881/why-defining-class-as-final-improves-jvm-performance this annotation does not really change anything, so my main concern was with the field itself being final.  So, while this patch does not change inlining characteristics, it does make it impossible to subclass Settings, which could be achieved with less noise by making the whole class final.
Alright, I agree, we shouldn't do this because of superstition
This doesn't look like java code
Does this mean that resize will not be done for ask?
There should also be a section about the actual ask/? change, move from ActorRef to pattern, with code samples v1.3 vs v2.0 Do separate v1.3/v2.0 samples in java also, since the syntax is changed.
I think some more things are needed in the migration kit. Add the needed implicit conversions in OldActor so that existing code works inside actors. Add needed implicit conversion in this package.scala file so that existing code works outside actors, i.e. in tests. 
ask() calls tell(), which should resize
good point, will do, including specific section in migration docs
Its a shame that the CPS plugin does not work for Java ;-) (good catch, will fix of course)
Document as internal and make final
I like to move it move it
should be private[akka]
I'd definitely recommend making DeadLetterActorRef a subtype of EmptyLocalActorRef, and drop DeadLetterActorRefLike
why is this class public, and not documented. I'm guessing this should be private[akka] and documented as internal (Java users will be able to use private[akka]-things, right?)
add ": Unit"
add ": Unit"
Does AskSupport need some overall ScalaDoc?
move f inside flow
move f inside flow
It dont work because constructor of ELAR supposed to `init()` while constructor of DLAR cant `init()`.
sure, everybody likes Davy Jones, dont they?
This will always be false, since `left` can at most be 1. I think the following would be more correct:   - `left` defaults to `throughput`  - decrementing will always to the right thing, also for negative values, since  - continuation condition is `left != 1` (unless I misremembered what negative throughput is supposed to do, I thought slurp all mode)  Also, wouldnt it cost less to check `deadlineNs == 0` instead of `!dispatcher.isThroughputDeadlineTimeDefined`? This can give a false positive and hence one potentially extraneous message processed in the exact nanosecond that nanoTime wraps around.  But overall I very much agree with the approach, much more readable.
correction: `deadlineNs` should have 1 ored in for a defined deadline to make sure that the value does not accidentally come out as zero. Im sure one extra nanosecond will not kill anybody.
I prefer L over l since l looks like 1 0l => 0L
Worrying about nanoTime wrap-around would also require changing to `(System.nanoTime - deadlineNs) < 0`, since that has the correct overflow behavior. This is only half-way academic, since nanoTime is documented to not bear any relation to absolute time, i.e. a malicious VM could potentially start that counter a few minutes shy of the wrap-around, just like the Linux kernel does with its `jiffies`.
Should of course be max here, so I fixed that.
I think this clears things up
Yes, it does: you chose to spend one or two extra cycles in order to keep this most central part of our source code readable. Fair enough ;-) (this shall also serve as my official smiley wrt. this pull request)
are we sure thats an immutable.Seq?
isnt this the default? (I mean Duration.Inf)
the local def was only used to get around the old clunky syntax, can make it nice and slick in one expression now 
Yes, Duration.Inf is internally treated as previous None
maybe also add an implicit for the makeDecider(causeAction) variant, which might be nice for composing strategies.
and you think I didn't try? I got a very strange compilation error about missing class $1 when using wildcard import in java
very strange, indeed. It would be possible to work around that compiler bug (it is adding the generated closure to the public static members, but it is broken presumably because it is indeed nested within the MODULE$) by using `lazy val`, but that is too expensive for this minor wart.
added it, with the twist that it needs to go into a new ...LowPriorityImplicits trait in order not to clash with this one for empty lists.
@havocp I suggest that we should add serialversionuid to all serializable config classes
I was thinking about and wasn't sure what to do. I read that by default the serialversionuid will be a hash of the class fields and methods, so if anything changes the version changes; that seems like a very safe behavior (will catch any kind of unsafe mismatch between serializer and deserializer)... the alternative is that if the serialization changes we remember to manually change the serialversionuid, right? Someone is going to screw that up sometimes, though it would also avoid "gratuitous" serialization format incompatibilities on occasion no doubt....  I doubt we should commit to freezing the serialization format... maybe the policy should be that you need the exact same library version on both sides ... (I don't even know exactly how akka is using serialization, so not sure). If the format is subject to change at any time, maybe it's better if it changes more often but reliably always complains about potential incompatibilities...  I need more context probably.
  4 feb 2012 kl. 23:40 skrev Havoc Pennington<reply@reply.github.com>:  >> @@ -28,7 +29,7 @@ >>  * with a one-level java.util.Map from paths to non-null values. Null values are >>  * not "in" the map. >>  */ >> -final class SimpleConfig implements Config, MergeableValue, java.io.Serializable { >> +final class SimpleConfig implements Config, MergeableValue, Serializable { >  > I was thinking about and wasn't sure what to do. I read that by default the serialversionuid will be a hash of the class fields and methods, so if anything changes the version changes; that seems like a very safe behavior (will catch any kind of unsafe mismatch between serializer and deserializer)  Yes, that is the problem, it is normally too safe, you can't do any changes without breaking serialization if you don't add the serialversionuid.  > ... the alternative is that if the serialization changes we remember to manually change the serialversionuid, right? We have to be careful, but then we have the option to do changes. Adding fields (and changing methods) is possible.  > Someone is going to screw that up sometimes, though it would also avoid "gratuitous" serialization format incompatibilities on occasion no doubt.... >  > I doubt we should commit to freezing the serialization format... maybe the policy should be that you need the exact same library version on both sides ... (I don't even know exactly how akka is using serialization, so not sure). If the format is subject to change at any time, maybe it's better if it changes more often but reliably always complains about potential incompatibilities... >  > I need more context probably.  Serialization is used in the remoting. Different versions might be needed when doing rolling upgrades and when talking cross different systems. Demanding exact same version on both sides is too limiting.  >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/298/files#r417003
so does the serialization need to be guaranteed just like the ABI? I think almost any substantive code change would break the default Java serializer... it's tempting to implement read/write by hand to avoid that risk if the serialization needs to be stable. then could e.g. make the deserializer handle both old and new versions, and things like reordering fields wouldn't break it.  if there's no guarantee, if you're doing a rolling upgrade or something how would you know whether a particular upgrade will have a problem... 
On Mon, Feb 6, 2012 at 9:17 PM, Havoc Pennington < reply@reply.github.com > wrote:  > > @@ -28,7 +29,7 @@ > >   * with a one-level java.util.Map from paths to non-null values. Null > values are > >   * not "in" the map. > >   */ > > -final class SimpleConfig implements Config, MergeableValue, > java.io.Serializable { > > +final class SimpleConfig implements Config, MergeableValue, > Serializable { > > It is even documented in JavaDoc of java.io.Serializable (last section) that it is strongly recommended that all serializable classes explicitly declare serialVersionUID values.   > so does the serialization need to be guaranteed just like the ABI? I think > almost any substantive code change would break the default Java > serializer...   No, it's not good, but not that bad. You can change methods (which is pretty useful) and add new fields, if you can use default values (or null).   > it's tempting to implement read/write by hand to avoid that risk if the > serialization needs to be stable. then could e.g. make the deserializer > handle both old and new versions, and things like reordering fields > wouldn't break it. >  Yes, that could be done, but I think the config lib will rather stable and I think that would be overkill. Reordering fields doesn't break serialization (afaik) if you define serialversionuid.   > > if there's no guarantee, if you're doing a rolling upgrade or something > how would you know whether a particular upgrade will have a problem... >  well, we could say cross minor or micro versions it is guaranteed to be compatible without serialversionuid we can't do any changes at all, not even recompile the code (may be different with different compilers)   > > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/298/files#r420554 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
sounds good. I'm not trying to avoid the serialVersionUID, just understand what the maintenance requirements are, thanks.  
I pushed a new 'serializable' branch to typesafehub/config with serialVersionUID and some tests that should fail if we make incompatible changes.
Will this mean anything since you don't know which system or port?
When is the Random instance cleaned up?
Shouldn't we make this configurable?
If we make PortRangeStart pull from Config in AbstractRemoteActorMultiJvmSpec, then we can do:  override def PortRangeStart = super.PortRangeStart + 2000
haha, nice bug
yes, this is good!
wouldnt it be better to keep it abstract in order to force explicit assignment per test?
Or even reserve a range so tests don't need to know other tests ports
This was old code I moved around. Been around since Akka 0.3. Perhaps does not make sense anymore.
Not cleaned up. Used for the lifetime of the router. 
It is configurable since you can override it.  I think a default is good, but make it abstract if you prefer.  The range is start + nr-of-nodes
But we don't really have to override it since multi-jvm tests are executed in sequence. We just need to make sure they use a different range than normal remoting tests. Don't think it is worth adding the extra complexity of being forced to override it in every test without it being necessary. 
adding ticket for this
Technically "unknown throwable"
What happens if the duration is 0? what if it's negative? Is it recommended to have delays that lasts for days?
My point is that you shouldn't have to know what range other tests use. So introducing, in the baseclass, a reservation method instead that they all can use:  val portrange = reservePorts(10)  10 nodes needed
Then here you could just do: portRange zip remotes
idx would be the port
Ok. You really think somebody cares? But I'll change it. 
Good question. I don't know. This router was contributed by some guy. What do you suggest? Set a cap? Do we do that for other timeouts? 
That's nice. I'll add PortRange method. 
Should we not have a check for negative Durations in Duration?  But I'll add a check for <= 0 in the Router. 
this is wrong, there is no else, so there will be NPE here if throwable eq null, put everything in the match instead      throwable match {       case null => "Unknown Throwable: was 'null'"       case ae: AkkaException  ae.toLongString       case e                  "%s:%s\n%s" format (e.getClass.getName, e.getMessage, stackTraceToString(e))
    if (within <= Duration.Zero)
    clientAddress.getOrElse("no address")
    clientAddress.getOrElse("no address")
    clientAddress.getOrElse("no address")
My point is that you don't. They run in sequence. No clashes. Just need to be different than remote test port range. Which this commit fixes. You want to generalize more than needed, feel free. 
Damn. Missing the else. Thanks Patrik. Sloppy of me to let this one slip through. I'll fix ASAP. 
I didn't write this, just formatted. But I'll fix these. 
I know. The reviews are good opportunity to cleanup whatever is noticed, that's the way we roll all the time.  5 feb 2012 kl. 09:05 skrev Jonas Bonr<reply@reply.github.com>:  >> @@ -135,11 +120,8 @@ case class RemoteServerClientDisconnected( >>   @BeanProperty val clientAddress: Option[Address]) extends RemoteServerLifeCycleEvent { >>   override def logLevel = Logging.DebugLevel >>   override def toString = >> -    "RemoteServerClientDisconnected@" + >> -      remote + >> -      ": Client[" + >> -      (if (clientAddress.isDefined) clientAddress.get else "no address") + >> -      "]" >> +    "RemoteServerClientDisconnected@" + remote + >> +      ": Client[" + (if (clientAddress.isDefined) clientAddress.get else "no address") + "]" >  > I didn't write this, just formatted. But I'll fix these.  >  > ---  > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/297/files#r417231
I know. It is good practice.  I'm just tired of this commit and ever more that I have to write this in 14 degrees Celsius at home now since the heating system is broken again. 
I very carefully test this :)  (sorry for the duplicate line)
Which license is this? Is it a complete rewrite?
See the the description of the PR and the FIXME comment below. The orignal Mozilla code is MPL. JChardet is a port of that, but my code is in fact closer to the original Mozilla code. Anyway, the license information is here:  http:jchardet.cvs.sourceforge.net/viewvc/jchardet/jchardet/LICENSE?revision=1.1&view=markup
btw, I do not know what is considered a complete rewrite. While the structure of my code is very different, the getNextState method is the same, and the vectors cclass and states are just the evaluated versions of the original ones that had expressions there.
I followed the established idiom in that file, will fix all occurrences of that same pattern.
Does it work with "deploy.config.getMilliseconds(retryKey).millis" ?
case (sender, _: ClusterDomainEvent | ClusterRouterActor.RetryTick) => List(Destination(sender, routeeProvider.context.self))
I'd probably prefer a simple if:      val interval = routeeProvider.settings.retryLookupInterval     if (interval.isFinite)  Retries enabled        retryTask = Some(context.system.scheduler.schedule(interval, interval, self, RetryTick)(context.dispatcher))
Doesn't schedule take a FiniteDuration?  11 maj 2013 kl. 19:57 skrev "Viktor Klang ()" <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/routing/ClusterRouterConfig.scala:  > @@ -308,6 +335,14 @@ private[akka] class ClusterRouterActor extends Router { >        routeeProvider.nodes = s.members.collect { case m if routeeProvider.isAvailable(m)  m.address } >        routeeProvider.createRoutees() > > +      if (routeeProvider.settings.isRouteesPathDefined && retryTask.isEmpty) > +        routeeProvider.settings.retryLookupInterval match { > +          case interval: FiniteDuration   I'd probably prefer a simple if:  val interval = routeeProvider.settings.retryLookupInterval if (interval.isFinite)  Retries enabled    retryTask = Some(context.system.scheduler.schedule(interval, interval, self, RetryTick)(context.dispatcher))   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1429/files#r4183728> .
Doh, you're right.
I guess so. Shall I change all places? I don't know why we have used this style everywhere?  11 maj 2013 kl. 19:17 skrev "Viktor Klang ()" <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/ClusterActorRefProvider.scala:  > @@ -91,13 +93,26 @@ private[akka] class ClusterDeployer(_settings: ActorSystem.Settings, _pm: Dynami >            if (deploy.routerConfig.isInstanceOf[RemoteRouterConfig]) >              throw new ConfigurationException("Cluster deployment can't be combined with [%s]".format(deploy.routerConfig)) > > +          val routeesPath = deploy.config.getString("cluster.routees-path") > +          val retryLookupInterval: Duration = { > +            if (routeesPath == "") Duration.Undefined > +            else { > +              val retryKey = "cluster.retry-lookup-interval" > +              deploy.config.getString(retryKey).toLowerCase match { > +                case "off"  Duration.Undefined > +                case _      Duration(deploy.config.getMilliseconds(retryKey), MILLISECONDS)  Does it work with "deploy.config.getMilliseconds(retryKey).millis" ?   Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1429/files#r4183645> .
No, leave it be. Would be nice with an approach that doesn't require ugly isInstanceOf and asInstanceOf but also doesn't require pointless "case _ => do nothing"
This val could be eliminated by just moving the expression into the first call of offerElement (eliminating the risk of the ``nanos`` val being improperly used in the code block)
Yup, sounds like a good idiom in order to avoid the usage in the locally defined method.
`log.warning("Stopping proxy after [{}] retries", numRetries)`  the actor path (self) is already logged since log comes from ActorLogging
This is not true any more, in master (and 2.2-M3) remote death watch takes care of this. Therefore I wonder if you need the special ProxyTerminated message. 
I'm not sure if that definition of 0 is good. It would be more natural to use 0 as "disable retry feature", and a high value (MaxInt, no special value is needed) as no limit.
ok, the unsent messages can be a reason
Given that `Terminated` is delivered, is this needed? Perhaps it is good with a limit anyway.
It will be the failure-detector actually, no?
ah, left over from my first implementation, thanks
I don't completely understand this part. What is the scenario here?
yes, way cleaner.
The idea is that if the leaving node is the only one remaining, it should be shutdown. This will only happen when it moves itself to Exiting (and all others have left). I added a test in SingletonClusterSpec that I think will fail without this. single node leaving itself.
But the comment "in case the Exiting gossip never arrives" is wrong!
That is what I guessed. Does it really need to move itself to Exiting? I mean this is fine as it is, it is not a scenario that is worth optimizing, I am just curious :)
this DOWN + EXITING relationship should be expressed by a method so that it can be kept consistent. wdyt?
Why 1 to 3?
This one? :)
I think it means gossip 3 times. Shouldn't this be configurable?
Yes, I know that :) But the question was why 1 to 3 and not 1 to 5.. i.e. why 3 times?
So that is covered by my second sentence :P
LOL, really? 3 is "best effort"? :D
Well, strictly speaking, the system will work without any gossip here, the idea is try to serve with our last breath :) 
Thank you @drewhk for clarifying my intent with this code. It is a special case, when the leader is leaving, the cluster will behave better if it tries to tell others that he is exiting before shutting down. Otherwise others will detect him as unreachable and so on. 3 felt like a good effort. I don't really see the point in making this configurable, but I can of course do that if that is our policy.
Ok, just document it (i.e. _why_ '3' was chosen, so the next guy knows how the magic happened)
Make it a named constant -- and we can decide if we want to expose this in config later. 
ok, thanks for feedback
I think this comment has gone a little stale now
recalculating upMembers and exitingMembers looks fragile, how about just `localMembers != newMembers`?
remember that equals for `Member` is based on the address only, so that kind of check would not detect status change
or better: derive newMembers from localMembers, upMembers and exitingMembers; keep things DRY
same here: calculate downedMembers and then use that to derive the newUnreachableMembers
I have not looked at the tests yet, but Id like to see it verified that without this extra gossip the new leader will actually do the right thing (eventually); is this possible?
I don't get the comment above this line ` Everyone else that is not Exiting stays as they are`. There is no check for `Exiting`. Everybody else stays as they are. It was even wrong before, since we removed `Down` members as well.
There will be no new leader (that performs any leader actions) until convergence, so the just shutdown node must first be detected as unreachable, and then downed (auto or manual), which will in the end result in a not so graceful leaving. That is why these extra gossips makes things better.
yes, the comment might be stale, but note that Down members can only exist in unreachable
so the comment was wrong before also, in another sense
which essentially means that we depend on these gossips for seamless operation of normally graceful operations, and hence it is more than pure optimization; it looks like this wants to be fixed in the sense that the designated new leader should take action in this case even though there is no full convergence
wont the ClusterCoreDaemon be shut down by cluster.shutdown(), thus making this superfluous?
When we discussed this solution we said that we should do best effort leaving. The real thing will be implemented in 2.3, when we really need hand off and stuff. Or do you have a suggestion of how to solve it? Changing leader semantics for leaving is not correct, I think. The only thing I can think of is to treat unreachable Leaving special, so that it is always handled as auto-down. However, I don't see the big problem, unreachable (and downing) can always happen.
yes, but after MemberExited it could come a MemberRemoved, which will trigger EndHeartbeat, before cluster is completely stopped, and the other side will stop monitoring (for a while), so this prevents this guy from sending out anything more.
The difference is that a leaving leader even breaks in the happy case: if any other node leaves, everything is just fine, but if it happens to be the leader then these three random gossips are all that stand between normal function and needing operator intervention, and the distinction is rather arbitrary (as any node could be the leader). If the new leader would just detect convergence if the previous leader is LEAVING and everybody else has seen that, then the problem would be solved.
okay, then this may want to have a comment somewhere
Just a quick question. Why didn't we allow `Exiting` nodes to become unreachable before? That feels weird.
but my point is that is unlikely, probably more unlikely than a normal unreachable failure  please don't complicate the reasoning around convergence just because of this, it is confusing as it is  it is not as simple as you suggest, what if that leader leaves, and then that leader...
that was a fix I did a few weeks ago, yes, that was not all good and therefore I reverted it here
I agree the `Down` and `Exiting` check should go into something like you did for the gossip with `convergenceSkipUnreachableWithMemberStatus`.
and if sending message to 3 different hosts fails, then the network **is** broken, and it is just correct to mark it as unreachable and require downing
Okay, convinced. Please add a note to the docs (if not already there) that downing might be required even when the failure happens late during a graceful exit, just so people are not surprised (it should have been obvious to the cluster  ).
*the* bug was here, missing the assignment to `state =` but I see now that the bug did not exist before my changes, so it was introduced by my changes ah, sorry, then it was not easy to spot 
in this case rebasing removed the evidence ;-)
This split makes it all much more readable. Very nice.
Shouldn't the "Normally this is handled..." be rephrased and broken out into a _real note_ that ends up in a box instead?
On a second thought, does this remove IDE related files?
yes, which I think is good: building a release should not be affected by anything which a clean checkout would not have contained, otherwise the build is not reproducible
+1 on a complete wipeout
So after release everyone has to regenerate the IDE settings? Not that I cannot live with the thought, but we should document it :D
thats the purpose of the prompt which needs a yes before the cleaning happens; is that not enough?
It is good, but I would add a warning at the top of the script to the description of the steps.
fine, a separate clone can also be used (convenient for other reasons as well)
"without exception" can be misleading. Either capitalize Exception, or rephrase "without providing an exception"
very true, brain turned off
I completely understand :D
Document as Internal only I recommend adding an @deprecated annotation that prints a warning that it's internal only
I think we should make it so we always use the behaviorStack (i.e. remove processingBehavior) and make it so you can't pop off the last entry on the stack
This should then drop all but the bottom entry
This file is probably not supposed to be here?
This should be: Stack.empty[Receive].push(receive)
no it was added by misstake
Wouldnt it be nice if the `processingBehavior` just were the bottom element of the stack at all times? One field removed, special dispatch logic removed,  On the other hand it would make a non-hotswapping actor slightly larger due to the Stacks object overhead (instead of using one common empty instance). Hmmm. I guess it might be fine as it is.
Hmm, that will pollute our own compiles. Isn't there a way to define a ordinary package private visibility in scala? I never thought I would miss that java feature, but here I actually do.
Pushed another commit without the processingBehavior. Digest and let us choose the best...
Don't know :S
val head = behaviorStack if(!head.isEmpty && !head.tail.isEmpty)   behaviorStack = head.pop
This method does nothing
I wanted to avoid new Stack, which is done by tail
thx, missed the assignment      behaviorStack  = Stack.empty[Receive].push(behaviorStack.last)
I think you can rip the match out and simply do:  if (behaviorStack.head.isDefinedAt(msg)) behaviorStack.head.apply(msg) else unhandled(msg)
Great observation, thanks
thx, of course one thought, would it be more efficient to assume that most messages are matched and catch MatchError instead of using isDefinedAt?
This can't be right
This can't be right either
true, it should of course be dispatcher.reportFailure(e) in all places
I think it's better to just report it and let the dispatcher log it
Or perhaps log this at DEBUG, wdyt?
OverKill with report AND log?
Remove logError and only use executor.reportFailure(e)
I don't think the dispatcher has enough context information to do proper logging. Perhaps the name reportFailure is misleading, it's really a decision about rethrow or not, as it is now.
same answer here
What is the purpose of this method?
What is the purpose of this?
I don't like this at all. It's the provider who should provide these, don't like the leakage.
Attention to detail :-)
Point is that it feels bad to retain memory just because someone happened to inspect it
Would this still be needed if serialization/deserialization was done by the provider?
it's documented on ActorPath, and it's used for cheaply injecting the transport address into its string rep (for local Addresses)
there should not be all that many Addresses around, and those which are will be inspected very often, so I think it pays off
I think "inbound" and "outbound" connections might be more appropriate since server and client are roles and not components really.
Is this still used?
is the space here correct?
yes, because system.provider does not know who is doing serializing right now, so how is the poor LocalActorRef supposed to find out its remote transport address in writeReplace()?
didn't change it 
no, forgot to forward-port your change
Isn't the system transitively reachable from the RARP?
Shouldn't this use the systems' threadfactorys' ClassLoader?
INTERNAL USE ONLY?
private[akka] on all of these, no-user api stuff?
Is this an issue?
Is anything of the file above changed? I don't like these sweeping changes :p
Why not just put the this value as the initial value?
this should probably move to the finally clause?
I would probably set these before setting the bootstrap val, to contain the mutable changes in a block
I dont know, I just copied your code into a new file 
This is not true
To be honest, I think we should remove settings.Port since people easily believes it's correct
Is that really true? Isnt the PassiveRemoteClient a bastard child in this respect?
I'm not sure I like this sweeping thing. Did you try only doing this at onBind?
good point, that one might well be preferable, will investigate.
no, the class loader I use is the same one as used for loading the RARP, which is the context class loader during ActorSystem creation (or the systems loader, if no context is set).
 or did I understand the class loader concept wrongly?
yup, sounds good
you were the one who requested it ;-)
no value other than `null` is available before this line which would be suitable
That doesn't answer the question :-)
you mean with a double-nested try-finally-setup for ultra protection?
then: yes, I believe I did some changes in this file. Its a shame that git doesnt track these better in case you copy around hunks between files ;-)
well, I didnt know which one would arrive first, and I dont have a test case for it, yet, because I dont know how to reliably trigger it; so this sounded like a hopefully reliable first shot which can later be optimized
we need to read the config somehow, so you are proposing a name change?
Yes and no, since the settings for Actives does not apply for passives.
Would be nice if we could just have 1 loader for everything :-)
After this is merged I can have a look and see if it's an issue anywhere 
:p so what am I looking for? :-)
so either assert true or do .set?
Either: 1) Remove it and pass "desiredPort" into the constructor or into the start()-method 2) Rename it to "DesiredPortFromConfig" :p
Add TODO about that
okay, changed it.
babystep towards cleaner initialization +1
perhaps time to extract this into method in ReflectiveAccess      def currentClassLoader(obj: AnyRef): ClassLoader =        Option(Thread.currentThread.getContextClassLoader) getOrElse obj.getClass.getClassLoader
and here also an alternative might be that the serializer used the ContextClassLoader, if any, which is now always set in our threads
can be used with or without space
actually: I forgot that I cleaned it up so much that even the RARP does not have a reference to its system anymore
getClass.getClassLoader will return the classLoader of RemoteActorRefProvider.class so it will not be able to load fqn if that is located in a child classloader, which could be the case. Perhaps ReflectiveAccess use contextClassLoader so it will work anyway, but feels wrong.
wrote a test for the NettySettings
why?       case ref: ActorRef  (ref.path.address.hostPort, 1)
found out that my scheme is b0rked, will refactor to store one ClassLoader in ActorSystem and use that everywhere
cutnpaste, is why. fixed.
As I wrote above: will make the decision exactly once in ActorSystemImpl and always use that from our code, then.
well, on second look: you need this when writing your own transport; shall we require this to always happen in the `akka` package?
No, the point was to only set it once, and the first one wins.
done, also on ActiveRemoteClient
I chose option 2
`ouput` is a perfectly fine actor name, but people might expect one more t in there ;-)
is the wildcard needed, i.e. is the name unpredictable?
u think so? thanks
it is unnamed, probably to avoid conflict with user name space
ah, right; it might be a tad annoying, but luckily concrete deployments take precedence over wildcard ones
here is another ouput (nice docs and solution by the way)
lol, epic typo
Perhaps also mention that wildcard can not be used to match part of the name, or can it? `/foo/bar*`
I understand that you mean `/user/*/sampeActor` => third, but I guess it's rather confusing if `user` is counted or not, since it's omitted in the deployment paths
Might be cheaper with a VectorBuilder
if sender == self => endless loop
so if someone sends this a buggy message, we don't want it logged in unhandled?
    case _: LeaderChanged | _:MemberEvent 
Gossip() creates a new Gossip instance every call, should return a cached zero.
we should pipe it to unhandled?
seems a bit noisy, I'd recommend just using "members.isEmpty"
This is not about unhandled stuff. It's about filtering out things that should only be published on convergence v.s. the ones that should be published for every gossip.
Yes, but it is only called on each node when it joins or leaves the cluster.
This is just updating an internal view of the cluster state that only cares about ClusterDomainEvents. I'll change it to check that the match is exhaustive instead so we get compile time warnings if we add new events.
I'm not sure about the naming here, these should be named as domain events, i.e. past tense `MemberBecameUnreachable` but thats rather long thoughts? suggestions?
Is the additional clearState on purpose? a minor style thing - I think you should extract `publishDone` to a method to make it similar to the other cases
I really don't see the semantic difference between `MemberUnreachable` and `UnreachableMember`. I think the difference in ordering makes it obvious that it's not a normal `MemberEvent`.
do we need this convergence flag any more? The state should always be based on converged gossip
also, the endless loop is important to address.
What would happen if `PublishChanges` only contained `newGossip`, since this holds `latestGossip` anyway?
Yes, the clearing is intentional. I'll extract it. The -loop has been addressed.
yes, that would be an error if sender == self
I think that the state must include the unreachable members from the latest gossip, and that doesn't always reflect a converged state. 
That would work. Add some overhead to the testing since you have to skip over the oldGossip changes to compare the real changes.
I don't understan that comment. We don't expose convergence in any other place (except in the internal `SeenChanged` event. When there are unreachable it's never convergence.  By the way, can you change `seenBy` and `getSeenBy` to `private[akka]`. That should only be internal api.
what do you think about creating two separate `diff` functions, one for the converged events and one for the others?
That's actually a good idea.
Yes, you're right, we can get rid of the flag, and let people check the unreachable.
I guess you might want to remove `val localGossip` to avoid confusion.
same here, or add a comment why not
`bothOld == bothNew` should always hold, no?
talking to Bjrn made me aware of how fragile this solution is (Members comparing equal when in fact they are meant to be different). Just think about what happens if `intersect` chose to pick the resulting set randomly from left and right? There is nothing in its contract prohibiting such behavior.
Was just writing up that comment as well. Will rewrite the code.
ah, I see now what makes re-appearing from UNREACHABLE a challenge; was this not supposed to happen only upon DOWN? @patriknw ?
I think that would be a good change. The background is that when I implemented this I had the cluster aware router in mind and thought it would be good to minimize the time the router would use the broken reference (dropping messages). DOWN require the full convergence cycle.
then the router should monitor the UnreachableMember messages instead (that should give the same timely feedback, right?)
Yes, that should be possible, perhaps that already done, when you mention it. At least for the cluster aware. The ordinary listen to Terminated, and can only do so.  /Patrik  7 dec 2012 kl. 13:42 skrev Roland Kuhn <notifications@github.com>:  In akka-cluster/src/main/scala/akka/cluster/ClusterEvent.scala:  > -           publish later, when convergence > -          leaderChangedState = Some(Left(x)) > - > -        case ConvergenceChanged(true) => > -           now it's convergence, publish eventual stashed LeaderChanged event > -          leaderChangedState match { > -            case Some(Left(x)) => > -              leaderChangedState = Some(Right(x)) > -              publish(x) > - > -            case _ =>  nothing stashed > -          } > -          publish(event) > - > -        case MemberUnreachable(m) => > +        case UnreachableMember(m) => >            publish(event) >             notify DeathWatch about unreachable node >            publish(AddressTerminated(m.address))  then the router should monitor the UnreachableMember messages instead (that should give the same timely feedback, right?)  -- Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/909/files#r2347834>.
could we qualify the name of this `diff` function, since the other one is named `diffConverged`?
should we / do we need to expose `convergence`? I'm afraid it's not complete. What if we start by receiving CurrentClusterState and then nothing more happens, then `_convergence` is not really defined.
obsolete comment, `MemberUnreachable`
how about `diffRaw`?
So this is in the akka private `ClusterReadView` and it's used in tests, some of which could probably be removed (awaitUpConvergence for example).  In `LargeCLusterSpec` for example, should we just check that we have the right number of nodes, that all are up, and that the leader is the expected one? That would probably do it I think.
Not too keen on `diffRaw` any other suggestions? Calling it `diffRest` or any such nondescript thing just for the sake of it feels wrong.
so, what does it do? without looking too hard I thought it would compute the raw diff between two gossips, as opposed to bailing out early in case of no convergence
yes, since we know that it was convergence when we receive the member events it should be enough to check those, which is done in awaitUpConvergence 
it's all about the unreachable stuff, so I suggest `diffUnreachable`
and make a third one, `diffSeen`, that handles SeenChanged, SeenChanged is only for test purpose (INTERNAL API) and I think that can and should be disabled by default, so it makes sense to place that into a separate method, which is a nop by default
      case Destination(_, `self`) :: Nil 
In the original code we sent the potentially wrapped "message" argument, but here we send the potentially unwrapped one. Is it guaranteed that the message is never wrapped in this case?
yes, I'm aware of that, and as far as I can tell that should be alright.  The purpose of wrapped messages (e.g. Broadcast) is when sending to the routees. Messages to `self` is Resize, AutoReceiveMessages, Terminate or other *administrative* messages, i.e. not wrapped. I don't think there was any deep thought into that, and you can also see that it was handled differently if it was one destination or many.
Ok, I just wanted some confirmation, before I say OK too quickly again :)
Move this out of the foreach, only needs to be done once
I'd use a PF instead:      applyRoute(s, message) foreach {       case Destination(sender, `self`) => super.tell(msg, sender)       case Destination(sender, recipient) =>  recipient.tell(msg, sender)     }
good point, thanks
yes, with the move of the unwrap that's great, I have changed
2 to 8 seems quite arbitrary from this point of view
well, 2 lines above it checks number 1, which is the initial resize, and then continues here from 2
this definitely is more uniform and pleasantly written, I just think we should also spec it (as in: adding it to the docs)
I have added docs describing behavior of RouterEnvelope resize and CurrentRoutees.
It won't win a beauty contest, but it will do the trick.
Are we sure that lookup is used for all getChildByX calls now?
Instead of this flag, could we check if lookup != underlying?
hmm, dont think so, since that would turn `true` a bit too early, no?
It won't matter, I'll experiment with that in master, would be nice to avoid accidental boolean-complexity
I would prefer  H I C   S V N T   D R A C O N E S
    !delimiter.isEmpty
No, that would be >0. This "if" is there to add additional tests for the   multi-byte delimiter case.
should set `delimiterFragment = None` here
please rename to `delimiterSubmatchLength` or some such
yes, this looks like a good idea
No the match could be inside the nextChunk, not just at the prefix.
oh, right, of course
Just one thing before the merge. I think it might make sense to compact the buffer at this point, too.
yes, that is a reasonable guess, but we might want to measure this as well
I'll leave as it is for now. We should benchmark IO to see if that makes   sense or not.
then we might want to have some TODOs in the code which remind us about what needs verification
What does this path parameter mean? It is not used in the docs for ordinary actors. I just want to learn
Um. Good question, I think it is not what I thought it is.
this is indeed misleading: path is used internally only AFAIK since Deploy doubles as the container of the parsed deployment section entries
ok, then my understanding was not totally off
this line needs to be moved to the end, see #1408.
Ah, I see. Nice catch. I'll push an update in a sec.
This closes over "this" and exposes it to another actor, i.e. when "this" is replaced by restarts or the actor terminates, it will be leaked by anyone holding onto the ChannelRegistration.
Benchmark and see if CCAS is faster or not?
Do we know that it was a child?
We don't know that the creation succeeded here
True. Even though "this" is not an actor (but the `ChannelRegistryImpl`) it is created and held by an Actor (the `SelectionHandler`). So, *if* the selection were allowed to be restarted there'd be a potential leak. However, the SelectionHandler cannot be restarted (is has a stopping supervisor strategy) and the only ones receiving a registration are its children, which are always going to be terminated before the SelectionHandler instance is terminated. Therefore we will never see this potential leak occur.
Since the SelectionHandler ref is not exposed to the user the only ones who can potentially call assign a watch are ourselves. So, we control for what actors we are seeing `Terminated` events, which are only children. I'll add a comment explaining that...
Would swapping with the next line help?
Even with the new Akka 2.2. Props swapping the lines would of course not help. Do you see an easy fix?
One fix is to:  create child increment watch decrement on Terminated
no need for this
no need for this
yes, my spidey senses were also triggered by this code, but from what I can tell it is fine
why is the channelActor implicit?
ChannelRegistry and ChannelRegistration needs some scaladoc/comments about expected behavior
Ok, I see. So, if the selector is limited (i.e. `MaxChannelsPerSelector > 0`) we are already doing the right thing here. For the unlimited case the problem of potentially counting failed child creations doesn't matter because we only use the childCount for enforcing the limit, which is not checked in the unlimited case. So I take it that no change is required.
Unfortunately it is needed since we create a *function* that creates the props and not the props themselves.
Simply because `register` is always called from the channelActor and we can spare the explicit `self` argument this way.
Good call, clearly needs a comment ;)
Yes, I'll add them.
yes, but for a different reason: if something throws an exception here then the selector will shut down, meaning that an off-by-one child count does not matter; if this is not correct then youll need to switch this line with the next
I wasn't sure how a problem with child creation would hit the SelectionHandler actor here. Since the actual creation happens asyncly this code would simply run through without seeing an exception, no?
before dispatching the asynchronous creation several things are already checked (e.g. that the name is okay and not already taken, that the mailbox matches the actor class, and possibly others in the future), which may in principle result in an exception being thrown from `actorOf`; Im not saying that I expect this to happen here, though, we are just being paranoid
Yes, exactly what @rkuhn says, encoding assumptions without documenting said assumptions will eventually lead to someone changing the code and risking to break those assumptions silently. Minimizing this is good for sleep purposes ;-)
Ok, got it. So, stepping back, I'd say swapping with the next line would increase robustness at least a little bit... I'll make the change.
Ouch, thanks, that was an omission.
might want to make `createAsker` private or something
this is nice!
Not daring to mix a match for InterruptedException plus custom extractor ;-)
I think this is where reportFailure is desirable
I think we should complete it with an ExecutionException so we don't let it time out.
case _: VirtualMachineError | _: ThreadDeath | _: InterruptedException => None
sounds good, like this?      case NonFatal(e)  p complete Left(e)     case e  p complete Left(new ExecutionException(e)); throw e
Mixing extractors and normal patterns can be troublesome, Roland, wdyt?
Harmless is an alternative name of this:      } catch {         case Harmless(e) =>  WDYT?
You mean like:      case e: InterruptedException      case NonFatal(e)   I think that would work, and I did that first, but I was annoyed by the need of copying the two duplicate lines, but that is probably more clean anyway.
Yeah, Roland, you agree here or not?
    case e @ (_: InterruptedException | NonFatal(_)) =>
this looks like it should be fine: the problems start when applying constraints to the extracted values
I think NonFatal describes more closely what it does, since Harmless is a lot more nebulous.
so, whats the rationale for not suspending the actor in this case? Or should it in fact be implicitly Resumed? (I mean in the Directive sense)
damn, that should of course be done, that was the two lines of copied code I talked about before. THANKS
This looks okay, but CTD vs. InterruptedException has been a bitch, so Jenkins will probably tell us if there is a problem.
why not unify this with the other logError in the same file?
This cannot ever happen: if `actor == null` then were suspended. Otherwise somethings else is broken and should not be masked by this protector clause.
I was able to make it null, I'll send you the code how to reproduce. I'm open to suggestions of how to handle it.
well, one was in the trait and one in the object, but I can unify them by passing in the class
Alright, I'll change it back to NonFatal then.
NonFatal is teh bomb
add "val cause: Throwable" and override def getCause() to return that.
fixed  On Fri, Feb 3, 2012 at 10:45 AM, viktorklang < reply@reply.github.com > wrote:  > > @@ -499,6 +499,13 @@ object Logging { > >    class EventHandlerException extends AkkaException > > > >    /** > > +   * Exception that wraps a LogEvent. > > +   */ > > +  class LogEventException(val event: LogEvent) extends NoStackTrace { > > add "val cause: Throwable" and override def getCause() to return that. > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/282/files#r413484 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
added at dubug level  On Fri, Feb 3, 2012 at 10:47 AM, viktorklang < reply@reply.github.com > wrote:  > > @@ -492,9 +483,7 @@ sealed trait Future[+T] extends japi.Future[T] with > Await.Awaitable[T] { > >          future complete (try { > >            Right(f(res)) > >          } catch { > > -          case e => > > -            logError("Future.map", e) > > -            Left(e) > > +          case NonFatal(e) => Left(e) > > executor.reportFailure(e) here? > > --- > Reply to this email directly or view it on GitHub: > https:github.com/jboner/akka/pull/282/files#r413490 >    --   Patrik Nordwall Typesafe <http:typesafe.com/> -  The software stack for applications that scale Twitter: @patriknw
What about removing this class and add the isTicking method to Deadline? The latter enjoys a nice DSL: `val deadline = 10 seconds fromNow`
this is not used, or is it?
wouldnt it make sense to create a `reference.conf` in akka-cluster and move this section there?
dont we have these already as RemoteLifeCycleEvents in RemoteTransport.scala?
same comment as for the cluster section in reference.conf
Sounds good. Will you do it or? 
True. I can remove it. 
in reply to your reply which is somehow not visible in the pull request diff: what is it that you are missing exactly, apart from `def isTicking: Boolean`?
ah, sorry, somehow I was under the wrong impression that the remote events were exceptions.
Definitely. I am on that now. Thanks.
I want to throw RemoteConnectionException so I can catch it and retry. How would the event help me? 
do we have a policy on these empty lines? because I remove them whenever I touch a file ;-)
Moving it as we speak. 
I want to have an empty line after the header. Just by habit, looks good. Formatting without thinking when reading code. But if you don't like that I can stop. 
I am not using isTicking, but a search shows that it is used by IOActor.  I think Deadline is sufficient. I guess I used it, but refactored it into not using it. 
okay, please create a ticket for removing Timer and Ill do it once this branch is merged into master.
I'm moving to Deadline. So if someone fixes IOActor we can remove the Timer. 
Do nodes start as Up?
Shouldn't these already be Durations?
What happens if one of these throw an exception?
mistake by me, changed from debug when debugging, fixing now
Seed nodes are consider Up. This node will try to join them on startup.  But I'm removing the concept of seed nodes soon anyway. 
Remove this code? use Git to revive if needed
quite pointless with system = system?
What do you suggest we do? 
point is, system should go first since I assume it is ubiquitous to the AFD
Don't like commented out tests, either fix or remove?
Is this used in remoting?
No. I'm working on this right now. 
Why is this needed? Aren't we going to have a ClusterActorRefProvider?
Ok. Can make it the first arg. 
should possibly be a finally to remove the k from remoteClients
Are we packaging the cluster module in 2.0?
Are we shipping the cluster docs in 2.0?
It is part of what I'm working on right now. I can delete the test from repo temporary if it would make you happy. 
Used it when Gossiper was in akka-remote. Can remove now. 
Just formatted it since it was too long. 
No. But I want it to build with regular build. How can we exclude it from packaging only? 
Yes. Cluster spec. 
Well, the question was what we do...in the finally...of course. 
do each in its own finally?
I could. But the full shutdown still fails. Only makes it slightly better. 
please add a space for better readability       # Which
perhaps """ instead fo \"
Is there something similar to       t.setUncaughtExceptionHandler(exceptionHandler)  ?
Make sure that you change all usages of these renamed properties. For example I don't see any change in AkkaSpec, where I know that they are used.
All tests pass.
:-) That doesn't mean that they are not used at other places. What is the problem with searching for them? Including rst and scala docs.
In general I like my results rather medium-rare  (just so I can comment on something)
please use `prerequisites.system.internalClassLoader`, and you may (I know you dont ;-) ) want to use this:      val args = Seq(classOf[Config] -> config, classOf[DispatcherPrerequisites] -> prerequisites)
I fixed the ones I found :-)
Don't know why this isn't using Akkas get Unsafe, it should.
We should have our own Adapted Runnable that doesn't eat as much memory
Why do you want to ``signalAll``?
Why only here?
Because this is the only blocking method which didn't call lockInterruptibly.
If we have multiple threads waiting we want to signal all of them (not just one). Otherwise we could end up in unnececssary waiting, or even deadlock if we're unlucky.
Just making sure :)
I guess it makes sense to signalAll waiters since we are purging the collection, it's only in the case where we have more threads than capacity that it is wasteful, which should be rare.
nope: never remove anything from .gitignore, that only leads to great pain when switching branches across this change
I'd argue that it should.  You really need to know about those files and remove them.
In other words, if you don't remove those files, they continue to cause issues.  This is your best mechanism to discover, I'd say.
No, we had that problem before and the conclusion was unanimous and generic: never remove anything from .gitignore. The reason is that the previously ignored files will eventually be committed into the repository inadvertently.  In this specific case we rely on the release script to clean things up, which is a step that also shall be retained even if technically not needed, it is a valuable safe-guard.  One thing we need to verify is how the pr-validator behaves when switching between 2.2 and 2.3 branches: the files should in principle cause test failures when going 2.2 -> 2.3 (but currently the relevant test does not run, AFAIK; we need to fix that anyway).
Alright, I can remove it if you feel it's best.  Also, I recommend using pax-exam for the tests. There's a junit bundle you can use which will allow you to run the same OSGi tests, but without the need for forking to maven, so you could include it in your full test suite by default.  As the tests are currently broken, I'd recommend trying that out.
Im trying to learn the sbt way, so why not the following?  ~~~ scala val dirAndNames = projects.map(p => (resourceDirectory in Compile in p) -> (normalizedName in p)).join dirAndNames map { dn =>   for {     (dir, project) <- dn     val conf = dir / "reference.conf"     if conf.exists   } yield conf -> project ) ~~~
Thanks! It would be great if we could hakk on that together sometime: pax-exam was tried and rejected by Christophe and Raman, they couldnt get it to work. But that will have to wait until after my vacation.
Doesn't quite pass the typesystem.   Too many levels.  Let's look at types:  ```scala val projects: Seq[Project] val tmp: Seq[(Initialize[File], Initialize[String])] =   projects.map(p => (resourceDirectory in Compile in p) -> (normalizedName in p)) val dirsAndNames: Initialize[Seq[(File, String)]] = tmp.join  value join is not a member of  Seq[(Initialize[File], Initialize[String])] ```  That is, the "join" helper isn't able to discover into a `Seq[Tuple2[Initialize[_],Initialzie[_]]]`.  Holy nested types batman!  This is actually the simplest I could do to preserve the existing semantics.  An even better approach (and one I want to do, once someone can answer my questions on repercussions) is to *NOT* dynamically depend on projects, but to depend on the `dependencyClasspath` setting.  Using this, we should be able to get all the dependent jars/classpath directories, and we can filter this to grab the relevant `reference.conf` files.  Then we're no longer used advanced sbt features, and we might see more stability.  
Sounds good.  I'll also be at the eng meetup early for hakking like this.  I was able to get pax-exam up and running in the Scala build, I'm certain we could do so for AKka given enough dedication :)   On Mon, Jul 15, 2013 at 11:58 AM, Roland Kuhn <notifications@github.com>wrote:  > In .gitignore: > > > @@ -40,7 +40,6 @@ akka-contrib/rst_preprocessed/ > >  akka-docs/_build/ > >  akka-docs/exts/ > >  akka-docs/rst_preprocessed/ > > -akka-osgi/src/main/resources/*.conf > > Thanks! It would be great if we could hakk on that together sometime: > pax-exam was tried and rejected by Christophe and Raman, they couldnt get > it to work. But that will have to wait until after my vacation. > >  > Reply to this email directly or view it on GitHub<https:github.com/akka/akka/pull/1617/files#r5193104> > . >
Ah, I see, thanks for the explanation. Classpaths will not work because neither forward nor reverse dependencies capture what we want to gather: akka-osgi does not depend onsayakka-remote, and the reverse is also not true. Filtering projects looks like the only solution to me.
Is this test run automatically? In that case why do we have so many spec wrappers for other junit tests?
this is just to verify compilability
ok, but then I think it should not be a junit test (remove the @Test) and write a comment about it. I have found junit tests before that were never run because of this and they were supposed to be run.
so very true.
  _and is_ completely oblivious  
please remove the lonely `s`
will be replied _to_
Yes it can be removed here. OTOH how can you add the plural 's' to these formatted words?
will be replied *with* ;)
well, actually: will be replied to with an 
these are fixed class names (for example), so they dont participate in normal grammar rules
notified of the termination
the `s` looks strange
I removed it already, but thanks!
this should probably be `actorSelection-java`
perhaps READER_IDLE | ALL_IDLE?
NettyRemoteTransport.setAddressFromChannel uses settings.host to set address.host, so these two are quite the same.
wrong: this is the time interval after which a HEARTBEAT will be generated. Nothing quite as dramatic as tearing down the connection.
I know, speed style, but:      cookie foreach (beat.setCookie(_))
Good catch, copy-paste error
yep, why not?
I think you can even get away with: cookie foreach beat.setCookie
NoScope doesn't make any sense.
does lookupDeploy = true  deploy = None make sense?
Okay, will change to NoConfigurationGivenForThisScope or some such.
Scopes are not explained
should probably add some docs at this point: systemService deploys locally no matter what, deploy can be passed in from a higher level to provide some override, and lookupDeploy can be set to false to prevent asking the Deployer again (e.g. if its result is already contained in the deploy argument)
return type bitte
creating routers should not be the hot path: it never was exactly cheap, and the actual router initialization is also not light-weight.
will add docs
okay, Ill clean up your mess ;-)
But don't you really only need to concat iterators apply the withFallback and then do a toList?
No, its not unscoped, its more no scope given, please use scope from other place
thats a good one, will change.
this looks mysteriously similar to code I've seen before in this commit. pull out, abstract, call?
isn't the deploy on Props?
great work man
Yes, I agree its tantalizingly similar. But not the same 
Thats a different deploy: Im passing the locally determined overrides to the remote system so that they can be applied to props there, together with any local configuration overrides the remote system may have.
Then drop the final ::: Nil
nice, never liked that one
That's a good name
we just removed this class completely, subsumed by Deadline.
Cool, I will remove it then. Do you think we need Deadline in the Scala Library? 
should we add hasTimeLeft as convenience to avoid !isOverdue
It already has: def timeLeft: Duration = this - Deadline.now
well, I was asking for hasAnyTimeLeft: Boolean to avoid negation which can be hard to see/read, but it was only a thought
good point, I also think it's wasteful to allocate a new Duration just to check time left
